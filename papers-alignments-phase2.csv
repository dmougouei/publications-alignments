expert_1,expert_2,paper_id,venue,theme,title,abstract,value,value item,justification,clarification,opinion,coder_1,coder_1_reason,coder_2,coder_2_reason,reconciled
Exp A,Exp H,361,ESEC/FSE,AI & Machine Learning,VulRepair: a T5-based automated software vulnerability repair,"As software vulnerabilities grow in volume and complexity, researchers proposed various Artificial Intelligence (AI)-based approaches to help under-resourced security analysts to find, detect, and localize vulnerabilities. However, security analysts still have to spend a huge amount of effort to manually fix or repair such vulnerable functions. Recent work proposed an NMT-based Automated Vulnerability Repair, but it is still far from perfect due to various limitations. In this paper, we propose VulRepair, a T5-based automated software vulnerability repair approach that leverages the pre-training and BPE components to address various technical limitations of prior work. Through an extensive experiment with over 8,482 vulnerability fixes from 1,754 real-world software projects, we find that our VulRepair achieves a Perfect Prediction of 44%, which is 13%-21% more accurate than competitive baseline approaches. These results lead us to conclude that our VulRepair is considerably more accurate than two baseline approaches, highlighting the substantial advancement of NMT-based Automated Vulnerability Repairs. Our additional investigation also shows that our VulRepair can accurately repair as many as 745 out of 1,706 real-world well-known vulnerabilities (e.g., Use After Free, Improper Input Validation, OS Command Injection), demonstrating the practicality and significance of our VulRepair for generating vulnerability repairs, helping under-resourced security analysts on fixing vulnerabilities.",Security,Healthy,"The paper's primary contribution is the development of VulRepair; which is aimed at repairing software vulnerabilities. This aligns with the value item ""Healthy"" (v6.1); as users value the health and security of their software. Healthy in this context refers to the stability and reliability of the software.","The paper's main contribution of VulRepair, an automated software vulnerability repair approach, aligns with the value item ""Healthy"" (v6.1) from Schwartz's Taxonomy. Users value the health and security of their software, and VulRepair aims to improve the stability and reliability of software by fixing vulnerabilities. By addressing vulnerabilities, VulRepair enhances the overall health and security of the software, aligning directly with the value item of ""Healthy.""",Agreed-Justified,Disagree,"the abstract is more concerned with achievement: successful, as it successfully identify vunlerabilities",Disagree,"The justification doesn't align with the value item of ""Healthy"" .",
Exp B,Exp H,361,ESEC/FSE,AI & Machine Learning,VulRepair: a T5-based automated software vulnerability repair,"As software vulnerabilities grow in volume and complexity, researchers proposed various Artificial Intelligence (AI)-based approaches to help under-resourced security analysts to find, detect, and localize vulnerabilities. However, security analysts still have to spend a huge amount of effort to manually fix or repair such vulnerable functions. Recent work proposed an NMT-based Automated Vulnerability Repair, but it is still far from perfect due to various limitations. In this paper, we propose VulRepair, a T5-based automated software vulnerability repair approach that leverages the pre-training and BPE components to address various technical limitations of prior work. Through an extensive experiment with over 8,482 vulnerability fixes from 1,754 real-world software projects, we find that our VulRepair achieves a Perfect Prediction of 44%, which is 13%-21% more accurate than competitive baseline approaches. These results lead us to conclude that our VulRepair is considerably more accurate than two baseline approaches, highlighting the substantial advancement of NMT-based Automated Vulnerability Repairs. Our additional investigation also shows that our VulRepair can accurately repair as many as 745 out of 1,706 real-world well-known vulnerabilities (e.g., Use After Free, Improper Input Validation, OS Command Injection), demonstrating the practicality and significance of our VulRepair for generating vulnerability repairs, helping under-resourced security analysts on fixing vulnerabilities.",Security,Family Security,"The proposed VulRepair in the paper helps in repairing vulnerabilities and will ensure safer usage of software for the users. This makes the software more dependable and reduces the risks associated with its use; thereby providing security and aligning with the value item ""Family Security"" (v6.2).","The proposed VulRepair in 'Paper X' directly aligns with the value item Family Security and its corresponding value Security from a ""Software User"" perspective. By automating vulnerability repair, VulRepair enhances the security of software, making it more dependable for users. This significantly reduces the risks associated with software vulnerabilities, ensuring the safety and protection of families using the software. Thus, it directly addresses the value of Family Security and provides the security needed for users' peace of mind.",Agreed-Justified,Disagree,"Related to Achievement: capable more, as it automatically identifies vulnerabilities",Disagree,Not directly related to family security. ,
Exp C,Exp I,361,ESEC/FSE,AI & Machine Learning,VulRepair: a T5-based automated software vulnerability repair,"As software vulnerabilities grow in volume and complexity, researchers proposed various Artificial Intelligence (AI)-based approaches to help under-resourced security analysts to find, detect, and localize vulnerabilities. However, security analysts still have to spend a huge amount of effort to manually fix or repair such vulnerable functions. Recent work proposed an NMT-based Automated Vulnerability Repair, but it is still far from perfect due to various limitations. In this paper, we propose VulRepair, a T5-based automated software vulnerability repair approach that leverages the pre-training and BPE components to address various technical limitations of prior work. Through an extensive experiment with over 8,482 vulnerability fixes from 1,754 real-world software projects, we find that our VulRepair achieves a Perfect Prediction of 44%, which is 13%-21% more accurate than competitive baseline approaches. These results lead us to conclude that our VulRepair is considerably more accurate than two baseline approaches, highlighting the substantial advancement of NMT-based Automated Vulnerability Repairs. Our additional investigation also shows that our VulRepair can accurately repair as many as 745 out of 1,706 real-world well-known vulnerabilities (e.g., Use After Free, Improper Input Validation, OS Command Injection), demonstrating the practicality and significance of our VulRepair for generating vulnerability repairs, helping under-resourced security analysts on fixing vulnerabilities.",Stimulation,Variation in Life,The abstract mentions addressing various technical limitations of prior work; suggesting changes and improvements in the way vulnerabilities are repaired. This aligns with the value of Stimulation and specifically with Variation in Life (v2.2); as software users may appreciate these changes that provide a new approach or method to dealing with software vulnerabilities.,"In the abstract, it is stated that the proposed approach, VulRepair, addresses various technical limitations of prior work in automated vulnerability repair. This implies that the approach introduces changes and improvements in the way vulnerabilities are repaired, which can be seen as a new and stimulating approach for software users. This aligns with the value item Variation in Life (v2.2) from Schwartz's Taxonomy, as it offers a different method or perspective for dealing with software vulnerabilities.",Agreed-Clarified,Agree,"As it talks about changes, it is a kind of variation",Agree,"not relevat to stimulation (variation), its related to security and/or achievement",
Exp D,Exp I,365,ESEC/FSE,AI & Machine Learning,Fuzzing deep-learning libraries via automated relational api inference,"Deep Learning (DL) has gained wide attention in recent years. Meanwhile, bugs in DL systems can lead to serious consequences, and may even threaten human lives. As a result, a growing body of research has been dedicated to DL model testing. However, there is still limited work on testing DL libraries, e.g., PyTorch and TensorFlow, which serve as the foundations for building, training, and running DL models. Prior work on fuzzing DL libraries can only generate tests for APIs which have been invoked by documentation examples, developer tests, or DL models, leaving a large number of APIs untested. In this paper, we propose DeepREL, the first approach to automatically inferring relational APIs for more effective DL library fuzzing. Our basic hypothesis is that for a DL library under test, there may exist a number of APIs sharing similar input parameters and outputs; in this way, we can easily ‚Äö√Ñ√∂‚àö√ë‚àö‚à´borrow‚Äö√Ñ√∂‚àö√ë‚àöœÄ test inputs from invoked APIs to test other relational APIs. Furthermore, we formalize the notion of value equivalence and status equivalence for relational APIs to serve as the oracle for effective bug finding. We have implemented DeepREL as a fully automated end-to-end relational API inference and fuzzing technique for DL libraries, which 1) automatically infers potential API relations based on API syntactic/semantic information, 2) synthesizes concrete test programs for invoking relational APIs, 3) validates the inferred relational APIs via representative test inputs, and finally 4) performs fuzzing on the verified relational APIs to find potential inconsistencies. Our evaluation on two of the most popular DL libraries, PyTorch and TensorFlow, demonstrates that DeepREL can cover 157% more APIs than state-of-the-art FreeFuzz. To date, DeepREL has detected 162 bugs in total, with 106 already confirmed by the developers as previously unknown bugs. Surprisingly, DeepREL has detected 13.5% of the high-priority bugs for the entire PyTorch issue-tracking system in a three-month period. Also, besides the 162 code bugs, we have also detected 14 documentation bugs (all confirmed).",Achievement,Intelligent,Paper X introduces DeepREL; a method for improving the effectiveness of DL library fuzzing; which involves intelligent handling of detecting and rectifying bugs. This aligns with the value item Intelligent and its corresponding value Achievement.,"In 'Paper X', the main contribution of DeepREL is to improve the effectiveness of DL library fuzzing by automatically inferring relational APIs and performing fuzzing. This involves intelligent handling of detecting and rectifying bugs in the DL libraries. By aligning with the value item Intelligent and its corresponding value Achievement, 'Paper X' demonstrates a focus on using intelligent methods to achieve successful bug detection and improvement in the software context, which is aligned with the perspective of a ""Software User"" looking for efficient and reliable software systems.",Agreed-Justified,Agree,,Agree,perhaps Successful too?,
Exp E,Exp J,365,ESEC/FSE,AI & Machine Learning,Fuzzing deep-learning libraries via automated relational api inference,"Deep Learning (DL) has gained wide attention in recent years. Meanwhile, bugs in DL systems can lead to serious consequences, and may even threaten human lives. As a result, a growing body of research has been dedicated to DL model testing. However, there is still limited work on testing DL libraries, e.g., PyTorch and TensorFlow, which serve as the foundations for building, training, and running DL models. Prior work on fuzzing DL libraries can only generate tests for APIs which have been invoked by documentation examples, developer tests, or DL models, leaving a large number of APIs untested. In this paper, we propose DeepREL, the first approach to automatically inferring relational APIs for more effective DL library fuzzing. Our basic hypothesis is that for a DL library under test, there may exist a number of APIs sharing similar input parameters and outputs; in this way, we can easily ‚Äö√Ñ√∂‚àö√ë‚àö‚à´borrow‚Äö√Ñ√∂‚àö√ë‚àöœÄ test inputs from invoked APIs to test other relational APIs. Furthermore, we formalize the notion of value equivalence and status equivalence for relational APIs to serve as the oracle for effective bug finding. We have implemented DeepREL as a fully automated end-to-end relational API inference and fuzzing technique for DL libraries, which 1) automatically infers potential API relations based on API syntactic/semantic information, 2) synthesizes concrete test programs for invoking relational APIs, 3) validates the inferred relational APIs via representative test inputs, and finally 4) performs fuzzing on the verified relational APIs to find potential inconsistencies. Our evaluation on two of the most popular DL libraries, PyTorch and TensorFlow, demonstrates that DeepREL can cover 157% more APIs than state-of-the-art FreeFuzz. To date, DeepREL has detected 162 bugs in total, with 106 already confirmed by the developers as previously unknown bugs. Surprisingly, DeepREL has detected 13.5% of the high-priority bugs for the entire PyTorch issue-tracking system in a three-month period. Also, besides the 162 code bugs, we have also detected 14 documentation bugs (all confirmed).",Security,Healthy,Paper X's innovation of DeepREL assists in detecting bugs in deep learning libraries and thereby improving system operation to ensure it keeps functioning properly. This caters to the value item Healthy and its corresponding value; Security.,"In the context of 'Paper X', the alignment with the value item Healthy and its corresponding value Security can be justified by the fact that the proposed technique, DeepREL, focuses on automatically inferring and testing relational APIs in deep learning libraries. By effectively detecting bugs and inconsistencies in these libraries, DeepREL contributes to ensuring the stability and integrity of the software systems built upon them, which ultimately contributes to the healthy functioning and security of the overall software infrastructure.",Agreed-Clarified,Agree,,Agree,,
Exp F,Exp J,365,ESEC/FSE,AI & Machine Learning,Fuzzing deep-learning libraries via automated relational api inference,"Deep Learning (DL) has gained wide attention in recent years. Meanwhile, bugs in DL systems can lead to serious consequences, and may even threaten human lives. As a result, a growing body of research has been dedicated to DL model testing. However, there is still limited work on testing DL libraries, e.g., PyTorch and TensorFlow, which serve as the foundations for building, training, and running DL models. Prior work on fuzzing DL libraries can only generate tests for APIs which have been invoked by documentation examples, developer tests, or DL models, leaving a large number of APIs untested. In this paper, we propose DeepREL, the first approach to automatically inferring relational APIs for more effective DL library fuzzing. Our basic hypothesis is that for a DL library under test, there may exist a number of APIs sharing similar input parameters and outputs; in this way, we can easily ‚Äö√Ñ√∂‚àö√ë‚àö‚à´borrow‚Äö√Ñ√∂‚àö√ë‚àöœÄ test inputs from invoked APIs to test other relational APIs. Furthermore, we formalize the notion of value equivalence and status equivalence for relational APIs to serve as the oracle for effective bug finding. We have implemented DeepREL as a fully automated end-to-end relational API inference and fuzzing technique for DL libraries, which 1) automatically infers potential API relations based on API syntactic/semantic information, 2) synthesizes concrete test programs for invoking relational APIs, 3) validates the inferred relational APIs via representative test inputs, and finally 4) performs fuzzing on the verified relational APIs to find potential inconsistencies. Our evaluation on two of the most popular DL libraries, PyTorch and TensorFlow, demonstrates that DeepREL can cover 157% more APIs than state-of-the-art FreeFuzz. To date, DeepREL has detected 162 bugs in total, with 106 already confirmed by the developers as previously unknown bugs. Surprisingly, DeepREL has detected 13.5% of the high-priority bugs for the entire PyTorch issue-tracking system in a three-month period. Also, besides the 162 code bugs, we have also detected 14 documentation bugs (all confirmed).",Security,Social Order,By locating; verifying; and treating bugs in DL systems; the proposal of DeepREL in Paper X promotes the order and stability of the software and its applications; which aligns with the value item Social Order and its corresponding value Security.,"The main contribution of Paper X, DeepREL, is the proposal of an approach to automatically infer relational APIs and perform fuzzing on DL libraries. By effectively detecting and treating bugs in DL systems, DeepREL promotes the stability and security of the software and its applications. This directly aligns with the value item Social Order, as maintaining order and stability in the software ecosystem is crucial for ensuring a secure and reliable environment for software users.",Agreed-Justified,Agree,,Agree,,
Exp G,Exp K,367,ESEC/FSE,Code Generation & Analysis,Incorporating domain knowledge through task augmentation for front-end JavaScript code generation,"Code generation aims to generate a code snippet automatically from natural language descriptions. Generally, the mainstream code generation methods rely on a large amount of paired training data, including both the natural language description and the code. However, in some domain-specific scenarios, building such a large paired corpus for code generation is difficult because there is no directly available pairing data, and a lot of effort is required to manually write the code descriptions to construct a high-quality training dataset. Due to the limited training data, the generation model cannot be well trained and is likely to be overfitting, making the model's performance unsatisfactory for real-world use. To this end, in this paper, we propose a task augmentation method that incorporates domain knowledge into code generation models through auxiliary tasks and a Subtoken-TranX model by extending the original TranX model to support subtoken-level code generation. To verify our proposed approach, we collect a real-world code generation dataset and conduct experiments on it. Our experimental results demonstrate that the subtoken-level TranX model outperforms the original TranX model and the Transformer model on our dataset, and the exact match accuracy of Subtoken-TranX improves significantly by 12.75% with the help of our task augmentation method. The model performance on several code categories has satisfied the requirements for application in industrial systems. Our proposed approach has been adopted by Alibaba's BizCook platform. To the best of our knowledge, this is the first domain code generation system adopted in industrial development environments.",Achievement,Successful,The paper contributes a method for code generation that has improved significantly in accuracy; aligns with the value item Successful and its corresponding value Achievement.,"The justification for aligning the main contributions of 'Paper X' with the value item Successful and its corresponding value Achievement from a ""Software User"" perspective is based on the explicit statement in the abstract that the proposed method for code generation has improved significantly in accuracy. Achieving higher accuracy in code generation can be seen as a successful outcome and is aligned with the value of achievement, as it demonstrates the ability to successfully generate accurate code from natural language descriptions, which can benefit software users by reducing the need for manual coding and improving overall software development efficiency.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,367,ESEC/FSE,Code Generation & Analysis,Incorporating domain knowledge through task augmentation for front-end JavaScript code generation,"Code generation aims to generate a code snippet automatically from natural language descriptions. Generally, the mainstream code generation methods rely on a large amount of paired training data, including both the natural language description and the code. However, in some domain-specific scenarios, building such a large paired corpus for code generation is difficult because there is no directly available pairing data, and a lot of effort is required to manually write the code descriptions to construct a high-quality training dataset. Due to the limited training data, the generation model cannot be well trained and is likely to be overfitting, making the model's performance unsatisfactory for real-world use. To this end, in this paper, we propose a task augmentation method that incorporates domain knowledge into code generation models through auxiliary tasks and a Subtoken-TranX model by extending the original TranX model to support subtoken-level code generation. To verify our proposed approach, we collect a real-world code generation dataset and conduct experiments on it. Our experimental results demonstrate that the subtoken-level TranX model outperforms the original TranX model and the Transformer model on our dataset, and the exact match accuracy of Subtoken-TranX improves significantly by 12.75% with the help of our task augmentation method. The model performance on several code categories has satisfied the requirements for application in industrial systems. Our proposed approach has been adopted by Alibaba's BizCook platform. To the best of our knowledge, this is the first domain code generation system adopted in industrial development environments.",Achievement,Intelligent,The contribution of a subtoken-level TranX model that demonstrates superior performance aligns with the value item Intelligent and its corresponding value Achievement.,"The justification for aligning the contribution of a subtoken-level TranX model with the value item Intelligent and its corresponding value Achievement is based on the fact that the subtoken-level TranX model outperforms the original TranX model and the Transformer model on a real-world code generation dataset. This superior performance demonstrates the model's intelligence and capability, which directly aligns with the value of Achievement in terms of being successful, capable, and intelligent. Therefore, the contribution of the subtoken-level TranX model can be seen as an achievement in the context of software use, where users would value intelligent and capable code generation systems.",Agreed-Justified,Agree,,Agree,,
Exp B,Exp H,367,ESEC/FSE,Code Generation & Analysis,Incorporating domain knowledge through task augmentation for front-end JavaScript code generation,"Code generation aims to generate a code snippet automatically from natural language descriptions. Generally, the mainstream code generation methods rely on a large amount of paired training data, including both the natural language description and the code. However, in some domain-specific scenarios, building such a large paired corpus for code generation is difficult because there is no directly available pairing data, and a lot of effort is required to manually write the code descriptions to construct a high-quality training dataset. Due to the limited training data, the generation model cannot be well trained and is likely to be overfitting, making the model's performance unsatisfactory for real-world use. To this end, in this paper, we propose a task augmentation method that incorporates domain knowledge into code generation models through auxiliary tasks and a Subtoken-TranX model by extending the original TranX model to support subtoken-level code generation. To verify our proposed approach, we collect a real-world code generation dataset and conduct experiments on it. Our experimental results demonstrate that the subtoken-level TranX model outperforms the original TranX model and the Transformer model on our dataset, and the exact match accuracy of Subtoken-TranX improves significantly by 12.75% with the help of our task augmentation method. The model performance on several code categories has satisfied the requirements for application in industrial systems. Our proposed approach has been adopted by Alibaba's BizCook platform. To the best of our knowledge, this is the first domain code generation system adopted in industrial development environments.",Power,Social Recognition,The paper‚Äö√Ñ√∂‚àö√ë‚àö‚àÇ‚Äö√†√∂‚àö√´‚Äö√†√∂‚Äö√†√á‚Äö√Ñ√∂‚àö‚Ä†‚àö‚àÇ‚Äö√†√∂‚àö¬¥‚Äö√Ñ√∂‚àö‚Ä†‚àö‚àÇ¬¨¬®‚Äö√Ñ¬¢s main contribution has been adopted by Alibaba's BizCook platform; which aligns with the value item Social Recognition and its corresponding value Power.,The adoption of 'Paper X's' proposed approach by Alibaba's BizCook platform aligns with the value item Social Recognition and its corresponding value Power because being adopted by a prominent industrial platform like Alibaba signifies recognition and acknowledgment of the value and impact of the paper's contribution in the software domain. This recognition from a powerful entity in the industry validates the effectiveness of the proposed approach and demonstrates its potential to exert influence and authority in the software development context.,Agreed-Clarified,Agree,,Agree,,
Exp C,Exp I,375,ESEC/FSE,Security & Privacy,Software security during modern code review: the developer's perspective,"To avoid software vulnerabilities, organizations are shifting security to earlier stages of the software development, such as at code review time. In this paper, we aim to understand the developers‚Äö√Ñ√∂‚àö√ë‚àö¬• perspective on assessing software security during code review, the challenges they encounter, and the support that companies and projects provide. To this end, we conduct a two-step investigation: we interview 10 professional developers and survey 182 practitioners about software security assessment during code review. The outcome is an overview of how developers perceive software security during code review and a set of identified challenges. Our study revealed that most developers do not immediately report to focus on security issues during code review. Only after being asked about software security, developers state to always consider it during review and acknowledge its importance. Most companies do not provide security training, yet expect developers to still ensure security during reviews. Accordingly, developers report the lack of training and security knowledge as the main challenges they face when checking for security issues. In addition, they have challenges with third-party libraries and to identify interactions between parts of code that could have security implications. Moreover, security may be disregarded during reviews due to developers‚Äö√Ñ√∂‚àö√ë‚àö¬• assumptions about the security dynamic of the application they develop. Preprint: https://arxiv.org/abs/2208.04261 Data and materials: https://doi.org/10.5281/zenodo.6969369",Security,Social Order,The paper contributes to understanding the developers' perspective on software security during code review; aligning with the value item 'Social Order' under the 'Security' value as it endeavors to improve and organize the software security protocols.,"I apologize for the confusion. My justification for labeling 'Paper X' as aligned with the value item 'Social Order' and its corresponding value 'Security' is based on the fact that the paper aims to improve and organize software security protocols by understanding the developers' perspective on software security during code review. By addressing the challenges faced by developers and highlighting the importance of software security in code review, the paper contributes to creating a more secure and orderly software development process that prioritizes the value of security.",Agreed-Justified,Agree,,Agree,,
Exp D,Exp I,375,ESEC/FSE,Security & Privacy,Software security during modern code review: the developer's perspective,"To avoid software vulnerabilities, organizations are shifting security to earlier stages of the software development, such as at code review time. In this paper, we aim to understand the developers‚Äö√Ñ√∂‚àö√ë‚àö¬• perspective on assessing software security during code review, the challenges they encounter, and the support that companies and projects provide. To this end, we conduct a two-step investigation: we interview 10 professional developers and survey 182 practitioners about software security assessment during code review. The outcome is an overview of how developers perceive software security during code review and a set of identified challenges. Our study revealed that most developers do not immediately report to focus on security issues during code review. Only after being asked about software security, developers state to always consider it during review and acknowledge its importance. Most companies do not provide security training, yet expect developers to still ensure security during reviews. Accordingly, developers report the lack of training and security knowledge as the main challenges they face when checking for security issues. In addition, they have challenges with third-party libraries and to identify interactions between parts of code that could have security implications. Moreover, security may be disregarded during reviews due to developers‚Äö√Ñ√∂‚àö√ë‚àö¬• assumptions about the security dynamic of the application they develop. Preprint: https://arxiv.org/abs/2208.04261 Data and materials: https://doi.org/10.5281/zenodo.6969369",Security,Healthy,The paper's focus on minimizing software vulnerabilities resonates with the 'Healthy' value item; as it is seeking to impart a better secured; and arguably 'healthier'; software experience for the user.,"The alignment between 'Paper X' and the value item Healthy and its corresponding value Security from a ""Software User"" perspective is justified because the paper explicitly states that it aims to understand developers' perspectives on assessing software security during code review, and the challenges they encounter. By addressing these challenges and improving software security, it ultimately contributes to providing a more secure and healthy software experience for the users, aligning with the value item Healthy and its corresponding value Security.",Agreed-Clarified,Agree,,Agree,,
Exp E,Exp J,375,ESEC/FSE,Security & Privacy,Software security during modern code review: the developer's perspective,"To avoid software vulnerabilities, organizations are shifting security to earlier stages of the software development, such as at code review time. In this paper, we aim to understand the developers‚Äö√Ñ√∂‚àö√ë‚àö¬• perspective on assessing software security during code review, the challenges they encounter, and the support that companies and projects provide. To this end, we conduct a two-step investigation: we interview 10 professional developers and survey 182 practitioners about software security assessment during code review. The outcome is an overview of how developers perceive software security during code review and a set of identified challenges. Our study revealed that most developers do not immediately report to focus on security issues during code review. Only after being asked about software security, developers state to always consider it during review and acknowledge its importance. Most companies do not provide security training, yet expect developers to still ensure security during reviews. Accordingly, developers report the lack of training and security knowledge as the main challenges they face when checking for security issues. In addition, they have challenges with third-party libraries and to identify interactions between parts of code that could have security implications. Moreover, security may be disregarded during reviews due to developers‚Äö√Ñ√∂‚àö√ë‚àö¬• assumptions about the security dynamic of the application they develop. Preprint: https://arxiv.org/abs/2208.04261 Data and materials: https://doi.org/10.5281/zenodo.6969369",Security,National Security,By advocating for security during the development stages; the paper indirectly promotes the value item 'National Security'. As software has a global impact; creating secure software is a part of maintaining cybersecurity on a national level.,"In the context of software development, ensuring security during the code review process aligns with the value item of National Security and its corresponding value, Security, from a ""Software User"" perspective. This alignment is justified because secure software plays a critical role in maintaining cybersecurity on a national level. By advocating for security at earlier stages of the software development lifecycle, such as code review, the paper indirectly contributes to the protection of sensitive information and infrastructure, ultimately fostering national security.",Agreed-Justified,Agree,,Agree,,
Exp F,Exp J,381,ESEC/FSE,Security & Privacy,On the vulnerability proneness of multilingual code,"Software construction using multiple languages has long been a norm, yet it is still unclear if multilingual code construction has significant security implications and real security consequences. This paper aims to address this question with a large-scale study of popular multi-language projects on GitHub and their evolution histories, enabled by our novel techniques for multilingual code characterization. We found statistically significant associations between the proneness of multilingual code to vulnerabilities (in general and of specific categories) and its language selection. We also found this association is correlated with that of the language interfacing mechanism, not that of individual languages. We validated our statistical findings with in-depth case studies on actual vulnerabilities, explained via the mechanism and language selection. Our results call for immediate actions to assess and defend against multilingual vulnerabilities, for which we provide practical recommendations.",Power,Social Recognition,The paper's main contribution revolves around multilingual code vulnerability; and their techniques to characterize it. Their work results in better security; therefore providing software users with increased social recognition as they engage with safer software systems; contributing to the value item 'Social Recognition' and its corresponding value 'Power'.,"In 'Paper X', the authors focus on the identification and analysis of multilingual code vulnerabilities, which ultimately leads to the improvement of software security. By addressing this issue and providing practical recommendations to defend against multilingual vulnerabilities, the paper's main contribution aligns with the value item of Social Recognition and the corresponding value of Power. This is because software users who engage with safer and more secure software systems are viewed as knowledgeable and influential in the software community, thus gaining social recognition and power.",Agreed-Clarified,Agree,,Agree,,
Exp G,Exp K,381,ESEC/FSE,Security & Privacy,On the vulnerability proneness of multilingual code,"Software construction using multiple languages has long been a norm, yet it is still unclear if multilingual code construction has significant security implications and real security consequences. This paper aims to address this question with a large-scale study of popular multi-language projects on GitHub and their evolution histories, enabled by our novel techniques for multilingual code characterization. We found statistically significant associations between the proneness of multilingual code to vulnerabilities (in general and of specific categories) and its language selection. We also found this association is correlated with that of the language interfacing mechanism, not that of individual languages. We validated our statistical findings with in-depth case studies on actual vulnerabilities, explained via the mechanism and language selection. Our results call for immediate actions to assess and defend against multilingual vulnerabilities, for which we provide practical recommendations.",Security,Social Order,"The paper addresses security risks related to multilingual code. Their large-scale study and the recommended practical steps can ensure the development of more secure software systems; directly aligning with the value item ""Social Order"" and corresponding value ""Security""; as a more secure software contributes to a more orderly digital social context.","In 'Paper X', the authors explicitly state that they found statistically significant associations between the proneness of multilingual code to vulnerabilities and its language selection, as well as the language interfacing mechanism. They also validate their findings with in-depth case studies on actual vulnerabilities. By addressing these vulnerabilities and providing practical recommendations to assess and defend against multilingual vulnerabilities, the paper directly aligns with the value item Social Order and its corresponding value Security. As software security is crucial for maintaining a more orderly digital social context, the contributions of 'Paper X' in ensuring the development of more secure software systems can be seen as directly supporting the value of Social Order within the software context.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,381,ESEC/FSE,Security & Privacy,On the vulnerability proneness of multilingual code,"Software construction using multiple languages has long been a norm, yet it is still unclear if multilingual code construction has significant security implications and real security consequences. This paper aims to address this question with a large-scale study of popular multi-language projects on GitHub and their evolution histories, enabled by our novel techniques for multilingual code characterization. We found statistically significant associations between the proneness of multilingual code to vulnerabilities (in general and of specific categories) and its language selection. We also found this association is correlated with that of the language interfacing mechanism, not that of individual languages. We validated our statistical findings with in-depth case studies on actual vulnerabilities, explained via the mechanism and language selection. Our results call for immediate actions to assess and defend against multilingual vulnerabilities, for which we provide practical recommendations.",Stimulation,Daring,"The use of novel techniques to characterize multilingual code shows an innovative and daring approach by the authors. This aligns with the value item ""Daring"" under the parent value ""Stimulation""; as using the results of this research might require users to engage with novel software structures and configurations.","The use of novel techniques for multilingual code characterization in 'Paper X' demonstrates a daring approach by the authors, which aligns with the value item ""Daring"" under the parent value ""Stimulation."" This alignment is evident as the research explores new methods and tools that may require software users to engage with innovative software structures and configurations. By studying the evolution histories of popular multi-language projects on GitHub, the paper offers insights into potential security implications and vulnerabilities associated with multilingual code. This innovative and daring approach of the authors contributes to stimulating software users' curiosity and excitement in exploring and understanding the complexities of multilingual code construction.",Agreed-Clarified,Agree,,Agree,,
Exp B,Exp H,382,ESEC/FSE,Software Testing & QA,Are elevator software robust against uncertainties? results and experiences from an industrial case study,"Industrial elevator systems are complex Cyber-Physical Systems operating in uncertain environments and experiencing uncertain passenger behaviors, hardware delays, and software errors. Identifying, understanding, and classifying such uncertainties are essential to enable system designers to reason about uncertainties and subsequently develop solutions for empowering elevator systems to deal with uncertainties systematically. To this end, we present a method, called RuCynefin, based on the Cynefin framework to classify uncertainties in industrial elevator systems from our industrial partner (Orona, Spain), results of which can then be used for assessing their robustness. RuCynefin is equipped with a novel classification algorithm to identify the Cynefin contexts for a variety of uncertainties in industrial elevator systems, and a novel metric for measuring the robustness using the uncertainty classification. We evaluated RuCynefin with an industrial case study of 90 dispatchers from Orona to assess their robustness against uncertainties. Results show that RuCynefin could effectively identify several situations for which certain dispatchers were not robust. Specifically, 93% of such versions showed some degree of low robustness against uncertainties. We also provide insights on the potential practical usages of RuCynefin, which are useful for practitioners in this field.",Security,Social Order,The paper presents a method called RuCynefin designed to classify uncertainties to improve the function of industrial elevator systems. This aligns directly with the value item Social Order within the value category Security; by creating a more orderly and predictable operation of elevator systems.,"The main contribution of 'Paper X' is the development of the RuCynefin method, which aims to classify uncertainties in industrial elevator systems and assess their robustness. By classifying uncertainties and improving the system's robustness, the paper aligns with the value item Social Order and its corresponding value Security from a ""Software User"" perspective. This is because a more orderly and predictable operation of elevator systems enhances the sense of security for software users who rely on these systems for safe and efficient transportation.",Agreed-Clarified,Disagree,"Related to Security: health, as it discusses the performance of elevator",Disagree,Is software oder social oder? I cannot see the link ,
Exp C,Exp I,382,ESEC/FSE,Software Testing & QA,Are elevator software robust against uncertainties? results and experiences from an industrial case study,"Industrial elevator systems are complex Cyber-Physical Systems operating in uncertain environments and experiencing uncertain passenger behaviors, hardware delays, and software errors. Identifying, understanding, and classifying such uncertainties are essential to enable system designers to reason about uncertainties and subsequently develop solutions for empowering elevator systems to deal with uncertainties systematically. To this end, we present a method, called RuCynefin, based on the Cynefin framework to classify uncertainties in industrial elevator systems from our industrial partner (Orona, Spain), results of which can then be used for assessing their robustness. RuCynefin is equipped with a novel classification algorithm to identify the Cynefin contexts for a variety of uncertainties in industrial elevator systems, and a novel metric for measuring the robustness using the uncertainty classification. We evaluated RuCynefin with an industrial case study of 90 dispatchers from Orona to assess their robustness against uncertainties. Results show that RuCynefin could effectively identify several situations for which certain dispatchers were not robust. Specifically, 93% of such versions showed some degree of low robustness against uncertainties. We also provide insights on the potential practical usages of RuCynefin, which are useful for practitioners in this field.",Security,Healthy,By improving the robustness and reliability of elevator systems; the method introduced in this paper indirectly contributes to the value item Healthy under the value Security. The systematic handling of uncertainties could potentially minimize accidents and ensure the wellness of users on a physical level.,"The justification for aligning the paper with the value item Healthy and its corresponding value Security is based on the premise that by improving the robustness and reliability of elevator systems through the systematic handling of uncertainties, the potential for accidents and incidents that could endanger the physical well-being of users is minimized. This aligns with the value of Security as it pertains to ensuring the safety and protection of individuals. By promoting a sense of security in elevator systems, the method presented in the paper indirectly contributes to the value item Healthy by creating an environment that prioritizes the physical wellness and safety of users.",Agreed-Justified,Agree,,Agree,,
Exp D,Exp I,382,ESEC/FSE,Software Testing & QA,Are elevator software robust against uncertainties? results and experiences from an industrial case study,"Industrial elevator systems are complex Cyber-Physical Systems operating in uncertain environments and experiencing uncertain passenger behaviors, hardware delays, and software errors. Identifying, understanding, and classifying such uncertainties are essential to enable system designers to reason about uncertainties and subsequently develop solutions for empowering elevator systems to deal with uncertainties systematically. To this end, we present a method, called RuCynefin, based on the Cynefin framework to classify uncertainties in industrial elevator systems from our industrial partner (Orona, Spain), results of which can then be used for assessing their robustness. RuCynefin is equipped with a novel classification algorithm to identify the Cynefin contexts for a variety of uncertainties in industrial elevator systems, and a novel metric for measuring the robustness using the uncertainty classification. We evaluated RuCynefin with an industrial case study of 90 dispatchers from Orona to assess their robustness against uncertainties. Results show that RuCynefin could effectively identify several situations for which certain dispatchers were not robust. Specifically, 93% of such versions showed some degree of low robustness against uncertainties. We also provide insights on the potential practical usages of RuCynefin, which are useful for practitioners in this field.",Achievement,Intelligent,The paper introduces a novel classification algorithm and evaluates the robustness of elevator systems; showing the potential for these systems to function intelligently in uncertain environments. This aligns with the value item Intelligent under the value Achievement; as the effective management of uncertainties can be viewed as a demonstration of the system's intelligence; competence; and capability.,"In the paper, the authors introduce a method called RuCynefin, which includes a classification algorithm and a metric for measuring robustness in elevator systems. By evaluating the robustness of these systems against uncertainties, the paper demonstrates the potential for elevators to function intelligently in uncertain environments. This aligns with the value item Intelligent under the value Achievement, as effectively managing uncertainties can be seen as a demonstration of the system's intelligence, competence, and capability.",Agreed-Justified,Agree,,Agree,,
Exp E,Exp J,383,ESEC/FSE,Mobile & IoT,Toward interactive bug reporting for (android app) end-users,"Many software bugs are reported manually, particularly bugs that manifest themselves visually in the user interface. End-users typically report these bugs via app reviewing websites, issue trackers, or in-app built-in bug reporting tools, if available. While these systems have various features that facilitate bug reporting (e.g., textual templates or forms), they often provide limited guidance, concrete feedback, or quality verification to end-users, who are often inexperienced at reporting bugs and submit low-quality bug reports that lead to excessive developer effort in bug report management tasks. We propose an interactive bug reporting system for end-users (Burt), implemented as a task-oriented chatbot. Unlike existing bug reporting systems, Burt provides guided reporting of essential bug report elements (i.e., the observed behavior, expected behavior, and steps to reproduce the bug), instant quality verification, and graphical suggestions for these elements. We implemented a version of Burt for Android and conducted an empirical evaluation study with end-users, who reported 12 bugs from six Android apps studied in prior work. The reporters found that Burt‚Äö√Ñ√∂‚àö√ë‚àö¬•s guidance and automated suggestions/clarifications are useful and Burt is easy to use. We found that Burt reports contain higher-quality information than reports collected via a template-based bug reporting system. Improvements to Burt, informed by the reporters, include support for various wordings to describe bug report elements and improved quality verification. Our work marks an important paradigm shift from static to interactive bug reporting for end-users.",Self Direction,Privacy,The paper contributes to 'Burt'; a tool that aids users in reporting software bugs in a more guided fashion; enhancing their privacy and autonomy in handling issues related to the software and hence aligns with the value of 'Privacy' under the 'Self Direction' value.,"The justification for aligning 'Paper X' with the value item Privacy and its corresponding value Self Direction from a ""Software User"" perspective is based on the fact that the proposed system, Burt, provides an interactive bug reporting system that guides users in reporting bugs in a more structured manner. By offering instant quality verification and graphical suggestions, Burt enables users to handle their software issues independently and privately. This aligns with the value of Privacy within the Self Direction value, as it empowers users to have control over their software experience, ensuring their autonomy and personal space in dealing with software bugs.",Agreed-Clarified,Disagree,The bug reporting systems provide guidance and feedback or quality verification to end users. To me this aligns with self direction but not convinced with privacy part.,Agree,,
Exp F,Exp J,383,ESEC/FSE,Mobile & IoT,Toward interactive bug reporting for (android app) end-users,"Many software bugs are reported manually, particularly bugs that manifest themselves visually in the user interface. End-users typically report these bugs via app reviewing websites, issue trackers, or in-app built-in bug reporting tools, if available. While these systems have various features that facilitate bug reporting (e.g., textual templates or forms), they often provide limited guidance, concrete feedback, or quality verification to end-users, who are often inexperienced at reporting bugs and submit low-quality bug reports that lead to excessive developer effort in bug report management tasks. We propose an interactive bug reporting system for end-users (Burt), implemented as a task-oriented chatbot. Unlike existing bug reporting systems, Burt provides guided reporting of essential bug report elements (i.e., the observed behavior, expected behavior, and steps to reproduce the bug), instant quality verification, and graphical suggestions for these elements. We implemented a version of Burt for Android and conducted an empirical evaluation study with end-users, who reported 12 bugs from six Android apps studied in prior work. The reporters found that Burt‚Äö√Ñ√∂‚àö√ë‚àö¬•s guidance and automated suggestions/clarifications are useful and Burt is easy to use. We found that Burt reports contain higher-quality information than reports collected via a template-based bug reporting system. Improvements to Burt, informed by the reporters, include support for various wordings to describe bug report elements and improved quality verification. Our work marks an important paradigm shift from static to interactive bug reporting for end-users.",Achievement,Successful,The paper; through its design and deployment of 'Burt'; can lead end-user to a more successful interaction with software in which they can more effectively report bugs; identify issues; and improve the software environment; thus serving the value item of 'Successful' in the value of 'Achievement'.,"In the context of a ""Software User,"" the alignment of 'Paper X' with the value item Successful and its corresponding value Achievement is evident through the implementation of 'Burt' as an interactive bug reporting system. By providing guided reporting, instant quality verification, and graphical suggestions, 'Burt' empowers end-users to effectively report bugs, identify issues, and improve the software environment. Through this improved bug reporting process, end-users can achieve successful interactions with the software, leading to higher-quality bug reports and reducing the developer effort required for bug report management tasks. This alignment highlights how 'Paper X' addresses the value of Achievement by enabling end-users to actively contribute to the improvement and success of the software they use.",Agreed-Clarified,Agree,,Agree,,
Exp G,Exp K,383,ESEC/FSE,Mobile & IoT,Toward interactive bug reporting for (android app) end-users,"Many software bugs are reported manually, particularly bugs that manifest themselves visually in the user interface. End-users typically report these bugs via app reviewing websites, issue trackers, or in-app built-in bug reporting tools, if available. While these systems have various features that facilitate bug reporting (e.g., textual templates or forms), they often provide limited guidance, concrete feedback, or quality verification to end-users, who are often inexperienced at reporting bugs and submit low-quality bug reports that lead to excessive developer effort in bug report management tasks. We propose an interactive bug reporting system for end-users (Burt), implemented as a task-oriented chatbot. Unlike existing bug reporting systems, Burt provides guided reporting of essential bug report elements (i.e., the observed behavior, expected behavior, and steps to reproduce the bug), instant quality verification, and graphical suggestions for these elements. We implemented a version of Burt for Android and conducted an empirical evaluation study with end-users, who reported 12 bugs from six Android apps studied in prior work. The reporters found that Burt‚Äö√Ñ√∂‚àö√ë‚àö¬•s guidance and automated suggestions/clarifications are useful and Burt is easy to use. We found that Burt reports contain higher-quality information than reports collected via a template-based bug reporting system. Improvements to Burt, informed by the reporters, include support for various wordings to describe bug report elements and improved quality verification. Our work marks an important paradigm shift from static to interactive bug reporting for end-users.",Stimulation,Variation in Life,The interactive chatbot in the bug reporting process proposed in the paper introduces a source of variation in the regular process of a user's interaction with the software; matching the value item of 'Variation in Life' under 'Stimulation'.,"In ""Paper X,"" the proposed interactive bug reporting system, Burt, introduces a novel and varied approach to the regular bug reporting process for software users. By utilizing a task-oriented chatbot, Burt provides guided reporting, instant quality verification, and graphical suggestions, thereby introducing a higher level of engagement and stimulation in the bug reporting experience. This enhanced interaction aligns with the value item of Variation in Life under Stimulation in Schwartz's Taxonomy as it offers a new and diverse way for software users to interact with the software and actively participate in the bug reporting process. This alignment highlights how the main contribution of Paper X directly addresses and enhances the value of Stimulation for software users.",Agreed-Justified,Agree,,Agree,,
Exp B,Exp H,385,ESEC/FSE,Software Testing & QA,Detecting non-crashing functional bugs in Android apps via deep-state differential analysis,"Non-crashing functional bugs of Android apps can seriously affect user experience. Often buried in rare program paths, such bugs are difficult to detect but lead to severe consequences. Unfortunately, very few automatic functional bug oracles for Android apps exist, and they are all specific to limited types of bugs. In this paper, we introduce a novel technique named deep-state differential analysis, which brings the classical ""bugs as deviant behaviors"" oracle to Android apps as a generic automatic test oracle. Our oracle utilizes the observations on the execution of automatically generated test inputs that (1) there can be a large number of traces reaching internal app states with similar GUI layouts, and only a small portion of them would reach an erroneous app state, and (2) when performing the same sequence of actions on similar GUI layouts, the outcomes will be limited. Therefore, for each set of test inputs terminating at similar GUI layouts, we manifest comparable app behaviors by appending the same events to these inputs, cluster the manifested behaviors, and identify minorities as possible anomalies. We also calibrate the distribution of these test inputs by a novel input calibration procedure, to ensure the distribution of these test inputs is balanced with rare bug occurrences. We implemented the deep-state differential analysis algorithm as an exploratory prototype Odin and evaluated it against 17 popular real-world Android apps. Odin successfully identified 28 non-crashing functional bugs (five of which were previously unknown) of various root causes with reasonable precision. Detailed comparisons and analyses show that a large fraction (11/28) of these bugs cannot be detected by state-of-the-art techniques.",Stimulation,Excitement in Life,By implementing a new technique for identifying and rectifying functional bugs; the paper promises to bring more excitement to the user experience of Android apps; as users can enjoy a smoother and more functional app performance.,"In the abstract of 'Paper X', it is stated that non-crashing functional bugs in Android apps can seriously affect user experience, and the paper introduces a novel technique called deep-state differential analysis to detect these bugs. By utilizing this technique, the paper aims to improve the performance of Android apps, which directly aligns with the value item of Excitement in Life and its corresponding value of Stimulation. This is because the successful identification and rectification of functional bugs can lead to a smoother and more functional app performance, ultimately enhancing the user's experience and generating excitement in the use of Android apps.",Agreed-Justified,Agree,,Agree,,
Exp C,Exp I,385,ESEC/FSE,Software Testing & QA,Detecting non-crashing functional bugs in Android apps via deep-state differential analysis,"Non-crashing functional bugs of Android apps can seriously affect user experience. Often buried in rare program paths, such bugs are difficult to detect but lead to severe consequences. Unfortunately, very few automatic functional bug oracles for Android apps exist, and they are all specific to limited types of bugs. In this paper, we introduce a novel technique named deep-state differential analysis, which brings the classical ""bugs as deviant behaviors"" oracle to Android apps as a generic automatic test oracle. Our oracle utilizes the observations on the execution of automatically generated test inputs that (1) there can be a large number of traces reaching internal app states with similar GUI layouts, and only a small portion of them would reach an erroneous app state, and (2) when performing the same sequence of actions on similar GUI layouts, the outcomes will be limited. Therefore, for each set of test inputs terminating at similar GUI layouts, we manifest comparable app behaviors by appending the same events to these inputs, cluster the manifested behaviors, and identify minorities as possible anomalies. We also calibrate the distribution of these test inputs by a novel input calibration procedure, to ensure the distribution of these test inputs is balanced with rare bug occurrences. We implemented the deep-state differential analysis algorithm as an exploratory prototype Odin and evaluated it against 17 popular real-world Android apps. Odin successfully identified 28 non-crashing functional bugs (five of which were previously unknown) of various root causes with reasonable precision. Detailed comparisons and analyses show that a large fraction (11/28) of these bugs cannot be detected by state-of-the-art techniques.",Security,Healthy,The paper directly contributes to improving the health of Android apps by providing a more effective way of detecting and fixing bugs; which significantly enhances the security and reliability of the software from a user's perspective.,"In the abstract of 'Paper X', it is stated that the proposed technique of deep-state differential analysis helps in detecting non-crashing functional bugs in Android apps. By improving bug detection and fixing, the paper directly contributes to enhancing the security and reliability of the software. This aligns with the value item of Healthy in Schwartz's Taxonomy, as it addresses the health of the apps by reducing the presence of bugs that can have serious consequences on user experience. In turn, this improved health contributes to the value of Security from a software user's perspective. Therefore, the alignment with the value item Healthy and its corresponding value Security is evident in the paper's contributions.",Agreed-Clarified,Agree,,Agree,,
Exp D,Exp I,394,ESEC/FSE,AI & Machine Learning,Uncertainty-aware transfer learning to evolve digital twins for industrial elevators,"Digital twins are increasingly developed to support the development, operation, and maintenance of cyber-physical systems such as industrial elevators. However, industrial elevators continuously evolve due to changes in physical installations, introducing new software features, updating existing ones, and making changes due to regulations (e.g., enforcing restricted elevator capacity due to COVID-19), etc. Thus, digital twin functionalities (often built on neural network-based models) need to evolve themselves constantly to be synchronized with the industrial elevators. Such an evolution is preferred to be automated, as manual evolution is time-consuming and error-prone. Moreover, collecting sufficient data to re-train neural network models of digital twins could be expensive or even infeasible. To this end, we propose unceRtaInty-aware tranSfer lEarning enriched Digital Twins LATTICE, a transfer learning based approach capable of transferring knowledge about the waiting time prediction capability of a digital twin of an industrial elevator across different scenarios. LATTICE also leverages uncertainty quantification to further improve its effectiveness. To evaluate LATTICE, we conducted experiments with 10 versions of an elevator dispatching software from Orona, Spain, which are deployed in a Software in the Loop (SiL) environment. Experiment results show that LATTICE, on average, improves the Mean Squared Error by 13.131% and the utilization of uncertainty quantification further improves it by 2.71%.",Self Direction,Creativity,The paper proposes LATTICE; an innovation using unceRtaInty-aware tranSfer lEarning for digital twins in industrial elevator systems. This creativity in solution aligns directly with the value item Creativity; corresponding to the value Self-Direction.,"The paper's proposal of LATTICE, an innovative approach using uncertainty-aware transfer learning for digital twins in industrial elevator systems, showcases creativity in finding a solution to the problem of constantly evolving digital twin functionalities. This aligns directly with the value item Creativity, which is associated with the value of Self-Direction. By thinking creatively and developing a novel approach, the paper demonstrates a commitment to self-direction and independent thinking to overcome challenges in the software context.",Agreed-Justified,Agree,,Agree,"Uncertainity and transfer learning makes it aligned to the value Intelligent not Creativity. Having read the abstrcat, the combination of terms like 'propose' 'learning' appears to be the reason for GPT to classify it as Self-direction and than creativity in that hierarchy.",
Exp E,Exp J,394,ESEC/FSE,AI & Machine Learning,Uncertainty-aware transfer learning to evolve digital twins for industrial elevators,"Digital twins are increasingly developed to support the development, operation, and maintenance of cyber-physical systems such as industrial elevators. However, industrial elevators continuously evolve due to changes in physical installations, introducing new software features, updating existing ones, and making changes due to regulations (e.g., enforcing restricted elevator capacity due to COVID-19), etc. Thus, digital twin functionalities (often built on neural network-based models) need to evolve themselves constantly to be synchronized with the industrial elevators. Such an evolution is preferred to be automated, as manual evolution is time-consuming and error-prone. Moreover, collecting sufficient data to re-train neural network models of digital twins could be expensive or even infeasible. To this end, we propose unceRtaInty-aware tranSfer lEarning enriched Digital Twins LATTICE, a transfer learning based approach capable of transferring knowledge about the waiting time prediction capability of a digital twin of an industrial elevator across different scenarios. LATTICE also leverages uncertainty quantification to further improve its effectiveness. To evaluate LATTICE, we conducted experiments with 10 versions of an elevator dispatching software from Orona, Spain, which are deployed in a Software in the Loop (SiL) environment. Experiment results show that LATTICE, on average, improves the Mean Squared Error by 13.131% and the utilization of uncertainty quantification further improves it by 2.71%.",Achievement,Intelligent,The LATTICE solution uses advanced techniques such as transfer learning and uncertainty quantification; which assumes intelligent users capable of understanding and implementing such advanced solutions. Therefore; it aligns with the value item Intelligent; corresponding to the value Achievement.,"The justification is that the LATTICE solution, by utilizing advanced techniques such as transfer learning and uncertainty quantification, requires users who possess a certain level of intelligence and understanding to effectively implement and utilize these techniques. Therefore, the alignment with the value item Intelligent and its corresponding value Achievement is evident, as the solution caters to software users who strive for intellectual growth and accomplishment in their utilization of advanced software solutions.",Agreed-Clarified,Agree,,Agree,,
Exp F,Exp J,394,ESEC/FSE,AI & Machine Learning,Uncertainty-aware transfer learning to evolve digital twins for industrial elevators,"Digital twins are increasingly developed to support the development, operation, and maintenance of cyber-physical systems such as industrial elevators. However, industrial elevators continuously evolve due to changes in physical installations, introducing new software features, updating existing ones, and making changes due to regulations (e.g., enforcing restricted elevator capacity due to COVID-19), etc. Thus, digital twin functionalities (often built on neural network-based models) need to evolve themselves constantly to be synchronized with the industrial elevators. Such an evolution is preferred to be automated, as manual evolution is time-consuming and error-prone. Moreover, collecting sufficient data to re-train neural network models of digital twins could be expensive or even infeasible. To this end, we propose unceRtaInty-aware tranSfer lEarning enriched Digital Twins LATTICE, a transfer learning based approach capable of transferring knowledge about the waiting time prediction capability of a digital twin of an industrial elevator across different scenarios. LATTICE also leverages uncertainty quantification to further improve its effectiveness. To evaluate LATTICE, we conducted experiments with 10 versions of an elevator dispatching software from Orona, Spain, which are deployed in a Software in the Loop (SiL) environment. Experiment results show that LATTICE, on average, improves the Mean Squared Error by 13.131% and the utilization of uncertainty quantification further improves it by 2.71%.",Security,Healthy,The paper mentions that the transfer learning based approach will aid the efficient maintenance of cyber-physical systems like industrial elevators; which can be seen as an indirect benefit to users' health by ensuring the safe operation of these systems. This aligns with the value item Healthy; corresponding to the value Security.,"Certainly! The justification is that the main contribution of 'Paper X' is the development of a transfer learning based approach that aids in the efficient maintenance of cyber-physical systems like industrial elevators. By ensuring the safe operation of these systems, the approach indirectly contributes to the users' health. For example, if the elevator is maintained properly, there is a lower risk of accidents or malfunctions that could potentially harm the users. This aligns with the value item Healthy, as it emphasizes the importance of maintaining systems that promote the well-being and safety of individuals.",Agreed-Clarified,Disagree,,Disagree,I dont think so if the proposed work is diectly related to Health or defined in Health domain. Based on my understanding of the abstract its broadly cover the industry not specific health.,
Exp G,Exp K,398,ESEC/FSE,AI & Machine Learning,RULER: discriminative and iterative adversarial training for deep neural network fairness,"Deep Neural Networks (DNNs) are becoming an integral part of many real-world applications, such as autonomous driving and financial management. While these models enable autonomy, there are however concerns regarding their ethics in decision making. For instance, fairness is an aspect that requires particular attention. A number of fairness testing techniques have been proposed to address this issue, e.g., by generating test cases called individual discriminatory instances for repairing DNNs. Although they have demonstrated great potential, they tend to generate many test cases that are not directly effective in improving fairness and incur substantial computation overhead. We propose a new model repair technique, RULER, by discriminating sensitive and non-sensitive attributes during test case generation for model repair. The generated cases are then used in training to improve DNN fairness. RULER balances the trade-off between accuracy and fairness by decomposing the training procedure into two phases and introducing a novel iterative adversarial training method for fairness. Compared to the state-of-the-art techniques on four datasets, RULER has 7-28 times more effective repair test cases generated, is 10-15 times faster in test generation, and has 26-43% more fairness improvement on‚Äö√Ñ√∂‚àö√ë‚àö√´average.",Universalism,Equality,The paper proposes ‚Äö√Ñ√∂‚àö√ë‚àö‚â§RULER‚Äö√Ñ√∂‚àö√ë‚àö¬•; a novel repair technique; to improve fairness in decision making with deep neural networks; which aligns with the value item Equality from the value Universalism.,"The justification for labeling 'Paper X' as aligning with the value item Equality and its corresponding value Universalism from a ""Software User"" perspective is based on the fact that the paper proposes a novel repair technique, RULER, to improve fairness in decision making with deep neural networks. Fairness, in this context, can be seen as treating all individuals equally and ensuring equal opportunities for all. By addressing fairness concerns in decision making, 'Paper X' directly aligns with the value item Equality and its corresponding value Universalism, as it aims to create a more equitable and just software system.",Agreed-Clarified,Agree,,Agree,,
Exp A,Exp H,398,ESEC/FSE,AI & Machine Learning,RULER: discriminative and iterative adversarial training for deep neural network fairness,"Deep Neural Networks (DNNs) are becoming an integral part of many real-world applications, such as autonomous driving and financial management. While these models enable autonomy, there are however concerns regarding their ethics in decision making. For instance, fairness is an aspect that requires particular attention. A number of fairness testing techniques have been proposed to address this issue, e.g., by generating test cases called individual discriminatory instances for repairing DNNs. Although they have demonstrated great potential, they tend to generate many test cases that are not directly effective in improving fairness and incur substantial computation overhead. We propose a new model repair technique, RULER, by discriminating sensitive and non-sensitive attributes during test case generation for model repair. The generated cases are then used in training to improve DNN fairness. RULER balances the trade-off between accuracy and fairness by decomposing the training procedure into two phases and introducing a novel iterative adversarial training method for fairness. Compared to the state-of-the-art techniques on four datasets, RULER has 7-28 times more effective repair test cases generated, is 10-15 times faster in test generation, and has 26-43% more fairness improvement on‚Äö√Ñ√∂‚àö√ë‚àö‚àÇ‚Äö√†√∂‚àö√´‚Äö√†√∂‚Äö√†√á‚Äö√Ñ√∂‚àö‚Ä†‚àö‚àÇ‚Äö√†√∂‚àö¬¥‚Äö√Ñ√∂‚àö‚Ä†‚àö‚àÇ‚Äö√†√∂¬¨¬•average.",Universalism,A World at Peace,The technique works towards giving fair judgement and maintaining a peace in the system; thus aligning with 'A World at Peace' from the value Universalism.,"My justification for aligning 'Paper X' with the value item A World at Peace and its corresponding value Universalism from a ""Software User"" perspective is based on the fact that the proposed technique, RULER, aims to improve fairness in decision-making, which is an important aspect of maintaining peace and harmony in a system. By discriminating sensitive and non-sensitive attributes during test case generation and introducing an iterative adversarial training method, RULER seeks to strike a balance between accuracy and fairness. This aligns with the value of Universalism, which emphasizes broadmindedness, equality, and social justice, all of which are essential for creating a world at peace.",Agreed-Justified,Agree,,Agree,,
Exp B,Exp H,398,ESEC/FSE,AI & Machine Learning,RULER: discriminative and iterative adversarial training for deep neural network fairness,"Deep Neural Networks (DNNs) are becoming an integral part of many real-world applications, such as autonomous driving and financial management. While these models enable autonomy, there are however concerns regarding their ethics in decision making. For instance, fairness is an aspect that requires particular attention. A number of fairness testing techniques have been proposed to address this issue, e.g., by generating test cases called individual discriminatory instances for repairing DNNs. Although they have demonstrated great potential, they tend to generate many test cases that are not directly effective in improving fairness and incur substantial computation overhead. We propose a new model repair technique, RULER, by discriminating sensitive and non-sensitive attributes during test case generation for model repair. The generated cases are then used in training to improve DNN fairness. RULER balances the trade-off between accuracy and fairness by decomposing the training procedure into two phases and introducing a novel iterative adversarial training method for fairness. Compared to the state-of-the-art techniques on four datasets, RULER has 7-28 times more effective repair test cases generated, is 10-15 times faster in test generation, and has 26-43% more fairness improvement on‚Äö√Ñ√∂‚àö√ë‚àö‚àÇ‚Äö√†√∂‚àö√´‚Äö√†√∂‚Äö√†√á‚Äö√Ñ√∂‚àö‚Ä†‚àö‚àÇ‚Äö√†√∂‚àö¬¥‚Äö√Ñ√∂‚àö‚Ä†‚àö‚àÇ‚Äö√†√∂¬¨¬•average.",Achievement,Successful,Although 'Achievement' mainly talks about what software users can achieve; in the case of 'RULER'; users can experience successful use of the software because of more fair outcomes. 'RULER' works towards maintaining fairness by being more effective and efficient in creating test cases; thus possibly making the software more successful.,"In the case of 'RULER,' the alignment with the value item Successful and its corresponding value Achievement from a ""Software User"" perspective is evident. By improving the fairness of the software through more effective and efficient test case generation, 'RULER' allows software users to experience successful outcomes. This aligns with the value of Achievement as users can achieve fair results and have a more successful experience with the software.",Agreed-Justified,Agree,,Agree,,
Exp C,Exp I,402,ESEC/FSE,Software Engineering Practices,In war and peace: the impact of world politics on software ecosystems,"Reliance on third-party libraries is now commonplace in contemporary software engineering. Being open source in nature, these libraries should advocate for a world where the freedoms and opportunities of open source software can be enjoyed by all. Yet, there is a growing concern related to maintainers using their influence to make political stances (i.e., referred to as protestware). In this paper, we reflect on the impact of world politics on software ecosystems, especially in the context of the ongoing War in Ukraine. We show three cases where world politics has had an impact on a software ecosystem, and how these incidents may result in either benign or malignant consequences. We further point to specific opportunities for research, and conclude with a research agenda with ten research questions to guide future research directions.",Universalism,Social Justice,The paper focuses on the understanding and management of political influences on software ecosystem which implies a concern for fairness and justice. This aligns with the value item Social Justice and its corresponding value Universalism.,"The paper's exploration of the impact of world politics on software ecosystems shows a concern for fairness and justice by examining the potential benign or malignant consequences of political influences. This aligns with the value item Social Justice, which emphasizes promoting fairness and equality in society. Additionally, as the paper aims to advocate for a world where open source software can be enjoyed by all, it aligns with the corresponding value of Universalism, which emphasizes equality and inclusivity across different groups and contexts.",Agreed-Justified,Agree,,Agree,,
Exp D,Exp I,402,ESEC/FSE,Software Engineering Practices,In war and peace: the impact of world politics on software ecosystems,"Reliance on third-party libraries is now commonplace in contemporary software engineering. Being open source in nature, these libraries should advocate for a world where the freedoms and opportunities of open source software can be enjoyed by all. Yet, there is a growing concern related to maintainers using their influence to make political stances (i.e., referred to as protestware). In this paper, we reflect on the impact of world politics on software ecosystems, especially in the context of the ongoing War in Ukraine. We show three cases where world politics has had an impact on a software ecosystem, and how these incidents may result in either benign or malignant consequences. We further point to specific opportunities for research, and conclude with a research agenda with ten research questions to guide future research directions.",Benevolence,Helpful,By solely providing detailed cases of influence; it indirectly pushes for improving well-being of larger community by urging for strategies to handle such situations. This might result in the software users contributing to the well-being of others. This aligns with the value item Helpful and its corresponding value Benevolence.,"In the abstract of 'Paper X', the author reflects on the impact of world politics on software ecosystems and highlights incidents that can have either benign or malignant consequences. By addressing these issues and suggesting specific opportunities for research, the paper indirectly promotes the well-being of the larger community of software users. This emphasis on improving the software ecosystem aligns with the value item Helpful and its corresponding value Benevolence, as it encourages strategies that contribute to the well-being of others and promotes a positive impact on software users.",Agreed-Clarified,Agree,,Agree,,
Exp F,Exp J,403,ESEC/FSE,Software Project Management,Corporate dominance in open source ecosystems: a case study of OpenStack,"Corporate participation plays an increasing role in Open Source Software (OSS) development. Unlike volunteers in OSS projects, companies are driven by business objectives. To pursue corporate interests, companies may try to dominate the development direction of OSS projects. One company's domination in OSS may 'crowd out' other contributors, changing the nature of the project, and jeopardizing the sustainability of the OSS ecosystem. Prior studies of corporate involvement in OSS have primarily focused on predominately positive aspects such as business strategies, contribution models, and collaboration patterns. However, there is a scarcity of research on the potential drawbacks of corporate engagement. In this paper, we investigate corporate dominance in OSS ecosystems. We draw on the field of Economics and quantify company domination using a dominance measure; we investigate the prevalence, patterns, and impact of domination in the evolution of the OpenStack ecosystem. We find evidence of company domination in over 73% of the repositories in OpenStack, and approximately 25% of companies dominate one or more repositories per version. We identify five patterns of corporate dominance: Early incubation, Full-time hosting, Growing domination, Occasional domination, and Last remaining. We find that domination has a significantly negative relationship with the survival probability of OSS projects. This study provides insights for building sustainable relationships between companies and the OSS ecosystems in which they seek to get involved.",Achievement,Capable,The paper investigates the dominant role of companies in open source software development with the objective of ensuring the persistence of such projects. This ensures that users have a reliable and continuously improving software; showing that the company is capable.,"The justification for aligning 'Paper X' with the value item Capable and its corresponding value Achievement from a ""Software User"" perspective is based on the fact that the paper investigates the prevalence and impact of corporate domination in open source software projects. By addressing the potential drawbacks of company involvement and emphasizing the need for sustainable relationships, the paper highlights the importance of capable companies in ensuring the persistence of these projects and providing users with reliable and continuously improving software. This aligns with the value of Achievement, as it focuses on the success and effectiveness in achieving business objectives and contributing to the development of high-quality software for users.",Agreed-Clarified,Agree,,Agree,,
Exp G,Exp K,403,ESEC/FSE,Software Project Management,Corporate dominance in open source ecosystems: a case study of OpenStack,"Corporate participation plays an increasing role in Open Source Software (OSS) development. Unlike volunteers in OSS projects, companies are driven by business objectives. To pursue corporate interests, companies may try to dominate the development direction of OSS projects. One company's domination in OSS may 'crowd out' other contributors, changing the nature of the project, and jeopardizing the sustainability of the OSS ecosystem. Prior studies of corporate involvement in OSS have primarily focused on predominately positive aspects such as business strategies, contribution models, and collaboration patterns. However, there is a scarcity of research on the potential drawbacks of corporate engagement. In this paper, we investigate corporate dominance in OSS ecosystems. We draw on the field of Economics and quantify company domination using a dominance measure; we investigate the prevalence, patterns, and impact of domination in the evolution of the OpenStack ecosystem. We find evidence of company domination in over 73% of the repositories in OpenStack, and approximately 25% of companies dominate one or more repositories per version. We identify five patterns of corporate dominance: Early incubation, Full-time hosting, Growing domination, Occasional domination, and Last remaining. We find that domination has a significantly negative relationship with the survival probability of OSS projects. This study provides insights for building sustainable relationships between companies and the OSS ecosystems in which they seek to get involved.",Security,National Security,The paper examines how companies' dominance could potentially jeopardize the sustainability of the Open Source Software ecosystem; aligning with the value item National Security as it represents maintaining the health of a system (OSS ecosystem) critical to many.,"In the given paper abstract, it is stated that company dominance in Open Source Software (OSS) development can jeopardize the sustainability of the OSS ecosystem. This aligns with the value item National Security, as the sustainability and health of the OSS ecosystem can be considered crucial for the software users, ensuring the secure and reliable functioning of the software they rely on. The concept of national security can be extended to encompass the security and stability of important systems or structures, such as the OSS ecosystem, which is relevant to software users.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,403,ESEC/FSE,Software Project Management,Corporate dominance in open source ecosystems: a case study of OpenStack,"Corporate participation plays an increasing role in Open Source Software (OSS) development. Unlike volunteers in OSS projects, companies are driven by business objectives. To pursue corporate interests, companies may try to dominate the development direction of OSS projects. One company's domination in OSS may 'crowd out' other contributors, changing the nature of the project, and jeopardizing the sustainability of the OSS ecosystem. Prior studies of corporate involvement in OSS have primarily focused on predominately positive aspects such as business strategies, contribution models, and collaboration patterns. However, there is a scarcity of research on the potential drawbacks of corporate engagement. In this paper, we investigate corporate dominance in OSS ecosystems. We draw on the field of Economics and quantify company domination using a dominance measure; we investigate the prevalence, patterns, and impact of domination in the evolution of the OpenStack ecosystem. We find evidence of company domination in over 73% of the repositories in OpenStack, and approximately 25% of companies dominate one or more repositories per version. We identify five patterns of corporate dominance: Early incubation, Full-time hosting, Growing domination, Occasional domination, and Last remaining. We find that domination has a significantly negative relationship with the survival probability of OSS projects. This study provides insights for building sustainable relationships between companies and the OSS ecosystems in which they seek to get involved.",Universalism,Social Justice,The paper delves into the impact of corporate dominance and its potential harm to the open source ecosystem which can be seen as advocating for 'Social Justice' in software development which benefits all users equally.,"In the abstract of 'Paper X', it is explicitly mentioned that corporate domination in open source projects can jeopardize the sustainability of the ecosystem. This aligns with the value of Social Justice, as it promotes fairness and equal opportunity for all users in the software context. By highlighting the potential harm caused by corporate dominance, the paper advocates for a more inclusive and diverse software development environment, where the interests of all users are represented and considered. This directly supports the value of Universalism, which emphasizes equality and the protection of rights and opportunities for all individuals in society, including software users.",Agreed-Justified,Disagree,reconcile with coder_2,Disagree,"The justification doesn't align with the value item of the Social justic
",
Exp B,Exp H,405,ESEC/FSE,Code Generation & Analysis,SolSEE: a source-level symbolic execution engine for solidity,"Most of the existing smart contract symbolic execution tools perform analysis on bytecode, which loses high-level semantic information presented in source code. This makes interactive analysis tasks‚Äö√Ñ√∂‚àö√ë‚àö‚àÇ‚Äö√†√∂‚àö√´‚Äö√†√∂‚Äö√†√á‚Äö√Ñ√∂‚àö‚Ä†‚àö‚àÇ‚Äö√†√∂‚àö¬¥‚Äö√Ñ√∂‚àö‚Ä†‚àö‚àÇ‚Äö√†√∂‚àö√∫such as visualization and debugging‚Äö√Ñ√∂‚àö√ë‚àö‚àÇ‚Äö√†√∂‚àö√´‚Äö√†√∂‚Äö√†√á‚Äö√Ñ√∂‚àö‚Ä†‚àö‚àÇ‚Äö√†√∂‚àö¬¥‚Äö√Ñ√∂‚àö‚Ä†‚àö‚àÇ‚Äö√†√∂‚àö√∫extremely challenging, and significantly limits the tool usability. In this paper, we present SolSEE, a source-level symbolic execution engine for Solidity smart contracts. We describe the design of SolSEE, highlight its key features, and demonstrate its usages through a Web-based user interface. SolSEE demonstrates advantages over other existing source-level analysis tools in the advanced Solidity language features it supports and analysis flexibility. A demonstration video is available at: https://sites.google.com/view/solsee/.",Achievement,Intelligent,Paper X highlights the development of SolSEE for the advanced analysis of Solidity smart contracts from a software user's perspective. The ability to execute symbolic execution at the source level empowers the software user to intelligently analyze and understand the behavior of smart contracts; thus aligning with the value item Intelligent and the corresponding value Achievement.,"In 'Paper X', the development of SolSEE allows for the advanced analysis of Solidity smart contracts from a software user's perspective. By enabling symbolic execution at the source level, software users gain the ability to deeply analyze and comprehend the behavior of smart contracts. This aligns with the value item Intelligent because it supports the software user in making informed and intelligent decisions based on their analysis. Additionally, the achievement value is also aligned as it represents the software user's ability to accomplish their analysis goals effectively and efficiently through the use of SolSEE.",Agreed-Justified,Agree,,Agree,,
Exp C,Exp I,405,ESEC/FSE,Code Generation & Analysis,SolSEE: a source-level symbolic execution engine for solidity,"Most of the existing smart contract symbolic execution tools perform analysis on bytecode, which loses high-level semantic information presented in source code. This makes interactive analysis tasks‚Äö√Ñ√∂‚àö√ë‚àö√Üsuch as visualization and debugging‚Äö√Ñ√∂‚àö√ë‚àö√Üextremely challenging, and significantly limits the tool usability. In this paper, we present SolSEE, a source-level symbolic execution engine for Solidity smart contracts. We describe the design of SolSEE, highlight its key features, and demonstrate its usages through a Web-based user interface. SolSEE demonstrates advantages over other existing source-level analysis tools in the advanced Solidity language features it supports and analysis flexibility. A demonstration video is available at: https://sites.google.com/view/solsee/.",Self Direction,Creativity,SolSEE's design features the flexibility of source-level analysis; which can stimulate the creativity of software users in troubleshooting and understanding complex smart contracts. This allows users to garner a more intuitive understanding of the technology; corresponding to the value item Creativity under the value Self Direction.,"The design features of SolSEE provide software users with the flexibility of source-level analysis, allowing them to troubleshoot and understand complex smart contracts in a more intuitive way. This aspect of the tool aligns with the value item Creativity from Schwartz's Taxonomy, under the value Self Direction, as it enables users to creatively explore and navigate the code, fostering a sense of independence and curiosity towards the technology.",Agreed-Clarified,Agree,,Agree,,
Exp D,Exp I,405,ESEC/FSE,Code Generation & Analysis,SolSEE: a source-level symbolic execution engine for solidity,"Most of the existing smart contract symbolic execution tools perform analysis on bytecode, which loses high-level semantic information presented in source code. This makes interactive analysis tasks‚Äö√Ñ√∂‚àö√ë‚àö√Üsuch as visualization and debugging‚Äö√Ñ√∂‚àö√ë‚àö√Üextremely challenging, and significantly limits the tool usability. In this paper, we present SolSEE, a source-level symbolic execution engine for Solidity smart contracts. We describe the design of SolSEE, highlight its key features, and demonstrate its usages through a Web-based user interface. SolSEE demonstrates advantages over other existing source-level analysis tools in the advanced Solidity language features it supports and analysis flexibility. A demonstration video is available at: https://sites.google.com/view/solsee/.",Stimulation,Excitement in Life,By offering a new; novel method of analyzing Solidity smart contracts via a source-level symbolic execution engine; Paper X introduces excitement into the software user's life by providing a new tool for interactive analysis. This aligns directly with the value item Excitement in life and the corresponding value Stimulation.,"In 'Paper X', the introduction of the SolSEE source-level symbolic execution engine for analyzing Solidity smart contracts directly aligns with the value item Excitement in Life and its corresponding value Stimulation. This is because the paper presents a new tool that enhances the software user's experience by enabling interactive analysis tasks such as visualization and debugging, which were previously challenging with existing tools. By offering a more user-friendly and efficient solution, 'Paper X' brings an element of excitement and stimulation to the software user's life, aligning directly with the value item and value mentioned.",Agreed-Justified,Agree,,Agree,,
Exp E,Exp J,412,ESEC/FSE,Software Deployment & Operations,AutoTSG: learning and synthesis for incident troubleshooting,"Incident management is a key aspect of operating large-scale cloud services. To aid with faster and efficient resolution of incidents, engineering teams document frequent troubleshooting steps in the form of Troubleshooting Guides (TSGs), to be used by on-call engineers (OCEs). However, TSGs are siloed, unstructured, and often incomplete, requiring developers to manually understand and execute necessary steps. This results in a plethora of issues such as on-call fatigue, reduced productivity, and human errors. In this work, we conduct a large-scale empirical study of over 4K+ TSGs mapped to incidents and find that TSGs are widely used and help significantly reduce mitigation efforts. We then analyze feedback on TSGs provided by 400+ OCEs and propose a taxonomy of issues that highlights significant gaps in TSG quality. To alleviate these gaps, we investigate the automation of TSGs and propose AutoTSG -- a novel framework for automation of TSGs to executable workflows by combining machine learning and program synthesis. Our evaluation of AutoTSG on 50 TSGs shows the effectiveness in both identifying TSG statements (accuracy 0.89) and parsing them for execution (precision 0.94 and recall 0.91). Lastly, we survey ten Microsoft engineers and show the importance of TSG automation and the usefulness of AutoTSG.",Achievement,Capable,The paper presents AutoTSG; a novel framework for automating troubleshooting guides. This aligns with Capable; the value item under the value of Achievement; since the users (the on-call engineers) are provided with a tool that enhances their capability to deal with incident management more effectively.,"In the paper, the authors introduce AutoTSG, a framework that automates troubleshooting guides for on-call engineers. By providing this tool, the engineers' capability to handle incident management is enhanced, allowing them to resolve issues more efficiently and effectively. This aligns with the value item Capable under the value of Achievement since it empowers software users (the on-call engineers) by improving their skills and abilities in handling and resolving incidents.",Agreed-Clarified,Agree,,Agree,,
Exp F,Exp J,412,ESEC/FSE,Software Deployment & Operations,AutoTSG: learning and synthesis for incident troubleshooting,"Incident management is a key aspect of operating large-scale cloud services. To aid with faster and efficient resolution of incidents, engineering teams document frequent troubleshooting steps in the form of Troubleshooting Guides (TSGs), to be used by on-call engineers (OCEs). However, TSGs are siloed, unstructured, and often incomplete, requiring developers to manually understand and execute necessary steps. This results in a plethora of issues such as on-call fatigue, reduced productivity, and human errors. In this work, we conduct a large-scale empirical study of over 4K+ TSGs mapped to incidents and find that TSGs are widely used and help significantly reduce mitigation efforts. We then analyze feedback on TSGs provided by 400+ OCEs and propose a taxonomy of issues that highlights significant gaps in TSG quality. To alleviate these gaps, we investigate the automation of TSGs and propose AutoTSG -- a novel framework for automation of TSGs to executable workflows by combining machine learning and program synthesis. Our evaluation of AutoTSG on 50 TSGs shows the effectiveness in both identifying TSG statements (accuracy 0.89) and parsing them for execution (precision 0.94 and recall 0.91). Lastly, we survey ten Microsoft engineers and show the importance of TSG automation and the usefulness of AutoTSG.",Power,Social Power,The AutoTSG framework presented in the paper contributes to Social Power; a value item under the value of Power; as it empowers the user (the on-call engineer) with a reliable tool that aids in maintaining the social order within the technical community by mitigating troubleshooting issues more efficiently.,"In 'Paper X', the AutoTSG framework aligns with the value item Social Power and its corresponding value Power from a ""Software User"" perspective. This alignment is justified as the framework empowers the on-call engineer, who is the user, by providing them with a reliable tool that helps in maintaining social order within the technical community. By automating troubleshooting steps, it reduces on-call fatigue, improves productivity, and minimizes human errors, thereby giving the user more control and influence over incident management processes. This enhancement in efficiency and effectiveness can be seen as a manifestation of social power, as the user gains authority and recognition within their professional community through their ability to resolve incidents more efficiently.",Agreed-Clarified,Agree,,Agree,,
Exp G,Exp K,412,ESEC/FSE,Software Deployment & Operations,AutoTSG: learning and synthesis for incident troubleshooting,"Incident management is a key aspect of operating large-scale cloud services. To aid with faster and efficient resolution of incidents, engineering teams document frequent troubleshooting steps in the form of Troubleshooting Guides (TSGs), to be used by on-call engineers (OCEs). However, TSGs are siloed, unstructured, and often incomplete, requiring developers to manually understand and execute necessary steps. This results in a plethora of issues such as on-call fatigue, reduced productivity, and human errors. In this work, we conduct a large-scale empirical study of over 4K+ TSGs mapped to incidents and find that TSGs are widely used and help significantly reduce mitigation efforts. We then analyze feedback on TSGs provided by 400+ OCEs and propose a taxonomy of issues that highlights significant gaps in TSG quality. To alleviate these gaps, we investigate the automation of TSGs and propose AutoTSG -- a novel framework for automation of TSGs to executable workflows by combining machine learning and program synthesis. Our evaluation of AutoTSG on 50 TSGs shows the effectiveness in both identifying TSG statements (accuracy 0.89) and parsing them for execution (precision 0.94 and recall 0.91). Lastly, we survey ten Microsoft engineers and show the importance of TSG automation and the usefulness of AutoTSG.",Security,Reciprocation of Favors,By automating the troubleshooting process; AutoTSG directly contributes to Reciprocation of Favors; a value item under the value of Security. The automation framework serves as a trusted aid; thus enhancing the feeling of security among the users (the on-call engineers) in resolving incidents.,"The automation of troubleshooting processes through the AutoTSG framework contributes to Reciprocation of Favors and the value of Security in several ways. By providing on-call engineers with a reliable and efficient tool to resolve incidents, AutoTSG reduces their workload and fatigue, enabling them to reciprocate the favors of the organization that entrusts them with incident management responsibilities. Additionally, the automation framework offers a sense of security by minimizing human errors and ensuring accurate execution of necessary steps, ultimately leading to a more secure and stable software environment.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,418,ESEC/FSE,Code Generation & Analysis,Cross-language Android permission specification,"The Android system manages access to sensitive APIs by permission enforcement. An application (app) must declare proper permissions before invoking specific Android APIs. However, there is no official documentation providing the complete list of permission-protected APIs and the corresponding permissions to date. Researchers have spent significant efforts extracting such API protection mapping from the Android API framework, which leverages static code analysis to determine if specific permissions are required before accessing an API. Nevertheless, none of them has attempted to analyze the protection mapping in the native library (i.e., code written in C and C++), an essential component of the Android framework that handles communication with the lower-level hardware, such as cameras and sensors. While the protection mapping can be utilized to detect various security vulnerabilities in Android apps, such as permission over-privilege, imprecise mapping will lead to false results in detecting such security vulnerabilities. To fill this gap, we thereby propose to construct the protection mapping involved in the native libraries of the Android framework to present a complete and accurate specification of Android API protection. We develop a prototype system, named NatiDroid, to facilitate the cross-language static analysis and compare its performance with two state-of-the-practice tools, termed Axplorer and Arcade. We evaluate NatiDroid on more than 11,000 Android apps, including system apps from custom Android ROMs and third-party apps from the Google Play. Our NatiDroid can identify up to 464 new API-permission mappings, in contrast to the worst-case results derived from both Axplorer and Arcade, where approximately 71% apps have at least one false positive in permission over-privilege. We have disclosed all the potential vulnerabilities detected to the stakeholders.",Security,Social Order,"Paper X advances the security of Android API by improving and thorough mapping of API protection; reducing false positives in permission over-privilege. This aligns with the value item ""Social Order"" under the value ""Security"" as it ensures a more reliably organized and secure system for the user.","The justification for aligning 'Paper X' with the value item ""Social Order"" and the corresponding value ""Security"" is based on the fact that the paper contributes to improving the mapping of API protection in the Android system. By reducing false positives in permission over-privilege, the paper helps create a more organized and secure system for the software user. This enhanced security contributes to maintaining social order within the software context, ensuring that users can trust the system and their sensitive data remains protected.",Agreed-Clarified,Agree,,Agree,,
Exp B,Exp H,418,ESEC/FSE,Code Generation & Analysis,Cross-language Android permission specification,"The Android system manages access to sensitive APIs by permission enforcement. An application (app) must declare proper permissions before invoking specific Android APIs. However, there is no official documentation providing the complete list of permission-protected APIs and the corresponding permissions to date. Researchers have spent significant efforts extracting such API protection mapping from the Android API framework, which leverages static code analysis to determine if specific permissions are required before accessing an API. Nevertheless, none of them has attempted to analyze the protection mapping in the native library (i.e., code written in C and C++), an essential component of the Android framework that handles communication with the lower-level hardware, such as cameras and sensors. While the protection mapping can be utilized to detect various security vulnerabilities in Android apps, such as permission over-privilege, imprecise mapping will lead to false results in detecting such security vulnerabilities. To fill this gap, we thereby propose to construct the protection mapping involved in the native libraries of the Android framework to present a complete and accurate specification of Android API protection. We develop a prototype system, named NatiDroid, to facilitate the cross-language static analysis and compare its performance with two state-of-the-practice tools, termed Axplorer and Arcade. We evaluate NatiDroid on more than 11,000 Android apps, including system apps from custom Android ROMs and third-party apps from the Google Play. Our NatiDroid can identify up to 464 new API-permission mappings, in contrast to the worst-case results derived from both Axplorer and Arcade, where approximately 71% apps have at least one false positive in permission over-privilege. We have disclosed all the potential vulnerabilities detected to the stakeholders.",Security,Healthy,"By minimizing false positives in permission over-privilege through thorough API mapping; the main contribution of ""Paper X"" reduces potential security issues improving the user's health in the software system; aligning with the ""Healthy"" value item under the ""Security"" value.","In the context of a software system, the main contribution of ""Paper X"" aligns with the value item ""Healthy"" and its corresponding value of ""Security"" from the perspective of a ""Software User"". This alignment is justified as the paper proposes a solution to minimize false positives in permission over-privilege through constructing a complete and accurate specification of Android API protection, specifically in the native libraries. By effectively identifying potential security vulnerabilities and reducing the risk of unauthorized access to sensitive APIs, the user's health in the software system is improved, as their personal information and sensitive data are better protected. This directly aligns with the value of ""Security"" and its corresponding value item of ""Healthy"" in Schwartz's Taxonomy.",Agreed-Justified,Disagree,reconcile with coder_2,Disagree,"The justification claims that the paper aligns with the value of 'Security'. aligning with the ""Healthy"" value item under the ""Security"" value is not supported by the evidence in the abstract, which focuses on 'The Android system manages access to sensitive APIs by permission enforcement.
",
Exp C,Exp I,418,ESEC/FSE,Code Generation & Analysis,Cross-language Android permission specification,"The Android system manages access to sensitive APIs by permission enforcement. An application (app) must declare proper permissions before invoking specific Android APIs. However, there is no official documentation providing the complete list of permission-protected APIs and the corresponding permissions to date. Researchers have spent significant efforts extracting such API protection mapping from the Android API framework, which leverages static code analysis to determine if specific permissions are required before accessing an API. Nevertheless, none of them has attempted to analyze the protection mapping in the native library (i.e., code written in C and C++), an essential component of the Android framework that handles communication with the lower-level hardware, such as cameras and sensors. While the protection mapping can be utilized to detect various security vulnerabilities in Android apps, such as permission over-privilege, imprecise mapping will lead to false results in detecting such security vulnerabilities. To fill this gap, we thereby propose to construct the protection mapping involved in the native libraries of the Android framework to present a complete and accurate specification of Android API protection. We develop a prototype system, named NatiDroid, to facilitate the cross-language static analysis and compare its performance with two state-of-the-practice tools, termed Axplorer and Arcade. We evaluate NatiDroid on more than 11,000 Android apps, including system apps from custom Android ROMs and third-party apps from the Google Play. Our NatiDroid can identify up to 464 new API-permission mappings, in contrast to the worst-case results derived from both Axplorer and Arcade, where approximately 71% apps have at least one false positive in permission over-privilege. We have disclosed all the potential vulnerabilities detected to the stakeholders.",Stimulation,Daring,"By proposing a new way to construct API protection mapping with NatiDroid; ""Paper X"" demonstrates a daring approach in enhancing the Android API framework; which corresponds to the value item ""Daring"" under the value ""Stimulation"" since users are engaged with a less risky and more secure software ecosystem.","In aligning 'Paper X' with the value item Daring and its corresponding value Stimulation, the justification lies in the novel approach proposed by the paper in constructing API protection mapping with NatiDroid. This approach goes beyond previous efforts and extends the analysis to native libraries, which are crucial components handling hardware communication. By embracing this daring approach, the paper explores new possibilities and pushes the boundaries of current practice, ultimately aiming to enhance the Android API framework. This aligns with the value of Stimulation as it introduces exciting advancements and creates a more engaging and secure software ecosystem for users.",Agreed-Justified,Agree,by protercting API users become more confident,Agree,,
Exp D,Exp I,419,ESEC/FSE,Security & Privacy,Tracking patches for open source software vulnerabilities,"Open source software (OSS) vulnerabilities threaten the security of software systems that use OSS. Vulnerability databases provide valuable information (e.g., vulnerable version and patch) to mitigate OSS vulnerabilities. There arises a growing concern about the information quality of vulnerability databases. However, it is unclear what the quality of patches in existing vulnerability databases is; and existing manual or heuristic-based approaches for patch tracking are either too expensive or too specific to apply to all OSS vulnerabilities. To address these problems, we first conduct an empirical study to understand the quality and characteristics of patches for OSS vulnerabilities in two industrial vulnerability databases. Inspired by our study, we then propose the first automated approach, Tracer, to track patches for OSS vulnerabilities from multiple knowledge sources. Our evaluation has demonstrated that i) Tracer can track patches for up to 273.8% more vulnerabilities than heuristic-based approaches while achieving a higher F1-score by up to 116.8%; and ii) Tracer can complement industrial vulnerability databases. Our evaluation has also indicated the generality and practical usefulness of Tracer.",Security,Healthy,The main contribution of 'Paper X' is an automated approach; Tracer; to track and patch OSS vulnerabilities; aiming to improve the health of software systems in terms of their security.,"In 'Paper X', the main contribution of the automated approach, Tracer, directly aligns with the value item Healthy and its corresponding value Security from a ""Software User"" perspective. By tracking and patching OSS vulnerabilities, Tracer aims to improve the security of software systems, which directly contributes to the health and well-being of the software users. The focus on mitigating vulnerabilities and ensuring the security of the software aligns with the value of security, which is an essential aspect for software users to feel secure and protected while using software systems. Therefore, the alignment with the value item Healthy and its corresponding value Security is evident in the direct contribution of 'Paper X'.",Agreed-Justified,Agree,,Agree,,
Exp E,Exp J,419,ESEC/FSE,Security & Privacy,Tracking patches for open source software vulnerabilities,"Open source software (OSS) vulnerabilities threaten the security of software systems that use OSS. Vulnerability databases provide valuable information (e.g., vulnerable version and patch) to mitigate OSS vulnerabilities. There arises a growing concern about the information quality of vulnerability databases. However, it is unclear what the quality of patches in existing vulnerability databases is; and existing manual or heuristic-based approaches for patch tracking are either too expensive or too specific to apply to all OSS vulnerabilities. To address these problems, we first conduct an empirical study to understand the quality and characteristics of patches for OSS vulnerabilities in two industrial vulnerability databases. Inspired by our study, we then propose the first automated approach, Tracer, to track patches for OSS vulnerabilities from multiple knowledge sources. Our evaluation has demonstrated that i) Tracer can track patches for up to 273.8% more vulnerabilities than heuristic-based approaches while achieving a higher F1-score by up to 116.8%; and ii) Tracer can complement industrial vulnerability databases. Our evaluation has also indicated the generality and practical usefulness of Tracer.",Security,Social Order,The paper proposes a solution to mitigate vulnerabilities in software systems; contributing towards maintaining social order in the context of software usage,"The justification for aligning 'Paper X' with the value item Social Order and its corresponding value Security from a ""Software User"" perspective is based on the fact that the paper aims to address the concern of OSS vulnerabilities in software systems. By proposing an automated approach to track patches for vulnerabilities, the paper contributes to enhancing the security of software systems. This improvement in security helps maintain social order within the context of software usage, as users can have greater confidence in the reliability and stability of the software they rely on for various tasks.",Agreed-Clarified,Agree,,Agree,,
Exp F,Exp J,419,ESEC/FSE,Security & Privacy,Tracking patches for open source software vulnerabilities,"Open source software (OSS) vulnerabilities threaten the security of software systems that use OSS. Vulnerability databases provide valuable information (e.g., vulnerable version and patch) to mitigate OSS vulnerabilities. There arises a growing concern about the information quality of vulnerability databases. However, it is unclear what the quality of patches in existing vulnerability databases is; and existing manual or heuristic-based approaches for patch tracking are either too expensive or too specific to apply to all OSS vulnerabilities. To address these problems, we first conduct an empirical study to understand the quality and characteristics of patches for OSS vulnerabilities in two industrial vulnerability databases. Inspired by our study, we then propose the first automated approach, Tracer, to track patches for OSS vulnerabilities from multiple knowledge sources. Our evaluation has demonstrated that i) Tracer can track patches for up to 273.8% more vulnerabilities than heuristic-based approaches while achieving a higher F1-score by up to 116.8%; and ii) Tracer can complement industrial vulnerability databases. Our evaluation has also indicated the generality and practical usefulness of Tracer.",Security,National Security,The paper addresses a growing concern about the information quality of vulnerability databases; potentially contributing to national security by enhancing the security of software systems used in nations.,"In the abstract, the paper explicitly states that there is a growing concern about the information quality of vulnerability databases. By addressing this concern and proposing an automated approach to track patches for OSS vulnerabilities, the paper aims to enhance the security of software systems that use OSS. Given the importance of software security in various sectors, including critical infrastructure and national defense, the paper's contributions align with the value of National Security under the broader value item of Security from a ""Software User"" perspective.",Agreed-Clarified,Agree,,Agree,,
Exp A,Exp H,423,ESEC/FSE,Software Engineering Practices,Psychologically-inspired unsupervised inference of perceptual groups of GUI widgets from GUI images,"Graphical User Interface (GUI) is not merely a collection of individual and unrelated widgets, but rather partitions discrete widgets into groups by various visual cues, thus forming higher-order perceptual units such as tab, menu, card or list. The ability to automatically segment a GUI into perceptual groups of widgets constitutes a fundamental component of visual intelligence to automate GUI design, implementation and automation tasks. Although humans can partition a GUI into meaningful perceptual groups of widgets in a highly reliable way, perceptual grouping is still an open challenge for computational approaches. Existing methods rely on ad-hoc heuristics or supervised machine learning that is dependent on specific GUI implementations and runtime information. Research in psychology and biological vision has formulated a set of principles (i.e., Gestalt theory of perception) that describe how humans group elements in visual scenes based on visual cues like connectivity, similarity, proximity and continuity. These principles are domain-independent and have been widely adopted by practitioners to structure content on GUIs to improve aesthetic pleasantness and usability. Inspired by these principles, we present a novel unsupervised image-based method for inferring perceptual groups of GUI widgets. Our method requires only GUI pixel images, is independent of GUI implementation, and does not require any training data. The evaluation on a dataset of 1,091 GUIs collected from 772 mobile apps and 20 UI design mockups shows that our method significantly outperforms the state-of-the-art ad-hoc heuristics-based baseline. Our perceptual grouping method creates opportunities for improving UI-related software engineering tasks.",Security,Healthy,The proposed method aims to improve the user experience by making it easier and more intuitive to interact with GUI's. This can indirectly contribute to Health (mental welfare in using the software) as it reduces the strain of using complex interfaces.,"The justification for aligning 'Paper X' with the value item Healthy and its corresponding value Security from a ""Software User"" perspective is that the proposed method of inferring perceptual groups of GUI widgets aims to improve the user experience by making it easier and more intuitive to interact with GUI's. This improved usability can indirectly contribute to the user's mental welfare in using the software, as it reduces the cognitive strain and frustration associated with navigating complex interfaces. Therefore, the alignment with the value item Healthy and its corresponding value Security is justified based on the positive impact on the user's well-being and overall satisfaction with the software.",Agreed-Clarified,Disagree,reconcile with coder_2,Disagree,"The justification claims that the paper aligns with the value of 'Security'.  This can indirectly contribute to Health (mental welfare in using the software) as it reduces the strain of using complex interfaces.
",
Exp B,Exp H,423,ESEC/FSE,Software Engineering Practices,Psychologically-inspired unsupervised inference of perceptual groups of GUI widgets from GUI images,"Graphical User Interface (GUI) is not merely a collection of individual and unrelated widgets, but rather partitions discrete widgets into groups by various visual cues, thus forming higher-order perceptual units such as tab, menu, card or list. The ability to automatically segment a GUI into perceptual groups of widgets constitutes a fundamental component of visual intelligence to automate GUI design, implementation and automation tasks. Although humans can partition a GUI into meaningful perceptual groups of widgets in a highly reliable way, perceptual grouping is still an open challenge for computational approaches. Existing methods rely on ad-hoc heuristics or supervised machine learning that is dependent on specific GUI implementations and runtime information. Research in psychology and biological vision has formulated a set of principles (i.e., Gestalt theory of perception) that describe how humans group elements in visual scenes based on visual cues like connectivity, similarity, proximity and continuity. These principles are domain-independent and have been widely adopted by practitioners to structure content on GUIs to improve aesthetic pleasantness and usability. Inspired by these principles, we present a novel unsupervised image-based method for inferring perceptual groups of GUI widgets. Our method requires only GUI pixel images, is independent of GUI implementation, and does not require any training data. The evaluation on a dataset of 1,091 GUIs collected from 772 mobile apps and 20 UI design mockups shows that our method significantly outperforms the state-of-the-art ad-hoc heuristics-based baseline. Our perceptual grouping method creates opportunities for improving UI-related software engineering tasks.",Tradition,Respect for Tradition,The paper's method is based on the principles of the Gestalt theory of perception; a tradition in psychology; and applies these principles into their method. This aligns with the value item 'Respect for Tradition.',"My justification for aligning 'Paper X' with the value item Respect for Tradition and its corresponding value Tradition is based on the fact that the paper draws inspiration from the principles of the Gestalt theory of perception, which are considered a tradition in the field of psychology. By incorporating these traditional principles into their method, the paper demonstrates respect for the knowledge and insights gained through years of research in psychology. This alignment with Respect for Tradition indicates a recognition of the value and importance of established practices and theories, which can contribute to the development of reliable and effective approaches in the field of GUI design.",Agreed-Clarified,Agree,,Agree,,
Exp C,Exp I,424,ESEC/FSE,Code Generation & Analysis,RecipeGen++: an automated trigger action programs generator,"Trigger Action Programs (TAPs) are event-driven rules that allow users to automate smart-devices and internet services. Users can write TAPs by specifying triggers and actions from a set of predefined channels and functions. Despite its simplicity, composing TAPs can still be challenging for users due to the enormous search space of available triggers and actions. The growing popularity of TAPs is followed by the increasing number of supported devices and services, resulting in a huge number of possible combinations between triggers and actions. Motivated by such a fact, we improve our prior work and propose RecipeGen++, a deep-learning-based approach that leverages Transformer seq2seq (sequence-to-sequence) architecture to generate TAPs given natural language descriptions. RecipeGen++ can generate TAPs in the Interactive, One-Click, or Functionality Discovery modes. In the Interactive mode, users can provide feedback to guide the prediction of a trigger or action component. In contrast, the One-Click mode allows users to generate all TAP components directly. Additionally, RecipeGen++ also enables users to discover functionalities at the channel level through the Functionality Discovery mode. We have evaluated RecipeGen++ on real-world datasets in all modes. Our results demonstrate that RecipeGen++ can outperform the baseline by 2.2%-16.2% in the gold-standard benchmark and 5%-29.2% in the noisy benchmark.",Self Direction,Freedom,The 'Paper X' introduces a system (RecipeGen++) that allows software users to automate tasks on their smart-devices and Internet services. This aligns with the value item Freedom (v1.2) and its corresponding value Self Direction (v1) because users gain the freedom to customize how their digital tools behave according to their needs and preferences.,"The justification for aligning 'Paper X' with the value item Freedom and its corresponding value Self Direction is that the paper introduces a system (RecipeGen++) that enables software users to have control and autonomy over their smart-devices and Internet services. By allowing users to automate tasks and customize the behavior of their digital tools according to their own needs and preferences, 'Paper X' empowers users with the freedom to make independent choices and have a sense of self-direction in their software usage.",Agreed-Justified,Agree,,Agree,,
Exp D,Exp I,424,ESEC/FSE,Code Generation & Analysis,RecipeGen++: an automated trigger action programs generator,"Trigger Action Programs (TAPs) are event-driven rules that allow users to automate smart-devices and internet services. Users can write TAPs by specifying triggers and actions from a set of predefined channels and functions. Despite its simplicity, composing TAPs can still be challenging for users due to the enormous search space of available triggers and actions. The growing popularity of TAPs is followed by the increasing number of supported devices and services, resulting in a huge number of possible combinations between triggers and actions. Motivated by such a fact, we improve our prior work and propose RecipeGen++, a deep-learning-based approach that leverages Transformer seq2seq (sequence-to-sequence) architecture to generate TAPs given natural language descriptions. RecipeGen++ can generate TAPs in the Interactive, One-Click, or Functionality Discovery modes. In the Interactive mode, users can provide feedback to guide the prediction of a trigger or action component. In contrast, the One-Click mode allows users to generate all TAP components directly. Additionally, RecipeGen++ also enables users to discover functionalities at the channel level through the Functionality Discovery mode. We have evaluated RecipeGen++ on real-world datasets in all modes. Our results demonstrate that RecipeGen++ can outperform the baseline by 2.2%-16.2% in the gold-standard benchmark and 5%-29.2% in the noisy benchmark.",Stimulation,Excitement in Life,With the various modes (Interactive; One-Click; and Functionality Discovery) of RecipeGen++; users experience an exciting way of automating their smart devices and internet services. This aligns with the value item Excitement in Life (v2.3) and its corresponding value Stimulation (v2) because the software adds an element of novelty into their digital experiences.,"The justification is based on the fact that RecipeGen++ provides users with different modes of generating TAPs, which adds novelty and excitement to their digital experiences. This aligns with the value item Excitement in Life (v2.3) and its corresponding value Stimulation (v2) because it allows users to explore and interact with the software in a dynamic and stimulating manner, enhancing their overall experience and providing them with a sense of excitement and novelty.",Agreed-Justified,Agree,,Disagree,More aligned with Helpful rather than stimulation and excitement,
Exp E,Exp J,424,ESEC/FSE,Code Generation & Analysis,RecipeGen++: an automated trigger action programs generator,"Trigger Action Programs (TAPs) are event-driven rules that allow users to automate smart-devices and internet services. Users can write TAPs by specifying triggers and actions from a set of predefined channels and functions. Despite its simplicity, composing TAPs can still be challenging for users due to the enormous search space of available triggers and actions. The growing popularity of TAPs is followed by the increasing number of supported devices and services, resulting in a huge number of possible combinations between triggers and actions. Motivated by such a fact, we improve our prior work and propose RecipeGen++, a deep-learning-based approach that leverages Transformer seq2seq (sequence-to-sequence) architecture to generate TAPs given natural language descriptions. RecipeGen++ can generate TAPs in the Interactive, One-Click, or Functionality Discovery modes. In the Interactive mode, users can provide feedback to guide the prediction of a trigger or action component. In contrast, the One-Click mode allows users to generate all TAP components directly. Additionally, RecipeGen++ also enables users to discover functionalities at the channel level through the Functionality Discovery mode. We have evaluated RecipeGen++ on real-world datasets in all modes. Our results demonstrate that RecipeGen++ can outperform the baseline by 2.2%-16.2% in the gold-standard benchmark and 5%-29.2% in the noisy benchmark.",Achievement,Successful,The introduction of RecipeGen++ gives users a tool that helps them achieve the goal of efficient automation of their smart devices and internet services. This aligns with the value item Successful (v4.3) and its corresponding value Achievement (v4). The success here refers to the successful implementation and execution of custom automations on the digital platforms and devices they use.,"The main contribution of 'Paper X', RecipeGen++, is directly aligned with the value item Successful (v4.3) and its corresponding value Achievement (v4) from a ""Software User"" perspective. The paper introduces a deep-learning-based approach that allows users to generate Trigger Action Programs (TAPs) for automating smart devices and internet services. By providing users with a tool that simplifies the process of composing TAPs and enables them to efficiently automate their digital platforms, RecipeGen++ helps users achieve the goal of successful implementation and execution of custom automations. This aligns with the value of Achievement, as users can successfully accomplish their automation goals and feel a sense of accomplishment and fulfillment by being able to effectively utilize their digital devices.",Agreed-Clarified,Agree,,Agree,,
Exp F,Exp J,425,ESEC/FSE,Software Testing & QA,MOSAT: finding safety violations of autonomous driving systems using multi-objective genetic algorithm,"Autonomous Driving Systems (ADSs) are safety-critical systems, and safety violations of Autonomous Vehicles (AVs) in real traffic will cause huge losses. Therefore, ADSs must be fully tested before deployed on real world roads. Simulation testing is essential to find safety violations of ADS. This paper proposes MOSAT, a multi-objective search-based testing framework, which constructs diverse and adversarial driving environment to expose safety violations of ADSs. Specifically, based on atomic driving maneuvers, MOSAT introduces motif pattern, which describes a sequence of maneuvers that can challenge ADS effectively. MOSAT constructs test scenarios by atomic maneuvers and motif patterns, and uses multi-objective genetic algorithm to search for adversarial and diverse test scenarios. Moreover, in order to test the performance of ADS comprehensively during long-mile driving, we design a novel continuous simulation testing technique, which runs the scenarios generated by multiple parallel search processes alternately in the simulator and can continuously create different perturbations to ADS. We demonstrate MOSAT on an industrial-grade platform, Baidu Apollo, and the experimental results show that MOSAT can effectively generate safety-critical scenarios to crash ADSs and it exposes 11 distinct types of safety violations in a short period of time. It also outperforms state-of-the-art techniques by finding more 6 distinct safety violations on the same road.",Security,Healthy,The paper introduces a testing framework (MOSAT) aimed to expose safety violations in Autonomous Driving Systems; which implicitly prioritizes the safety and health of end users. This aligns with the value item 'Healthy' and its corresponding value 'Security'.,"The justification for aligning 'Paper X' with the value item Healthy and its corresponding value Security from a ""Software User"" perspective is based on the fact that the paper proposes a testing framework (MOSAT) specifically designed to expose safety violations in Autonomous Driving Systems. By prioritizing safety, the paper indirectly aims to protect the health and well-being of the end users who will be interacting with these systems. Therefore, the alignment with the value item Healthy and its corresponding value Security is evident in the context of ensuring the safe and secure usage of autonomous vehicles.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,425,ESEC/FSE,Software Testing & QA,MOSAT: finding safety violations of autonomous driving systems using multi-objective genetic algorithm,"Autonomous Driving Systems (ADSs) are safety-critical systems, and safety violations of Autonomous Vehicles (AVs) in real traffic will cause huge losses. Therefore, ADSs must be fully tested before deployed on real world roads. Simulation testing is essential to find safety violations of ADS. This paper proposes MOSAT, a multi-objective search-based testing framework, which constructs diverse and adversarial driving environment to expose safety violations of ADSs. Specifically, based on atomic driving maneuvers, MOSAT introduces motif pattern, which describes a sequence of maneuvers that can challenge ADS effectively. MOSAT constructs test scenarios by atomic maneuvers and motif patterns, and uses multi-objective genetic algorithm to search for adversarial and diverse test scenarios. Moreover, in order to test the performance of ADS comprehensively during long-mile driving, we design a novel continuous simulation testing technique, which runs the scenarios generated by multiple parallel search processes alternately in the simulator and can continuously create different perturbations to ADS. We demonstrate MOSAT on an industrial-grade platform, Baidu Apollo, and the experimental results show that MOSAT can effectively generate safety-critical scenarios to crash ADSs and it exposes 11 distinct types of safety violations in a short period of time. It also outperforms state-of-the-art techniques by finding more 6 distinct safety violations on the same road.",Security,National Security,The proposed MOSAT tool ensures that Autonomous Vehicles are properly tested before deployment; ensuring 'National Security' by preventing accidents that may occur due to safety violations. This aligns with the value item 'National Security' and its corresponding value 'Security'.,"My justification is that the main contribution of 'Paper X' is the MOSAT tool, which ensures that Autonomous Vehicles are thoroughly tested before deployment. By identifying safety violations through diverse and adversarial test scenarios, MOSAT enhances the security of autonomous driving systems, ultimately contributing to national security. This directly aligns with the value item 'National Security' and its corresponding value 'Security', as it addresses potential safety risks that could lead to accidents and harm to individuals and society.",Agreed-Justified,Agree,,Agree,,
Exp C,Exp I,428,ESEC/FSE,Code Generation & Analysis,MANDO-GURU: vulnerability detection for smart contract source code by heterogeneous graph embeddings,"Smart contracts are increasingly used with blockchain systems for high-value applications. It is highly desired to ensure the quality of smart contract source code before they are deployed. This paper proposes a new deep learning-based tool, MANDO-GURU, that aims to accurately detect vulnerabilities in smart contracts at both coarse-grained contract-level and fine-grained line-level. Using a combination of control-flow graphs and call graphs of Solidity code, we design new heterogeneous graph attention neural networks to encode more structural and potentially semantic relations among different types of nodes and edges of such graphs and use the encoded embeddings of the graphs and nodes to detect vulnerabilities. Our validation of real-world smart contract datasets shows that MANDO-GURU can significantly improve many other vulnerability detection techniques by up to 24% in terms of the F1-score at the contract level, depending on vulnerability types. It is the first learning-based tool for Ethereum smart contracts that identify vulnerabilities at the line level and significantly improves the traditional code analysis-based techniques by up to 63.4%. Our tool is publicly available at https://github.com/MANDO-Project/ge-sc-machine. A test version is currently deployed at http://mandoguru.com, and a demo video of our tool is available at http://mandoguru.com/demo-video.",Security,Healthy,By aiming to prevent potential breaches; MANDO-GURU can improve the safety of using smart contracts; aligning with the value item Healthy which is part of the Security value.,"In the context of a ""Software User,"" the alignment between the main contributions of 'Paper X' and the value item ""Healthy"" within the value of ""Security"" is evident. The paper proposes MANDO-GURU, a deep learning-based tool, which aims to accurately detect vulnerabilities in smart contracts. By detecting vulnerabilities, MANDO-GURU can help prevent potential breaches and improve the overall safety of using smart contracts. This directly aligns with the value item ""Healthy,"" which emphasizes the importance of ensuring security and minimizing risks in the software context.",Agreed-Justified,Agree,,Agree,,
Exp E,Exp J,435,ESEC/FSE,Security & Privacy,Automated unearthing of dangerous issue reports,"The coordinated vulnerability disclosure (CVD) process is commonly adopted for open source software (OSS) vulnerability management, which suggests to privately report the discovered vulnerabilities and keep relevant information secret until the official disclosure. However, in practice, due to various reasons (e.g., lacking security domain expertise or the sense of security management), many vulnerabilities are first reported via public issue reports (IRs) before its official disclosure. Such IRs are dangerous IRs, since attackers can take advantages of the leaked vulnerability information to launch zero-day attacks. It is crucial to identify such dangerous IRs at an early stage, such that OSS users can start the vulnerability remediation process earlier and OSS maintainers can timely manage the dangerous IRs. In this paper, we propose and evaluate a deep learning based approach, namely MemVul, to automatically identify dangerous IRs at the time they are reported. MemVul augments the neural networks with a memory component, which stores the external vulnerability knowledge from Common Weakness Enumeration (CWE). We rely on publicly accessible CVE-referred IRs (CIRs) to operationalize the concept of dangerous IR. We mine 3,937 CIRs distributed across 1,390 OSS projects hosted on GitHub. Evaluated under a practical scenario of high data imbalance, MemVul achieves the best trade-off between precision and recall among all baselines. In particular, the F1-score of MemVul (i.e., 0.49) improves the best performing baseline by 44%. For IRs that are predicted as CIRs but not reported to CVE, we conduct a user study to investigate their usefulness to OSS stakeholders. We observe that 82% (41 out of 50) of these IRs are security-related and 28 of them are suggested by security experts to be publicly disclosed, indicating MemVul is capable of identifying undisclosed dangerous IRs.",Power,Social Recognition,The paper contributes to an automated method to identify dangerous IRs; offering software users more security and ensuring accuracy and timeliness; which can lead to social recognition for the user in terms of identification of vulnerabilities.,"In the context of 'Paper X', the main contributions of the paper align with the value item Social Recognition and its corresponding value Power from a ""Software User"" perspective because the proposed automated method of identifying dangerous IRs enhances the user's ability to identify vulnerabilities and take timely security measures. By actively participating in vulnerability remediation, the user demonstrates their knowledge and expertise in software security, leading to potential recognition from their peers and the larger software user community. This recognition aligns with the value of Power, as the user gains influence and authority in the software context through their proactive engagement in security management.",Agreed-Clarified,Agree,,Agree,,
Exp F,Exp J,444,ESEC/FSE,AI & Machine Learning,Testing of machine learning models with limited samples: an industrial vacuum pumping application,"There is often a scarcity of training data for machine learning (ML) classification and regression models in industrial production, especially for time-consuming or sparsely run manufacturing processes. Traditionally, a majority of the limited ground-truth data is used for training, while a handful of samples are left for testing. In that case, the number of test samples is inadequate to properly evaluate the robustness of the ML models under test (i.e., the system under test) for classification and regression. Furthermore, the output of these ML models may be inaccurate or even fail if the input data differ from the expected. This is the case for ML models used in the Electroslag Remelting (ESR) process in the refined steel industry to predict the pressure in a vacuum chamber. A vacuum pumping event that occurs once a workday generates a few hundred samples in a year of pumping for training and testing. In the absence of adequate training and test samples, this paper first presents a method to generate a fresh set of augmented samples based on vacuum pumping principles. Based on the generated augmented samples, three test scenarios and one test oracle are presented to assess the robustness of an ML model used for production on an industrial scale. Experiments are conducted with real industrial production data obtained from Uddeholms AB steel company. The evaluations indicate that Ensemble and Neural Network are the most robust when trained on augmented data using the proposed testing strategy. The evaluation also demonstrates the proposed method's effectiveness in checking and improving ML algorithms' robustness in such situations. The work improves software testing's state-of-the-art robustness testing in similar settings. Finally, the paper presents an MLOps implementation of the proposed approach for real-time ML model prediction and action on the edge node and automated continuous delivery of ML software from the cloud.",Achievement,Intelligent,Paper X presents a method for improving the robustness of ML models; which can be viewed as increasing the software user's intelligence by providing them with more accurate predictions; thus aligning with the value item 'Intelligent' and its corresponding value 'Achievement'.,"In 'Paper X', the main contribution is the development of a method to enhance the robustness of ML models used in an industrial production setting. This improved robustness can be seen as increasing the software user's intelligence by providing them with more accurate predictions and insights. By aligning with the value item 'Intelligent' and its corresponding value 'Achievement', 'Paper X' directly addresses the user's need for reliable and intelligent software solutions that can enhance their productivity and success in their job role.",Agreed-Clarified,Agree,,Agree,,
Exp G,Exp K,444,ESEC/FSE,AI & Machine Learning,Testing of machine learning models with limited samples: an industrial vacuum pumping application,"There is often a scarcity of training data for machine learning (ML) classification and regression models in industrial production, especially for time-consuming or sparsely run manufacturing processes. Traditionally, a majority of the limited ground-truth data is used for training, while a handful of samples are left for testing. In that case, the number of test samples is inadequate to properly evaluate the robustness of the ML models under test (i.e., the system under test) for classification and regression. Furthermore, the output of these ML models may be inaccurate or even fail if the input data differ from the expected. This is the case for ML models used in the Electroslag Remelting (ESR) process in the refined steel industry to predict the pressure in a vacuum chamber. A vacuum pumping event that occurs once a workday generates a few hundred samples in a year of pumping for training and testing. In the absence of adequate training and test samples, this paper first presents a method to generate a fresh set of augmented samples based on vacuum pumping principles. Based on the generated augmented samples, three test scenarios and one test oracle are presented to assess the robustness of an ML model used for production on an industrial scale. Experiments are conducted with real industrial production data obtained from Uddeholms AB steel company. The evaluations indicate that Ensemble and Neural Network are the most robust when trained on augmented data using the proposed testing strategy. The evaluation also demonstrates the proposed method's effectiveness in checking and improving ML algorithms' robustness in such situations. The work improves software testing's state-of-the-art robustness testing in similar settings. Finally, the paper presents an MLOps implementation of the proposed approach for real-time ML model prediction and action on the edge node and automated continuous delivery of ML software from the cloud.",Security,Social Order,Paper X implements the proposed approach in a real-time ML model prediction and continuous delivery setup; which will ensure system responses are optimized and secure; aligning it directly with the value item 'Social Order' and its corresponding value 'Security'.,"In 'Paper X', the proposed approach of implementing real-time ML model prediction and continuous delivery contributes to the value item 'Social Order' by ensuring that the system responses are optimized and secure. By continuously delivering ML software from the cloud, there is a systematic and organized process in place to maintain the stability and orderliness of the software system, ultimately promoting social order. Furthermore, through the emphasis on security in the delivery and prediction of ML models, the proposed approach aligns with the corresponding value of 'Security'. This alignment is crucial in ensuring that the software system is reliable and protected against potential risks or threats, thereby contributing to the overall sense of security for software users.",Agreed-Clarified,Agree,,Agree,,
Exp A,Exp H,444,ESEC/FSE,AI & Machine Learning,Testing of machine learning models with limited samples: an industrial vacuum pumping application,"There is often a scarcity of training data for machine learning (ML) classification and regression models in industrial production, especially for time-consuming or sparsely run manufacturing processes. Traditionally, a majority of the limited ground-truth data is used for training, while a handful of samples are left for testing. In that case, the number of test samples is inadequate to properly evaluate the robustness of the ML models under test (i.e., the system under test) for classification and regression. Furthermore, the output of these ML models may be inaccurate or even fail if the input data differ from the expected. This is the case for ML models used in the Electroslag Remelting (ESR) process in the refined steel industry to predict the pressure in a vacuum chamber. A vacuum pumping event that occurs once a workday generates a few hundred samples in a year of pumping for training and testing. In the absence of adequate training and test samples, this paper first presents a method to generate a fresh set of augmented samples based on vacuum pumping principles. Based on the generated augmented samples, three test scenarios and one test oracle are presented to assess the robustness of an ML model used for production on an industrial scale. Experiments are conducted with real industrial production data obtained from Uddeholms AB steel company. The evaluations indicate that Ensemble and Neural Network are the most robust when trained on augmented data using the proposed testing strategy. The evaluation also demonstrates the proposed method's effectiveness in checking and improving ML algorithms' robustness in such situations. The work improves software testing's state-of-the-art robustness testing in similar settings. Finally, the paper presents an MLOps implementation of the proposed approach for real-time ML model prediction and action on the edge node and automated continuous delivery of ML software from the cloud.",Stimulation,Variation in Life,The paper introduces a novel testing strategy for ML models; which brings a new approach and a variation to the existing testing methods; aligning with the value item 'Variation in Life' and its corresponding value 'Stimulation'.,"In the context of the abstract, the paper's main contribution lies in introducing a new testing strategy for ML models used in industrial production. This strategy brings variation and novelty to the traditional methods, as it focuses on generating augmented samples based on vacuum pumping principles. The alignment with the value item ""Variation in Life"" and its corresponding value ""Stimulation"" is evident because the paper offers an innovative approach that introduces diversity and excitement to the testing process in software development, particularly from the perspective of a software user.",Agreed-Clarified,Agree,,Agree,,
Exp D,Exp I,451,ESEC/FSE,Code Generation & Analysis,Detecting Simulink compiler bugs via controllable zombie blocks mutation,"As a popular Cyber-Physical System (CPS) development tool chain, MathWorks Simulink is widely used to prototype CPS models in safety-critical applications, e.g., aerospace and healthcare. It is crucial to ensure the correctness and reliability of Simulink compiler (i.e., the compiler module of Simulink) in practice since all CPS models depend on compilation. However, Simulink compiler testing is challenging due to millions of lines of source code and the lack of the complete formal language specification. Although several methods have been proposed to automatically test Simulink compiler, there still remains two challenges to be tackled, namely the limited variant space and the insufficient mutation diversity. To address these challenges, we propose COMBAT, a new differential testing method for Simulink compiler testing. COMBAT includes an EMI (Equivalence Modulo Input) mutation component and a diverse variant generation component. The EMI mutation component inserts assertion statements (e.g., If /While blocks) at arbitrary points of the seed CPS model. These statements break each insertion point into true and false branches. Then, COMBAT feeds all the data passed through the insertion point into the true branch to preserve the equivalence of CPS variants. In such a way, the body of the false branch could be viewed as a new variant space, thus addressing the first challenge. The diverse variant generation component uses Markov chain Monte Carlo optimization to sample the seed CPS model and generate complex mutations of long sequences of blocks in the variant space, thus addressing the second challenge. Experiments demonstrate that COMBAT significantly outperforms the state-of-the-art approaches in Simulink compiler testing. Within five months, COMBAT has reported 16 valid bugs for Simulink R2021b, of which 11 bugs have been confirmed as new bugs by MathWorks Support.",Security,Healthy,By enhancing the accuracy of the Simulink compiler. The software user is ensured of a reliable tool; enhancing their well-being by ensuring safety in critical fields like aerospace and healthcare; thus aligning with the value item healthy and its corresponding value security.,"The main contribution of 'Paper X' is the proposal of a new testing method, COMBAT, for the Simulink compiler. This method aims to address the challenges of limited variant space and insufficient mutation diversity in compiler testing. By enhancing the accuracy and reliability of the Simulink compiler, the software user can have confidence in the correctness of their CPS models, particularly in safety-critical applications like aerospace and healthcare. This alignment with the value item Healthy and its corresponding value Security is justified as it ensures the well-being and safety of the software user by providing a secure and reliable tool for their critical tasks.",Agreed-Clarified,Agree,,Agree,,
Exp E,Exp J,452,ESEC/FSE,Code Generation & Analysis,KVS: a tool for knowledge-driven vulnerability searching,"It is difficult to quickly locate and search for specific vulnerabilities and their solutions because vulnerability information is scattered in the existing vulnerability management library. To alleviate this problem, we extract knowledge from vulnerability reports and organize the vulnerability information into the form of a knowledge graph. Then, we implement a tool for knowledge-driven vulnerability searching, KVS. This tool mainly uses the BERT model to realize the vulnerability named entity recognition and construct the vulnerability knowledge graph (VulKG). Finally, we can search vulnerabilities of interest-based on VulKG. The URL of this tool is https://cinnqi.github.io/Neo4j-D3-VKG/. Video of our demo is available at https://youtu.be/FT1BaLUGPk0.",Security,Healthy,The paper presents a tool; KVS; for searching vulnerabilities which helps in maintaining and increasing the software health; therefore aligning with the value item Healthy and its corresponding value Security.,"My justification for aligning 'Paper X' with the value item Healthy and its corresponding value Security is that the paper explicitly states that the tool, KVS, helps in searching vulnerabilities to maintain and increase software health. This aligns with the value of Security because it addresses the need to protect the software from vulnerabilities and potential threats. By ensuring the software's health, users can have a sense of security in using it.",Agreed-Clarified,Agree,,Agree,,
Exp F,Exp J,452,ESEC/FSE,Code Generation & Analysis,KVS: a tool for knowledge-driven vulnerability searching,"It is difficult to quickly locate and search for specific vulnerabilities and their solutions because vulnerability information is scattered in the existing vulnerability management library. To alleviate this problem, we extract knowledge from vulnerability reports and organize the vulnerability information into the form of a knowledge graph. Then, we implement a tool for knowledge-driven vulnerability searching, KVS. This tool mainly uses the BERT model to realize the vulnerability named entity recognition and construct the vulnerability knowledge graph (VulKG). Finally, we can search vulnerabilities of interest-based on VulKG. The URL of this tool is https://cinnqi.github.io/Neo4j-D3-VKG/. Video of our demo is available at https://youtu.be/FT1BaLUGPk0.",Security,Social Order,The tool proposed by the paper provides information about software vulnerabilities; contributing to social order by making the digital space safer thus representing the value item Social Order and its corresponding value Security.,"The tool introduced in 'Paper X' directly contributes to social order by organizing and providing information about software vulnerabilities. By improving the management of vulnerabilities, the tool enhances security in the digital space, aligning with the value item of Social Order and its corresponding value of Security.",Agreed-Justified,Agree,,Agree,,
Exp G,Exp K,452,ESEC/FSE,Code Generation & Analysis,KVS: a tool for knowledge-driven vulnerability searching,"It is difficult to quickly locate and search for specific vulnerabilities and their solutions because vulnerability information is scattered in the existing vulnerability management library. To alleviate this problem, we extract knowledge from vulnerability reports and organize the vulnerability information into the form of a knowledge graph. Then, we implement a tool for knowledge-driven vulnerability searching, KVS. This tool mainly uses the BERT model to realize the vulnerability named entity recognition and construct the vulnerability knowledge graph (VulKG). Finally, we can search vulnerabilities of interest-based on VulKG. The URL of this tool is https://cinnqi.github.io/Neo4j-D3-VKG/. Video of our demo is available at https://youtu.be/FT1BaLUGPk0.",Universalism,Equality,By providing a tool that anyone can use to search for software vulnerabilities; the paper indirectly contributes to equality by allowing every software user to access the same level of security; thus aligning with the value item Equality and its corresponding value Universalism.,"I apologize for any confusion. The justification for aligning 'Paper X' with the value item Equality and its corresponding value Universalism is based on the fact that the paper's contribution of a knowledge-driven vulnerability searching tool allows all software users to have equal access to security information. This aligns with the value of Universalism as it promotes equal opportunity and fairness in the software context. By providing a tool that enables anyone to search for vulnerabilities, the paper indirectly contributes to equality by ensuring that all users can access the same level of security information, regardless of their background or technical expertise.",Agreed-Clarified,Agree,,Agree,,
Exp A,Exp H,454,ESEC/FSE,Security & Privacy,Demystifying the underground ecosystem of account registration bots,"Member services are a core part of most online systems. For example, member services in online social networks and video platforms make it possible to serve users customized content or track their footprint for a recommendation. However, there is a dark side to membership that lurks behind influencer marketing, coupon harvesting, and spreading fake news. All these activities rely heavily on owning masses of fake accounts, and to create new accounts efficiently, malicious registrants use automated registration bots with anti-human verification services that can easily bypass a website‚Äö√Ñ√∂‚àö√ë‚àö‚àÇ‚Äö√†√∂‚àö√´‚Äö√†√∂‚Äö√†√á‚Äö√Ñ√∂‚àö‚Ä†‚àö‚àÇ‚Äö√†√∂‚àö¬¥‚Äö√Ñ√∂‚àö‚Ä†‚àö‚àÇ¬¨¬®‚Äö√Ñ¬¢s security strategies. In this paper, we take the first step toward understanding the underground ecosystem of account registration bots, and in particular, the anti-human verification services they use. From a comprehensive analysis, we determined the three most popular types of anti-human verification services. We then conducted experiments on these services from an attacker‚Äö√Ñ√∂‚àö√ë‚àö‚àÇ‚Äö√†√∂‚àö√´‚Äö√†√∂‚Äö√†√á‚Äö√Ñ√∂‚àö‚Ä†‚àö‚àÇ‚Äö√†√∂‚àö¬¥‚Äö√Ñ√∂‚àö‚Ä†‚àö‚àÇ¬¨¬®‚Äö√Ñ¬¢s perspective to verify their effectiveness. The results show that all can easily bypass the security strategies website providers put in place to prevent fake registrations, such as SMS verification, CAPTCHA and IP monitoring. We further estimated the market size of the underground registration ecosystem, placing it at about US $4.8M-128.1 million per year. Our study demonstrates the urgency with which we to think about the effectiveness of our registration security strategies and should prompt us to develop new strategies for better protection.",Security,Social Order,The paper addresses the need for better registration security strategies; which relates to securing social order in online platforms; aligning with the value of Security.,"In the abstract of 'Paper X', the authors specifically mention the need to develop new strategies for better protection against activities such as influencer marketing, coupon harvesting, and spreading fake news. This directly aligns with the value of Security, as it addresses the importance of implementing stronger security measures to maintain the social order and integrity of online platforms. By focusing on enhancing registration security strategies, the paper aims to ensure a secure environment for software users, thereby promoting social order within the software context.",Agreed-Justified,Agree,,Agree,,
Exp B,Exp H,454,ESEC/FSE,Security & Privacy,Demystifying the underground ecosystem of account registration bots,"Member services are a core part of most online systems. For example, member services in online social networks and video platforms make it possible to serve users customized content or track their footprint for a recommendation. However, there is a dark side to membership that lurks behind influencer marketing, coupon harvesting, and spreading fake news. All these activities rely heavily on owning masses of fake accounts, and to create new accounts efficiently, malicious registrants use automated registration bots with anti-human verification services that can easily bypass a website‚Äö√Ñ√∂‚àö√ë‚àö‚àÇ‚Äö√†√∂‚àö√´‚Äö√†√∂‚Äö√†√á‚Äö√Ñ√∂‚àö‚Ä†‚àö‚àÇ‚Äö√†√∂‚àö¬¥‚Äö√Ñ√∂‚àö‚Ä†‚àö‚àÇ¬¨¬®‚Äö√Ñ¬¢s security strategies. In this paper, we take the first step toward understanding the underground ecosystem of account registration bots, and in particular, the anti-human verification services they use. From a comprehensive analysis, we determined the three most popular types of anti-human verification services. We then conducted experiments on these services from an attacker‚Äö√Ñ√∂‚àö√ë‚àö‚àÇ‚Äö√†√∂‚àö√´‚Äö√†√∂‚Äö√†√á‚Äö√Ñ√∂‚àö‚Ä†‚àö‚àÇ‚Äö√†√∂‚àö¬¥‚Äö√Ñ√∂‚àö‚Ä†‚àö‚àÇ¬¨¬®‚Äö√Ñ¬¢s perspective to verify their effectiveness. The results show that all can easily bypass the security strategies website providers put in place to prevent fake registrations, such as SMS verification, CAPTCHA and IP monitoring. We further estimated the market size of the underground registration ecosystem, placing it at about US $4.8M-128.1 million per year. Our study demonstrates the urgency with which we to think about the effectiveness of our registration security strategies and should prompt us to develop new strategies for better protection.",Benevolence,Honesty,By identifying the effectiveness of registration bots and their usage in creating fake accounts; the paper helps maintain honesty in online platforms from a user perspective; thus aligning with Honor within the Benevolence value.,"Sure, I'd be happy to clarify my justification. The main contribution of 'Paper X' is to understand the underground ecosystem of account registration bots and the anti-human verification services they use. By conducting experiments on these services, the paper shows that these bots can easily bypass security strategies like SMS verification and CAPTCHA. This understanding is crucial for maintaining honesty in online platforms because it highlights the vulnerabilities in current registration security strategies. By aligning with the value item Honesty in Schwartz's Taxonomy, the paper emphasizes the importance of developing new strategies to protect against fake registrations, ultimately benefiting software users by promoting a more honest online environment.",Agreed-Justified,Agree,,Agree,,
Exp E,Exp J,457,ESEC/FSE,Security & Privacy,Security code smells in apps: are we getting better?,"Users increasingly rely on mobile apps for everyday tasks, including security- and privacy-sensitive tasks such as online banking, e-health, and e-government. Additionally, a wealth of sensors captures the movements and habits of the users for fitness tracking and convenience. Despite legal regulations imposing requirements and limits on the processing of privacy-sensitive data, users must still trust the app developers to apply suffcient protections. In this paper, we investigate the state of security in Android apps and how security-related code smells have evolved since the introduction of the Android operating system. With an analysis of 300 apps per year over 12 years between 2010 and 2021 from the Google Play Store, we find that the number of code scanner findings per thousand lines of code decreases over time. Still, this development is offset by the increase in code size. Apps have more and more findings, suggesting that the overall security level decreases. This trend is driven by flaws in the use of cryptography, insecure compiler flags, insecure uses of WebView components, and insecure uses of language features such as reflection. Based on our data, we argue for stricter controls on apps before admission to the store.",Security,Social Order,The paper suggests stricter controls on apps before admission to the store; which reflects the importance of Social Order among mobile apps by ensuring only secure and safe apps are allowed and its corresponding value Security.,"In the paper, the authors argue for stricter controls on apps before admission to the store. This aligns with the value item of Social Order, as it emphasizes the importance of maintaining order and ensuring that only secure and safe apps are allowed in the app store. By promoting stricter controls, the authors aim to enhance the overall security level in Android apps, which directly corresponds to the value of Security. Therefore, the alignment between 'Paper X' and the value item Social Order and its corresponding value Security is evident based on the explicitly stated contribution of promoting stricter controls for app admission.",Agreed-Justified,Agree,,Agree,,
Exp F,Exp J,457,ESEC/FSE,Security & Privacy,Security code smells in apps: are we getting better?,"Users increasingly rely on mobile apps for everyday tasks, including security- and privacy-sensitive tasks such as online banking, e-health, and e-government. Additionally, a wealth of sensors captures the movements and habits of the users for fitness tracking and convenience. Despite legal regulations imposing requirements and limits on the processing of privacy-sensitive data, users must still trust the app developers to apply suffcient protections. In this paper, we investigate the state of security in Android apps and how security-related code smells have evolved since the introduction of the Android operating system. With an analysis of 300 apps per year over 12 years between 2010 and 2021 from the Google Play Store, we find that the number of code scanner findings per thousand lines of code decreases over time. Still, this development is offset by the increase in code size. Apps have more and more findings, suggesting that the overall security level decreases. This trend is driven by flaws in the use of cryptography, insecure compiler flags, insecure uses of WebView components, and insecure uses of language features such as reflection. Based on our data, we argue for stricter controls on apps before admission to the store.",Universalism,A World at Peace,The paper's focus on increasing security in Android apps aligns with creating 'A World at Peace' by reducing cyber threats and attacks; corresponding to the value Universalism.,"The justification for aligning 'Paper X' with the value item A World at Peace and its corresponding value Universalism from a ""Software User"" perspective is that by increasing the security in Android apps, the paper aims to reduce cyber threats and attacks. This aligns with the value of Universalism, which emphasizes equality, social justice, and protecting the environment. By creating a more secure software environment, users can have peace of mind and a sense of safety, contributing towards a world at peace.",Agreed-Clarified,Agree,,Agree,,
Exp G,Exp K,462,ESEC/FSE,Code Generation & Analysis,Automated generation of test oracles for RESTful APIs,"Test case generation tools for RESTful APIs have proliferated in recent years. However, despite their promising results, they all share the same limitation: they can only detect crashes (i.e., server errors) and disconformities with the API specification. In this paper, we present a technique for the automated generation of test oracles for RESTful APIs through the detection of invariants. In practice, our approach aims to learn the expected properties of the output by analysing previous API requests and their corresponding responses. For this, we extended the popular tool Daikon for dynamic detection of likely invariants. A preliminary evaluation conducted on a set of 8 operations from 6 industrial APIs reveals a total precision of 66.5% (reaching 100% in 2 operations). Moreover, our approach revealed 6 reproducible bugs in APIs with millions of users: Amadeus, GitHub and OMDb.",Achievement,Intelligent,The paper contributes to improved intelligence by developing a method for detecting invariants in RESTful API's; which aid the software user in gaining more accurate and reliable results; aligning with the value item Intelligence of the value Achievement.,"The paper aligns with the value item Intelligent and its corresponding value Achievement from a ""Software User"" perspective because it presents a technique for detecting invariants in RESTful APIs, which can improve the accuracy and reliability of the results obtained by the software user. By analyzing previous API requests and responses, the approach aims to learn the expected properties of the output, contributing to the software user's ability to make intelligent decisions and achieve successful outcomes.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,462,ESEC/FSE,Code Generation & Analysis,Automated generation of test oracles for RESTful APIs,"Test case generation tools for RESTful APIs have proliferated in recent years. However, despite their promising results, they all share the same limitation: they can only detect crashes (i.e., server errors) and disconformities with the API specification. In this paper, we present a technique for the automated generation of test oracles for RESTful APIs through the detection of invariants. In practice, our approach aims to learn the expected properties of the output by analysing previous API requests and their corresponding responses. For this, we extended the popular tool Daikon for dynamic detection of likely invariants. A preliminary evaluation conducted on a set of 8 operations from 6 industrial APIs reveals a total precision of 66.5% (reaching 100% in 2 operations). Moreover, our approach revealed 6 reproducible bugs in APIs with millions of users: Amadeus, GitHub and OMDb.",Achievement,Successful,The automated generation of test oracles is likely to eliminate failures and server errors to a great extent; directly leading to the successful operation for software users. This aligns with the value item Successful from the value Achievement.,"The alignment between 'Paper X' and the value item Successful from the value Achievement is justified by the fact that the automated generation of test oracles, as described in the abstract, aims to ensure the expected properties of the output by detecting invariants. This detection of invariants ultimately helps in eliminating failures and server errors, which directly contributes to the successful operation of the software for users. Therefore, the alignment with the value item Successful reflects the importance of achieving reliable and error-free software performance for user satisfaction.",Agreed-Justified,Agree,,Agree,,
Exp B,Exp H,462,ESEC/FSE,Code Generation & Analysis,Automated generation of test oracles for RESTful APIs,"Test case generation tools for RESTful APIs have proliferated in recent years. However, despite their promising results, they all share the same limitation: they can only detect crashes (i.e., server errors) and disconformities with the API specification. In this paper, we present a technique for the automated generation of test oracles for RESTful APIs through the detection of invariants. In practice, our approach aims to learn the expected properties of the output by analysing previous API requests and their corresponding responses. For this, we extended the popular tool Daikon for dynamic detection of likely invariants. A preliminary evaluation conducted on a set of 8 operations from 6 industrial APIs reveals a total precision of 66.5% (reaching 100% in 2 operations). Moreover, our approach revealed 6 reproducible bugs in APIs with millions of users: Amadeus, GitHub and OMDb.",Security,Healthy,The detection of bugs in the popular APIs mentioned contributes to ensuring a healthier software experience for the users by preventing them from encountering errors; relating to the value item Healthy from the value Security.,"By detecting bugs in widely-used APIs, 'Paper X' contributes to the value item Healthy and its corresponding value Security by ensuring a more secure and stable software experience for users. This directly aligns with the value of Security, as it prevents users from encountering errors and potential vulnerabilities in the APIs they rely on, which can have a negative impact on their overall software experience and trust in the system.",Agreed-Clarified,Disagree,reconcile with coder_2,Disagree,"The justification claims that the paper aligns with the value 'Security', but relating to the value item Healthy from the value Security is not supported by the evidence in the abstract.
",
Exp D,Exp I,463,ESEC/FSE,Security & Privacy,What did you pack in my app? a systematic analysis of commercial Android packers,"Commercial Android packers have been widely used by developers as a way to protect their apps from being tampered with. However, app packer is usually provided as an online service developed by security vendors, and the packed apps are well protected. It is thus hard to know what exactly is packed in the app, and few existing studies in the community have systematically analyzed the behaviors of commercial app packers. In this paper, we propose PackDiff, a dynamic analysis system to inspect the fine-grained behaviors of commercial packers. By instrumenting the Android system, PackDiff records the runtime behaviors of Android apps (e.g., Linux system call invocations, Java API calls, Binder interactions, etc.), which are further processed to pinpoint the additional sensitive behaviors introduced by packers. By applying PackDiff to roughly 200 apps protected by seven commercial packers, we observe the disappointing facts of existing commercial packers. Most app packers have introduced unnecessary behaviors (e.g., accessing sensitive data), serious performance and compatibility issues, and they can even be abused to create evasive malware and repackaged apps, which contradicts with their design purposes.",Security,National Security,By identifying issues and contradictions in the behaviors of commercial packers; the paper indirectly contributes to improving the overall security of the digital environment; similar to National Security.,"The main contribution of 'Paper X' is in identifying the issues and contradictions in the behaviors of commercial packers, which can be abused to create evasive malware and repackaged apps. This aligns with the value item National Security and its corresponding value Security from a ""Software User"" perspective because improving the security of the digital environment is an important aspect of ensuring national security and protecting users' data and privacy.",Agreed-Justified,Agree,,Agree,,
Exp E,Exp J,463,ESEC/FSE,Security & Privacy,What did you pack in my app? a systematic analysis of commercial Android packers,"Commercial Android packers have been widely used by developers as a way to protect their apps from being tampered with. However, app packer is usually provided as an online service developed by security vendors, and the packed apps are well protected. It is thus hard to know what exactly is packed in the app, and few existing studies in the community have systematically analyzed the behaviors of commercial app packers. In this paper, we propose PackDiff, a dynamic analysis system to inspect the fine-grained behaviors of commercial packers. By instrumenting the Android system, PackDiff records the runtime behaviors of Android apps (e.g., Linux system call invocations, Java API calls, Binder interactions, etc.), which are further processed to pinpoint the additional sensitive behaviors introduced by packers. By applying PackDiff to roughly 200 apps protected by seven commercial packers, we observe the disappointing facts of existing commercial packers. Most app packers have introduced unnecessary behaviors (e.g., accessing sensitive data), serious performance and compatibility issues, and they can even be abused to create evasive malware and repackaged apps, which contradicts with their design purposes.",Achievement,Successful,Through identifying the unnecessary behaviours introduced by commercial app packers; the system proposed in the paper helps the user to potentially optimize the performance of the app; thus leading to a more successful user experience.,"In aligning with the value item Successful and its corresponding value Achievement, the paper's proposed system, PackDiff, identifies unnecessary behaviors introduced by commercial app packers. By doing so, it enables the software user to have a more optimized and efficient app performance, which directly contributes to a successful user experience.",Agreed-Justified,Agree,,Agree,,
Exp F,Exp J,466,ESEC/FSE,Software Engineering Practices,Using nudges to accelerate code reviews at scale,"We describe a large-scale study to reduce the amount of time code review takes. Each quarter at Meta we survey developers. Combining sentiment data from a developer experience survey and telemetry data from our diff review tool, we address, ‚Äö√Ñ√∂‚àö√ë‚àö‚à´When does a diff review feel too slow?‚Äö√Ñ√∂‚àö√ë‚àöœÄ From the sentiment data alone, we learn that 84.7% of developers are satisfied with the time their diffs spend in review. By enriching the survey results with telemetry for each respondent, we determined that sentiment is closely associated with the 75th percentile time in review for that respondent‚Äö√Ñ√∂‚àö√ë‚àö¬•s diffs, ie those that take more than 24 hours. To encourage developers to act on stale diffs that have had no action for 24 or more hours, we designed a NudgeBot to notify, ie nudge, reviewers. To determine who to nudge when a diff is stale, we created a model to rank the reviewers based on the probability that they will make a comment or perform some other action on a diff. This model outperformed models that looked at files the reviewer had modified in the past. Combining this information with prior author-review relationships, we achieved an MRR and AUC of .81 and .88, respectively. To evaluate NudgeBot in production, we conducted an A/B cluster-randomized experiment on over 30k engineers. We observed substantial statistically significant decrease in both time in review (-6.8%, p=0.049) and time to first reviewer action (-9.9%, p=0.010). We also used guard metrics to ensure that most reviews were still done in fewer than 24 hours and that reviewers still spend the same amount of time looking at diffs, and saw no statistically significant change in these metrics. NudgeBot is now rolled out company wide and is used daily by thousands of engineers at Meta.",Achievement,Capable,The NudgeBot tool described in 'Paper X' analysis method helps to rank the reviewers based on their likely responsiveness; implying they are competent to take actions thus aligning with the value item Capable and its corresponding value Achievement.,"Based on the analysis of 'Paper X', the NudgeBot tool directly aligns with the value item Capable and its corresponding value Achievement from a ""Software User"" perspective. The tool's ability to rank reviewers based on their likelihood of taking action demonstrates their competence and capability to contribute effectively to the code review process. By improving the efficiency and timeliness of the review process, the tool enables users to achieve their desired outcomes and goals in a more successful and influential manner, thus aligning with the values of Achievement and Capable.",Agreed-Justified,Agree,,Agree,,
Exp G,Exp K,466,ESEC/FSE,Software Engineering Practices,Using nudges to accelerate code reviews at scale,"We describe a large-scale study to reduce the amount of time code review takes. Each quarter at Meta we survey developers. Combining sentiment data from a developer experience survey and telemetry data from our diff review tool, we address, ‚Äö√Ñ√∂‚àö√ë‚àö‚à´When does a diff review feel too slow?‚Äö√Ñ√∂‚àö√ë‚àöœÄ From the sentiment data alone, we learn that 84.7% of developers are satisfied with the time their diffs spend in review. By enriching the survey results with telemetry for each respondent, we determined that sentiment is closely associated with the 75th percentile time in review for that respondent‚Äö√Ñ√∂‚àö√ë‚àö¬•s diffs, ie those that take more than 24 hours. To encourage developers to act on stale diffs that have had no action for 24 or more hours, we designed a NudgeBot to notify, ie nudge, reviewers. To determine who to nudge when a diff is stale, we created a model to rank the reviewers based on the probability that they will make a comment or perform some other action on a diff. This model outperformed models that looked at files the reviewer had modified in the past. Combining this information with prior author-review relationships, we achieved an MRR and AUC of .81 and .88, respectively. To evaluate NudgeBot in production, we conducted an A/B cluster-randomized experiment on over 30k engineers. We observed substantial statistically significant decrease in both time in review (-6.8%, p=0.049) and time to first reviewer action (-9.9%, p=0.010). We also used guard metrics to ensure that most reviews were still done in fewer than 24 hours and that reviewers still spend the same amount of time looking at diffs, and saw no statistically significant change in these metrics. NudgeBot is now rolled out company wide and is used daily by thousands of engineers at Meta.",Achievement,Successful,The success and effectiveness of the NudgeBot are confirmed with substantial statistically significant decrease in time in review and time to first reviewer action. This confirms the software's success; aligning with the value item Successful and its corresponding value Achievement.,"In the abstract of 'Paper X', it is explicitly stated that the implementation of NudgeBot resulted in a substantial statistically significant decrease in both time in review and time to first reviewer action. These positive outcomes demonstrate the success and effectiveness of the software in improving the code review process. This aligns directly with the value item Successful and its corresponding value Achievement from a ""Software User"" perspective, as the software's contributions lead to tangible improvements and accomplishments in terms of efficiency and productivity.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,466,ESEC/FSE,Software Engineering Practices,Using nudges to accelerate code reviews at scale,"We describe a large-scale study to reduce the amount of time code review takes. Each quarter at Meta we survey developers. Combining sentiment data from a developer experience survey and telemetry data from our diff review tool, we address, ‚Äö√Ñ√∂‚àö√ë‚àö‚àÇ‚Äö√†√∂‚àö√´‚Äö√†√∂‚Äö√†√á‚Äö√Ñ√∂‚àö‚Ä†‚àö‚àÇ‚Äö√†√∂‚àö¬¥‚Äö√Ñ√∂‚àö‚Ä†‚àö‚àÇ‚Äö√Ñ√∂‚àö‚Ä†¬¨¬•When does a diff review feel too slow?‚Äö√Ñ√∂‚àö√ë‚àö‚àÇ‚Äö√†√∂‚àö√´‚Äö√†√∂‚Äö√†√á‚Äö√Ñ√∂‚àö‚Ä†‚àö‚àÇ‚Äö√†√∂‚àö¬¥‚Äö√Ñ√∂‚àö‚Ä†‚àö‚àÇ‚âà√¨‚àö√ë From the sentiment data alone, we learn that 84.7% of developers are satisfied with the time their diffs spend in review. By enriching the survey results with telemetry for each respondent, we determined that sentiment is closely associated with the 75th percentile time in review for that respondent‚Äö√Ñ√∂‚àö√ë‚àö‚àÇ‚Äö√†√∂‚àö√´‚Äö√†√∂‚Äö√†√á‚Äö√Ñ√∂‚àö‚Ä†‚àö‚àÇ‚Äö√†√∂‚àö¬¥‚Äö√Ñ√∂‚àö‚Ä†‚àö‚àÇ¬¨¬®‚Äö√Ñ¬¢s diffs, ie those that take more than 24 hours. To encourage developers to act on stale diffs that have had no action for 24 or more hours, we designed a NudgeBot to notify, ie nudge, reviewers. To determine who to nudge when a diff is stale, we created a model to rank the reviewers based on the probability that they will make a comment or perform some other action on a diff. This model outperformed models that looked at files the reviewer had modified in the past. Combining this information with prior author-review relationships, we achieved an MRR and AUC of .81 and .88, respectively. To evaluate NudgeBot in production, we conducted an A/B cluster-randomized experiment on over 30k engineers. We observed substantial statistically significant decrease in both time in review (-6.8%, p=0.049) and time to first reviewer action (-9.9%, p=0.010). We also used guard metrics to ensure that most reviews were still done in fewer than 24 hours and that reviewers still spend the same amount of time looking at diffs, and saw no statistically significant change in these metrics. NudgeBot is now rolled out company wide and is used daily by thousands of engineers at Meta.",Power,Social Recognition,The large-scale study described in Paper X; and the significance of its findings suggest that it's a notable contribution in its area; likely garnering acknowledgement and visibility from the community; aligning to value item Social Recognition and corresponding value Power.,"In ""Paper X,"" the authors describe a large-scale study that addresses the issue of reducing the time for code reviews. This study is significant and likely to gain recognition and visibility within the software development community. The focus on improving the efficiency of code reviews aligns with the value item of Social Recognition, as the successful implementation of the proposed approach can lead to acknowledgment and respect from peers and colleagues. This recognition is a manifestation of the corresponding value of Power, as the authors' work has the potential to influence the code review processes and practices in software development.",Agreed-Clarified,Agree,,Agree,,
Exp B,Exp H,468,ESEC/FSE,Accessibility & User Experience,AccessiText: automated detection of text accessibility issues in Android apps,"For 15% of the world population with disabilities, accessibility is arguably the most critical software quality attribute. The growing reliance of users with disability on mobile apps to complete their day-to-day tasks further stresses the need for accessible software. Mobile operating systems, such as iOS and Android, provide various integrated assistive services to help individuals with disabilities perform tasks that could otherwise be difficult or not possible. However, for these assistive services to work correctly, developers have to support them in their app by following a set of best practices and accessibility guidelines. Text Scaling Assistive Service (TSAS) is utilized by people with low vision, to increase the text size and make apps accessible to them. However, the use of TSAS with incompatible apps can result in unexpected behavior introducing accessibility barriers to users. This paper presents approach, an automated testing technique for text accessibility issues arising from incompatibility between apps and TSAS. As a first step, we identify five different types of text accessibility by analyzing more than 600 candidate issues reported by users in (i) app reviews for Android and iOS, and (ii) Twitter data collected from public Twitter accounts. To automatically detect such issues, approach utilizes the UI screenshots and various metadata information extracted using dynamic analysis, and then applies various heuristics informed by the different types of text accessibility issues identified earlier. Evaluation of approach on 30 real-world Android apps corroborates its effectiveness by achieving 88.27% precision and 95.76% recall on average in detecting text accessibility issues.",Universalism,Protecting the Environment,The paper contributes to improving the accessibility of mobile applications for individuals with disabilities. It directly aligns with the value item 'Protecting the Environment' in a way that the paper promotes accessible environments by involving practices and actions that contribute to the preservation of the software Accessibility; a part of the users' digital environment; corresponds to the value 'Universalism'.,"The justification for aligning 'Paper X' with the value item Protecting the Environment and its corresponding value Universalism is based on the understanding that the paper's contributions towards improving the accessibility of mobile applications indirectly contribute to creating an accessible environment for individuals with disabilities. This aligns with the value of Universalism, which emphasizes the importance of recognizing the interconnectedness of all individuals and promoting a world that is inclusive and accessible to everyone. By ensuring that mobile apps are compatible with assistive services and guidelines, the paper helps to protect the environment of software accessibility, which is an essential aspect of the digital environment for software users. This alignment highlights the commitment to creating a world that values and respects the needs and rights of all individuals, including those with disabilities.",Agreed-Clarified,Agree,,Agree,,
Exp C,Exp I,468,ESEC/FSE,Accessibility & User Experience,AccessiText: automated detection of text accessibility issues in Android apps,"For 15% of the world population with disabilities, accessibility is arguably the most critical software quality attribute. The growing reliance of users with disability on mobile apps to complete their day-to-day tasks further stresses the need for accessible software. Mobile operating systems, such as iOS and Android, provide various integrated assistive services to help individuals with disabilities perform tasks that could otherwise be difficult or not possible. However, for these assistive services to work correctly, developers have to support them in their app by following a set of best practices and accessibility guidelines. Text Scaling Assistive Service (TSAS) is utilized by people with low vision, to increase the text size and make apps accessible to them. However, the use of TSAS with incompatible apps can result in unexpected behavior introducing accessibility barriers to users. This paper presents approach, an automated testing technique for text accessibility issues arising from incompatibility between apps and TSAS. As a first step, we identify five different types of text accessibility by analyzing more than 600 candidate issues reported by users in (i) app reviews for Android and iOS, and (ii) Twitter data collected from public Twitter accounts. To automatically detect such issues, approach utilizes the UI screenshots and various metadata information extracted using dynamic analysis, and then applies various heuristics informed by the different types of text accessibility issues identified earlier. Evaluation of approach on 30 real-world Android apps corroborates its effectiveness by achieving 88.27% precision and 95.76% recall on average in detecting text accessibility issues.",Security,Healthy,By aiming to ensure that mobile applications are properly scaled for users with visual impairments; the authors contribute to the health and well-being of the users; directly aligning with the 'Healthy' value item and the overarching value of 'Security'.,"In 'Paper X', the authors address the issue of text accessibility for individuals with low vision by proposing an automated testing technique to detect text accessibility issues arising from incompatibility between apps and assistive services. By prioritizing the proper scaling of mobile applications for users with visual impairments, the authors contribute to the overall health and well-being of the users. This aligns directly with the value item 'Healthy' and the overarching value of 'Security' as it ensures that individuals with disabilities have access to software that meets their accessibility needs, providing a sense of security and inclusiveness in using technology.",Agreed-Justified,Agree,The research want to solve a problem of low vision people,Agree,Universalism/Achievement,
Exp D,Exp I,468,ESEC/FSE,Accessibility & User Experience,AccessiText: automated detection of text accessibility issues in Android apps,"For 15% of the world population with disabilities, accessibility is arguably the most critical software quality attribute. The growing reliance of users with disability on mobile apps to complete their day-to-day tasks further stresses the need for accessible software. Mobile operating systems, such as iOS and Android, provide various integrated assistive services to help individuals with disabilities perform tasks that could otherwise be difficult or not possible. However, for these assistive services to work correctly, developers have to support them in their app by following a set of best practices and accessibility guidelines. Text Scaling Assistive Service (TSAS) is utilized by people with low vision, to increase the text size and make apps accessible to them. However, the use of TSAS with incompatible apps can result in unexpected behavior introducing accessibility barriers to users. This paper presents approach, an automated testing technique for text accessibility issues arising from incompatibility between apps and TSAS. As a first step, we identify five different types of text accessibility by analyzing more than 600 candidate issues reported by users in (i) app reviews for Android and iOS, and (ii) Twitter data collected from public Twitter accounts. To automatically detect such issues, approach utilizes the UI screenshots and various metadata information extracted using dynamic analysis, and then applies various heuristics informed by the different types of text accessibility issues identified earlier. Evaluation of approach on 30 real-world Android apps corroborates its effectiveness by achieving 88.27% precision and 95.76% recall on average in detecting text accessibility issues.",Stimulation,Daring,By promoting accessible software; it allows users with visual impairments to dare and try new activities or tasks on mobile applications; aligning with the value item 'Daring' under the broader value 'Stimulation'.,"In the context of 'Paper X', promoting accessible software aligns with the value item 'Daring' and its corresponding value 'Stimulation' from a ""Software User"" perspective because it enables individuals with visual impairments to confidently and boldly explore new activities and tasks on mobile applications. By providing the necessary accessibility features, users are empowered to overcome barriers and engage in stimulating experiences, ultimately contributing to their personal growth and sense of adventure.",Agreed-Justified,Agree,,Agree,,
Exp E,Exp J,469,ESEC/FSE,Code Generation & Analysis,Avgust: automating usage-based test generation from videos of app executions,"Writing and maintaining UI tests for mobile apps is a time-consuming and tedious task. While decades of research have produced auto- mated approaches for UI test generation, these approaches typically focus on testing for crashes or maximizing code coverage. By contrast, recent research has shown that developers prefer usage-based tests, which center around specific uses of app features, to help support activities such as regression testing. Very few existing techniques support the generation of such tests, as doing so requires automating the difficult task of understanding the semantics of UI screens and user inputs. In this paper, we introduce Avgust, which automates key steps of generating usage-based tests. Avgust uses neural models for image understanding to process video recordings of app uses to synthesize an app-agnostic state-machine encoding of those uses. Then, Avgust uses this encoding to synthesize test cases for a new target app. We evaluate Avgust on 374 videos of common uses of 18 popular apps and show that 69% of the tests Avgust generates successfully execute the desired usage, and that Avgust‚Äö√Ñ√∂‚àö√ë‚àö¬•s classifiers outperform the state of the art.",Achievement,Capable,"The paper presents Avgust; a tool that demonstrates a software user's capability by automating the process of usage-based tests generation; fitting the value item ""Capable"" from the value ""Achievement"".","The justification for aligning 'Paper X' with the value item Capable and its corresponding value Achievement from a ""Software User"" perspective is based on the fact that the paper introduces Avgust, a tool that automates the process of generating usage-based tests for mobile apps. By automating this task, Avgust enables software users to efficiently and effectively test app features, showcasing their capability to achieve successful app usage and ensuring high-quality performance. This aligns with the value item Capable, as it emphasizes the ability to accomplish tasks and the value Achievement, as it pertains to demonstrating competence and success in app usage.",Agreed-Clarified,Agree,,Agree,,
Exp F,Exp J,469,ESEC/FSE,Code Generation & Analysis,Avgust: automating usage-based test generation from videos of app executions,"Writing and maintaining UI tests for mobile apps is a time-consuming and tedious task. While decades of research have produced auto- mated approaches for UI test generation, these approaches typically focus on testing for crashes or maximizing code coverage. By contrast, recent research has shown that developers prefer usage-based tests, which center around specific uses of app features, to help support activities such as regression testing. Very few existing techniques support the generation of such tests, as doing so requires automating the difficult task of understanding the semantics of UI screens and user inputs. In this paper, we introduce Avgust, which automates key steps of generating usage-based tests. Avgust uses neural models for image understanding to process video recordings of app uses to synthesize an app-agnostic state-machine encoding of those uses. Then, Avgust uses this encoding to synthesize test cases for a new target app. We evaluate Avgust on 374 videos of common uses of 18 popular apps and show that 69% of the tests Avgust generates successfully execute the desired usage, and that Avgust‚Äö√Ñ√∂‚àö√ë‚àö¬•s classifiers outperform the state of the art.",Stimulation,Excitement in Life,"Avgust provides excitement for software users by automating the generation of usage-based tests from video recordings of apps; aligning with ""Excitement in Life"" from the ""Stimulation"" value.","In the paper abstract, it is stated that developers prefer usage-based tests to support activities such as regression testing. Avgust, the proposed approach, automates the generation of these tests by using video recordings of app uses to synthesize test cases for a new target app. This automation process provides excitement for software users as it eliminates the time-consuming and tedious task of writing and maintaining UI tests. This directly aligns with the value item ""Excitement in Life"" and its corresponding value ""Stimulation"" from Schwartz's Taxonomy.",Agreed-Clarified,Disagree,,Disagree,This paper's main focus is testing and introducing a tool for automating it.,
Exp G,Exp K,469,ESEC/FSE,Code Generation & Analysis,Avgust: automating usage-based test generation from videos of app executions,"Writing and maintaining UI tests for mobile apps is a time-consuming and tedious task. While decades of research have produced auto- mated approaches for UI test generation, these approaches typically focus on testing for crashes or maximizing code coverage. By contrast, recent research has shown that developers prefer usage-based tests, which center around specific uses of app features, to help support activities such as regression testing. Very few existing techniques support the generation of such tests, as doing so requires automating the difficult task of understanding the semantics of UI screens and user inputs. In this paper, we introduce Avgust, which automates key steps of generating usage-based tests. Avgust uses neural models for image understanding to process video recordings of app uses to synthesize an app-agnostic state-machine encoding of those uses. Then, Avgust uses this encoding to synthesize test cases for a new target app. We evaluate Avgust on 374 videos of common uses of 18 popular apps and show that 69% of the tests Avgust generates successfully execute the desired usage, and that Avgust‚Äö√Ñ√∂‚àö√ë‚àö¬•s classifiers outperform the state of the art.",Benevolence,Helpful,"By automating the key steps generating usage-based tests; Avgust aids the end user; directly aligning with the ""Helpful"" value item under the ""Benevolence"" value.","In the context of software testing, the value item Helpful and its corresponding value Benevolence align with the main contributions of 'Paper X' in a ""Software User"" perspective. The automation of generating usage-based tests by Avgust provides assistance to the end user, making the testing process more efficient and user-friendly. This aligns with the value of being helpful towards users, as it aims to support activities such as regression testing and improve the overall user experience of mobile apps.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,471,ESEC/FSE,Code Generation & Analysis,Unite: an adapter for transforming analysis tools to web services via OSLC,"This paper describes Unite, a new tool intended as an adapter for transforming non-interactive command-line analysis tools to OSLC-compliant web services. Unite aims to make such tools easier to adopt and more convenient to use by allowing them to be accessible, both locally and remotely, in a unified way and to be easily integrated into various development environments. Open Services for Lifecycle Collaboration (OSLC) is an open standard for tool integration and was chosen for this task due to its robustness, extensibility, support of data from various domains, and its growing popularity. The work is motivated by allowing existing analysis tools to be more widely used with a strong emphasis on widening their industrial usage. We have implemented Unite and used it with multiple existing static as well as dynamic analysis and verification tools, and then successfully deployed it internationally in the industry to automate verification tasks for development teams in Honeywell. We discuss Honeywell's experience with using Unite and with OSLC in general. Moreover, we also provide the Unite Client (UniC) for Eclipse to allow users to easily run various analysis tools directly from the Eclipse IDE.",Achievement,Intelligent,The paper contributes a tool named Unite that transforms non-interactive command-line analysis tools to web services; something that would make such tools easier to adopt and more convenient; therefore improving the overall intelligence of the software users.,"The contribution of the Unite tool in 'Paper X' aligns with the value item Intelligent and its corresponding value Achievement from a ""Software User"" perspective because by enabling the transformation of non-interactive command-line analysis tools to web services, Unite enhances the intelligence and capability of software users. It allows for easier adoption of such tools and convenient integration into various development environments, ultimately improving the efficiency and effectiveness of software analysis and verification tasks. This aligns with the value of Achievement as software users can achieve greater success and productivity through the use of Unite and its ability to automate and streamline analysis processes.",Agreed-Justified,Agree,,Agree,,
Exp B,Exp H,471,ESEC/FSE,Code Generation & Analysis,Unite: an adapter for transforming analysis tools to web services via OSLC,"This paper describes Unite, a new tool intended as an adapter for transforming non-interactive command-line analysis tools to OSLC-compliant web services. Unite aims to make such tools easier to adopt and more convenient to use by allowing them to be accessible, both locally and remotely, in a unified way and to be easily integrated into various development environments. Open Services for Lifecycle Collaboration (OSLC) is an open standard for tool integration and was chosen for this task due to its robustness, extensibility, support of data from various domains, and its growing popularity. The work is motivated by allowing existing analysis tools to be more widely used with a strong emphasis on widening their industrial usage. We have implemented Unite and used it with multiple existing static as well as dynamic analysis and verification tools, and then successfully deployed it internationally in the industry to automate verification tasks for development teams in Honeywell. We discuss Honeywell's experience with using Unite and with OSLC in general. Moreover, we also provide the Unite Client (UniC) for Eclipse to allow users to easily run various analysis tools directly from the Eclipse IDE.",Security,Healthy,By automating verification tasks for development teams in Honeywell and providing the Unite Client (UniC) for Eclipse; the software users would be able to maintain a more reliable and stable software environment; contributing to a state of being healthy.,"By automating verification tasks and providing the Unite Client for Eclipse, 'Paper X' is aligned with the value item Healthy and its corresponding value Security from a ""Software User"" perspective. This alignment is justified by the fact that the software users can ensure the reliability and stability of their software environment, leading to a sense of security in their work. This is particularly important in industries such as Honeywell, where the deployment of automated verification tasks can help prevent software errors and vulnerabilities, ultimately contributing to a healthier software ecosystem.",Agreed-Clarified,Disagree,reconcile with coder_2,Disagree,"The justification claims that the paper aligns with the value 'Security', but relating to the value item Healthy from the value Security is not supported by the evidence in the abstract.
",
Exp C,Exp I,471,ESEC/FSE,Code Generation & Analysis,Unite: an adapter for transforming analysis tools to web services via OSLC,"This paper describes Unite, a new tool intended as an adapter for transforming non-interactive command-line analysis tools to OSLC-compliant web services. Unite aims to make such tools easier to adopt and more convenient to use by allowing them to be accessible, both locally and remotely, in a unified way and to be easily integrated into various development environments. Open Services for Lifecycle Collaboration (OSLC) is an open standard for tool integration and was chosen for this task due to its robustness, extensibility, support of data from various domains, and its growing popularity. The work is motivated by allowing existing analysis tools to be more widely used with a strong emphasis on widening their industrial usage. We have implemented Unite and used it with multiple existing static as well as dynamic analysis and verification tools, and then successfully deployed it internationally in the industry to automate verification tasks for development teams in Honeywell. We discuss Honeywell's experience with using Unite and with OSLC in general. Moreover, we also provide the Unite Client (UniC) for Eclipse to allow users to easily run various analysis tools directly from the Eclipse IDE.",Universalism,Broadmindedness,The paper explicitly states that the tool Unite is aimed at making the use of analysis tools unified and; therefore; more approachable for software users of various backgrounds; hence contributing to Broadmindedness.,"The paper clearly mentions that Unite aims to make analysis tools more accessible and easier to use by providing a unified approach. By doing so, it enables software users with different backgrounds to easily adopt and utilize these tools. This aligns with the value item of Broadmindedness as it promotes open-mindedness and inclusivity by allowing users from diverse backgrounds to utilize and benefit from analysis tools, contributing to a broader perspective and understanding in the software context. Furthermore, this aligns with the corresponding value of Universalism, which emphasizes equality and the consideration of the broader impact and benefits for all users.",Agreed-Clarified,Agree,,Agree,,
Exp D,Exp I,472,ESEC/FSE,Software Engineering Practices,Demystifying √ø¬£¬¨¬¢√ø¬¢‚Äö√á¬®√ø¬¢‚âà√¨removed reviews√ø¬£¬¨¬¢√ø¬¢‚Äö√á¬®√ø¬¢‚Äö√Ñ√• in iOS app store,"The app markets enable users to submit feedback for downloaded apps in the form of star ratings and text reviews, which are meant to be helpful and trustworthy for decision making to both developers and other users. App markets have released strict guidelines/policies for user review submissions. However, there has been growing evidence showing the untrustworthy and poor-quality of app reviews, making the app store review environment a shambles. Therefore, review removal is a common practice, and market maintainers have to remove undesired reviews from the market periodically in a reactive manner. Although some reports and news outlets have mentioned removed reviews, our research community still lacks the comprehensive understanding of the landscape of this kind of reviews. To fill the void, in this paper, we present a large-scale and longitudinal study of removed reviews in iOS App Store. We first collaborate with our industry partner to collect over 30 million removed reviews for 33,665 popular apps over the course of a full year in 2020. This comprehensive dataset enables us to characterize the overall landscape of removed reviews. We next investigate the practical reasons leading to the removal of policy-violating reviews, and summarize several interesting reasons, including fake reviews, offensive reviews, etc. More importantly, most of these mis-behaviors can be reflected on reviews‚Äö√Ñ√∂‚àö√ë‚àö¬• basic information including the posters, narrative content, and posting time. It motivates us to design an automated approach to flag the policy-violation reviews, and our experiment result on the labelled benchmark can achieve a good performance (F1=97%). We further make an attempt to apply our approach to the large-scale industry setting, and the result suggests the promising industry usage scenario of our approach. Our approach can act as a gatekeeper to pinpoint policy-violation reviews beforehand, which will be quite effective in improving the maintenance process of app reviews in the industrial setting.",Conformity,Obedience,"The 'Paper X' proposal to build an approach to flag inappropriate app reviews aligns with the value item ""Obedience"" in the ""Conformity"" value; as it will encourage users to comply with the existing review policies in the app markets.","The justification for the alignment of 'Paper X' with the value item Obedience and its corresponding value Conformity is based on the fact that the proposed approach aims to flag and remove policy-violating reviews in app markets. By doing so, it encourages users to comply with the existing review policies, thus promoting obedience to the established regulations and conformity with the expected behavior within the app market environment.",Agreed-Justified,Agree,,Agree,,
Exp E,Exp J,472,ESEC/FSE,Software Engineering Practices,Demystifying √ø¬£¬¨¬¢√ø¬¢‚Äö√á¬®√ø¬¢‚âà√¨removed reviews√ø¬£¬¨¬¢√ø¬¢‚Äö√á¬®√ø¬¢‚Äö√Ñ√• in iOS app store,"The app markets enable users to submit feedback for downloaded apps in the form of star ratings and text reviews, which are meant to be helpful and trustworthy for decision making to both developers and other users. App markets have released strict guidelines/policies for user review submissions. However, there has been growing evidence showing the untrustworthy and poor-quality of app reviews, making the app store review environment a shambles. Therefore, review removal is a common practice, and market maintainers have to remove undesired reviews from the market periodically in a reactive manner. Although some reports and news outlets have mentioned removed reviews, our research community still lacks the comprehensive understanding of the landscape of this kind of reviews. To fill the void, in this paper, we present a large-scale and longitudinal study of removed reviews in iOS App Store. We first collaborate with our industry partner to collect over 30 million removed reviews for 33,665 popular apps over the course of a full year in 2020. This comprehensive dataset enables us to characterize the overall landscape of removed reviews. We next investigate the practical reasons leading to the removal of policy-violating reviews, and summarize several interesting reasons, including fake reviews, offensive reviews, etc. More importantly, most of these mis-behaviors can be reflected on reviews‚Äö√Ñ√∂‚àö√ë‚àö¬• basic information including the posters, narrative content, and posting time. It motivates us to design an automated approach to flag the policy-violation reviews, and our experiment result on the labelled benchmark can achieve a good performance (F1=97%). We further make an attempt to apply our approach to the large-scale industry setting, and the result suggests the promising industry usage scenario of our approach. Our approach can act as a gatekeeper to pinpoint policy-violation reviews beforehand, which will be quite effective in improving the maintenance process of app reviews in the industrial setting.",Security,Family Security,"The 'Paper X' describes an attempt to improve the reliability of user reviews which directly aligns with the ""Family Security"" value item as well as its corresponding ""Security"" value as it will make users' decisions more secure and trustworthy.","The main contribution of 'Paper X' is to present a large-scale study of removed reviews in the iOS App Store, with a focus on identifying policy-violating reviews. By developing an automated approach to flag these reviews, the paper aims to improve the maintenance process of app reviews and increase their reliability. This aligns with the value item of Family Security and its corresponding value of Security because ensuring the trustworthiness and accuracy of user reviews will contribute to a more secure and reliable decision-making process for software users, ultimately providing a sense of security for them and their families when using apps.",Agreed-Clarified,Agree,,Agree,,
Exp F,Exp J,472,ESEC/FSE,Software Engineering Practices,Demystifying √ø¬£¬¨¬¢√ø¬¢‚Äö√á¬®√ø¬¢‚âà√¨removed reviews√ø¬£¬¨¬¢√ø¬¢‚Äö√á¬®√ø¬¢‚Äö√Ñ√• in iOS app store,"The app markets enable users to submit feedback for downloaded apps in the form of star ratings and text reviews, which are meant to be helpful and trustworthy for decision making to both developers and other users. App markets have released strict guidelines/policies for user review submissions. However, there has been growing evidence showing the untrustworthy and poor-quality of app reviews, making the app store review environment a shambles. Therefore, review removal is a common practice, and market maintainers have to remove undesired reviews from the market periodically in a reactive manner. Although some reports and news outlets have mentioned removed reviews, our research community still lacks the comprehensive understanding of the landscape of this kind of reviews. To fill the void, in this paper, we present a large-scale and longitudinal study of removed reviews in iOS App Store. We first collaborate with our industry partner to collect over 30 million removed reviews for 33,665 popular apps over the course of a full year in 2020. This comprehensive dataset enables us to characterize the overall landscape of removed reviews. We next investigate the practical reasons leading to the removal of policy-violating reviews, and summarize several interesting reasons, including fake reviews, offensive reviews, etc. More importantly, most of these mis-behaviors can be reflected on reviews‚Äö√Ñ√∂‚àö√ë‚àö¬• basic information including the posters, narrative content, and posting time. It motivates us to design an automated approach to flag the policy-violation reviews, and our experiment result on the labelled benchmark can achieve a good performance (F1=97%). We further make an attempt to apply our approach to the large-scale industry setting, and the result suggests the promising industry usage scenario of our approach. Our approach can act as a gatekeeper to pinpoint policy-violation reviews beforehand, which will be quite effective in improving the maintenance process of app reviews in the industrial setting.",Universalism,Equality,"By filtering out untrustworthy reviews; 'Paper X' proposes a solution to establish a fairer environment on app platforms; thus aligning with the value of ""Universalism"" and specifically the value item ""Equality"".","In the context of the software user, 'Paper X' aligns with the value item Equality and its corresponding value Universalism because it proposes a solution to filter out untrustworthy reviews, which creates a fairer environment on app platforms. This aligns with the value of Universalism as it promotes equality by ensuring that all users have access to reliable and trustworthy information when making decisions about downloading apps.",Agreed-Clarified,Agree,,Agree,,
Exp G,Exp K,476,ESEC/FSE,Security & Privacy,Exploring the under-explored terrain of non-open source data for software engineering through the lens of federated learning,"The availability of open source projects on platforms like GitHub has led to the wide use of the artifacts from these projects in software engineering research. These publicly available artifacts have been used to train artificial intelligence models used in various empirical studies and the development of tools. However, these advancements have missed out on the artifacts from non-open source projects due to the unavailability of the data. A major cause for the unavailability of the data from non-open source repositories is the issue concerning data privacy. In this paper, we propose using federated learning to address the issue of data privacy to enable the use of data from non-open source to train AI models used in software engineering research. We believe that this can potentially enable industries to collaborate with software engineering researchers without concerns about privacy. We present the preliminary evaluation of the use of federated learning to train a classifier to label bug-fix commits from an existing study to demonstrate its feasibility. The federated approach achieved an F1 score of 0.83 compared to a score of 0.84 using the centralized approach. We also present our vision of the potential implications of the use of federated learning in software engineering research.",Self Direction,Privacy,The paper proposes a method that addresses data privacy concerns enabling the use of non-public data in software engineering research. This ensures the privacy of data originating from users; aligning with the value item 'Privacy' under the value 'Self Direction'.,"In 'Paper X', the authors propose using federated learning as a solution to the issue of data privacy in software engineering research. By utilizing this approach, they enable the use of non-public data while ensuring the privacy of the data originating from users. This aligns with the value item 'Privacy' under the value 'Self Direction' because it prioritizes individuals' ability to control their personal information and make decisions regarding its usage. The focus on protecting user data reflects a commitment to self-directed autonomy and freedom in the software context.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,476,ESEC/FSE,Security & Privacy,Exploring the under-explored terrain of non-open source data for software engineering through the lens of federated learning,"The availability of open source projects on platforms like GitHub has led to the wide use of the artifacts from these projects in software engineering research. These publicly available artifacts have been used to train artificial intelligence models used in various empirical studies and the development of tools. However, these advancements have missed out on the artifacts from non-open source projects due to the unavailability of the data. A major cause for the unavailability of the data from non-open source repositories is the issue concerning data privacy. In this paper, we propose using federated learning to address the issue of data privacy to enable the use of data from non-open source to train AI models used in software engineering research. We believe that this can potentially enable industries to collaborate with software engineering researchers without concerns about privacy. We present the preliminary evaluation of the use of federated learning to train a classifier to label bug-fix commits from an existing study to demonstrate its feasibility. The federated approach achieved an F1 score of 0.83 compared to a score of 0.84 using the centralized approach. We also present our vision of the potential implications of the use of federated learning in software engineering research.",Security,Healthy,The use of federated learning in this paper can potentially make software less prone to data breaches; indirectly contributing to users' psychological health by reducing anxieties & fears related to personal data exposure; thus aligning with 'Healthy' under the value 'Security'.,"I apologize for any confusion caused. In my previous justification, I aligned 'Paper X' with the value item Healthy and its corresponding value Security under Schwartz's Taxonomy. This alignment is based on the fact that the proposed use of federated learning in the paper can help enhance data privacy and security in software engineering research. By reducing concerns about personal data exposure and potential data breaches, the use of federated learning indirectly contributes to users' psychological well-being and aligns with the value of 'Healthy' in terms of security.",Agreed-Justified,Agree,,Agree,,
Exp B,Exp H,476,ESEC/FSE,Security & Privacy,Exploring the under-explored terrain of non-open source data for software engineering through the lens of federated learning,"The availability of open source projects on platforms like GitHub has led to the wide use of the artifacts from these projects in software engineering research. These publicly available artifacts have been used to train artificial intelligence models used in various empirical studies and the development of tools. However, these advancements have missed out on the artifacts from non-open source projects due to the unavailability of the data. A major cause for the unavailability of the data from non-open source repositories is the issue concerning data privacy. In this paper, we propose using federated learning to address the issue of data privacy to enable the use of data from non-open source to train AI models used in software engineering research. We believe that this can potentially enable industries to collaborate with software engineering researchers without concerns about privacy. We present the preliminary evaluation of the use of federated learning to train a classifier to label bug-fix commits from an existing study to demonstrate its feasibility. The federated approach achieved an F1 score of 0.83 compared to a score of 0.84 using the centralized approach. We also present our vision of the potential implications of the use of federated learning in software engineering research.",Power,Preserving My Public Image,The federated learning approach presented in this paper allows industries to collaborate with researchers without compromising data privacy; enabling them to maintain their public image without compromising the privacy of their users' data. This aligns with the value item 'Preserving My Public Image' under the value 'Power'.,"In the paper abstract, it is explicitly stated that the use of federated learning addresses the issue of data privacy, allowing industries to collaborate with researchers without privacy concerns. This aligns with the value item 'Preserving My Public Image' under the value 'Power' because it emphasizes the importance of maintaining a positive public image and reputation by taking appropriate measures to protect user data and privacy. By adopting a privacy-preserving approach like federated learning, software users can trust that their data is being handled responsibly, which directly aligns with the value items under the value of 'Power'.",Agreed-Justified,Agree,reconcile with coder_2,Agree,,
Exp C,Exp I,479,ESEC/FSE,AI & Machine Learning,CheapET-3: cost-efficient use of remote DNN models,"On complex problems, state of the art prediction accuracy of Deep Neural Networks (DNN) can be achieved using very large-scale models, consisting of billions of parameters. Such models can only be run on dedicated servers, typically provided by a 3th party service, which leads to a substantial monetary cost for every prediction. We propose a new software architecture for client-side applications, where a small local DNN is used alongside a remote large-scale model, aiming to make easy predictions locally at negligible monetary cost, while still leveraging the benefits of a large model for challenging inputs. In a proof of concept we reduce prediction cost by up to 50% without negatively impacting system accuracy.",Stimulation,Variation in Life,The new software architecture proposed in 'Paper X' brings variation into the user's experience by having a dual system of prediction - local DNN for easy predictions and remote large-scale model for more complex ones.,"In 'Paper X', the proposed software architecture introduces variation in the user's experience by implementing a dual system of prediction. By utilizing a small local DNN for easy predictions and a remote large-scale model for more complex inputs, the user is exposed to different levels of stimulation and novelty in the prediction process. This aligns with the value item ""Variation in Life"" and its corresponding value ""Stimulation"" from a software user perspective, as it introduces different levels of excitement and engagement in the prediction tasks performed by the software.",Agreed-Justified,Agree,,Agree,,
Exp D,Exp I,479,ESEC/FSE,AI & Machine Learning,CheapET-3: cost-efficient use of remote DNN models,"On complex problems, state of the art prediction accuracy of Deep Neural Networks (DNN) can be achieved using very large-scale models, consisting of billions of parameters. Such models can only be run on dedicated servers, typically provided by a 3th party service, which leads to a substantial monetary cost for every prediction. We propose a new software architecture for client-side applications, where a small local DNN is used alongside a remote large-scale model, aiming to make easy predictions locally at negligible monetary cost, while still leveraging the benefits of a large model for challenging inputs. In a proof of concept we reduce prediction cost by up to 50% without negatively impacting system accuracy.",Achievement,Capable,The proposed software architecture in 'Paper X' allows users to use local DNN for inexpensive predictions and be capable of handling their requirements efficiently; aligning with the value item 'Capable' and its corresponding value 'Achievement'.,"In 'Paper X', the proposed software architecture enables software users to achieve their goals efficiently by using a small local DNN for inexpensive predictions. This aligns with the value item 'Capable' as it empowers users to accomplish tasks effectively and demonstrates the value of 'Achievement' as users can successfully meet their requirements with the system.",Agreed-Justified,Agree,,Agree,,
Exp E,Exp J,479,ESEC/FSE,AI & Machine Learning,CheapET-3: cost-efficient use of remote DNN models,"On complex problems, state of the art prediction accuracy of Deep Neural Networks (DNN) can be achieved using very large-scale models, consisting of billions of parameters. Such models can only be run on dedicated servers, typically provided by a 3th party service, which leads to a substantial monetary cost for every prediction. We propose a new software architecture for client-side applications, where a small local DNN is used alongside a remote large-scale model, aiming to make easy predictions locally at negligible monetary cost, while still leveraging the benefits of a large model for challenging inputs. In a proof of concept we reduce prediction cost by up to 50% without negatively impacting system accuracy.",Security,Healthy,The software architecture proposed in 'Paper X' aligns with the value item 'Healthy' as it aims at reducing monetary cost of predictions; thus contributing to financial health of software users.,"In 'Paper X', the proposed software architecture directly aligns with the value item 'Healthy' by addressing the financial health of software users. By reducing the monetary cost of predictions, the paper contributes to the overall financial well-being of software users, as it enables easy predictions at a negligible cost. This aligns with the value of security, as users can make predictions without incurring substantial monetary burdens, leading to a more secure and stable financial situation for the software users.",Agreed-Justified,Agree,,Agree,,
Exp F,Exp J,483,ESEC/FSE,Security & Privacy,TSA: a tool to detect and quantify network side-channels,"Mobile applications, Internet of Things devices and web services are pervasive and they all encrypt the communications between servers and clients to not have information leakages. While the network traffic is encrypted, packet sizes and timings are still visible to an eavesdropper and these properties can leak information and sacrifice user privacy. We present TSA, a black box network side-channel analysis tool which detects and quantifies side-channel information leakages. TSA provides the users with the means to automate trace gathering by providing a framework in which the users can write mutators for the inputs to the system under analysis. TSA can also take as input traces directly for analysis if the user prefers to gather them separately. TSA is open-source and available as a Python package and a command-line tool. TSA demo, tool and benchmarks are available at https://github.com/kadron/tsa-tool.",Self Direction,Privacy,The contribution of TSA as a tool is linked to the value item 'Privacy' under the 'Self Direction' value because it prevents information leakages and affirms user's privacy by encrypting communication on various software platforms.,"The contribution of 'Paper X' aligns with the value item 'Privacy' and its corresponding value 'Self Direction' from the perspective of a software user because TSA, the black box network side-channel analysis tool presented in the paper, aims to detect and quantify side-channel information leakages. By encrypting communication on various software platforms, TSA helps prevent information leakages and thus ensures user privacy, which is a key aspect of self-direction and independent decision-making in the digital realm.",Agreed-Justified,Agree,,Agree,,
Exp G,Exp K,483,ESEC/FSE,Security & Privacy,TSA: a tool to detect and quantify network side-channels,"Mobile applications, Internet of Things devices and web services are pervasive and they all encrypt the communications between servers and clients to not have information leakages. While the network traffic is encrypted, packet sizes and timings are still visible to an eavesdropper and these properties can leak information and sacrifice user privacy. We present TSA, a black box network side-channel analysis tool which detects and quantifies side-channel information leakages. TSA provides the users with the means to automate trace gathering by providing a framework in which the users can write mutators for the inputs to the system under analysis. TSA can also take as input traces directly for analysis if the user prefers to gather them separately. TSA is open-source and available as a Python package and a command-line tool. TSA demo, tool and benchmarks are available at https://github.com/kadron/tsa-tool.",Security,Healthy,TSA allows for a healthy interaction between users and the software systems of mobile applications; web services; and IoT devices by ensuring that their data is safe from side-channel leakages. This aligns with the 'Healthy' value item under the 'Security' value.,"TSA's ability to detect and quantify side-channel information leakages ensures the security of user data in software systems such as mobile applications, web services, and IoT devices. By preventing information leakages, TSA promotes a healthy interaction between users and these software systems, as users can trust that their sensitive information is protected. This alignment with the 'Healthy' value item under the 'Security' value highlights the importance of maintaining a secure and safe environment for software users.",Agreed-Clarified,Agree,,Agree,,
Exp A,Exp H,483,ESEC/FSE,Security & Privacy,TSA: a tool to detect and quantify network side-channels,"Mobile applications, Internet of Things devices and web services are pervasive and they all encrypt the communications between servers and clients to not have information leakages. While the network traffic is encrypted, packet sizes and timings are still visible to an eavesdropper and these properties can leak information and sacrifice user privacy. We present TSA, a black box network side-channel analysis tool which detects and quantifies side-channel information leakages. TSA provides the users with the means to automate trace gathering by providing a framework in which the users can write mutators for the inputs to the system under analysis. TSA can also take as input traces directly for analysis if the user prefers to gather them separately. TSA is open-source and available as a Python package and a command-line tool. TSA demo, tool and benchmarks are available at https://github.com/kadron/tsa-tool.",Benevolence,Honesty,By creating a framework where users can construct mutators for input analysis and through offering flexibility in gathering input traces; TSA emphasizes 'Honesty' under the 'Benevolence' value by providing an open-source and trustworthy solution to users.,"In the context of the ""Software User,"" the alignment of 'Paper X' with the value item Honesty and its corresponding value Benevolence from Schwartz's Taxonomy can be justified by the fact that TSA is an open-source tool. This transparency and openness demonstrate a commitment to providing a trustworthy solution to users, which aligns with the values of Honesty and Benevolence. By allowing users to have access to the code and making it openly available, 'Paper X' prioritizes the users' best interests and fosters a sense of responsibility and honesty in the software development process.",Agreed-Clarified,Agree,,Agree,,
Exp B,Exp H,485,ESEC/FSE,Code Generation & Analysis,Minerva: browser API fuzzing with dynamic mod-ref analysis,"Browser APIs are essential to the modern web experience. Due to their large number and complexity, they vastly expand the attack surface of browsers. To detect vulnerabilities in these APIs, fuzzers generate test cases with a large amount of random API invocations. However, the massive search space formed by arbitrary API combinations hinders their effectiveness: since randomly-picked API invocations unlikely interfere with each other (i.e., compute on partially shared data), few interesting API interactions are explored. Consequently, reducing the search space by revealing inter-API relations is a major challenge in browser fuzzing. We propose Minerva, an efficient browser fuzzer for browser API bug detection. The key idea is to leverage API interference relations to reduce redundancy and improve coverage. Minerva consists of two modules: dynamic mod-ref analysis and guided code generation. Before fuzzing starts, the dynamic mod-ref analysis module builds an API interference graph. It first automatically identifies individual browser APIs from the browser‚Äö√Ñ√∂‚àö√ë‚àö‚àÇ‚Äö√†√∂‚àö√´‚Äö√†√∂‚Äö√†√á‚Äö√Ñ√∂‚àö‚Ä†‚àö‚àÇ‚Äö√†√∂‚àö¬¥‚Äö√Ñ√∂‚àö‚Ä†‚àö‚àÇ¬¨¬®‚Äö√Ñ¬¢s code base. Next, it instruments the browser to dynamically collect mod-ref relations between APIs. During fuzzing, the guided code generation module synthesizes highly-relevant API invocations guided by the mod-ref relations. We evaluate Minerva on three mainstream browsers, i.e. Safari, FireFox, and Chromium. Compared to state-of-the-art fuzzers, Minerva improves edge coverage by 19.63% to 229.62% and finds 2x to 3x more unique bugs. Besides, Minerva has discovered 35 previously-unknown bugs out of which 20 have been fixed with 5 CVEs assigned and acknowledged by browser vendors.",Achievement,Intelligent,The main contribution of 'Paper X' aligns with the value item 'Intelligent' under 'Achievement' value; since 'Minerva' leverages inter-API relations to reduce redundancy and improve effectiveness in detecting bugs; showcasing an intelligent approach.,The main contribution of 'Paper X' aligns with the value item 'Intelligent' under the value of 'Achievement' because 'Minerva' demonstrates an intelligent approach by leveraging inter-API relations to reduce redundancy and improve bug detection effectiveness. This shows a high level of capability and competence in utilizing innovative techniques to improve the efficiency and effectiveness of browser fuzzing.,Agreed-Justified,Agree,,Agree,,
Exp C,Exp I,485,ESEC/FSE,Code Generation & Analysis,Minerva: browser API fuzzing with dynamic mod-ref analysis,"Browser APIs are essential to the modern web experience. Due to their large number and complexity, they vastly expand the attack surface of browsers. To detect vulnerabilities in these APIs, fuzzers generate test cases with a large amount of random API invocations. However, the massive search space formed by arbitrary API combinations hinders their effectiveness: since randomly-picked API invocations unlikely interfere with each other (i.e., compute on partially shared data), few interesting API interactions are explored. Consequently, reducing the search space by revealing inter-API relations is a major challenge in browser fuzzing. We propose Minerva, an efficient browser fuzzer for browser API bug detection. The key idea is to leverage API interference relations to reduce redundancy and improve coverage. Minerva consists of two modules: dynamic mod-ref analysis and guided code generation. Before fuzzing starts, the dynamic mod-ref analysis module builds an API interference graph. It first automatically identifies individual browser APIs from the browser‚Äö√Ñ√∂‚àö√ë‚àö¬•s code base. Next, it instruments the browser to dynamically collect mod-ref relations between APIs. During fuzzing, the guided code generation module synthesizes highly-relevant API invocations guided by the mod-ref relations. We evaluate Minerva on three mainstream browsers, i.e. Safari, FireFox, and Chromium. Compared to state-of-the-art fuzzers, Minerva improves edge coverage by 19.63% to 229.62% and finds 2x to 3x more unique bugs. Besides, Minerva has discovered 35 previously-unknown bugs out of which 20 have been fixed with 5 CVEs assigned and acknowledged by browser vendors.",Security,Healthy,The paper indirectly contributes to 'Healthy' under the 'Security' value by improving the security of browsers; making web browsing a safer experience for the user by detecting vulnerabilities more efficiently.,"In the context of a ""Software User,"" the paper's main contribution is aligned with the value item of 'Healthy' and its corresponding value of 'Security.' By proposing Minerva, an efficient browser fuzzer for detecting vulnerabilities in browser APIs, the paper directly addresses the security aspect of the user's software experience. By reducing redundancy and improving coverage through the use of API interference relations, Minerva enhances the overall security of browsers, resulting in a safer web browsing experience for the user. Therefore, the paper's contribution aligns with the value item of 'Healthy' under the 'Security' value from the perspective of a ""Software User.""",Agreed-Reconciled,Agree,,Agree,,
Exp D,Exp I,485,ESEC/FSE,Code Generation & Analysis,Minerva: browser API fuzzing with dynamic mod-ref analysis,"Browser APIs are essential to the modern web experience. Due to their large number and complexity, they vastly expand the attack surface of browsers. To detect vulnerabilities in these APIs, fuzzers generate test cases with a large amount of random API invocations. However, the massive search space formed by arbitrary API combinations hinders their effectiveness: since randomly-picked API invocations unlikely interfere with each other (i.e., compute on partially shared data), few interesting API interactions are explored. Consequently, reducing the search space by revealing inter-API relations is a major challenge in browser fuzzing. We propose Minerva, an efficient browser fuzzer for browser API bug detection. The key idea is to leverage API interference relations to reduce redundancy and improve coverage. Minerva consists of two modules: dynamic mod-ref analysis and guided code generation. Before fuzzing starts, the dynamic mod-ref analysis module builds an API interference graph. It first automatically identifies individual browser APIs from the browser‚Äö√Ñ√∂‚àö√ë‚àö¬•s code base. Next, it instruments the browser to dynamically collect mod-ref relations between APIs. During fuzzing, the guided code generation module synthesizes highly-relevant API invocations guided by the mod-ref relations. We evaluate Minerva on three mainstream browsers, i.e. Safari, FireFox, and Chromium. Compared to state-of-the-art fuzzers, Minerva improves edge coverage by 19.63% to 229.62% and finds 2x to 3x more unique bugs. Besides, Minerva has discovered 35 previously-unknown bugs out of which 20 have been fixed with 5 CVEs assigned and acknowledged by browser vendors.",Security,National Security,The development and implementation of Minerva support the value item 'National Security' in 'Security' by helping to protect users' internet activities from potential threats; making web experience more secure for the users at a larger scale.,"The development and implementation of Minerva directly align with the value item 'National Security' and its corresponding value 'Security' from a ""Software User"" perspective. By leveraging API interference relations and reducing the search space in browser fuzzing, Minerva helps to detect vulnerabilities in browser APIs and improve coverage. This contributes to making the web experience more secure for users, protecting their internet activities from potential threats at a larger scale.",Agreed-Clarified,Agree,,Agree,,
Exp F,Exp J,486,ESEC/FSE,Software Engineering Practices,Workgraph: personal focus vs. interruption for engineers at Meta,"All engineers dislike interruptions because it takes away from the deep focus time needed to write complex code. Our goal is to reduce unnecessary interruptions at . We first describe our Workgraph platform that logs how engineers use our internal work tools at . Using these anonymized logs, we create sessions. sessions are defined in opposition to interruption and are the amount of time until the engineer is interrupted by, for example, a work chat message. We describe descriptive statistics related to how long engineers are able to focus. We find that at Meta, Engineers have a total of 14.25 hours of personal-focus time per week. These numbers are comparable with those reported by other software firms. We then create a Random Forest model to understand which factors influence the median daily personal-focus time. We find that the more time an engineer spends in the IDE the longer their focus. We also find that the more central an engineer is in the social work network, the shorter their personal-focus time. Other factors such as role and domain/pillar have little impact on personal-focus at Meta. To help engineers achieve longer blocks of personal-focus and help them stay in flow, Meta developed the AutoFocus tool that blocks work chat notifications when an engineer is working on code for 12 minutes or longer. AutoFocus allows the sender to still force a work chat message using ‚Äö√Ñ√∂‚àö√ë‚àö‚à´@notify‚Äö√Ñ√∂‚àö√ë‚àöœÄ ensuring that urgent messages still get through, but allowing the sender to reflect on the importance of the message. In a large experiment, we find that AutoFocus increases the amount of personal-focus time by 20.27%, and it has now been rolled out widely at Meta.",Achievement,Successful,The paper reveals that AutoFocus increases the amount of personal-focus time by 20.27%. This improvement in focus can contribute to engineers becoming more successful in their work. This aligns with the value item Successful and its corresponding value Achievement.,"The justification for aligning 'Paper X' with the value item Successful and its corresponding value Achievement is that the improvement in personal-focus time resulting from the implementation of AutoFocus can lead to increased productivity and effectiveness in engineers' work. By allowing engineers to stay in the flow and focus on complex coding tasks for longer periods without unnecessary interruptions, AutoFocus contributes to their ability to achieve their goals and make significant achievements in their software development work.",Agreed-Clarified,Agree,,Agree,,
Exp G,Exp K,486,ESEC/FSE,Software Engineering Practices,Workgraph: personal focus vs. interruption for engineers at Meta,"All engineers dislike interruptions because it takes away from the deep focus time needed to write complex code. Our goal is to reduce unnecessary interruptions at . We first describe our Workgraph platform that logs how engineers use our internal work tools at . Using these anonymized logs, we create sessions. sessions are defined in opposition to interruption and are the amount of time until the engineer is interrupted by, for example, a work chat message. We describe descriptive statistics related to how long engineers are able to focus. We find that at Meta, Engineers have a total of 14.25 hours of personal-focus time per week. These numbers are comparable with those reported by other software firms. We then create a Random Forest model to understand which factors influence the median daily personal-focus time. We find that the more time an engineer spends in the IDE the longer their focus. We also find that the more central an engineer is in the social work network, the shorter their personal-focus time. Other factors such as role and domain/pillar have little impact on personal-focus at Meta. To help engineers achieve longer blocks of personal-focus and help them stay in flow, Meta developed the AutoFocus tool that blocks work chat notifications when an engineer is working on code for 12 minutes or longer. AutoFocus allows the sender to still force a work chat message using ‚Äö√Ñ√∂‚àö√ë‚àö‚à´@notify‚Äö√Ñ√∂‚àö√ë‚àöœÄ ensuring that urgent messages still get through, but allowing the sender to reflect on the importance of the message. In a large experiment, we find that AutoFocus increases the amount of personal-focus time by 20.27%, and it has now been rolled out widely at Meta.",Security,Healthy,The paper's proposal of reducing interruptions can result in a healthier work environment by reducing stress and fatigue caused by constant distractions. This aligns with the value item Healthy and its corresponding value Security.,"Certainly! The reduction of interruptions proposed by 'Paper X' aligns with the value item Healthy and its corresponding value Security because it creates a healthier work environment by minimizing stress and fatigue caused by constant distractions. When software users are able to focus for longer blocks of time without interruptions, they can cultivate a sense of security in their work, knowing that they have the necessary focus and concentration to tackle complex tasks effectively. This promotes their overall well-being and contributes to a healthier work-life balance.",Agreed-Clarified,Agree,,Agree,,
Exp A,Exp H,487,ESEC/FSE,AI & Machine Learning,COREQQA: a COmpliance REQuirements understanding using question answering tool,"We introduce COREQQA, a tool for assisting requirements engineers in acquiring a better understanding of compliance requirements by means of automated Question Answering. Extracting compliance-related requirements by manually navigating through a legal document is both time-consuming and error-prone. COREQQA enables requirements engineers to pose questions in natural language about a compliance-related topic given some legal document, e.g., asking about data breach. The tool then automatically navigates through the legal document and returns to the requirements engineer a list of text passages containing the possible answers to the input question. For better readability, the tool also highlights the likely answers in these passages. The engineer can then use this output for specifying compliance requirements. COREQQA is developed using advanced large-scale language models from BERT's family. COREQQA has been evaluated on four legal documents. The results of this evaluation are briefly presented in the paper. The tool is publicly available on Zenodo (DOI: 10.5281/zenodo.6653514).",Achievement,Intelligent,The COREQQA tool aids requirements engineers in obtaining a clear understanding of compliance-related requirements; hence contributing to the users becoming more Intelligent in their tasks and aligning with the value of Achievement.,"The justification for aligning 'Paper X' with the value item Intelligent and its corresponding value Achievement from a ""Software User"" perspective lies in the fact that the COREQQA tool enhances the capabilities of requirements engineers by providing them with automated question-answering capabilities for compliance-related topics. By utilizing this tool, requirements engineers are able to acquire a deeper understanding of compliance requirements, which in turn facilitates their decision-making process and contributes to their overall expertise and intelligence in handling such tasks. This aligns with the value of Achievement as users are able to demonstrate their competence and proficiency in their field through the successful utilization of the COREQQA tool.",Agreed-Justified,Agree,,Agree,,
Exp B,Exp H,487,ESEC/FSE,AI & Machine Learning,COREQQA: a COmpliance REQuirements understanding using question answering tool,"We introduce COREQQA, a tool for assisting requirements engineers in acquiring a better understanding of compliance requirements by means of automated Question Answering. Extracting compliance-related requirements by manually navigating through a legal document is both time-consuming and error-prone. COREQQA enables requirements engineers to pose questions in natural language about a compliance-related topic given some legal document, e.g., asking about data breach. The tool then automatically navigates through the legal document and returns to the requirements engineer a list of text passages containing the possible answers to the input question. For better readability, the tool also highlights the likely answers in these passages. The engineer can then use this output for specifying compliance requirements. COREQQA is developed using advanced large-scale language models from BERT's family. COREQQA has been evaluated on four legal documents. The results of this evaluation are briefly presented in the paper. The tool is publicly available on Zenodo (DOI: 10.5281/zenodo.6653514).",Achievement,Capable,The ability of users to pose questions in natural language and receive detailed text passages containing the possible answers contributes to users becoming more Capable; aligning with the value of Achievement.,"The justification for aligning 'Paper X' with the value item Capable and its corresponding value Achievement is based on the fact that the tool introduced in the paper allows users to pose questions in natural language and receive detailed text passages containing possible answers. By providing users with this capability, the paper enhances their ability to gather information and acquire knowledge, thus contributing to their sense of capability and achievement.",Agreed-Clarified,Agree,,Agree,,
Exp C,Exp I,487,ESEC/FSE,AI & Machine Learning,COREQQA: a COmpliance REQuirements understanding using question answering tool,"We introduce COREQQA, a tool for assisting requirements engineers in acquiring a better understanding of compliance requirements by means of automated Question Answering. Extracting compliance-related requirements by manually navigating through a legal document is both time-consuming and error-prone. COREQQA enables requirements engineers to pose questions in natural language about a compliance-related topic given some legal document, e.g., asking about data breach. The tool then automatically navigates through the legal document and returns to the requirements engineer a list of text passages containing the possible answers to the input question. For better readability, the tool also highlights the likely answers in these passages. The engineer can then use this output for specifying compliance requirements. COREQQA is developed using advanced large-scale language models from BERT's family. COREQQA has been evaluated on four legal documents. The results of this evaluation are briefly presented in the paper. The tool is publicly available on Zenodo (DOI: 10.5281/zenodo.6653514).",Security,Healthy,By reducing the possibility of error in compliance requirements; COREQQA fits in enhancing the Healthiness of software; signifying a connection with the value of Security.,"COREQQA, as described in the abstract of 'Paper X', addresses the challenge of extracting compliance-related requirements from legal documents efficiently and accurately. By automating the process of navigating through the document and providing a list of text passages containing possible answers to compliance-related questions, COREQQA reduces the time-consuming and error-prone nature of manual extraction. This directly aligns with the value item Healthy and its corresponding value Security from a ""Software User"" perspective because it enhances the healthiness of the software by minimizing errors in compliance requirements. This improvement in accuracy and efficiency contributes to the overall security of the software system, ensuring that it complies with relevant regulations and protects sensitive data.",Agreed-Reconciled,Agree,,Agree,,
Exp D,Exp I,488,ESEC/FSE,Software Engineering Practices,Industry practice of configuration auto-tuning for cloud applications and services,"Auto-tuning attracts increasing attention in industry practice to optimize the performance of a system with many configurable parameters. It is particularly useful for cloud applications and services since they have complex system hierarchies and intricate knob correlations. However, existing tools and algorithms rarely consider practical problems such as workload pressure control, the support for distributed deployment, and expensive time costs, etc., which are utterly important for enterprise cloud applications and services. In this work, we significantly extend an open source tuning tool ‚Äö√Ñ√∂‚àö√ë‚àö¬® KeenTune to optimize several typical enterprise cloud applications and services. Our practice is in collaboration with enterprise users and tuning tool developers to address the aforementioned problems. Specifically, we highlight five key challenges from our experiences and provide a set of solutions accordingly. Through applying the improved tuning tool to different application scenarios, we achieve 2%-14% improvements for the performance of MySQL, OceanBase, nginx, ingress-nginx, and 5%-70% improvements for the performance of ACK cloud container service.",Achievement,Successful,The paper aims to optimize the performance of cloud applications and services that affect the user's success in their tasks; aligning with the value item Successful of Achievement.,"In the context of a software user, the optimization of performance for cloud applications and services directly contributes to the user's success in their tasks. By improving the performance of these applications, the software user can experience faster and more efficient workflows, leading to increased productivity and achievement of their goals. This alignment with the value item Successful from the Achievement value demonstrates the direct impact of 'Paper X' on enhancing the user's success in a software context.",Agreed-Justified,Agree,,Agree,,
Exp F,Exp J,488,ESEC/FSE,Software Engineering Practices,Industry practice of configuration auto-tuning for cloud applications and services,"Auto-tuning attracts increasing attention in industry practice to optimize the performance of a system with many configurable parameters. It is particularly useful for cloud applications and services since they have complex system hierarchies and intricate knob correlations. However, existing tools and algorithms rarely consider practical problems such as workload pressure control, the support for distributed deployment, and expensive time costs, etc., which are utterly important for enterprise cloud applications and services. In this work, we significantly extend an open source tuning tool ‚Äö√Ñ√∂‚àö√ë‚àö¬® KeenTune to optimize several typical enterprise cloud applications and services. Our practice is in collaboration with enterprise users and tuning tool developers to address the aforementioned problems. Specifically, we highlight five key challenges from our experiences and provide a set of solutions accordingly. Through applying the improved tuning tool to different application scenarios, we achieve 2%-14% improvements for the performance of MySQL, OceanBase, nginx, ingress-nginx, and 5%-70% improvements for the performance of ACK cloud container service.",Security,Healthy,By aiming to improve the performance of various software applications; which in result helps in smoother operations and less issues while using them; the paper aligns with the value item Healthy under the value Security.,"In the context of a ""Software User,"" the alignment of ""Paper X"" with the value item Healthy and its corresponding value Security is justified because by optimizing the performance of software applications, issues and disruptions are minimized, resulting in a smoother and healthier experience for users. This aligns with the value of Security as it reduces the risk of potential problems or vulnerabilities that could compromise the integrity and stability of the software being used. Overall, the focus on improving the performance and reliability of software applications contributes to a healthier and more secure user experience.",Agreed-Justified,Agree,,Agree,,
Exp G,Exp K,493,ESEC/FSE,Software Testing & QA,Testing of autonomous driving systems: where are we and where should we go?,"Autonomous driving has shown great potential to reform modern transportation. Yet its reliability and safety have drawn a lot of attention and concerns. Compared with traditional software systems, autonomous driving systems (ADSs) often use deep neural networks in tandem with logic-based modules. This new paradigm poses unique challenges for software testing. Despite the recent development of new ADS testing techniques, it is not clear to what extent those techniques have addressed the needs of ADS practitioners. To fill this gap, we present the first comprehensive study to identify the current practices and needs of ADS testing. We conducted semi-structured interviews with developers from 10 autonomous driving companies and surveyed 100 developers who have worked on autonomous driving systems. A systematic analysis of the interview and survey data revealed 7 common practices and 4 emerging needs of autonomous driving testing. Through a comprehensive literature review, we developed a taxonomy of existing ADS testing techniques and analyzed the gap between ADS research and practitioners‚Äö√Ñ√∂‚àö√ë‚àö¬• needs. Finally, we proposed several future directions for SE researchers, such as developing test reduction techniques to accelerate simulation-based ADS testing.",Power,Social Recognition,The paper addresses the concern of reliability and safety in autonomous driving systems; which could lead to increased social recognition for users of such systems; corresponding to the value item Social Recognition and the value Power.,"In the paper abstract of 'Paper X', it is mentioned that autonomous driving systems have the potential to reform modern transportation but their reliability and safety are a concern. By addressing these concerns, the paper indirectly aligns with the value item Social Recognition and its corresponding value Power. This is because if autonomous driving systems are made more reliable and safe, users of such systems would gain social recognition and influence, as they would be seen as early adopters of advanced technology and would have the power to make safer choices in transportation.",Agreed-Clarified,Disagree,This clarification is not related to the social recognition. ,Disagree,Cannot be traced in the abstract. ,
Exp A,Exp H,493,ESEC/FSE,Software Testing & QA,Testing of autonomous driving systems: where are we and where should we go?,"Autonomous driving has shown great potential to reform modern transportation. Yet its reliability and safety have drawn a lot of attention and concerns. Compared with traditional software systems, autonomous driving systems (ADSs) often use deep neural networks in tandem with logic-based modules. This new paradigm poses unique challenges for software testing. Despite the recent development of new ADS testing techniques, it is not clear to what extent those techniques have addressed the needs of ADS practitioners. To fill this gap, we present the first comprehensive study to identify the current practices and needs of ADS testing. We conducted semi-structured interviews with developers from 10 autonomous driving companies and surveyed 100 developers who have worked on autonomous driving systems. A systematic analysis of the interview and survey data revealed 7 common practices and 4 emerging needs of autonomous driving testing. Through a comprehensive literature review, we developed a taxonomy of existing ADS testing techniques and analyzed the gap between ADS research and practitioners‚Äö√Ñ√∂‚àö√ë‚àö‚àÇ‚Äö√†√∂‚àö√´‚Äö√†√∂‚Äö√†√á‚Äö√Ñ√∂‚àö‚Ä†‚àö‚àÇ‚Äö√†√∂‚àö¬¥‚Äö√Ñ√∂‚àö‚Ä†‚àö‚àÇ¬¨¬®‚Äö√Ñ¬¢ needs. Finally, we proposed several future directions for SE researchers, such as developing test reduction techniques to accelerate simulation-based ADS testing.",Security,Healthy,By identifying the prevalent practices and emergent needs of autonomous driving testing; the paper contributes to enhancing the health of users by improving the safety of autonomous driving systems. This aligns with the value item Healthy and its corresponding value Security.,"The paper's contribution to enhancing the health of users is evidenced by its focus on identifying practices and needs of autonomous driving testing, which ultimately improves the safety of autonomous driving systems. By addressing these needs and improving the reliability and safety of autonomous driving, the paper aligns with the value item Healthy and its corresponding value Security, as it directly emphasizes the importance of ensuring the well-being and security of software users.",Agreed-Justified,Agree,,Agree,,
Exp B,Exp H,493,ESEC/FSE,Software Testing & QA,Testing of autonomous driving systems: where are we and where should we go?,"Autonomous driving has shown great potential to reform modern transportation. Yet its reliability and safety have drawn a lot of attention and concerns. Compared with traditional software systems, autonomous driving systems (ADSs) often use deep neural networks in tandem with logic-based modules. This new paradigm poses unique challenges for software testing. Despite the recent development of new ADS testing techniques, it is not clear to what extent those techniques have addressed the needs of ADS practitioners. To fill this gap, we present the first comprehensive study to identify the current practices and needs of ADS testing. We conducted semi-structured interviews with developers from 10 autonomous driving companies and surveyed 100 developers who have worked on autonomous driving systems. A systematic analysis of the interview and survey data revealed 7 common practices and 4 emerging needs of autonomous driving testing. Through a comprehensive literature review, we developed a taxonomy of existing ADS testing techniques and analyzed the gap between ADS research and practitioners‚Äö√Ñ√∂‚àö√ë‚àö‚àÇ‚Äö√†√∂‚àö√´‚Äö√†√∂‚Äö√†√á‚Äö√Ñ√∂‚àö‚Ä†‚àö‚àÇ‚Äö√†√∂‚àö¬¥‚Äö√Ñ√∂‚àö‚Ä†‚àö‚àÇ¬¨¬®‚Äö√Ñ¬¢ needs. Finally, we proposed several future directions for SE researchers, such as developing test reduction techniques to accelerate simulation-based ADS testing.",Security,National Security,The paper's focus on the reliability and safety issues in autonomous driving systems also aligns with the concept of National Security; as improving these areas can contribute to safer national transport infrastructure.,"The paper's emphasis on addressing the reliability and safety concerns in autonomous driving systems directly aligns with the value item National Security and its corresponding value of Security. By improving the reliability and safety of autonomous driving systems, the paper contributes to creating a safer national transport infrastructure, which ultimately enhances national security.",Agreed-Justified,Agree,,Agree,,
Exp C,Exp I,498,ESEC/FSE,Code Generation & Analysis,NL2Viz: natural language to visualization via constrained syntax-guided synthesis,"Recent development in NL2CODE (Natural Language to Code) research allows end-users, especially novice programmers to create a concrete implementation of their ideas such as data visualization by providing natural language (NL) instructions. An NL2CODE system often fails to achieve its goal due to three major challenges: the user's words have contextual semantics, the user may not include all details needed for code generation, and the system results are imperfect and require further refinement. To address the aforementioned three challenges for NL to Visualization, we propose a new approach and its supporting tool named NL2VIZ with three salient features: (1) leveraging not only the user's NL input but also the data and program context that the NL query is upon, (2) using hard/soft constraints to reflect different confidence levels in the constraints retrieved from the user input and data/program context, and (3) providing support for result refinement and reuse. We implement NL2VIZ in the Jupyter Notebook environment and evaluate NL2VIZ on a real-world visualization benchmark and a public dataset to show the effectiveness of NL2VIZ. We also conduct a user study involving 6 data scientist professionals to demonstrate the usability of NL2VIZ, the readability of the generated code, and NL2VIZ's effectiveness in helping users generate desired visualizations effectively and efficiently.",Stimulation,Excitement in Life,The paper presents NL2VIZ; a tool that increases Excitement in Life (Stimulation) by enabling end users to convert natural language into data visualizations. This tool creates an exciting experience by making data visualization more accessible to novice programmers.,"The main contribution of 'Paper X' is the development of NL2VIZ, a tool that enables end-users to convert natural language instructions into data visualizations. This aligns with the value item Excitement in Life and its corresponding value Stimulation because NL2VIZ makes the process of creating visualizations more accessible to novice programmers, allowing them to bring their ideas to life and experience the excitement of visualizing their data in a tangible and meaningful way. By providing a user-friendly interface and bridging the gap between natural language and code, NL2VIZ enhances the stimulation and excitement users feel when exploring and analyzing their data through visualizations.",Agreed-Justified,Agree,visualizing  words or sentences is interesting for user,Agree,Achievement,
Exp D,Exp I,498,ESEC/FSE,Code Generation & Analysis,NL2Viz: natural language to visualization via constrained syntax-guided synthesis,"Recent development in NL2CODE (Natural Language to Code) research allows end-users, especially novice programmers to create a concrete implementation of their ideas such as data visualization by providing natural language (NL) instructions. An NL2CODE system often fails to achieve its goal due to three major challenges: the user's words have contextual semantics, the user may not include all details needed for code generation, and the system results are imperfect and require further refinement. To address the aforementioned three challenges for NL to Visualization, we propose a new approach and its supporting tool named NL2VIZ with three salient features: (1) leveraging not only the user's NL input but also the data and program context that the NL query is upon, (2) using hard/soft constraints to reflect different confidence levels in the constraints retrieved from the user input and data/program context, and (3) providing support for result refinement and reuse. We implement NL2VIZ in the Jupyter Notebook environment and evaluate NL2VIZ on a real-world visualization benchmark and a public dataset to show the effectiveness of NL2VIZ. We also conduct a user study involving 6 data scientist professionals to demonstrate the usability of NL2VIZ, the readability of the generated code, and NL2VIZ's effectiveness in helping users generate desired visualizations effectively and efficiently.",Achievement,Successful,The paper contributes to Successful (Achievement) value by proposing a tool that allows the user to effectively and efficiently generate desired visualizations; contributing to their success in data analysis and visualization.,"The paper aligns with the value item Successful and its corresponding value Achievement from a ""Software User"" perspective, as it proposes a tool (NL2VIZ) that enables users to generate desired visualizations effectively and efficiently. By providing support for result refinement and reuse, the tool helps users achieve success in their data analysis and visualization tasks. This aligns with the value of Achievement, as it enables users to accomplish their goals and create concrete implementations of their ideas.",Agreed-Justified,Agree,,Agree,,
Exp E,Exp J,498,ESEC/FSE,Code Generation & Analysis,NL2Viz: natural language to visualization via constrained syntax-guided synthesis,"Recent development in NL2CODE (Natural Language to Code) research allows end-users, especially novice programmers to create a concrete implementation of their ideas such as data visualization by providing natural language (NL) instructions. An NL2CODE system often fails to achieve its goal due to three major challenges: the user's words have contextual semantics, the user may not include all details needed for code generation, and the system results are imperfect and require further refinement. To address the aforementioned three challenges for NL to Visualization, we propose a new approach and its supporting tool named NL2VIZ with three salient features: (1) leveraging not only the user's NL input but also the data and program context that the NL query is upon, (2) using hard/soft constraints to reflect different confidence levels in the constraints retrieved from the user input and data/program context, and (3) providing support for result refinement and reuse. We implement NL2VIZ in the Jupyter Notebook environment and evaluate NL2VIZ on a real-world visualization benchmark and a public dataset to show the effectiveness of NL2VIZ. We also conduct a user study involving 6 data scientist professionals to demonstrate the usability of NL2VIZ, the readability of the generated code, and NL2VIZ's effectiveness in helping users generate desired visualizations effectively and efficiently.",Benevolence,Helpful,The paper aligns with the Helpful (Benevolence) value by proposing a new approach and supporting tool (NL2VIZ) that helps users (specifically novice programmers and data scientists) transform natural language instructions into concrete visualizations; assisting their tasks.,"The main contribution of 'Paper X' is the proposal of a new approach and a supporting tool named NL2VIZ, which directly aligns with the value item Helpful and its corresponding value Benevolence from a ""Software User"" perspective. This alignment is justified by the fact that the NL2VIZ system aims to assist users, specifically novice programmers and data scientists, in transforming their natural language instructions into concrete visualizations, thereby providing support and help in their tasks. By enabling users to easily generate desired visualizations effectively and efficiently, NL2VIZ demonstrates benevolence by providing a valuable tool that aids users in achieving their goals.",Agreed-Justified,Agree,,Agree,,
Exp F,Exp J,501,ESEC/FSE,Software Development Methodologies,Achievement unlocked: a case study on gamifying DevOps practices in industry,"Gamification is the use of game elements such as points, leaderboards, and badges in a non-game context to encourage a desired behavior from individuals interacting with an environment. Recently, gamification has found its way into software engineering contexts as a means to promote certain activities to practitioners. Previous studies investigated the use of gamification to promote the adoption of a variety of tools and practices, however, these studies were either performed in an educational environment or in small to medium-sized teams of developers in the industry. We performed a large-scale mixed-methods study on the effects of badge-based gamification in promoting the adoption of DevOps practices in a very large company and evaluated how practice adoption is associated with changes in key delivery, quality, and throughput metrics of 333 software projects. We observed an accelerated adoption of some gamified DevOps practices by at least 60%, with increased adoption rates up to 6x. We found mixed results when associating badge adoption and metric changes: teams that earned testing badges showed an increase in bug fixing commits but output fewer commits and pull requests; teams that earned code review and quality tooling badges exhibited faster delivery metrics. Finally, our empirical study was supplemented by a survey with 45 developers where 73% of respondents found badges to be helpful for learning about and adopting new standardized practices. Our results contribute to the rich knowledge on gamification with a unique and important perspective from real industry practitioners.",Achievement,Successful,The paper described a large-scale mixed-methods study on the effect of badge-based gamification in a software context; and observed an accelerated adoption of gamified DevOps practices by 60% with rates up to 6x. This indicates the software's contribution to promoting user success; explicitly aligning with the value item Successful and its corresponding value Achievement.,"In 'Paper X', the authors conducted a large-scale study on the effects of badge-based gamification in promoting the adoption of DevOps practices in a software context. The findings showed an accelerated adoption of gamified practices, indicating the software's contribution to promoting user success. This aligns directly with the value item Successful and its corresponding value Achievement from a ""Software User"" perspective, as the paper explicitly states the positive impact of the software in achieving higher adoption rates and improving key delivery and quality metrics.",Agreed-Justified,Agree,,Agree,,
Exp G,Exp K,501,ESEC/FSE,Software Development Methodologies,Achievement unlocked: a case study on gamifying DevOps practices in industry,"Gamification is the use of game elements such as points, leaderboards, and badges in a non-game context to encourage a desired behavior from individuals interacting with an environment. Recently, gamification has found its way into software engineering contexts as a means to promote certain activities to practitioners. Previous studies investigated the use of gamification to promote the adoption of a variety of tools and practices, however, these studies were either performed in an educational environment or in small to medium-sized teams of developers in the industry. We performed a large-scale mixed-methods study on the effects of badge-based gamification in promoting the adoption of DevOps practices in a very large company and evaluated how practice adoption is associated with changes in key delivery, quality, and throughput metrics of 333 software projects. We observed an accelerated adoption of some gamified DevOps practices by at least 60%, with increased adoption rates up to 6x. We found mixed results when associating badge adoption and metric changes: teams that earned testing badges showed an increase in bug fixing commits but output fewer commits and pull requests; teams that earned code review and quality tooling badges exhibited faster delivery metrics. Finally, our empirical study was supplemented by a survey with 45 developers where 73% of respondents found badges to be helpful for learning about and adopting new standardized practices. Our results contribute to the rich knowledge on gamification with a unique and important perspective from real industry practitioners.",Stimulation,Variation in Life,The paper introduces gamification elements like badges in a non-game software context; adding a new and exciting dimension to the user experience. It therefore aligns with the value item Variation in Life; contributing to the value Stimulation for the software user.,"In my previous justification, I stated that the paper aligns with the value item Variation in Life and its corresponding value Stimulation from a ""Software User"" perspective. This is because the introduction of gamification elements like badges in a non-game software context adds a new and exciting dimension to the user experience. This element of variation in the software experience can provide stimulation and engage the user in a more dynamic and interactive way, enhancing their overall satisfaction and involvement with the software.",Agreed-Clarified,Agree,,Agree,,
Exp A,Exp H,501,ESEC/FSE,Software Development Methodologies,Achievement unlocked: a case study on gamifying DevOps practices in industry,"Gamification is the use of game elements such as points, leaderboards, and badges in a non-game context to encourage a desired behavior from individuals interacting with an environment. Recently, gamification has found its way into software engineering contexts as a means to promote certain activities to practitioners. Previous studies investigated the use of gamification to promote the adoption of a variety of tools and practices, however, these studies were either performed in an educational environment or in small to medium-sized teams of developers in the industry. We performed a large-scale mixed-methods study on the effects of badge-based gamification in promoting the adoption of DevOps practices in a very large company and evaluated how practice adoption is associated with changes in key delivery, quality, and throughput metrics of 333 software projects. We observed an accelerated adoption of some gamified DevOps practices by at least 60%, with increased adoption rates up to 6x. We found mixed results when associating badge adoption and metric changes: teams that earned testing badges showed an increase in bug fixing commits but output fewer commits and pull requests; teams that earned code review and quality tooling badges exhibited faster delivery metrics. Finally, our empirical study was supplemented by a survey with 45 developers where 73% of respondents found badges to be helpful for learning about and adopting new standardized practices. Our results contribute to the rich knowledge on gamification with a unique and important perspective from real industry practitioners.",Benevolence,Helpful,73% of developers found badges to be helpful for learning about and adopting new standardized practices. This feature would help software users become more helpful to others by promoting better practices; aligning with the value item Helpful and its corresponding value Benevolence.,"The justification for aligning 'Paper X' with the value item Helpful and its corresponding value Benevolence from a ""Software User"" perspective is based on the finding from the survey that 73% of developers found badges to be helpful for learning and adopting new standardized practices. By promoting the adoption of these practices, software users are likely to become more knowledgeable and skilled, enabling them to be more helpful to others in the software context. This aligns directly with the value item Helpful, which emphasizes the importance of assisting and supporting others, and its corresponding value Benevolence, which focuses on contributing to the welfare and well-being of others.",Agreed-Clarified,Agree,,Agree,,
Exp B,Exp H,502,ESEC/FSE,Software Engineering Practices,This is your cue! assisting search behaviour with resource style properties,"When learning a software technology, programmers face a large variety of resources in different styles and catering to different requirements. Although search engines are helpful to filter relevant resources, programmers are still required to manually go through a number of resources before they find one pertinent to their needs. Prior work has largely concentrated on helping programmers find the precise location of relevant information within a resource. Our work focuses on helping programmers assess the pertinence of resources to differentiate between resources. We investigated how programmers find learning resources online via a diary and interview study, and observed that programmers use certain cues to determine whether to access a resource. Based on our findings, we investigate the extent to which we can support the cue-following process via a prototype tool. Our research supports programmers‚Äö√Ñ√∂‚àö√ë‚àö‚àÇ‚Äö√†√∂‚àö√´‚Äö√†√∂‚Äö√†√á‚Äö√Ñ√∂‚àö‚Ä†‚àö‚àÇ‚Äö√†√∂‚àö¬¥‚Äö√Ñ√∂‚àö‚Ä†‚àö‚àÇ¬¨¬®‚Äö√Ñ¬¢ search behaviour for software technology learning resources to inform resource creators on important factors that programmers look for during their search.",Self Direction,Independent,Paper X develops a prototype tool to help programmers differentiate between resources for learning new software technologies. This supports Independent software use; a value item under Self Direction; as it empowers programmers to make informed choices about their learning resources.,"In 'Paper X', the development of a prototype tool to help programmers assess the pertinence of learning resources aligns with the value item Independent and its corresponding value Self Direction from the perspective of a software user. By providing programmers with a tool that assists them in differentiating between resources, they are empowered to make independent and informed choices about their learning materials. This supports their autonomy and freedom to determine the most relevant resources for their needs, contributing to their sense of self-direction in their software learning journey.",Agreed-Justified,Agree,,Agree,,
Exp C,Exp I,502,ESEC/FSE,Software Engineering Practices,This is your cue! assisting search behaviour with resource style properties,"When learning a software technology, programmers face a large variety of resources in different styles and catering to different requirements. Although search engines are helpful to filter relevant resources, programmers are still required to manually go through a number of resources before they find one pertinent to their needs. Prior work has largely concentrated on helping programmers find the precise location of relevant information within a resource. Our work focuses on helping programmers assess the pertinence of resources to differentiate between resources. We investigated how programmers find learning resources online via a diary and interview study, and observed that programmers use certain cues to determine whether to access a resource. Based on our findings, we investigate the extent to which we can support the cue-following process via a prototype tool. Our research supports programmers‚Äö√Ñ√∂‚àö√ë‚àö¬• search behaviour for software technology learning resources to inform resource creators on important factors that programmers look for during their search.",Self Direction,Freedom,By providing support for the cue-following process in programmers‚Äö√Ñ√∂‚àö√ë‚àö¬• search behaviour; Paper X directly contributes to empowering the programmers with Freedom; another value item under the value of Self Direction; as they have the freedom to choose the best resources for their learning.,"In the context of 'Paper X', the main contribution of providing support for the cue-following process in programmers' search behavior aligns with the value item of Freedom and its corresponding value of Self Direction. This alignment is evident because by empowering programmers with the tools to assess the pertinence of learning resources, they have the freedom to choose the best resources that align with their specific needs and goals. This aligns with the value of Self Direction as programmers are able to independently make choices and have control over their learning process.",Agreed-Justified,Agree,,Agree,,
Exp D,Exp I,502,ESEC/FSE,Software Engineering Practices,This is your cue! assisting search behaviour with resource style properties,"When learning a software technology, programmers face a large variety of resources in different styles and catering to different requirements. Although search engines are helpful to filter relevant resources, programmers are still required to manually go through a number of resources before they find one pertinent to their needs. Prior work has largely concentrated on helping programmers find the precise location of relevant information within a resource. Our work focuses on helping programmers assess the pertinence of resources to differentiate between resources. We investigated how programmers find learning resources online via a diary and interview study, and observed that programmers use certain cues to determine whether to access a resource. Based on our findings, we investigate the extent to which we can support the cue-following process via a prototype tool. Our research supports programmers‚Äö√Ñ√∂‚àö√ë‚àö¬• search behaviour for software technology learning resources to inform resource creators on important factors that programmers look for during their search.",Stimulation,Excitement in Life,The tool developed in Paper X; which helps to identify relevant learning resources; provides Excitement in life; a value item under Stimulation; by introducing novelty and variety into the programmers' learning process.,"In the context of software technology learning resources, the tool developed in Paper X aligns with the value item Excitement in Life and its corresponding value Stimulation. This alignment is based on the fact that the tool introduces novelty and variety into the programmers' learning process, allowing them to explore a large variety of resources in different styles and catering to different requirements. By providing programmers with a more diverse range of learning resources and helping them discover new and relevant information, the tool enhances the learning experience and brings an element of excitement and stimulation into their lives.",Agreed-Clarified,Agree,,Agree,,
Exp E,Exp J,611,ICSE,Data Management & Processing,Training data debugging for the fairness of machine learning software,"With the widespread application of machine learning (ML) software, especially in high-risk tasks, the concern about their unfairness has been raised towards both developers and users of ML software. The unfairness of ML software indicates the software behavior affected by the sensitive features (e.g., sex), which leads to biased and illegal decisions and has become a worthy problem for the whole software engineering community. According to the ‚Äö√Ñ√∂‚àö√ë‚àö‚à´data-driven‚Äö√Ñ√∂‚àö√ë‚àöœÄ programming paradigm of ML software, we consider the root cause of the unfairness as biased features in training data. Inspired by software debugging, we propose a novel method, Linear-regression based Training Data Debugging (LTDD), to debug feature values in training data, i.e., (a) identify which features and which parts of them are biased, and (b) exclude the biased parts of such features to recover as much valuable and unbiased information as possible to build fair ML software. We conduct an extensive study on nine data sets and three classifiers to evaluate the effect of our method LTDD compared with four baseline methods. Experimental results show that (a) LTDD can better improve the fairness of ML software with less or comparable damage to the performance, and (b) LTDD is more actionable for fairness improvement in realistic scenarios.",Universalism,Social Justice,The paper contributes a method for improving fairness in software; particularly in the application of machine learning. This aligns with the value item Social Justice and its corresponding value Universalism.,"The paper's contribution of a method for improving fairness in software, specifically in the context of machine learning, aligns with the value item of Social Justice and its corresponding value of Universalism. By addressing the issue of unfairness in ML software, the paper aims to promote fairness and equal treatment for all individuals who interact with these systems. This aligns with the value of Social Justice, which emphasizes the importance of equality and fairness in society. Furthermore, the paper's focus on building fair ML software reflects the value of Universalism, which emphasizes the welfare and equal rights of all individuals. Overall, the paper's contributions align with these values by striving to address and mitigate biases and unfairness in ML software, ultimately promoting social justice and universal principles.",Agreed-Clarified,Agree,,Agree,,
Exp F,Exp J,611,ICSE,Data Management & Processing,Training data debugging for the fairness of machine learning software,"With the widespread application of machine learning (ML) software, especially in high-risk tasks, the concern about their unfairness has been raised towards both developers and users of ML software. The unfairness of ML software indicates the software behavior affected by the sensitive features (e.g., sex), which leads to biased and illegal decisions and has become a worthy problem for the whole software engineering community. According to the ‚Äö√Ñ√∂‚àö√ë‚àö‚à´data-driven‚Äö√Ñ√∂‚àö√ë‚àöœÄ programming paradigm of ML software, we consider the root cause of the unfairness as biased features in training data. Inspired by software debugging, we propose a novel method, Linear-regression based Training Data Debugging (LTDD), to debug feature values in training data, i.e., (a) identify which features and which parts of them are biased, and (b) exclude the biased parts of such features to recover as much valuable and unbiased information as possible to build fair ML software. We conduct an extensive study on nine data sets and three classifiers to evaluate the effect of our method LTDD compared with four baseline methods. Experimental results show that (a) LTDD can better improve the fairness of ML software with less or comparable damage to the performance, and (b) LTDD is more actionable for fairness improvement in realistic scenarios.",Achievement,Capable,The paper presents a novel method for improving the ability of machine learning software to make fair and unbiased decisions. This aligns with the value item Capable and its corresponding value Achievement.,"The justification for aligning 'Paper X' with the value item Capable and its corresponding value Achievement is based on the paper's focus on improving the fairness and unbiasedness of machine learning software. By proposing a novel method to debug biased features in training data, the paper aims to enhance the capability of ML software to make fair decisions. This aligns with the value item Capable, which represents the desire for competence and effectiveness in achieving desired outcomes, and the corresponding value Achievement, which signifies the pursuit of success and accomplishment.",Agreed-Clarified,Agree,,Agree,,
Exp A,Exp H,614,ICSE,Code Generation & Analysis,Free lunch for testing: Fuzzing deep-learning libraries from open source,"Deep learning (DL) systems can make our life much easier, and thus are gaining more and more attention from both academia and industry. Meanwhile, bugs in DL systems can be disastrous, and can even threaten human lives in safety-critical applications. To date, a huge body of research efforts have been dedicated to testing DL models. However, interestingly, there is still limited work for testing the underlying DL libraries, which are the foundation for building, optimizing, and running DL models. One potential reason is that test generation for the underlying DL libraries can be rather challenging since their public APIs are mainly exposed in Python, making it even hard to automatically determine the API input parameter types due to dynamic typing. In this paper, we propose FreeFuzz, the first approach to fuzzing DL libraries via mining from open source. More specifically, FreeFuzz obtains code/models from three different sources: 1) code snippets from the library documentation, 2) library developer tests, and 3) DL models in the wild. Then, FreeFuzz automatically runs all the collected code/models with instrumentation to trace the dynamic information for each covered API, including the types and values of each parameter during invocation, and shapes of input/output tensors. Lastly, FreeFuzz will leverage the traced dynamic information to perform fuzz testing for each covered API. The extensive study of FreeFuzz on PyTorch and TensorFlow, two of the most popular DL libraries, shows that FreeFuzz is able to automatically trace valid dynamic information for fuzzing 1158 popular APIs, 9X more than state-of-the-art LEMON with 3.5X lower overhead than LEMON. To date, FreeFuzz has detected 49 bugs for PyTorch and TensorFlow (with 38 already confirmed by developers as previously unknown).",Security,Healthy,The main goal of Paper X is to detect and solve bugs in deep learning systems; enhancing the safety and functionality. This principle aligns with the health value item under the security value; as it aims to reduce risks associated with software bugs.,"In Paper X, the main contribution is focused on developing a approach to fuzzing deep learning libraries in order to detect and solve bugs. This aligns with the value item Healthy under the value Security because by reducing risks associated with software bugs, the overall health and safety of the system is improved. Detecting and solving bugs in deep learning systems contributes to the security and functionality of the software, which in turn enhances its health and overall well-being.",Agreed-Justified,Disagree,"related to Achievement: capable more, as it automatically identifies bugs",Disagree,The justification claiming that the paper aligns with the value 'Security' because of 'The main goal of Paper X is to detect and solve bugs in deep learning systems; enhancing the safety and functionality.  This principle aligns with the health value item under the security value; as it aims to reduce risks associated with software bugs.,
Exp B,Exp H,614,ICSE,Code Generation & Analysis,Free lunch for testing: Fuzzing deep-learning libraries from open source,"Deep learning (DL) systems can make our life much easier, and thus are gaining more and more attention from both academia and industry. Meanwhile, bugs in DL systems can be disastrous, and can even threaten human lives in safety-critical applications. To date, a huge body of research efforts have been dedicated to testing DL models. However, interestingly, there is still limited work for testing the underlying DL libraries, which are the foundation for building, optimizing, and running DL models. One potential reason is that test generation for the underlying DL libraries can be rather challenging since their public APIs are mainly exposed in Python, making it even hard to automatically determine the API input parameter types due to dynamic typing. In this paper, we propose FreeFuzz, the first approach to fuzzing DL libraries via mining from open source. More specifically, FreeFuzz obtains code/models from three different sources: 1) code snippets from the library documentation, 2) library developer tests, and 3) DL models in the wild. Then, FreeFuzz automatically runs all the collected code/models with instrumentation to trace the dynamic information for each covered API, including the types and values of each parameter during invocation, and shapes of input/output tensors. Lastly, FreeFuzz will leverage the traced dynamic information to perform fuzz testing for each covered API. The extensive study of FreeFuzz on PyTorch and TensorFlow, two of the most popular DL libraries, shows that FreeFuzz is able to automatically trace valid dynamic information for fuzzing 1158 popular APIs, 9X more than state-of-the-art LEMON with 3.5X lower overhead than LEMON. To date, FreeFuzz has detected 49 bugs for PyTorch and TensorFlow (with 38 already confirmed by developers as previously unknown).",Conformity,Self-Discipline,FreeFuzz; the proposed method in the paper; operates with self-discipline; remaining consistent and meticulous in its process to ensure no errors in the DL system are overlooked. This falls under the Conformity value in Schwartz's taxonomy.,"The justification for aligning 'Paper X' with the value item Self-Discipline and its corresponding value Conformity is based on the fact that FreeFuzz, the proposed method in the paper, demonstrates self-discipline by consistently and meticulously running code/models and tracing dynamic information for each covered API. This disciplined approach ensures that no errors in the DL system are overlooked, which aligns with the value of conformity in adhering to established rules and standards within a software context.",Agreed-Clarified,Agree,,Agree,,
Exp C,Exp I,614,ICSE,Code Generation & Analysis,Free lunch for testing: Fuzzing deep-learning libraries from open source,"Deep learning (DL) systems can make our life much easier, and thus are gaining more and more attention from both academia and industry. Meanwhile, bugs in DL systems can be disastrous, and can even threaten human lives in safety-critical applications. To date, a huge body of research efforts have been dedicated to testing DL models. However, interestingly, there is still limited work for testing the underlying DL libraries, which are the foundation for building, optimizing, and running DL models. One potential reason is that test generation for the underlying DL libraries can be rather challenging since their public APIs are mainly exposed in Python, making it even hard to automatically determine the API input parameter types due to dynamic typing. In this paper, we propose FreeFuzz, the first approach to fuzzing DL libraries via mining from open source. More specifically, FreeFuzz obtains code/models from three different sources: 1) code snippets from the library documentation, 2) library developer tests, and 3) DL models in the wild. Then, FreeFuzz automatically runs all the collected code/models with instrumentation to trace the dynamic information for each covered API, including the types and values of each parameter during invocation, and shapes of input/output tensors. Lastly, FreeFuzz will leverage the traced dynamic information to perform fuzz testing for each covered API. The extensive study of FreeFuzz on PyTorch and TensorFlow, two of the most popular DL libraries, shows that FreeFuzz is able to automatically trace valid dynamic information for fuzzing 1158 popular APIs, 9X more than state-of-the-art LEMON with 3.5X lower overhead than LEMON. To date, FreeFuzz has detected 49 bugs for PyTorch and TensorFlow (with 38 already confirmed by developers as previously unknown).",Achievement,Capable,The paper contributes to enabling the software users to be capable of using Deep Learning systems without having the risk of bugs. This aligns with the Capability value item under Achievement value in Schwartz's taxonomy.,"By proposing FreeFuzz, the paper addresses the potential bugs in Deep Learning systems and ensures the capability of software users in utilizing these systems without the risk of bugs. This aligns with the value item of Capable under the Achievement value in Schwartz's Taxonomy. Specifically, by automatically tracing dynamic information and performing fuzz testing on the underlying DL libraries, FreeFuzz enhances the users' capability to utilize DL systems effectively, thereby achieving the value of being capable in a software context.",Agreed-Justified,Agree,,Agree,,
Exp D,Exp I,615,ICSE,Code Generation & Analysis,Demystifying the vulnerability propagation and its evolution via dependency trees in the npm ecosystem,"Third-party libraries with rich functionalities facilitate the fast development of JavaScript software, leading to the explosive growth of the NPM ecosystem. However, it also brings new security threats that vulnerabilities could be introduced through dependencies from third-party libraries. In particular, the threats could be excessively amplified by transitive dependencies. Existing research only considers direct dependencies or reasoning transitive dependencies based on reachability analysis, which neglects the NPM-specific dependency resolution rules as adapted during real installation, resulting in wrongly resolved dependencies. Consequently, further fine-grained analysis, such as precise vulnerability propagation and their evolution over time in dependencies, cannot be carried out precisely at a large scale, as well as deriving ecosystem-wide solutions for vulnerabilities in dependencies. To fill this gap, we propose a knowledge graph-based dependency resolution, which resolves the inner dependency relations of dependencies as trees (i.e., dependency trees), and investigates the security threats from vulnerabilities in dependency trees at a large scale. Specifically, we first construct a complete dependency-vulnerability knowledge graph (DVGraph) that captures the whole NPM ecosystem (over 10 million library versions and 60 million well-resolved dependency relations). Based on it, we propose a novel algorithm (DTResolver) to statically and precisely resolve dependency trees, as well as transitive vulnerability propagation paths, for each package by taking the official dependency resolution rules into account. Based on that, we carry out an ecosystem-wide empirical study on vulnerability propagation and its evolution in dependency trees. Our study unveils lots of useful findings, and we further discuss the lessons learned and solutions for different stakeholders to mitigate the vulnerability impact in NPM based on our findings. For example, we implement a dependency tree based vulnerability remediation method (DTReme) for NPM packages, and receive much better performance than the official tool (npm audit fix).",Security,Healthy,The paper proposes a method to increase security and mitigate vulnerability threats in the NPM ecosystem. This aligns with the value item Healthy and its corresponding value Security.,"The proposed method in 'Paper X' directly aligns with the value item Healthy and its corresponding value Security because it focuses on resolving vulnerabilities and increasing the overall security of the NPM ecosystem. By constructing a knowledge graph-based dependency resolution and analyzing vulnerability propagation in dependency trees, the paper aims to provide solutions for mitigating the vulnerability impact in NPM packages. A more secure software environment contributes to the overall health and well-being of users, as it reduces the risk of potential security breaches and potential harm that could arise from vulnerabilities in third-party libraries.",Agreed-Clarified,Agree,,Agree,,
Exp E,Exp J,615,ICSE,Code Generation & Analysis,Demystifying the vulnerability propagation and its evolution via dependency trees in the npm ecosystem,"Third-party libraries with rich functionalities facilitate the fast development of JavaScript software, leading to the explosive growth of the NPM ecosystem. However, it also brings new security threats that vulnerabilities could be introduced through dependencies from third-party libraries. In particular, the threats could be excessively amplified by transitive dependencies. Existing research only considers direct dependencies or reasoning transitive dependencies based on reachability analysis, which neglects the NPM-specific dependency resolution rules as adapted during real installation, resulting in wrongly resolved dependencies. Consequently, further fine-grained analysis, such as precise vulnerability propagation and their evolution over time in dependencies, cannot be carried out precisely at a large scale, as well as deriving ecosystem-wide solutions for vulnerabilities in dependencies. To fill this gap, we propose a knowledge graph-based dependency resolution, which resolves the inner dependency relations of dependencies as trees (i.e., dependency trees), and investigates the security threats from vulnerabilities in dependency trees at a large scale. Specifically, we first construct a complete dependency-vulnerability knowledge graph (DVGraph) that captures the whole NPM ecosystem (over 10 million library versions and 60 million well-resolved dependency relations). Based on it, we propose a novel algorithm (DTResolver) to statically and precisely resolve dependency trees, as well as transitive vulnerability propagation paths, for each package by taking the official dependency resolution rules into account. Based on that, we carry out an ecosystem-wide empirical study on vulnerability propagation and its evolution in dependency trees. Our study unveils lots of useful findings, and we further discuss the lessons learned and solutions for different stakeholders to mitigate the vulnerability impact in NPM based on our findings. For example, we implement a dependency tree based vulnerability remediation method (DTReme) for NPM packages, and receive much better performance than the official tool (npm audit fix).",Security,Social Order,By proposing an approach to improve security threats from vulnerabilities in dependencies; the paper contributes to maintaining order in the software. This aligns with the value item Social Order and its corresponding value Security.,"The paper's contribution to maintaining security in software aligns with the value item Social Order because social order refers to the maintenance of stability and harmony within a society. In the context of software, social order can be seen as the establishment and maintenance of security measures to ensure the smooth functioning of software systems. By addressing security threats through the proposed approach, the paper directly aligns with the value item of social order as it contributes to the stability and harmony of the software ecosystem.",Agreed-Clarified,Agree,,Agree,,
Exp G,Exp K,617,ICSE,Code Generation & Analysis,VulCNN: An image-inspired scalable vulnerability detection system,"Since deep learning (DL) can automatically learn features from source code, it has been widely used to detect source code vulnerability. To achieve scalable vulnerability scanning, some prior studies intend to process the source code directly by treating them as text. To achieve accurate vulnerability detection, other approaches consider distilling the program semantics into graph representations and using them to detect vulnerability. In practice, text-based techniques are scalable but not accurate due to the lack of program semantics. Graph-based methods are accurate but not scalable since graph analysis is typically time-consuming. In this paper, we aim to achieve both scalability and accuracy on scanning large-scale source code vulnerabilities. Inspired by existing DL-based image classification which has the ability to analyze millions of images accurately, we prefer to use these techniques to accomplish our purpose. Specifically, we propose a novel idea that can efficiently convert the source code of a function into an image while preserving the program details. We implement Vul-CNN and evaluate it on a dataset of 13,687 vulnerable functions and 26,970 non-vulnerable functions. Experimental results report that VulCNN can achieve better accuracy than eight state-of-the-art vul-nerability detectors (i.e., Checkmarx, FlawFinder, RATS, TokenCNN, VulDeePecker, SySeVR, VulDeeLocator, and Devign). As for scalability, VulCNN is about four times faster than VulDeePecker and SySeVR, about 15 times faster than VulDeeLocator, and about six times faster than Devign. Furthermore, we conduct a case study on more than 25 million lines of code and the result indicates that VulCNN can detect large-scale vulnerability. Through the scanning reports, we finally discover 73 vulnerabilities that are not reported in NVD.",Achievement,Successful,The paper contributes to a software; VulCNN; which enables better accuracy in vulnerability detection. This aligns with the value item Successful and its corresponding value Achievement.,"The justification for aligning 'Paper X' with the value item Successful and its corresponding value Achievement is based on the fact that the paper introduces VulCNN, a software that improves the accuracy of vulnerability detection. By achieving better accuracy than existing state-of-the-art vulnerability detectors, the paper demonstrates a successful contribution in the field of software security, which aligns with the value of achieving success and making advancements in the software domain.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,617,ICSE,Code Generation & Analysis,VulCNN: An image-inspired scalable vulnerability detection system,"Since deep learning (DL) can automatically learn features from source code, it has been widely used to detect source code vulnerability. To achieve scalable vulnerability scanning, some prior studies intend to process the source code directly by treating them as text. To achieve accurate vulnerability detection, other approaches consider distilling the program semantics into graph representations and using them to detect vulnerability. In practice, text-based techniques are scalable but not accurate due to the lack of program semantics. Graph-based methods are accurate but not scalable since graph analysis is typically time-consuming. In this paper, we aim to achieve both scalability and accuracy on scanning large-scale source code vulnerabilities. Inspired by existing DL-based image classification which has the ability to analyze millions of images accurately, we prefer to use these techniques to accomplish our purpose. Specifically, we propose a novel idea that can efficiently convert the source code of a function into an image while preserving the program details. We implement Vul-CNN and evaluate it on a dataset of 13,687 vulnerable functions and 26,970 non-vulnerable functions. Experimental results report that VulCNN can achieve better accuracy than eight state-of-the-art vul-nerability detectors (i.e., Checkmarx, FlawFinder, RATS, TokenCNN, VulDeePecker, SySeVR, VulDeeLocator, and Devign). As for scalability, VulCNN is about four times faster than VulDeePecker and SySeVR, about 15 times faster than VulDeeLocator, and about six times faster than Devign. Furthermore, we conduct a case study on more than 25 million lines of code and the result indicates that VulCNN can detect large-scale vulnerability. Through the scanning reports, we finally discover 73 vulnerabilities that are not reported in NVD.",Security,Healthy,The paper proposes VulCNN for efficient vulnerability detection in source code; contributing to the value item Healthy and its corresponding value Security as it can protect the software system's health by detecting vulnerabilities.,"The justification for aligning 'Paper X' with the value item Healthy and its corresponding value Security from a ""Software User"" perspective is based on the fact that the proposed VulCNN can efficiently detect vulnerabilities in source code. By detecting and addressing vulnerabilities, the software system's health and security are protected. This aligns with the value item Healthy, as a secure system ensures the well-being and stability of the software, and it aligns with the corresponding value Security, as it directly contributes to safeguarding the software against potential threats or attacks.",Agreed-Justified,Disagree,"Related to Achievement: capable more, as it automatically identifies vulnerabilities",Disagree,"The justification claiming that the paper aligns with the value 'Security' because of 'The paper proposes VulCNN for efficient vulnerability detection in source code; contributing to the value item Healthy and its corresponding value Security as it can protect the software system's health by detecting vulnerabilities. ' is not supported by the evidence in the abstract, which focuses on 'Since deep learning (DL) can automatically learn features from source code, it has been widely used.",
Exp B,Exp H,617,ICSE,Code Generation & Analysis,VulCNN: An image-inspired scalable vulnerability detection system,"Since deep learning (DL) can automatically learn features from source code, it has been widely used to detect source code vulnerability. To achieve scalable vulnerability scanning, some prior studies intend to process the source code directly by treating them as text. To achieve accurate vulnerability detection, other approaches consider distilling the program semantics into graph representations and using them to detect vulnerability. In practice, text-based techniques are scalable but not accurate due to the lack of program semantics. Graph-based methods are accurate but not scalable since graph analysis is typically time-consuming. In this paper, we aim to achieve both scalability and accuracy on scanning large-scale source code vulnerabilities. Inspired by existing DL-based image classification which has the ability to analyze millions of images accurately, we prefer to use these techniques to accomplish our purpose. Specifically, we propose a novel idea that can efficiently convert the source code of a function into an image while preserving the program details. We implement Vul-CNN and evaluate it on a dataset of 13,687 vulnerable functions and 26,970 non-vulnerable functions. Experimental results report that VulCNN can achieve better accuracy than eight state-of-the-art vul-nerability detectors (i.e., Checkmarx, FlawFinder, RATS, TokenCNN, VulDeePecker, SySeVR, VulDeeLocator, and Devign). As for scalability, VulCNN is about four times faster than VulDeePecker and SySeVR, about 15 times faster than VulDeeLocator, and about six times faster than Devign. Furthermore, we conduct a case study on more than 25 million lines of code and the result indicates that VulCNN can detect large-scale vulnerability. Through the scanning reports, we finally discover 73 vulnerabilities that are not reported in NVD.",Universalism,Protecting the Environment,The paper includes a large-scale case study indicating that VulCNN can detect large-scale vulnerability; preventing potential damage to the software environment; which aligns with the value item Protecting the Environment and its corresponding value Universalism.,"In the paper, the authors propose Vul-CNN, a novel approach that converts source code into images to efficiently detect source code vulnerabilities. They conduct a large-scale case study and are able to detect 73 vulnerabilities that were not reported before. By identifying and preventing these vulnerabilities, the paper contributes to protecting the software environment. This aligns with the value item ""Protecting the Environment"" from Schwartz's Taxonomy. Additionally, the approach aims to achieve scalability and accuracy, which can be considered as values of Universalism, as it emphasizes broad and equal benefits for software users. Therefore, the main contributions of the paper directly align with the value item Protecting the Environment and its corresponding value Universalism.",Agreed-Clarified,Agree,reconcile with coder_2,Agree,,
Exp C,Exp I,618,ICSE,Code Generation & Analysis,Towards automatically repairing compatibility issues in published android apps,"The heavy fragmentation of the Android ecosystem has led to se-vere compatibility issues with apps, including those that crash at runtime or cannot be installed on certain devices but work well on other devices. To address this problem, various approaches have been proposed to detect and fix compatibility issues automatically. However, these all come with various limitations on fixing the com-patibility issues, e.g., can only fix one specific type of issues, cannot deal with multi-invocation issues in a single line and issues in re-leased apps. To overcome these limitations, we propose a generic approach that aims at fixing more types of compatibility issues in released Android apps. To this end, our prototype tool, Repair-Droid, provides a generic app patch description language for users to create fix templates for compatibility issues. The created tem-plates will then be leveraged by RepairDroid to automatically fix the corresponding issue at the bytecode level (e.g., right before users install the app). RepairDroid can support template creations for OS-induced, device-specific and inter-callback compatibility issues detected by three state-of-the-art approaches. Our experimental re-sults show that RepairDroid can fix 7,660 out of 8,976 compatibility issues in 1,000 randomly selected Google Play apps. RepairDroid is generic to configure new compatibility issues and outperforms the state-of-the-art on effectively repairing compatibility issues in released Android apps.",Security,Healthy,RepairDroid aims at fixing more types of compatibility issues in released Android apps; enhancing the usage experience and indirectly contributing to the software users' digital health in terms of using apps free from issues; crashing; or inability to install; which aligns with the value item Healthy and its corresponding value Security.,"In ""Paper X,"" RepairDroid aims to fix compatibility issues in Android apps, which directly contributes to the software users' digital health and security. By addressing issues such as crashing and inability to install, RepairDroid enhances the overall usage experience for software users and ensures they can use apps without encountering any detrimental effects, thus aligning with the value item Healthy and its corresponding value Security in Schwartz's Taxonomy.",Agreed-Clarified,Agree,,Agree,,
Exp D,Exp I,618,ICSE,Code Generation & Analysis,Towards automatically repairing compatibility issues in published android apps,"The heavy fragmentation of the Android ecosystem has led to se-vere compatibility issues with apps, including those that crash at runtime or cannot be installed on certain devices but work well on other devices. To address this problem, various approaches have been proposed to detect and fix compatibility issues automatically. However, these all come with various limitations on fixing the com-patibility issues, e.g., can only fix one specific type of issues, cannot deal with multi-invocation issues in a single line and issues in re-leased apps. To overcome these limitations, we propose a generic approach that aims at fixing more types of compatibility issues in released Android apps. To this end, our prototype tool, Repair-Droid, provides a generic app patch description language for users to create fix templates for compatibility issues. The created tem-plates will then be leveraged by RepairDroid to automatically fix the corresponding issue at the bytecode level (e.g., right before users install the app). RepairDroid can support template creations for OS-induced, device-specific and inter-callback compatibility issues detected by three state-of-the-art approaches. Our experimental re-sults show that RepairDroid can fix 7,660 out of 8,976 compatibility issues in 1,000 randomly selected Google Play apps. RepairDroid is generic to configure new compatibility issues and outperforms the state-of-the-art on effectively repairing compatibility issues in released Android apps.",Self Direction,Freedom,RepairDroid allows users to create fix templates for compatibility issues indicating a level of freedom in customization; which aligns with the value item Freedom and its corresponding value Self Direction.,"RepairDroid provides users with the ability to create fix templates for compatibility issues, allowing them to customize the fixes according to their own preferences and needs. This level of freedom aligns with the value item ""Freedom"" and its corresponding value ""Self Direction"" from Schwartz's Taxonomy. By empowering software users with the freedom to personalize the fixes, RepairDroid promotes self-direction and autonomy in the software context, thus demonstrating a direct alignment with the value item ""Freedom.""",Agreed-Justified,Agree,,Agree,,
Exp E,Exp J,618,ICSE,Code Generation & Analysis,Towards automatically repairing compatibility issues in published android apps,"The heavy fragmentation of the Android ecosystem has led to se-vere compatibility issues with apps, including those that crash at runtime or cannot be installed on certain devices but work well on other devices. To address this problem, various approaches have been proposed to detect and fix compatibility issues automatically. However, these all come with various limitations on fixing the com-patibility issues, e.g., can only fix one specific type of issues, cannot deal with multi-invocation issues in a single line and issues in re-leased apps. To overcome these limitations, we propose a generic approach that aims at fixing more types of compatibility issues in released Android apps. To this end, our prototype tool, Repair-Droid, provides a generic app patch description language for users to create fix templates for compatibility issues. The created tem-plates will then be leveraged by RepairDroid to automatically fix the corresponding issue at the bytecode level (e.g., right before users install the app). RepairDroid can support template creations for OS-induced, device-specific and inter-callback compatibility issues detected by three state-of-the-art approaches. Our experimental re-sults show that RepairDroid can fix 7,660 out of 8,976 compatibility issues in 1,000 randomly selected Google Play apps. RepairDroid is generic to configure new compatibility issues and outperforms the state-of-the-art on effectively repairing compatibility issues in released Android apps.",Achievement,Influential,By using the RepairDroid tool; software users indirectly are given the power to fix apps's issuess and thus be influential over their digital environment; aligning with the value item Influential and its corresponding value Achievement.,"In utilizing the RepairDroid tool, software users are empowered to directly address compatibility issues within their digital environment, enabling them to exercise influence over the performance and functioning of their chosen apps. This aligns with the value item Influential and its corresponding value Achievement from a ""Software User"" perspective, as it allows individuals to take control and achieve the desired outcomes in their software experience.",Agreed-Justified,Agree,,Agree,,
Exp F,Exp J,622,ICSE,Software Project Management,What makes effective leadership in agile software development teams?,"Effective leadership is one of the key drivers of business and project success, and one of the most active areas of management research. But how does leadership work in agile software development, which emphasizes self-management and self-organization and marginalizes traditional leadership roles? To find out, this study examines agile leadership from the perspective of thirteen professionals who identify as agile leaders, in different roles, at ten different software development companies of varying sizes. Data from semi-structured interviews reveals that leadership: (1) is dynamically shared among team members; (2) engenders a sense of belonging to the team; and (3) involves balancing competing organizational cultures (e.g. balancing the new agile culture with the old milestone-driven culture). In other words, agile leadership is a property of a team, not a role, and effectiveness depends on agile team members' identifying with the team, accepting responsibility, and being sensitive to cultural conflict.",Security,Family Security,The abstract articulates leadership in agile software development creates a sense of belonging within the team; thus creating a more secure environment for the software user.,"In the abstract of 'Paper X', it is mentioned that agile leadership in software development engenders a sense of belonging to the team. This implies that the team members, including software users, feel secure and supported within the team environment. This alignment with the value item Family Security and its corresponding value Security from a ""Software User"" perspective can be seen as the software user becomes part of a team that values their security and provides a safe and stable atmosphere for their work and contributions.",Agreed-Justified,Disagree,,Disagree,"Strongly, this paper is about leadership in agile team not mentioning any points about security or family security.",
Exp G,Exp K,622,ICSE,Software Project Management,What makes effective leadership in agile software development teams?,"Effective leadership is one of the key drivers of business and project success, and one of the most active areas of management research. But how does leadership work in agile software development, which emphasizes self-management and self-organization and marginalizes traditional leadership roles? To find out, this study examines agile leadership from the perspective of thirteen professionals who identify as agile leaders, in different roles, at ten different software development companies of varying sizes. Data from semi-structured interviews reveals that leadership: (1) is dynamically shared among team members; (2) engenders a sense of belonging to the team; and (3) involves balancing competing organizational cultures (e.g. balancing the new agile culture with the old milestone-driven culture). In other words, agile leadership is a property of a team, not a role, and effectiveness depends on agile team members' identifying with the team, accepting responsibility, and being sensitive to cultural conflict.",Benevolence,Responsibility,The abstract explains that agile leadership is about team members taking responsibility. This implies software they are developing could be reliable and trustworthy thus aligning with the value item 'Responsibility'.,"In the abstract, it is stated that agile leadership involves team members taking responsibility. This implies that the software being developed under agile leadership would be reliable and trustworthy, aligning with the value item of Responsibility. As a software user, relying on responsible development practices is essential for ensuring the quality and dependability of the software, making this alignment a relevant and important consideration.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,622,ICSE,Software Project Management,What makes effective leadership in agile software development teams?,"Effective leadership is one of the key drivers of business and project success, and one of the most active areas of management research. But how does leadership work in agile software development, which emphasizes self-management and self-organization and marginalizes traditional leadership roles? To find out, this study examines agile leadership from the perspective of thirteen professionals who identify as agile leaders, in different roles, at ten different software development companies of varying sizes. Data from semi-structured interviews reveals that leadership: (1) is dynamically shared among team members; (2) engenders a sense of belonging to the team; and (3) involves balancing competing organizational cultures (e.g. balancing the new agile culture with the old milestone-driven culture). In other words, agile leadership is a property of a team, not a role, and effectiveness depends on agile team members' identifying with the team, accepting responsibility, and being sensitive to cultural conflict.",Self Direction,Choosing Own Goals,The abstract shows contributing in an agile team leadership could allow software users to have input; letting them have a role in setting their own goals; thus aligning with the value item 'Choosing Own Goals'.,"The justification for aligning 'Paper X' with the value item ""Choosing Own Goals"" and its corresponding value ""Self Direction"" from a ""Software User"" perspective is based on the abstract's mention of agile leadership being dynamically shared among team members. This implies that software users, as part of the agile team, have a role in setting their own goals and have autonomy in directing their work. This aligns with the value of self-direction, as software users are empowered to make choices and decisions related to their goals within the agile software development process.",Agreed-Justified,Agree,,Agree,,
Exp B,Exp H,632,ESEC/FSE,Code Generation & Analysis,All you need is logs: improving code completion by learning from anonymous IDE usage logs,"In this work, we propose an approach for collecting completion usage logs from the users in an IDE and using them to train a machine learning based model for ranking completion candidates. We developed a set of features that describe completion candidates and their context, and deployed their anonymized collection in the Early Access Program of IntelliJ-based IDEs. We used the logs to collect a dataset of code completions from users, and employed it to train a ranking CatBoost model. Then, we evaluated it in two settings: on a held-out set of the collected completions and in a separate A/B test on two different groups of users in the IDE. Our evaluation shows that using a simple ranking model trained on the past user behavior logs significantly improved code completion experience. Compared to the default heuristics-based ranking, our model demonstrated a decrease in the number of typing actions necessary to perform the completion in the IDE from 2.073 to 1.832. The approach adheres to privacy requirements and legal constraints, since it does not require collecting personal information, performing all the necessary anonymization on the client's side. Importantly, it can be improved continuously: implementing new features, collecting new data, and evaluating new models - this way, we have been using it in production since the end of 2020.",Stimulation,Variation in Life,The paper contributes an approach that improves the variation in the code completion experience for the user by proposing a machine learning model based on user's past behavior. This directly aligns with the value item 'Variation in Life' and its corresponding value 'Stimulation' as it brings variation to the user's experience with the integrated development environment (IDE).,"The justification for aligning 'Paper X' with the value item Variation in Life and its corresponding value Stimulation is based on the fact that the paper proposes a machine learning model that improves the code completion experience for users. By incorporating this model, users are exposed to a greater variety of completion suggestions, leading to a more diverse and stimulating experience within the IDE. This aligns with the value of Stimulation, as it introduces novelty and excitement into the user's coding process, thereby supporting the value item Variation in Life.",Agreed-Clarified,Agree,,Agree,,
Exp D,Exp I,632,ESEC/FSE,Code Generation & Analysis,All you need is logs: improving code completion by learning from anonymous IDE usage logs,"In this work, we propose an approach for collecting completion usage logs from the users in an IDE and using them to train a machine learning based model for ranking completion candidates. We developed a set of features that describe completion candidates and their context, and deployed their anonymized collection in the Early Access Program of IntelliJ-based IDEs. We used the logs to collect a dataset of code completions from users, and employed it to train a ranking CatBoost model. Then, we evaluated it in two settings: on a held-out set of the collected completions and in a separate A/B test on two different groups of users in the IDE. Our evaluation shows that using a simple ranking model trained on the past user behavior logs significantly improved code completion experience. Compared to the default heuristics-based ranking, our model demonstrated a decrease in the number of typing actions necessary to perform the completion in the IDE from 2.073 to 1.832. The approach adheres to privacy requirements and legal constraints, since it does not require collecting personal information, performing all the necessary anonymization on the client's side. Importantly, it can be improved continuously: implementing new features, collecting new data, and evaluating new models - this way, we have been using it in production since the end of 2020.",Self Direction,Independent,The paper's main contribution is a self-improving system that adapts to the users' behaviour in order to optimize code completion in the IDEs. As this approach enables users to work more independently; it aligns with the  'Independent' Value item from the 'Self Direction' Value.,"The main contribution of 'Paper X' is a machine learning-based model for ranking completion candidates in an IDE. This model utilizes completion usage logs collected from users to improve the code completion experience. By adapting to users' behavior and optimizing code completion, the system empowers software users to work more independently, as they can perform completions with fewer typing actions. This alignment with the value item of ""Independent"" from the ""Self Direction"" value in Schwartz's Taxonomy is evident from the abstract as it directly reflects the aim of enabling users to have freedom, choice in setting their goals, and making their own decisions while using the IDE.",Agreed-Clarified,Agree,,Agree,,
Exp E,Exp J,633,ESEC/FSE,AI & Machine Learning,Machine learning and natural language processing for automating software testing (tutorial),"In this tutorial, we see how natural language processing and machine learning can help us address the open challenges of software testing. We overview the open challenges of testing autonomous and self-adaptive software systems, discuss the leading-edge technologies that can address the core issues, and see the latest progresses and future prospective of natural language processing and machine learning to cope with core problems. Automating test case and oracle generation are still largely open issues. Autonomous and self-adaptive systems, like self-driving cars, smart cities, and smart buildings, raise new issues that further toughen the already challenging scenarios. In the tutorial we understand the growing importance of field testing to address failures that emerge in production, the role of dynamic analysis and deep learning in revealing failure-prone scenarios, the need of symbolic fuzzing to explore unexpected scenarios, and the potentiality of reinforcement learning and natural language processing to generate test cases and oracles. We see in details state-of-the-art approaches that exploit natural language processing to automatically generate executable test oracles, as well as semantic matching, deep and reinforcement learning to automatically generate test cases and reveal failure-prone scenarios in production. The tutorial is designed for both researchers, whose research roadmap focuses on software testing and applications of natural language processing and machine learning to software engineering, and practitioners, who see important professional opportunities from autonomous and self-adaptive systems. It is particularly well suited to PhD students and postdoctoral researchers who aim to address new challenges with novel technologies. The tutorial is self-contained, and is designed for a software engineering audience, who many not have a specific background in natural language processing and machine learning.",Achievement,Capable,The tutorial overviews the open challenges of testing autonomous and self-adaptive software systems; discussing leading-edge technologies that facilitate competency in executing tasks; thus aligning with the value item 'Capable' and its corresponding value 'Achievement'.,"In the abstract of 'Paper X', it is explicitly stated that the tutorial discusses the leading-edge technologies that can address the core challenges of testing autonomous and self-adaptive software systems. By addressing these challenges and providing solutions, the tutorial enables software users to have capable systems that are competent in executing tasks. This aligns with the value item 'Capable' and its corresponding value 'Achievement' from Schwartz's Taxonomy.",Agreed-Clarified,Agree,,Agree,,
Exp F,Exp J,633,ESEC/FSE,AI & Machine Learning,Machine learning and natural language processing for automating software testing (tutorial),"In this tutorial, we see how natural language processing and machine learning can help us address the open challenges of software testing. We overview the open challenges of testing autonomous and self-adaptive software systems, discuss the leading-edge technologies that can address the core issues, and see the latest progresses and future prospective of natural language processing and machine learning to cope with core problems. Automating test case and oracle generation are still largely open issues. Autonomous and self-adaptive systems, like self-driving cars, smart cities, and smart buildings, raise new issues that further toughen the already challenging scenarios. In the tutorial we understand the growing importance of field testing to address failures that emerge in production, the role of dynamic analysis and deep learning in revealing failure-prone scenarios, the need of symbolic fuzzing to explore unexpected scenarios, and the potentiality of reinforcement learning and natural language processing to generate test cases and oracles. We see in details state-of-the-art approaches that exploit natural language processing to automatically generate executable test oracles, as well as semantic matching, deep and reinforcement learning to automatically generate test cases and reveal failure-prone scenarios in production. The tutorial is designed for both researchers, whose research roadmap focuses on software testing and applications of natural language processing and machine learning to software engineering, and practitioners, who see important professional opportunities from autonomous and self-adaptive systems. It is particularly well suited to PhD students and postdoctoral researchers who aim to address new challenges with novel technologies. The tutorial is self-contained, and is designed for a software engineering audience, who many not have a specific background in natural language processing and machine learning.",Security,Healthy,The tutorial discusses the importance of software testing in autonomous and self-adaptive systems which ensures such systems are robust and function reliably; thereby aligning with the value item 'Healthy' and its corresponding value 'Security'.,"By addressing the open challenges of software testing in autonomous and self-adaptive systems, the paper aligns with the value item 'Healthy' and its corresponding value 'Security'. The tutorial emphasizes the importance of testing in ensuring the robustness and reliability of such systems, which directly contributes to the overall security and well-being of software users. This alignment is evident in the abstract where the paper discusses the role of field testing, dynamic analysis, and deep learning to reveal failure-prone scenarios, ultimately aiming to enhance the security and healthiness of autonomous and self-adaptive systems.",Agreed-Justified,Agree,,Agree,,
Exp G,Exp K,633,ESEC/FSE,AI & Machine Learning,Machine learning and natural language processing for automating software testing (tutorial),"In this tutorial, we see how natural language processing and machine learning can help us address the open challenges of software testing. We overview the open challenges of testing autonomous and self-adaptive software systems, discuss the leading-edge technologies that can address the core issues, and see the latest progresses and future prospective of natural language processing and machine learning to cope with core problems. Automating test case and oracle generation are still largely open issues. Autonomous and self-adaptive systems, like self-driving cars, smart cities, and smart buildings, raise new issues that further toughen the already challenging scenarios. In the tutorial we understand the growing importance of field testing to address failures that emerge in production, the role of dynamic analysis and deep learning in revealing failure-prone scenarios, the need of symbolic fuzzing to explore unexpected scenarios, and the potentiality of reinforcement learning and natural language processing to generate test cases and oracles. We see in details state-of-the-art approaches that exploit natural language processing to automatically generate executable test oracles, as well as semantic matching, deep and reinforcement learning to automatically generate test cases and reveal failure-prone scenarios in production. The tutorial is designed for both researchers, whose research roadmap focuses on software testing and applications of natural language processing and machine learning to software engineering, and practitioners, who see important professional opportunities from autonomous and self-adaptive systems. It is particularly well suited to PhD students and postdoctoral researchers who aim to address new challenges with novel technologies. The tutorial is self-contained, and is designed for a software engineering audience, who many not have a specific background in natural language processing and machine learning.",Benevolence,A Spiritual Life,By discussing the potential of natural language processing and machine learning in software testing; the tutorial brings in a spiritual life to the field by introducing novel; enlightened ways of addressing problems; thus aligning with the value item 'A Spiritual Life' and its corresponding value 'Benevolence'.,"In the context of 'Paper X', the alignment with the value item A Spiritual Life and its corresponding value Benevolence can be justified by considering the impact of natural language processing and machine learning on software testing. These technologies offer innovative approaches to address the challenges of software testing, contributing to the betterment of software systems and the overall user experience. By enabling more efficient and effective testing methods, 'Paper X' promotes a sense of benevolence towards software users, as it seeks to enhance the quality and reliability of software systems, ultimately benefiting the individuals who use them. This aligns with the value item A Spiritual Life, as it exemplifies the pursuit of higher ideals and the concern for the well-being of others, in this case, the software users.",Agreed-Justified,Agree,,Agree,,
Exp C,Exp I,636,ICSE,Code Generation & Analysis,Fairness-aware configuration of machine learning libraries,"This paper investigates the parameter space of machine learning (ML) algorithms in aggravating or mitigating fairness bugs. Data-driven software is increasingly applied in social-critical applications where ensuring fairness is of paramount importance. The existing approaches focus on addressing fairness bugs by either modifying the input dataset or modifying the learning algorithms. On the other hand, the selection of hyperparameters, which provide finer controls of ML algorithms, may enable a less intrusive approach to influence the fairness. Can hyperparameters amplify or suppress discrimination present in the input dataset? How can we help programmers in detecting, understanding, and exploiting the role of hyperparameters to improve the fairness? We design three search-based software testing algorithms to un-cover the precision-fairness frontier of the hyperparameter space. We complement these algorithms with statistical debugging to explain the role of these parameters in improving fairness. We implement the proposed approaches in the tool Parfait-ML (PARameter FAIrness Testing for ML Libraries) and show its effectiveness and utility over five mature ML algorithms as used in six social-critical applications. In these applications, our approach successfully iden-tified hyperparameters that significantly improve (vis-a-vis the state-of-the-art techniques) the fairness without sacrificing precision. Surprisingly, for some algorithms (e.g., random forest), our approach showed that certain configuration of hyperparameters (e.g., restricting the search space of attributes) can amplify biases across applications. Upon further investigation, we found intuitive explanations of these phenomena, and the results corroborate simi-lar observations from the literature.",Universalism,Social Justice,The tool implemented as part of the research efforts; Parfait-ML; plays a crucial role in improving fairness in machine learning libraries; thereby contributing to social justice. This aligns with the value item Social Justice and its corresponding value Universalism.,"The justification for labeling 'Paper X' as aligning with the value item Social Justice and its corresponding value Universalism is based on the fact that the implementation of the tool Parfait-ML in the research efforts of the paper aims to improve fairness in machine learning libraries. By addressing fairness concerns in data-driven software, the paper contributes to promoting equal opportunities and equitable outcomes, which are fundamental principles of social justice. The focus on fairness and the use of the Parfait-ML tool align with the value item Social Justice and its corresponding value Universalism, which emphasizes the importance of promoting fairness, equality, and justice for all individuals in society.",Agreed-Clarified,Agree,,Agree,,
Exp D,Exp I,649,ICSE,Software Engineering Practices,Autonomy is an acquired taste: Exploring developer preferences for GitHub bots,"Software bots fulfill an important role in collective software development, and their adoption by developers promises increased productivity. Past research has identified that bots that communicate too often can irritate developers, which affects the utility of the bot. However, it is not clear what other properties of human-bot collaboration affect developers' preferences, or what impact these properties might have. The main idea of this paper is to explore characteristics affecting developer preferences for interactions between humans and bots, in the context of GitHub pull requests. We carried out an exploratory sequential study with interviews and a subsequent vignette-based survey. We find developers generally prefer bots that are personable but show little autonomy, however, more experienced developers tend to prefer more autonomous bots. Based on this empirical evidence, we recommend bot developers increase configuration options for bots so that individual developers and projects can configure bots to best align with their own preferences and project cultures.",Achievement,Successful,The paper examines the ways in which software bots can be configured to increase developer productivity and align with individual preferences and project cultures. This aligns with the value item Successful and its corresponding value Achievement; as it aids the software users in their pursuit of success within their context.,"The paper's exploration of configuring software bots to align with individual preferences and project cultures directly aligns with the value item Successful and its corresponding value Achievement. By providing developers with the means to customize their bot interactions, the paper aims to enhance productivity and ultimately contribute to the software users' pursuit of success in their software development endeavors.",Agreed-Justified,Agree,,Agree,,
Exp F,Exp J,649,ICSE,Software Engineering Practices,Autonomy is an acquired taste: Exploring developer preferences for GitHub bots,"Software bots fulfill an important role in collective software development, and their adoption by developers promises increased productivity. Past research has identified that bots that communicate too often can irritate developers, which affects the utility of the bot. However, it is not clear what other properties of human-bot collaboration affect developers' preferences, or what impact these properties might have. The main idea of this paper is to explore characteristics affecting developer preferences for interactions between humans and bots, in the context of GitHub pull requests. We carried out an exploratory sequential study with interviews and a subsequent vignette-based survey. We find developers generally prefer bots that are personable but show little autonomy, however, more experienced developers tend to prefer more autonomous bots. Based on this empirical evidence, we recommend bot developers increase configuration options for bots so that individual developers and projects can configure bots to best align with their own preferences and project cultures.",Power,Preserving My Public Image,The paper provides recommendation to bot developers suggesting an increase in configuration options for software bots aligning with developers' preferences and project cultures. This keeps developers' public image aligned with software usefulness and efficiency; preserving their public image; which aligns with the value item Preserving My Public Image and its corresponding value Power.,"In the context of the software development community, where individuals' public image and reputation can have significant impacts on their professional success and opportunities, aligning with the value item Preserving My Public Image and its corresponding value Power is essential. The paper recommends increasing configuration options for software bots, allowing developers to tailor the bots to their preferences and project cultures. By doing so, developers can maintain control over the behavior and interactions of the bots, ensuring that they align with their own goals and meet their desired standards of efficiency and usefulness. This control over the bots helps developers maintain a positive public image by ensuring that the bots are perceived as valuable tools that enhance their productivity and effectiveness, rather than irritants or distractions. Therefore, the alignment with Preserving My Public Image and Power values is evident as it focuses on the significance of maintaining developers' reputation and influence within the software development community.",Agreed-Justified,Disagree,,Disagree,The paper talks about how developers should develop bots so they will be choice of developers.,
Exp G,Exp K,654,ICSE,Security & Privacy,Measuring secure coding practice and culture: A finger pointing at the moon is not the moon,"Software security research has a core problem: it is impossible to prove the security of complex software. A low number of known defects may simply indicate that the software has not been attacked yet, or that successful attacks have not been detected. A high defect count may be the result of white-hat hacker targeting, or of a successful bug bounty program which prevented insecurities from persisting in the wild. This makes it difficult to measure the security of non-trivial software. Researchers instead usually measure effort directed towards ensuring software security. However, different researchers use their own tailored measures, usually devised from industry secure coding guidelines. Not only is there no agreed way to measure effort, there is also no agreement on what effort entails. Qualitative studies emphasise the importance of security culture in an organisation. Where software security practices are introduced solely to ensure compliance with legislative or industry standards, a box-ticking attitude to security may result. The security culture may be weak or non-existent, making it likely that precautions not explicitly mentioned in the standards will be missed. Thus, researchers need both a way to assess software security practice and a way to measure software security culture. To assess security practice, we converted the empirically-established 12 most common software security activities into questions. To assess security culture, we devised a number of questions grounded in prior literature. We ran a secure development survey with both sets of questions, obtaining organic responses from 1,100 software coders in 59 countries. We used proven common activities to assess security practice, and made a first attempt to quantitatively assess aspects of security culture in the broad developer population. Our results show that some coders still work in environments where there is little to no attempt to ensure code security. Security practice and culture do not always correlate, and some organisations with strong secure coding practice have weak secure coding culture. This may lead to problems in defect prevention and sustained software security effort.",Security,Healthy,The paper contributes to improving the software's security system thereby making it healthier for end users. This aligns with the value item Healthy and its corresponding value Security.,"In 'Paper X', the main contribution is focused on assessing software security practices and culture. By improving the software's security system and ensuring compliance with industry standards, the paper aims to create a secure environment for end users. This aligns with the value item Healthy, as it promotes the well-being and protection of software users by safeguarding their data and privacy. Security is a fundamental aspect of ensuring the overall health and trustworthiness of software systems, making it a relevant alignment with the value item Healthy from a user's perspective.",Agreed-Clarified,Agree,,Agree,,
Exp A,Exp H,654,ICSE,Security & Privacy,Measuring secure coding practice and culture: A finger pointing at the moon is not the moon,"Software security research has a core problem: it is impossible to prove the security of complex software. A low number of known defects may simply indicate that the software has not been attacked yet, or that successful attacks have not been detected. A high defect count may be the result of white-hat hacker targeting, or of a successful bug bounty program which prevented insecurities from persisting in the wild. This makes it difficult to measure the security of non-trivial software. Researchers instead usually measure effort directed towards ensuring software security. However, different researchers use their own tailored measures, usually devised from industry secure coding guidelines. Not only is there no agreed way to measure effort, there is also no agreement on what effort entails. Qualitative studies emphasise the importance of security culture in an organisation. Where software security practices are introduced solely to ensure compliance with legislative or industry standards, a box-ticking attitude to security may result. The security culture may be weak or non-existent, making it likely that precautions not explicitly mentioned in the standards will be missed. Thus, researchers need both a way to assess software security practice and a way to measure software security culture. To assess security practice, we converted the empirically-established 12 most common software security activities into questions. To assess security culture, we devised a number of questions grounded in prior literature. We ran a secure development survey with both sets of questions, obtaining organic responses from 1,100 software coders in 59 countries. We used proven common activities to assess security practice, and made a first attempt to quantitatively assess aspects of security culture in the broad developer population. Our results show that some coders still work in environments where there is little to no attempt to ensure code security. Security practice and culture do not always correlate, and some organisations with strong secure coding practice have weak secure coding culture. This may lead to problems in defect prevention and sustained software security effort.",Security,Family Security,The paper outlines a method for more effectively securing software; which in turn contributes to the Security of the user's digital 'Family'; or network of devices. This aligns with the value item Family Security and its corresponding value Security.,"The paper's method for enhancing software security directly contributes to the overall security of a user's digital family or network of devices. By addressing vulnerabilities and preventing potential attacks, the paper aligns with the value item of Family Security, which emphasizes ensuring the protection and safety of one's loved ones or close relationships. This alignment is rooted in the value of Security, as the paper's contributions aim to provide a secure software environment for users and their families.",Agreed-Justified,Disagree,reconcile with coder_2,Disagree,The justification claiming that the paper aligns with the value 'Security' because of 'The paper outlines a method for more effectively securing software; which in turn contributes to the Security of the user's digital 'Family'; or network of devices.  This aligns with the value item Family Security and its corresponding value Security.,
Exp B,Exp H,654,ICSE,Security & Privacy,Measuring secure coding practice and culture: A finger pointing at the moon is not the moon,"Software security research has a core problem: it is impossible to prove the security of complex software. A low number of known defects may simply indicate that the software has not been attacked yet, or that successful attacks have not been detected. A high defect count may be the result of white-hat hacker targeting, or of a successful bug bounty program which prevented insecurities from persisting in the wild. This makes it difficult to measure the security of non-trivial software. Researchers instead usually measure effort directed towards ensuring software security. However, different researchers use their own tailored measures, usually devised from industry secure coding guidelines. Not only is there no agreed way to measure effort, there is also no agreement on what effort entails. Qualitative studies emphasise the importance of security culture in an organisation. Where software security practices are introduced solely to ensure compliance with legislative or industry standards, a box-ticking attitude to security may result. The security culture may be weak or non-existent, making it likely that precautions not explicitly mentioned in the standards will be missed. Thus, researchers need both a way to assess software security practice and a way to measure software security culture. To assess security practice, we converted the empirically-established 12 most common software security activities into questions. To assess security culture, we devised a number of questions grounded in prior literature. We ran a secure development survey with both sets of questions, obtaining organic responses from 1,100 software coders in 59 countries. We used proven common activities to assess security practice, and made a first attempt to quantitatively assess aspects of security culture in the broad developer population. Our results show that some coders still work in environments where there is little to no attempt to ensure code security. Security practice and culture do not always correlate, and some organisations with strong secure coding practice have weak secure coding culture. This may lead to problems in defect prevention and sustained software security effort.",Security,Social Order,The paper explores ways to reliably measure and enhance software security; contributing to maintaining the user's Social Order within digital spaces. This aligns with the value item Social Order and its corresponding value Security.,"In the paper abstract, it is mentioned that the lack of security in software can result in problems such as defect prevention and sustained software security effort. By focusing on assessing software security practice and culture, the paper aims to enhance security measures. For software users, this contributes to maintaining social order within digital spaces by ensuring a safer and more secure environment for their interactions and activities. This aligns with the value item Social Order and its corresponding value Security, as the paper's contributions directly address the importance of software security for users.",Agreed-Justified,Agree,,Agree,,
Exp C,Exp I,660,ICSE,Software Engineering Practices,Sustainability is stratified: Toward a better theory of sustainable software engineering,"Background: Sustainable software engineering (SSE) means creating software in a way that meets present needs without undermining our collective capacity to meet our future needs. It is typically conceptualized as several intersecting dimensions or ‚Äö√Ñ√∂‚àö√ë‚àö‚à´pillars‚Äö√Ñ√∂‚àö√ë‚àöœÄ-environmental, social, economic, technical and in-dividual. However; these pillars are theoretically underdeveloped and require refinement. Objectives: The objective of this paper is to generate a better theory of SSE. Method: First, a scoping review was conducted to understand the state of research on SSE and identify existing models thereof. Next, a meta-synthesis of qualitative research on SSE was conducted to critique and improve the existing models identified. Results: 961 potentially relevant articles were extracted from five article databases. These articles were de-duplicated and then screened independently by two screeners, leaving 243 articles to examine. Of these, 109 were non-empirical, the most common empirical method was systematic review, and no randomized controlled experiments were found. Most papers focus on ecological sustainability (158) and the sustainability of software products (148) rather than processes. A meta-synthesis of 36 qualitative studies produced several key propositions, most notably, that sustainability is stratified (has different meanings at different levels of abstraction) and multisystemic (emerges from interactions among multiple social, technical, and sociotechnical systems). Conclusion: The academic literature on SSE is surprisingly non-empirical. More empirical evaluations of specific sustainability interventions are needed. The sustainability of software development products and processes should be conceptualized as multisystemic and stratified, and assessed accordingly.",Universalism,Protecting the Environment,Paper X's focus on sustainable software engineering directly contributes to the value item Protecting the Environment which is part of Universalism value as it emphasizes on creating software in a way that meets present needs without undermining our collective capacity to meet our future needs.,"The main contribution of 'Paper X' aligns with the value item Protecting the Environment and its corresponding value Universalism from a ""Software User"" perspective because the paper explicitly states that sustainable software engineering aims to create software that meets present needs without undermining our collective capacity to meet our future needs. This focus on sustainability directly implies a commitment to protecting the environment by developing software in a way that minimizes negative environmental impacts and promotes long-term ecological well-being. Thus, the alignment with Protecting the Environment and Universalism values is evident in the stated objective and focus of the paper.",Agreed-Clarified,Agree,,Agree,,
Exp D,Exp I,660,ICSE,Software Engineering Practices,Sustainability is stratified: Toward a better theory of sustainable software engineering,"Background: Sustainable software engineering (SSE) means creating software in a way that meets present needs without undermining our collective capacity to meet our future needs. It is typically conceptualized as several intersecting dimensions or ‚Äö√Ñ√∂‚àö√ë‚àö‚à´pillars‚Äö√Ñ√∂‚àö√ë‚àöœÄ-environmental, social, economic, technical and in-dividual. However; these pillars are theoretically underdeveloped and require refinement. Objectives: The objective of this paper is to generate a better theory of SSE. Method: First, a scoping review was conducted to understand the state of research on SSE and identify existing models thereof. Next, a meta-synthesis of qualitative research on SSE was conducted to critique and improve the existing models identified. Results: 961 potentially relevant articles were extracted from five article databases. These articles were de-duplicated and then screened independently by two screeners, leaving 243 articles to examine. Of these, 109 were non-empirical, the most common empirical method was systematic review, and no randomized controlled experiments were found. Most papers focus on ecological sustainability (158) and the sustainability of software products (148) rather than processes. A meta-synthesis of 36 qualitative studies produced several key propositions, most notably, that sustainability is stratified (has different meanings at different levels of abstraction) and multisystemic (emerges from interactions among multiple social, technical, and sociotechnical systems). Conclusion: The academic literature on SSE is surprisingly non-empirical. More empirical evaluations of specific sustainability interventions are needed. The sustainability of software development products and processes should be conceptualized as multisystemic and stratified, and assessed accordingly.",Universalism,A World at Peace,"The paper's objective of creating a better theory of SSE also aligns with promoting ""A World at Peace;"" another item under the value Universalism. This is because optimizing software sustainability can contribute to social stability by addressing environmental concerns.","The paper's objective of creating a better theory of SSE aligns with promoting ""A World at Peace"" under the value Universalism because optimizing software sustainability can have a positive impact on environmental concerns, which in turn can contribute to the overall goal of social stability and a more peaceful world. By addressing the environmental dimensions of sustainable software engineering, such as reducing emissions and promoting eco-friendly practices, the paper's contributions align with the value of Universalism and the aspiration for a world that is more harmonious and at peace.",Agreed-Clarified,Agree,,Agree,,
Exp E,Exp J,660,ICSE,Software Engineering Practices,Sustainability is stratified: Toward a better theory of sustainable software engineering,"Background: Sustainable software engineering (SSE) means creating software in a way that meets present needs without undermining our collective capacity to meet our future needs. It is typically conceptualized as several intersecting dimensions or ‚Äö√Ñ√∂‚àö√ë‚àö‚à´pillars‚Äö√Ñ√∂‚àö√ë‚àöœÄ-environmental, social, economic, technical and in-dividual. However; these pillars are theoretically underdeveloped and require refinement. Objectives: The objective of this paper is to generate a better theory of SSE. Method: First, a scoping review was conducted to understand the state of research on SSE and identify existing models thereof. Next, a meta-synthesis of qualitative research on SSE was conducted to critique and improve the existing models identified. Results: 961 potentially relevant articles were extracted from five article databases. These articles were de-duplicated and then screened independently by two screeners, leaving 243 articles to examine. Of these, 109 were non-empirical, the most common empirical method was systematic review, and no randomized controlled experiments were found. Most papers focus on ecological sustainability (158) and the sustainability of software products (148) rather than processes. A meta-synthesis of 36 qualitative studies produced several key propositions, most notably, that sustainability is stratified (has different meanings at different levels of abstraction) and multisystemic (emerges from interactions among multiple social, technical, and sociotechnical systems). Conclusion: The academic literature on SSE is surprisingly non-empirical. More empirical evaluations of specific sustainability interventions are needed. The sustainability of software development products and processes should be conceptualized as multisystemic and stratified, and assessed accordingly.",Universalism,Social Justice,The emphasis on multisystemic sustainability indirectly contributes to Social Justice; another value item under Universalism. The concept being proposed here suggests that creating sustainable software requires proactive efforts in addressing the interdependence of social; technical; and sociotechnical systems.,"The justification for aligning 'Paper X' with the value item Social Justice and its corresponding value Universalism from a ""Software User"" perspective is based on the understanding that sustainability in software development requires acknowledging the interdependence of social, technical, and sociotechnical systems. By promoting a multisystemic approach to sustainability, the paper indirectly addresses the need for equitable distribution of resources and opportunities across different social groups, thereby contributing to the value of Social Justice. This aligns with the value item Universalism, which emphasizes equality and fairness in creating a world that is just and harmonious.",Agreed-Clarified,Agree,,Agree,,
Exp F,Exp J,662,ICSE,Code Generation & Analysis,Fine-grained commit-level vulnerability type prediction by CWE tree structure,"Identifying security patches via code commits to allow early warnings and timely fixes for Open Source Software (OSS) has received increasing attention. However, the existing detection methods can only identify the presence of a patch (i.e., a binary classification) but fail to pinpoint the vulnerability type. In this work, we take the first step to categorize the security patches into fine-grained vulnerability types. Specifically, we use the Common Weakness Enumeration (CWE) as the label and perform fine-grained classification using categories at the third level of the CWE tree. We first formulate the task as a Hierarchical Multi-label Classification (HMC) problem, i.e., inferring a path (a sequence of CWE nodes) from the root of the CWE tree to the node at the target depth. We then propose an approach named TreeVul with a hierarchical and chained architecture, which manages to utilize the structure information of the CWE tree as prior knowledge of the classification task. We further propose a tree structure aware and beam search based inference algorithm for retrieving the optimal path with the highest merged probability. We collect a large security patch dataset from NVD, consisting of 6,541 commits from 1,560 GitHub OSS repositories. Experimental results show that Tree-vulsignificantly outperforms the best performing baselines, with improvements of 5.9%, 25.0%, and 7.7% in terms of weighted F1-score, macro F1-score, and MCC, respectively. We further conduct a user study and a case study to verify the practical value of TreeVul in enriching the binary patch detection results and improving the data quality of NVD, respectively.",Power,Social Recognition,"In the abstract of the 'Paper X'; it is stated that the research work has the potential to improve the data quality of NVD (National Vulnerability Database) and provide early warnings for security vulnerabilities in Open Source Software (OSS). This achievement and the recognition the software could gain from users due to this improved functionality aligns with the value item ""Social Recognition"" and its corresponding value ""Power"".","In the abstract of 'Paper X', it explicitly states that the main contribution is to provide early warnings and timely fixes for security vulnerabilities in Open Source Software (OSS), which can improve the data quality of the National Vulnerability Database (NVD). By achieving this, the software can gain recognition from users for its improved functionality and the power it holds in enhancing software security. This aligns with the value item ""Social Recognition"" and its corresponding value ""Power"" as it focuses on the software's ability to contribute to the greater security of OSS and gain recognition for its impact in this domain.",Agreed-Clarified,Agree,,Agree,,
Exp G,Exp K,662,ICSE,Code Generation & Analysis,Fine-grained commit-level vulnerability type prediction by CWE tree structure,"Identifying security patches via code commits to allow early warnings and timely fixes for Open Source Software (OSS) has received increasing attention. However, the existing detection methods can only identify the presence of a patch (i.e., a binary classification) but fail to pinpoint the vulnerability type. In this work, we take the first step to categorize the security patches into fine-grained vulnerability types. Specifically, we use the Common Weakness Enumeration (CWE) as the label and perform fine-grained classification using categories at the third level of the CWE tree. We first formulate the task as a Hierarchical Multi-label Classification (HMC) problem, i.e., inferring a path (a sequence of CWE nodes) from the root of the CWE tree to the node at the target depth. We then propose an approach named TreeVul with a hierarchical and chained architecture, which manages to utilize the structure information of the CWE tree as prior knowledge of the classification task. We further propose a tree structure aware and beam search based inference algorithm for retrieving the optimal path with the highest merged probability. We collect a large security patch dataset from NVD, consisting of 6,541 commits from 1,560 GitHub OSS repositories. Experimental results show that Tree-vulsignificantly outperforms the best performing baselines, with improvements of 5.9%, 25.0%, and 7.7% in terms of weighted F1-score, macro F1-score, and MCC, respectively. We further conduct a user study and a case study to verify the practical value of TreeVul in enriching the binary patch detection results and improving the data quality of NVD, respectively.",Security,Healthy,"The software contribution in 'Paper X' is aimed at identifying and categorizing security patches for vulnerabilities in a software. This proactive approach to software health management directly aligns with the value item ""Healthy"" and its corresponding value ""Security"".","The justification for aligning 'Paper X' with the value item Healthy and its corresponding value Security is based on the contribution of the paper in identifying and categorizing security patches for vulnerabilities in software. By providing early warnings and timely fixes, the paper aims to enhance the security of software systems, which is directly associated with the notion of a healthy software environment. Ensuring security can contribute to the overall well-being and stability of a software system, aligning with the value of Security within Schwartz's Taxonomy.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,662,ICSE,Code Generation & Analysis,Fine-grained commit-level vulnerability type prediction by CWE tree structure,"Identifying security patches via code commits to allow early warnings and timely fixes for Open Source Software (OSS) has received increasing attention. However, the existing detection methods can only identify the presence of a patch (i.e., a binary classification) but fail to pinpoint the vulnerability type. In this work, we take the first step to categorize the security patches into fine-grained vulnerability types. Specifically, we use the Common Weakness Enumeration (CWE) as the label and perform fine-grained classification using categories at the third level of the CWE tree. We first formulate the task as a Hierarchical Multi-label Classification (HMC) problem, i.e., inferring a path (a sequence of CWE nodes) from the root of the CWE tree to the node at the target depth. We then propose an approach named TreeVul with a hierarchical and chained architecture, which manages to utilize the structure information of the CWE tree as prior knowledge of the classification task. We further propose a tree structure aware and beam search based inference algorithm for retrieving the optimal path with the highest merged probability. We collect a large security patch dataset from NVD, consisting of 6,541 commits from 1,560 GitHub OSS repositories. Experimental results show that Tree-vulsignificantly outperforms the best performing baselines, with improvements of 5.9%, 25.0%, and 7.7% in terms of weighted F1-score, macro F1-score, and MCC, respectively. We further conduct a user study and a case study to verify the practical value of TreeVul in enriching the binary patch detection results and improving the data quality of NVD, respectively.",Security,National Security,"The technology in 'Paper X' assists in improving the overall security of open source software (OSS) by identifying security patches and categorising vulnerability types. This can be aligned with the value item ‚Äö√Ñ√∂‚àö√ë‚àö‚àÇ‚Äö√†√∂‚àö√´‚Äö√†√∂‚Äö√†√á‚Äö√Ñ√∂‚àö‚Ä†‚àö‚àÇ‚Äö√†√∂‚àö¬¥‚Äö√Ñ√∂‚àö‚Ä†‚àö‚àÇ‚Äö√Ñ√∂‚àö‚Ä†¬¨¬•National Security‚Äö√Ñ√∂‚àö√ë‚àö‚àÇ‚Äö√†√∂‚àö√´‚Äö√†√∂‚Äö√†√á‚Äö√Ñ√∂‚àö‚Ä†‚àö‚àÇ‚Äö√†√∂‚àö¬¥‚Äö√Ñ√∂‚àö‚Ä†‚àö‚àÇ‚âà√¨‚àö√ë which in this context refers to the security of a community of software users; thus aligning with the value ""Security"".","In 'Paper X', the main contribution is focused on identifying security patches and categorizing vulnerability types in open source software. By improving the overall security of software, particularly in the context of the OSS community, it aligns with the value item of ""National Security"" as it enhances the security and protection of a collective group of software users. This directly addresses the value of ""Security"" as it contributes to safeguarding the interests and well-being of the community in a software context.",Agreed-Justified,Agree,,Agree,,
Exp B,Exp H,664,ICSE,Code Generation & Analysis,Enhancing Deep Learning-based Vulnerability Detection by Building Behavior Graph Model,"Software vulnerabilities have posed huge threats to the cyberspace security, and there is an increasing demand for automated vulnerability detection (VD). In recent years, deep learning-based (DL-based) vulnerability detection systems have been proposed for the purpose of automatic feature extraction from source code. Although these methods can achieve ideal performance on synthetic datasets, the accuracy drops a lot when detecting real-world vulnerability datasets. Moreover, these approaches limit their scopes within a single function, being not able to leverage the information between functions. In this paper, we attempt to extract the function's abstract behaviors, figure out the relationships between functions, and use this global information to assist DL-based VD to achieve higher performance. To this end, we build a Behavior Graph Model and use it to design a novel framework, namely VulBG. To examine the ability of our constructed Behavior Graph Model, we choose several existing DL-based VD models (e.g., TextCNN, ASTGRU, CodeBERT, Devign, and VulCNN) as our baseline models and conduct evaluations on two real-world datasets: the balanced $\text{FFMpeg}+\text{Qemu}$ dataset and the unbalanced $\text{Chrome} +\text{Debian}$ dataset. Experimental results indicate that VulBG enables all baseline models to detect more real vulnerabilities, thus improving the overall detection performance.",Power,Wealth,The paper contributes to improved vulnerability detection in software; which could indirectly result in financial benefit; e.g.; by preventing damage associated with security breaches. This could be seen as aligning with the value item 'Wealth' under the value 'Power'.,"In the context of software vulnerability detection, the alignment of ""Paper X"" with the value item Wealth and its corresponding value Power can be justified based on the potential financial benefit that improved vulnerability detection can provide. By preventing security breaches and their associated damages, organizations can avoid financial losses and maintain their reputation, which aligns with the desire for power and control over one's economic resources. The focus on the perspective of a software user emphasizes the indirect impact on wealth through enhanced software security.",Agreed-Clarified,Agree,,Agree,,
Exp D,Exp I,664,ICSE,Code Generation & Analysis,Enhancing Deep Learning-based Vulnerability Detection by Building Behavior Graph Model,"Software vulnerabilities have posed huge threats to the cyberspace security, and there is an increasing demand for automated vulnerability detection (VD). In recent years, deep learning-based (DL-based) vulnerability detection systems have been proposed for the purpose of automatic feature extraction from source code. Although these methods can achieve ideal performance on synthetic datasets, the accuracy drops a lot when detecting real-world vulnerability datasets. Moreover, these approaches limit their scopes within a single function, being not able to leverage the information between functions. In this paper, we attempt to extract the function's abstract behaviors, figure out the relationships between functions, and use this global information to assist DL-based VD to achieve higher performance. To this end, we build a Behavior Graph Model and use it to design a novel framework, namely VulBG. To examine the ability of our constructed Behavior Graph Model, we choose several existing DL-based VD models (e.g., TextCNN, ASTGRU, CodeBERT, Devign, and VulCNN) as our baseline models and conduct evaluations on two real-world datasets: the balanced $\text{FFMpeg}+\text{Qemu}$ dataset and the unbalanced $\text{Chrome} +\text{Debian}$ dataset. Experimental results indicate that VulBG enables all baseline models to detect more real vulnerabilities, thus improving the overall detection performance.",Security,National Security,The paper's contribution to enhanced vulnerability detection in software could be seen as contributing to 'National Security'; as it can prevent security breaches that could potentially have national-level implications. This aligns with 'Security' value.,"In the context of software vulnerabilities, the improved vulnerability detection provided by 'Paper X' can directly align with the value item National Security and its corresponding value Security. By enhancing the ability to detect vulnerabilities in software, the paper contributes to preventing potential security breaches that could have significant impacts at a national level. This alignment is evident as the paper explicitly states that its novel framework enables all baseline models to detect more real vulnerabilities, thus improving overall detection performance, which directly addresses the value of Security and its association with National Security.",Agreed-Clarified,Disagree,nothing related to National security discussed or relates with topic,Disagree,"Security perhaps a yes in a very broader and subjective context, national security not sure. The value beter aligns with Aachievement:Capable",
Exp E,Exp J,665,ICSE,Security & Privacy,FedSlice: Protecting Federated Learning Models from Malicious Participants with Model Slicing,"Crowdsourcing Federated learning (CFL) is a new crowdsourcing development paradigm for the Deep Neural Network (DNN) models, also called ‚Äö√Ñ√∂‚àö√ë‚àö‚à´software 2.0‚Äö√Ñ√∂‚àö√ë‚àöœÄ. In practice, the privacy of CFL can be compromised by many attacks, such as free-rider attacks, adversarial attacks, gradient leakage attacks, and inference attacks. Conventional defensive techniques have low efficiency because they deploy heavy encryption techniques or rely on Trusted Execution Environments (TEEs). To improve the efficiency of protecting CFL from these attacks, this paper proposes FedSlice to prevent malicious participants from getting the whole server-side model while keeping the performance goal of CFL. FedSlice breaks the server-side model into several slices and delivers one slice to each participant. Thus, a malicious participant can only get a subset of the server-side model, preventing them from effectively conducting effective attacks. We evaluate FedSlice against these attacks, and results show that FedSlice provides effective defense: the server-side model leakage is reduced from 100% to 43.45%, the success rate of adversarial attacks is reduced from 100% to 11.66%, the average accuracy of membership inference is reduced from 71.91% to 51.58%, and the data leakage from shared gradients is reduced to the level of random guesses. Besides, FedSlice only introduces less than 2% accuracy loss and about 14% computation overhead. To the best of our knowledge, this is the first paper to discuss defense methods against these attacks to the CFL framework.",Security,Healthy,The paper introduces 'FedSlice' as a method for improving the security of Crowdsourcing Federated Learning; which should provide a safer and more secure environment for the software user; aligning with the value item 'Healthy' and its corresponding value 'Security'.,"In the abstract, it is mentioned that FedSlice prevents malicious participants from accessing the whole server-side model, thus reducing the risk of attacks such as inference attacks and adversarial attacks. By improving the security of Crowdsourcing Federated Learning, the software user can have a safer and more secure environment for their data and privacy. This aligns with the value item 'Healthy' as it encompasses the user's well-being and protection, and its corresponding value 'Security' as it emphasizes the importance of safeguarding personal information and avoiding vulnerabilities in the software context.",Agreed-Clarified,Agree,,Agree,,
Exp F,Exp J,665,ICSE,Security & Privacy,FedSlice: Protecting Federated Learning Models from Malicious Participants with Model Slicing,"Crowdsourcing Federated learning (CFL) is a new crowdsourcing development paradigm for the Deep Neural Network (DNN) models, also called ‚Äö√Ñ√∂‚àö√ë‚àö‚à´software 2.0‚Äö√Ñ√∂‚àö√ë‚àöœÄ. In practice, the privacy of CFL can be compromised by many attacks, such as free-rider attacks, adversarial attacks, gradient leakage attacks, and inference attacks. Conventional defensive techniques have low efficiency because they deploy heavy encryption techniques or rely on Trusted Execution Environments (TEEs). To improve the efficiency of protecting CFL from these attacks, this paper proposes FedSlice to prevent malicious participants from getting the whole server-side model while keeping the performance goal of CFL. FedSlice breaks the server-side model into several slices and delivers one slice to each participant. Thus, a malicious participant can only get a subset of the server-side model, preventing them from effectively conducting effective attacks. We evaluate FedSlice against these attacks, and results show that FedSlice provides effective defense: the server-side model leakage is reduced from 100% to 43.45%, the success rate of adversarial attacks is reduced from 100% to 11.66%, the average accuracy of membership inference is reduced from 71.91% to 51.58%, and the data leakage from shared gradients is reduced to the level of random guesses. Besides, FedSlice only introduces less than 2% accuracy loss and about 14% computation overhead. To the best of our knowledge, this is the first paper to discuss defense methods against these attacks to the CFL framework.",Security,Social Order,The paper contributes to a defensive technique that delivers only a slice of the server-side model to each participant; reducing malicious participant's potential of conducting effective attacks. This contributes to maintaining 'Social Order' within the software environment by minimizing threats; which aligns with the value of 'Security'.,"The justification for aligning 'Paper X' with the value item Social Order and its corresponding value Security is based on the fact that the paper proposes a defensive technique, FedSlice, which distributes slices of the server-side model to participants. By doing so, the paper aims to prevent malicious participants from conducting effective attacks, thus minimizing threats and contributing to maintaining order within the software environment. This aligns with the value of Security, as it addresses the need to protect the system and its users from potential harm or disruption.",Agreed-Clarified,Agree,,Agree,,
Exp G,Exp K,665,ICSE,Security & Privacy,FedSlice: Protecting Federated Learning Models from Malicious Participants with Model Slicing,"Crowdsourcing Federated learning (CFL) is a new crowdsourcing development paradigm for the Deep Neural Network (DNN) models, also called ‚Äö√Ñ√∂‚àö√ë‚àö‚à´software 2.0‚Äö√Ñ√∂‚àö√ë‚àöœÄ. In practice, the privacy of CFL can be compromised by many attacks, such as free-rider attacks, adversarial attacks, gradient leakage attacks, and inference attacks. Conventional defensive techniques have low efficiency because they deploy heavy encryption techniques or rely on Trusted Execution Environments (TEEs). To improve the efficiency of protecting CFL from these attacks, this paper proposes FedSlice to prevent malicious participants from getting the whole server-side model while keeping the performance goal of CFL. FedSlice breaks the server-side model into several slices and delivers one slice to each participant. Thus, a malicious participant can only get a subset of the server-side model, preventing them from effectively conducting effective attacks. We evaluate FedSlice against these attacks, and results show that FedSlice provides effective defense: the server-side model leakage is reduced from 100% to 43.45%, the success rate of adversarial attacks is reduced from 100% to 11.66%, the average accuracy of membership inference is reduced from 71.91% to 51.58%, and the data leakage from shared gradients is reduced to the level of random guesses. Besides, FedSlice only introduces less than 2% accuracy loss and about 14% computation overhead. To the best of our knowledge, this is the first paper to discuss defense methods against these attacks to the CFL framework.",Achievement,Successful,The paper presents 'FedSlice' as a method that effectively minimizes threats such as data leakage; accuracy loss; and computation overhead; contributing to the success of the software user by providing a more secure and efficient tool; aligning with the value item 'Successful' under the value 'Achievement'.,"In the abstract of 'Paper X', it is stated that FedSlice, the proposed method, effectively reduces threats such as data leakage, accuracy loss, and computation overhead. This directly aligns with the value item 'Successful' because by providing a more secure and efficient tool, it contributes to the success of the software user in achieving their goals and desired outcomes. Therefore, the main contributions of 'Paper X' are aligned with the value item Successful and its corresponding value Achievement from a ""Software User"" perspective.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,668,ICSE,AI & Machine Learning,Practical and Efficient Model Extraction of Sentiment Analysis APIs,"Despite their stunning performance, developing deep learning models from scratch is a formidable task. Therefore, it popularizes Machine-Learning-as-a-Service (MLaaS), where general users can access the trained models of MLaaS providers via Application Programming Interfaces (APIs) on a pay-per-query basis. Unfortunately, the success of MLaaS is under threat from model extraction attacks, where attackers intend to extract a local model of equivalent functionality to the target MLaaS model. However, existing studies on model extraction of text analytics APIs frequently assume adversaries have strong knowledge about the victim model, like its architecture and parameters, which hardly holds in practice. Besides, since the attacker's and the victim's training data can be considerably discrepant, it is non-trivial to perform efficient model extraction. In this paper, to advance the understanding of such attacks, we propose a framework, PEEP, for practical and efficient model extraction of sentiment analysis APIs with only query access. Specifically, PEEP features a learning-based scheme, which employs out-of-domain public corpora and a novel query strategy to construct proxy training data for model extraction. Besides, PEEP introduces a greedy search algorithm to settle an appropriate architecture for the extracted model. We conducted extensive experiments with two victim models across three datasets and two real-life commercial sentiment analysis APIs. Experimental results corroborate that PEEP can consistently outperform the state-of-the-art baselines in terms of effectiveness and efficiency.",Achievement,Influential,The paper contributes to the enhancement of software user influence through proposing PEEP; a more effective and efficient method for model extraction in Machine-Learning-as-a-Service application. This aligns with the value item Influential and its corresponding value Achievement.,"In the context of a ""Software User,"" the paper's alignment with the value item ""Influential"" and its corresponding value ""Achievement"" is evident through the proposal of PEEP, a framework that enhances the user's ability to influence the extraction of models in Machine-Learning-as-a-Service (MLaaS) applications. By providing a more effective and efficient approach to model extraction, the paper empowers the software user to achieve greater influence and control over the MLaaS models they interact with, thereby aligning with the value of Achievement and the desire to have a significant impact on the outcome of their software usage.",Agreed-Clarified,Agree,,Agree,,
Exp B,Exp H,668,ICSE,AI & Machine Learning,Practical and Efficient Model Extraction of Sentiment Analysis APIs,"Despite their stunning performance, developing deep learning models from scratch is a formidable task. Therefore, it popularizes Machine-Learning-as-a-Service (MLaaS), where general users can access the trained models of MLaaS providers via Application Programming Interfaces (APIs) on a pay-per-query basis. Unfortunately, the success of MLaaS is under threat from model extraction attacks, where attackers intend to extract a local model of equivalent functionality to the target MLaaS model. However, existing studies on model extraction of text analytics APIs frequently assume adversaries have strong knowledge about the victim model, like its architecture and parameters, which hardly holds in practice. Besides, since the attacker's and the victim's training data can be considerably discrepant, it is non-trivial to perform efficient model extraction. In this paper, to advance the understanding of such attacks, we propose a framework, PEEP, for practical and efficient model extraction of sentiment analysis APIs with only query access. Specifically, PEEP features a learning-based scheme, which employs out-of-domain public corpora and a novel query strategy to construct proxy training data for model extraction. Besides, PEEP introduces a greedy search algorithm to settle an appropriate architecture for the extracted model. We conducted extensive experiments with two victim models across three datasets and two real-life commercial sentiment analysis APIs. Experimental results corroborate that PEEP can consistently outperform the state-of-the-art baselines in terms of effectiveness and efficiency.",Stimulation,Daring,The paper presents a daring approach - PEEP - in addressing the model extraction problem in sentiment analysis APIs in a practical and application-focused manner. This aligns with the value item Daring and its corresponding value Stimulation.,"The justification for aligning 'Paper X' with the value item Daring and its corresponding value Stimulation is based on the fact that the paper presents a novel and daring approach, PEEP, for efficient model extraction in sentiment analysis APIs. This approach challenges the traditional assumptions and addresses the limitations of existing studies, demonstrating a bold and innovative solution. By pushing the boundaries and introducing a new methodology, 'Paper X' stimulates advancements in the field and encourages further exploration and improvement in sentiment analysis within the software context.",Agreed-Clarified,Agree,,Agree,,
Exp D,Exp I,673,ICSE,Software Engineering Practices,On-Demand Security Requirements Synthesis with Relational Generative Adversarial Networks,"Security requirements engineering is a manual and error-prone activity that is often neglected due to the knowledge gap between cybersecurity professionals and software requirements engineers. In this paper, we aim to automate the process of recommending and synthesizing security requirements specifications and therefore supporting requirements engineers in soliciting and specifying security requirements. We investigate the use of Relational Generative Adversarial Networks (GANs) in automatically synthesizing security requirements specifications. We evaluate our approach using a real case study of the Court Case Management System (CCMS) developed for the Indiana Supreme Court's Division of State Court Administration. We present an approach based on RelGAN to generate security requirements specifications for the CCMS. We show that RelGAN is practical for synthesizing security requirements specifications as indicated by subject matter experts. Based on this study, we demonstrate promising results for the use of GANs in the software requirements synthesis domain. We also provide a baseline for synthesizing requirements, highlight limitations and weaknesses of RelGAN and define opportunities for further investigations.",Security,Healthy,The paper presents a method for automating the process of recommending and synthesizing security requirements specifications; which directly contributes to a more secure and therefore healthier software environment for the user.,"In the paper, the authors propose an approach to automate the process of synthesizing security requirements specifications. By providing a more efficient and accurate way to specify security requirements, the paper directly contributes to enhancing the security of software systems. This, in turn, creates a healthier software environment for software users as their data and privacy are better protected. Therefore, the alignment of the paper's contributions with the value item ""Healthy"" and its corresponding value ""Security"" is evident.",Agreed-Clarified,Agree,,Agree,,
Exp E,Exp J,673,ICSE,Software Engineering Practices,On-Demand Security Requirements Synthesis with Relational Generative Adversarial Networks,"Security requirements engineering is a manual and error-prone activity that is often neglected due to the knowledge gap between cybersecurity professionals and software requirements engineers. In this paper, we aim to automate the process of recommending and synthesizing security requirements specifications and therefore supporting requirements engineers in soliciting and specifying security requirements. We investigate the use of Relational Generative Adversarial Networks (GANs) in automatically synthesizing security requirements specifications. We evaluate our approach using a real case study of the Court Case Management System (CCMS) developed for the Indiana Supreme Court's Division of State Court Administration. We present an approach based on RelGAN to generate security requirements specifications for the CCMS. We show that RelGAN is practical for synthesizing security requirements specifications as indicated by subject matter experts. Based on this study, we demonstrate promising results for the use of GANs in the software requirements synthesis domain. We also provide a baseline for synthesizing requirements, highlight limitations and weaknesses of RelGAN and define opportunities for further investigations.",Security,Social Order,By synthesizing security requirements; this paper contributes to the establishment and maintenance of social order within the software context; a particular value item which relates to the security value.,"In the paper abstract, the authors clearly state that their aim is to automate the process of recommending and synthesizing security requirements specifications. By doing so, they contribute to the establishment and maintenance of social order within the software context. Social order refers to the structure and stability of society, and in the software context, it can be understood as ensuring the security and stability of software systems. As security is a crucial aspect of social order, the alignment between the main contributions of the paper and the value item of Social Order is evident. Additionally, security is also associated with the value of Security, which further supports the justification for the alignment with Social Order.",Agreed-Clarified,Agree,,Agree,,
Exp F,Exp J,673,ICSE,Software Engineering Practices,On-Demand Security Requirements Synthesis with Relational Generative Adversarial Networks,"Security requirements engineering is a manual and error-prone activity that is often neglected due to the knowledge gap between cybersecurity professionals and software requirements engineers. In this paper, we aim to automate the process of recommending and synthesizing security requirements specifications and therefore supporting requirements engineers in soliciting and specifying security requirements. We investigate the use of Relational Generative Adversarial Networks (GANs) in automatically synthesizing security requirements specifications. We evaluate our approach using a real case study of the Court Case Management System (CCMS) developed for the Indiana Supreme Court's Division of State Court Administration. We present an approach based on RelGAN to generate security requirements specifications for the CCMS. We show that RelGAN is practical for synthesizing security requirements specifications as indicated by subject matter experts. Based on this study, we demonstrate promising results for the use of GANs in the software requirements synthesis domain. We also provide a baseline for synthesizing requirements, highlight limitations and weaknesses of RelGAN and define opportunities for further investigations.",Achievement,Capable,The use of automated methods to create security requirements makes users more capable of maintaining and enhancing the security of their software; aligning with the value item Capable and its corresponding value Achievement.,"In 'Paper X', the authors propose the use of automated methods, specifically Relational Generative Adversarial Networks (GANs), to generate security requirements specifications for software systems. By automating this process, it empowers software users to be more capable in maintaining and enhancing the security of their software. This aligns with the value item Capable and its corresponding value Achievement from Schwartz's Taxonomy, as it enables users to achieve their goal of improving software security and demonstrates their competence in effectively addressing security requirements.",Agreed-Clarified,Agree,,Agree,,
Exp G,Exp K,681,ICSE,Software Project Management,On the Self-Governance and Episodic Changes in Apache Incubator Projects: An Empirical Study,"Sustainable Open Source Software (OSS) projects are characterized by the ability to attract new project members and maintain an energetic project community. Building sustainable OSS projects from a nascent state requires effective project governance and socio-technical structure to be interleaved, in a complex and dynamic process. Although individual disciplines have studied each separately, little is known about how governance and software development work together in practice toward sustainability. Prior work has shown that many OSS projects experience large, episodic changes over short periods of time, which can propel them or drag them down. However, sustainable projects typically manage to come out unscathed from such changes, while others do not. The natural questions arise: Can we identify the back-and-forth between governance and socio-technical structure that lead to sustainability following episodic events? And, how about those that do not lead to sustainability? From a data set of social, technical, and policy digital traces from 262 sustainability-labeled ASF incubator projects, here we employ a large-scale empirical study to characterize episodic changes in socio-technical aspects measured by Change Intervals (CI), governance rules and regulations in a form of Institutional Statements (IS), and the temporal relationships between them. We find that sustainable projects during episodic changes can adapt themselves to institutional statements more efficiently, and that institutional discussions can lead to episodic changes intervals in socio-technical aspects of the projects, and vice versa. In practice, these results can provide timely guidance beyond socio-technical considerations, adding rules and regulations in the mix, toward a unified analytical framework for OSS project sustainability.",Self Direction,Freedom,The paper addresses the software user's need for Freedom (v1.2) in navigating open source software (OSS) projects by highlighting the interplay of governance and socio-technical structure in ensuring sustainable OSS projects; and hence; user freedom in OSS use.,"In 'Paper X', the alignment with the value item Freedom (v1.2) and its corresponding value Self Direction is evident in the focus on the interplay between governance and socio-technical structure in sustainable open source software (OSS) projects. By studying the relationship between these factors, the paper addresses the software user's need for freedom in navigating OSS projects, allowing them to have greater control and autonomy in their software usage. This aligns with the value item Freedom as it emphasizes the importance of independence and the ability to make choices in the context of software usage.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,681,ICSE,Software Project Management,On the Self-Governance and Episodic Changes in Apache Incubator Projects: An Empirical Study,"Sustainable Open Source Software (OSS) projects are characterized by the ability to attract new project members and maintain an energetic project community. Building sustainable OSS projects from a nascent state requires effective project governance and socio-technical structure to be interleaved, in a complex and dynamic process. Although individual disciplines have studied each separately, little is known about how governance and software development work together in practice toward sustainability. Prior work has shown that many OSS projects experience large, episodic changes over short periods of time, which can propel them or drag them down. However, sustainable projects typically manage to come out unscathed from such changes, while others do not. The natural questions arise: Can we identify the back-and-forth between governance and socio-technical structure that lead to sustainability following episodic events? And, how about those that do not lead to sustainability? From a data set of social, technical, and policy digital traces from 262 sustainability-labeled ASF incubator projects, here we employ a large-scale empirical study to characterize episodic changes in socio-technical aspects measured by Change Intervals (CI), governance rules and regulations in a form of Institutional Statements (IS), and the temporal relationships between them. We find that sustainable projects during episodic changes can adapt themselves to institutional statements more efficiently, and that institutional discussions can lead to episodic changes intervals in socio-technical aspects of the projects, and vice versa. In practice, these results can provide timely guidance beyond socio-technical considerations, adding rules and regulations in the mix, toward a unified analytical framework for OSS project sustainability.",Hedonism,Enjoying Life,By conducting an empirical study on how governance and socio-technical structures lead to OSS project sustainability; this paper contributes to users' value of Enjoying Life (v3.1) by ensuring robust; efficient; and sustainable software that potentially reduces user stress and responsibilities.,"The justification provided aligns 'Paper X' with the value item of Enjoying Life and its corresponding value Hedonism from a ""Software User"" perspective. The empirical study conducted in the paper focuses on how governance and socio-technical structures contribute to Open Source Software (OSS) project sustainability. By ensuring robust, efficient, and sustainable software, the contributions of 'Paper X' potentially reduce user stress and responsibilities. This directly aligns with the value item Enjoying Life (v3.1) as users can enjoy their software experience without significant disruptions or concerns, enhancing their overall enjoyment and satisfaction with the software.",Agreed-Justified,Disagree,reconcile with coder_2,Disagree,"The justification doesn't align with the value item of Enjoying Life
",
Exp B,Exp H,681,ICSE,Software Project Management,On the Self-Governance and Episodic Changes in Apache Incubator Projects: An Empirical Study,"Sustainable Open Source Software (OSS) projects are characterized by the ability to attract new project members and maintain an energetic project community. Building sustainable OSS projects from a nascent state requires effective project governance and socio-technical structure to be interleaved, in a complex and dynamic process. Although individual disciplines have studied each separately, little is known about how governance and software development work together in practice toward sustainability. Prior work has shown that many OSS projects experience large, episodic changes over short periods of time, which can propel them or drag them down. However, sustainable projects typically manage to come out unscathed from such changes, while others do not. The natural questions arise: Can we identify the back-and-forth between governance and socio-technical structure that lead to sustainability following episodic events? And, how about those that do not lead to sustainability? From a data set of social, technical, and policy digital traces from 262 sustainability-labeled ASF incubator projects, here we employ a large-scale empirical study to characterize episodic changes in socio-technical aspects measured by Change Intervals (CI), governance rules and regulations in a form of Institutional Statements (IS), and the temporal relationships between them. We find that sustainable projects during episodic changes can adapt themselves to institutional statements more efficiently, and that institutional discussions can lead to episodic changes intervals in socio-technical aspects of the projects, and vice versa. In practice, these results can provide timely guidance beyond socio-technical considerations, adding rules and regulations in the mix, toward a unified analytical framework for OSS project sustainability.",Security,Social Order,This study's focus on the societal factors (e.g.; governance; rules and regulations) involved in OSS project sustainability aligns with the value item Social Order (v6.3) under the larger value of Security (v6) as it emphasizes the importance of structure and order in the context of software use.,"One of the main contributions of 'Paper X' is the examination of governance and socio-technical structure in sustainable OSS projects. This focus on the societal factors involved in project sustainability aligns with the value item Social Order (v6.3) from Schwartz's Taxonomy. Social Order values emphasize the importance of structure and order in maintaining societal stability and security. In the context of software use, this alignment suggests that establishing and adhering to governance rules and regulations is crucial for ensuring a secure and ordered environment for software users.",Agreed-Clarified,Agree,,Agree,,
Exp C,Exp I,682,ICSE,Security & Privacy,Demystifying privacy policy of third-party libraries in mobile apps,"The privacy of personal information has received significant attention in mobile software. Although researchers have designed methods to identify the conflict between app behavior and privacy policies, little is known about the privacy compliance issues relevant to third-party libraries (TPLs). The regulators enacted articles to regulate the usage of personal information for TPLs (e.g., the CCPA requires businesses clearly notify consumers if they share consumers' data with third parties or not). However, it remains challenging to investigate the privacy compliance issues of TPLs due to three reasons: 1) Difficulties in collecting TPLs' privacy policies. In contrast to Android apps, which are distributed through markets like Google Play and must provide privacy policies, there is no unique platform for collecting privacy policies of TPLs. 2) Difficulties in analyzing TPL's user privacy access behaviors. TPLs are mainly provided in binary files, such as jar or aar, and their whole functionalities usually cannot be executed independently without host apps. 3) Difficulties in identifying consistency between TPL's functionalities and privacy policies, and host app's privacy policy and data sharing with TPLs. This requires analyzing not only the privacy policies of TPLs and host apps but also their functionalities. In this paper, we propose an automated system named ATPChecker to analyze whether Android TPLs comply with the privacy-related regulations. We construct a data set that contains a list of 458 TPLs, 247 TPL's privacy policies, 187 TPL's binary files and 641 host apps and their privacy policies. Then, we analyze the bytecode of TPLs and host apps, design natural language processing systems to analyze privacy policies, and implement an expert system to identify TPL usage-related regulation compliance. The experimental results show that 23% TPLs violate regulation requirements for providing privacy policies. Over 47% TPLs miss disclosing data usage in their privacy policies. Over 65% host apps share user data with TPLs while 65% of them miss disclosing interactions with TPLs. Our findings remind developers to be mindful of TPL usage when developing apps or writing privacy policies to avoid violating regulations,",Self Direction,Privacy,The paper contributes by proposing an automated system to ensure the privacy of the users' data in mobile software; aligns with the value item Privacy and its corresponding value Self Direction.,"The justification for labeling 'Paper X' as aligning with the value item Privacy and its corresponding value Self Direction from a ""Software User"" perspective is based on the fact that the paper proposes an automated system, named ATPChecker, which aims to analyze whether Android TPLs (third-party libraries) comply with privacy-related regulations. By ensuring compliance with privacy regulations, the paper contributes to protecting users' personal data, allowing them to have control over their own information and make independent decisions regarding sharing and privacy settings, which aligns with the value of Self Direction.",Agreed-Justified,Agree,,Agree,,
Exp E,Exp J,682,ICSE,Security & Privacy,Demystifying privacy policy of third-party libraries in mobile apps,"The privacy of personal information has received significant attention in mobile software. Although researchers have designed methods to identify the conflict between app behavior and privacy policies, little is known about the privacy compliance issues relevant to third-party libraries (TPLs). The regulators enacted articles to regulate the usage of personal information for TPLs (e.g., the CCPA requires businesses clearly notify consumers if they share consumers' data with third parties or not). However, it remains challenging to investigate the privacy compliance issues of TPLs due to three reasons: 1) Difficulties in collecting TPLs' privacy policies. In contrast to Android apps, which are distributed through markets like Google Play and must provide privacy policies, there is no unique platform for collecting privacy policies of TPLs. 2) Difficulties in analyzing TPL's user privacy access behaviors. TPLs are mainly provided in binary files, such as jar or aar, and their whole functionalities usually cannot be executed independently without host apps. 3) Difficulties in identifying consistency between TPL's functionalities and privacy policies, and host app's privacy policy and data sharing with TPLs. This requires analyzing not only the privacy policies of TPLs and host apps but also their functionalities. In this paper, we propose an automated system named ATPChecker to analyze whether Android TPLs comply with the privacy-related regulations. We construct a data set that contains a list of 458 TPLs, 247 TPL's privacy policies, 187 TPL's binary files and 641 host apps and their privacy policies. Then, we analyze the bytecode of TPLs and host apps, design natural language processing systems to analyze privacy policies, and implement an expert system to identify TPL usage-related regulation compliance. The experimental results show that 23% TPLs violate regulation requirements for providing privacy policies. Over 47% TPLs miss disclosing data usage in their privacy policies. Over 65% host apps share user data with TPLs while 65% of them miss disclosing interactions with TPLs. Our findings remind developers to be mindful of TPL usage when developing apps or writing privacy policies to avoid violating regulations,",Security,Sense of Belonging,The paper contributes to providing a sense of belonging and trust to the users by checking the compliance of privacy policies; aligns with the value item Sense of Belonging and its corresponding value Security.,"The justification for aligning 'Paper X' with the value item Sense of Belonging and its corresponding value Security is that the paper directly addresses the concerns of software users by analyzing the compliance of privacy policies of third-party libraries (TPLs). By ensuring that TPLs comply with privacy-related regulations and disclose their data usage, the paper aims to provide a sense of belonging and trust to the users, as they can feel secure in knowing that their personal information is being handled appropriately.",Agreed-Clarified,Agree,,Agree,,
Exp F,Exp J,686,ICSE,Accessibility & User Experience,Aidui: Toward automated recognition of dark patterns in user interfaces,"Past studies have illustrated the prevalence of UI dark patterns, or user interfaces that can lead end-users toward (unknowingly) taking actions that they may not have intended. Such deceptive UI designs can be either intentional (to benefit an online service) or unintentional (through complicit design practices) and can result in adverse effects on end users, such as oversharing personal information or financial loss. While significant research progress has been made toward the development of dark pattern taxonomies across different software domains, developers and users currently lack guidance to help recognize, avoid, and navigate these often subtle design motifs. However, automated recognition of dark patterns is a challenging task, as the instantiation of a single type of pattern can take many forms, leading to significant variability. In this paper, we take the first step toward understanding the extent to which common UI dark patterns can be automatically recognized in modern software applications. To do this, we introduce AidUI, a novel automated approach that uses computer vision and natural language processing techniques to recognize a set of visual and textual cues in application screenshots that signify the presence of ten unique UI dark patterns, allowing for their detection, classification, and localization. To evaluate our approach, we have constructed ContextDP, the current largest dataset of fully-localized UI dark patterns that spans 175 mobile and 83 web UI screenshots containing 301 dark pattern instances. The results of our evaluation illustrate that AidUI achieves an overall precision of 0.66, recall of 0.67, F1-score of 0.65 in detecting dark pattern instances, reports few false positives, and is able to localize detected patterns with an IoU score of 0.84. Furthermore, a significant subset of our studied dark patterns can be detected quite reliably (F1 score of over 0.82), and future research directions may allow for improved detection of additional patterns. This work demonstrates the plausibility of developing tools to aid developers in recognizing and appropriately rectifying deceptive UI patterns.",Self Direction,Freedom,The paper introduces AidUI; which detects deceptive UI patterns in software. This empowers users with Freedom to make informed decisions and act according to their own choices.,"In 'Paper X', the introduction of AidUI, a tool that detects deceptive UI patterns in software, directly aligns with the value item Freedom and the corresponding value Self Direction from a ""Software User"" perspective. By recognizing and highlighting these dark patterns, AidUI empowers users with the freedom to make informed decisions and act according to their own choices. This directly addresses the concern of users unknowingly taking actions that they may not have intended, allowing them to exercise their self-direction and have greater control over their interactions with software applications.",Agreed-Clarified,Agree,,Agree,,
Exp G,Exp K,686,ICSE,Accessibility & User Experience,Aidui: Toward automated recognition of dark patterns in user interfaces,"Past studies have illustrated the prevalence of UI dark patterns, or user interfaces that can lead end-users toward (unknowingly) taking actions that they may not have intended. Such deceptive UI designs can be either intentional (to benefit an online service) or unintentional (through complicit design practices) and can result in adverse effects on end users, such as oversharing personal information or financial loss. While significant research progress has been made toward the development of dark pattern taxonomies across different software domains, developers and users currently lack guidance to help recognize, avoid, and navigate these often subtle design motifs. However, automated recognition of dark patterns is a challenging task, as the instantiation of a single type of pattern can take many forms, leading to significant variability. In this paper, we take the first step toward understanding the extent to which common UI dark patterns can be automatically recognized in modern software applications. To do this, we introduce AidUI, a novel automated approach that uses computer vision and natural language processing techniques to recognize a set of visual and textual cues in application screenshots that signify the presence of ten unique UI dark patterns, allowing for their detection, classification, and localization. To evaluate our approach, we have constructed ContextDP, the current largest dataset of fully-localized UI dark patterns that spans 175 mobile and 83 web UI screenshots containing 301 dark pattern instances. The results of our evaluation illustrate that AidUI achieves an overall precision of 0.66, recall of 0.67, F1-score of 0.65 in detecting dark pattern instances, reports few false positives, and is able to localize detected patterns with an IoU score of 0.84. Furthermore, a significant subset of our studied dark patterns can be detected quite reliably (F1 score of over 0.82), and future research directions may allow for improved detection of additional patterns. This work demonstrates the plausibility of developing tools to aid developers in recognizing and appropriately rectifying deceptive UI patterns.",Security,Healthy,The detection; classification; and localization of dark patterns by AidUI can allow software users to avoid potential harms such as oversharing personal information or financial loss. This corresponds to the user's value of being Healthy and safe in their software interactions.,"The detection, classification, and localization of dark patterns by AidUI align with the value item Healthy and its corresponding value Security from a ""Software User"" perspective because it enables users to protect their personal information and avoid financial loss. By recognizing and flagging deceptive UI designs, users can make informed decisions, avoid unintended actions, and maintain their security and well-being while interacting with software applications.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,686,ICSE,Accessibility & User Experience,Aidui: Toward automated recognition of dark patterns in user interfaces,"Past studies have illustrated the prevalence of UI dark patterns, or user interfaces that can lead end-users toward (unknowingly) taking actions that they may not have intended. Such deceptive UI designs can be either intentional (to benefit an online service) or unintentional (through complicit design practices) and can result in adverse effects on end users, such as oversharing personal information or financial loss. While significant research progress has been made toward the development of dark pattern taxonomies across different software domains, developers and users currently lack guidance to help recognize, avoid, and navigate these often subtle design motifs. However, automated recognition of dark patterns is a challenging task, as the instantiation of a single type of pattern can take many forms, leading to significant variability. In this paper, we take the first step toward understanding the extent to which common UI dark patterns can be automatically recognized in modern software applications. To do this, we introduce AidUI, a novel automated approach that uses computer vision and natural language processing techniques to recognize a set of visual and textual cues in application screenshots that signify the presence of ten unique UI dark patterns, allowing for their detection, classification, and localization. To evaluate our approach, we have constructed ContextDP, the current largest dataset of fully-localized UI dark patterns that spans 175 mobile and 83 web UI screenshots containing 301 dark pattern instances. The results of our evaluation illustrate that AidUI achieves an overall precision of 0.66, recall of 0.67, F1-score of 0.65 in detecting dark pattern instances, reports few false positives, and is able to localize detected patterns with an IoU score of 0.84. Furthermore, a significant subset of our studied dark patterns can be detected quite reliably (F1 score of over 0.82), and future research directions may allow for improved detection of additional patterns. This work demonstrates the plausibility of developing tools to aid developers in recognizing and appropriately rectifying deceptive UI patterns.",Universalism,Social Justice,By preventing users from falling into deceptive UI traps; AidUI contributes to promoting Social Justice by equalizing the power balance between software providers and users.,"In my justification, I mentioned that AidUI, by preventing users from falling into deceptive UI traps, helps promote social justice. This is because it equalizes the power balance between software providers and users. Deceptive UI patterns often exploit users by leading them to take unintended actions, resulting in potential harms such as oversharing personal information or financial loss. By automatically recognizing and detecting these patterns, AidUI empowers users to make informed choices and avoid being manipulated. This aligns with the value item of Social Justice, which is part of the broader value of Universalism, as it advocates for fairness, equal rights, and protecting individuals from exploitation. Therefore, AidUI's contribution directly supports the values of Social Justice and Universalism in a software user context.",Agreed-Clarified,Disagree,reconcile with coder_2,Disagree,"The justification doesn't align with the value item of social justic
",
Exp B,Exp H,691,ICSE,AI & Machine Learning,Towards understanding fairness and its composition in ensemble machine learning,"Machine Learning (ML) software has been widely adopted in modern society, with reported fairness implications for minority groups based on race, sex, age, etc. Many recent works have proposed methods to measure and mitigate algorithmic bias in ML models. The existing approaches focus on single classifier-based ML models. However, real-world ML models are often composed of multiple independent or dependent learners in an ensemble (e.g., Random Forest), where the fairness composes in a non-trivial way. How does fairness compose in ensembles? What are the fairness impacts of the learners on the ultimate fairness of the ensemble? Can fair learners result in an unfair ensemble? Furthermore, studies have shown that hyperparameters influence the fairness of ML models. Ensemble hyperparameters are more complex since they affect how learners are combined in different categories of ensembles. Understanding the impact of ensemble hyperparameters on fairness will help programmers design fair ensembles. Today, we do not understand these fully for different ensemble algorithms. In this paper, we comprehensively study popular real-world ensembles: Bagging, Boosting, Stacking, and Voting. We have developed a benchmark of 168 ensemble models collected from Kaggle on four popular fairness datasets. We use existing fairness metrics to understand the composition of fairness. Our results show that ensembles can be designed to be fairer without using mitigation techniques. We also identify the interplay between fairness composition and data characteristics to guide fair ensemble design. Finally, our benchmark can be leveraged for further research on fair ensembles. To the best of our knowledge, this is one of the first and largest studies on fairness composition in ensembles yet presented in the literature.",Universalism,Equality,The paper primarily aims to study and guide the fair design of ensemble ML models; emphasizing fair outcomes for all users without discrimination; which aligns with the value item Equality and its corresponding value Universalism.,"The main contributions of 'Paper X' involve studying and guiding the fair design of ensemble ML models. By focusing on fair outcomes for all software users without discrimination, the paper aligns with the value item Equality and its corresponding value Universalism, which emphasizes the importance of fairness and equal treatment for all individuals.",Agreed-Clarified,Agree,,Agree,,
Exp C,Exp I,691,ICSE,AI & Machine Learning,Towards understanding fairness and its composition in ensemble machine learning,"Machine Learning (ML) software has been widely adopted in modern society, with reported fairness implications for minority groups based on race, sex, age, etc. Many recent works have proposed methods to measure and mitigate algorithmic bias in ML models. The existing approaches focus on single classifier-based ML models. However, real-world ML models are often composed of multiple independent or dependent learners in an ensemble (e.g., Random Forest), where the fairness composes in a non-trivial way. How does fairness compose in ensembles? What are the fairness impacts of the learners on the ultimate fairness of the ensemble? Can fair learners result in an unfair ensemble? Furthermore, studies have shown that hyperparameters influence the fairness of ML models. Ensemble hyperparameters are more complex since they affect how learners are combined in different categories of ensembles. Understanding the impact of ensemble hyperparameters on fairness will help programmers design fair ensembles. Today, we do not understand these fully for different ensemble algorithms. In this paper, we comprehensively study popular real-world ensembles: Bagging, Boosting, Stacking, and Voting. We have developed a benchmark of 168 ensemble models collected from Kaggle on four popular fairness datasets. We use existing fairness metrics to understand the composition of fairness. Our results show that ensembles can be designed to be fairer without using mitigation techniques. We also identify the interplay between fairness composition and data characteristics to guide fair ensemble design. Finally, our benchmark can be leveraged for further research on fair ensembles. To the best of our knowledge, this is one of the first and largest studies on fairness composition in ensembles yet presented in the literature.",Security,Social Order,The paper contributes to ensuring 'Social Order' by providing unbiased and fair results from the ML models. The research helps to maintain trust in ML software; contributing to social stability; which aligns with the value item Social Order and the corresponding value of Security.,"The paper's contribution to ensuring 'Social Order' can be seen through its focus on mitigating algorithmic bias in ML models within ensembles. By addressing fairness concerns and providing unbiased results, the research helps to maintain trust in ML software, which is essential for social stability. Aligning with the value item of Social Order and its corresponding value of Security, the paper's efforts contribute to creating a more secure and stable software environment for users.",Agreed-Clarified,Agree,,Agree,,
Exp E,Exp J,693,ICSE,Accessibility & User Experience,Detecting Dialog-Related Keyboard Navigation Failures in Web Applications,"The ability to navigate the Web via the keyboard interface is critical to people with various types of disabilities. However, modern websites often violate web accessibility guidelines for keyboard navigability with respect to web dialogs. In this paper, we present a novel approach for automatically detecting web accessibility bugs that prevent or hinder keyboard users' ability to navigate dialogs in web pages. An extensive evaluation of our technique on real-world subjects showed that our technique is effective in detecting these dialog-related keyboard navigation failures.",Security,Healthy,The paper aims at improving the health of software users by providing a solution to overcome accessibility issues. This aligns with the value item Healthy and its corresponding value Security.,"In 'Paper X', the authors specifically mention that their approach aims to automatically detect web accessibility bugs that prevent or hinder keyboard users' ability to navigate web dialogs. By addressing these bugs and improving accessibility, the paper contributes to enhancing the user experience and overall health of software users. This aligns with the value item Healthy from Schwartz's Taxonomy, as it highlights the importance of ensuring the well-being and smooth functioning of individuals in a software context. Furthermore, improving accessibility also contributes to the value of Security, as it promotes a sense of safety and inclusivity for software users who may have disabilities or limitations in their ability to navigate websites effectively.",Agreed-Clarified,Agree,,Agree,,
Exp F,Exp J,693,ICSE,Accessibility & User Experience,Detecting Dialog-Related Keyboard Navigation Failures in Web Applications,"The ability to navigate the Web via the keyboard interface is critical to people with various types of disabilities. However, modern websites often violate web accessibility guidelines for keyboard navigability with respect to web dialogs. In this paper, we present a novel approach for automatically detecting web accessibility bugs that prevent or hinder keyboard users' ability to navigate dialogs in web pages. An extensive evaluation of our technique on real-world subjects showed that our technique is effective in detecting these dialog-related keyboard navigation failures.",Benevolence,Helpful,"The paper's contribution is intended to be ""helpful"" for those with disabilities in navigating websites. This aligns with the value item Helpful and its corresponding value Benevolence.","The paper's main contribution of automatically detecting web accessibility bugs and improving keyboard navigation for people with disabilities can be seen as aligned with the value item ""Helpful"" and its corresponding value of Benevolence. By addressing the needs of individuals with disabilities and striving to make web dialogs more navigable, the paper demonstrates a helpful and benevolent approach towards improving the software experience for users who face accessibility challenges.",Agreed-Justified,Agree,,Agree,,
Exp G,Exp K,693,ICSE,Accessibility & User Experience,Detecting Dialog-Related Keyboard Navigation Failures in Web Applications,"The ability to navigate the Web via the keyboard interface is critical to people with various types of disabilities. However, modern websites often violate web accessibility guidelines for keyboard navigability with respect to web dialogs. In this paper, we present a novel approach for automatically detecting web accessibility bugs that prevent or hinder keyboard users' ability to navigate dialogs in web pages. An extensive evaluation of our technique on real-world subjects showed that our technique is effective in detecting these dialog-related keyboard navigation failures.",Universalism,Wisdom,The paper develops a technique that increases wisdom; as it helps those with disabilities to effectively navigate websites. This aligns with the value item Wisdom and its corresponding value Universalism.,"The justification for aligning 'Paper X' with the value item Wisdom and its corresponding value Universalism is based on the fact that the paper presents a technique that helps people with disabilities navigate websites effectively. By improving web accessibility for keyboard users, the paper contributes to the wisdom value of understanding and respect for diverse user needs. This aligns with the value of Universalism, which emphasizes broadmindedness, equality, and social justice in the software context.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,695,ICSE,Security & Privacy,Keyword Extraction From Specification Documents for Planning Security Mechanisms,"Software development companies heavily invest both time and money to provide post-production support to fix security vulnerabilities in their products. Current techniques identify vulnerabilities from source code using static and dynamic analyses. However, this does not help integrate security mechanisms early in the architectural design phase. We develop VDocScan, a technique for predicting vulnerabilities based on specification documents, even before the development stage. We evaluate VDocScan using an extensive dataset of CVE vulnerability reports mapped to over 3600 product documentations. An evaluation of 8 CWE vulnerability pillars shows that even interpretable whitebox classifiers predict vulnerabilities with up to 61.1% precision and 78% recall. Further, using strategies to improve the relevance of extracted keywords, addressing class imbalance, segregating products into categories such as Operating Systems, Web applications, and Hardware, and using blackbox ensemble models such as the random forest classifier improves the performance to 96% precision and 91.1% recall. The high precision and recall shows that VDocScan can anticipate vulnerabilities detected in a product's lifetime ahead of time during the Design phase to incorporate necessary security mechanisms. The performance is consistently high for vulnerabilities with the mode of introduction: architecture and design.",Achievement,Capable,The paper develops VDocScan to predict vulnerabilities in software products ahead of time during the design phase; demonstrating a user's capabilities in ensuring software security. This aligns with the value item Capable and its corresponding value Achievement.,"Certainly! In the abstract, 'Paper X' introduces VDocScan, a technique that predicts vulnerabilities in software products based on specification documents. By identifying vulnerabilities early in the design phase, software users can incorporate necessary security mechanisms and demonstrate their capability in ensuring software security. This aligns with the value item Capable and its corresponding value Achievement because it emphasizes the user's ability to proactively address potential vulnerabilities and achieve secure software development.",Agreed-Justified,Agree,,Agree,,
Exp B,Exp H,695,ICSE,Security & Privacy,Keyword Extraction From Specification Documents for Planning Security Mechanisms,"Software development companies heavily invest both time and money to provide post-production support to fix security vulnerabilities in their products. Current techniques identify vulnerabilities from source code using static and dynamic analyses. However, this does not help integrate security mechanisms early in the architectural design phase. We develop VDocScan, a technique for predicting vulnerabilities based on specification documents, even before the development stage. We evaluate VDocScan using an extensive dataset of CVE vulnerability reports mapped to over 3600 product documentations. An evaluation of 8 CWE vulnerability pillars shows that even interpretable whitebox classifiers predict vulnerabilities with up to 61.1% precision and 78% recall. Further, using strategies to improve the relevance of extracted keywords, addressing class imbalance, segregating products into categories such as Operating Systems, Web applications, and Hardware, and using blackbox ensemble models such as the random forest classifier improves the performance to 96% precision and 91.1% recall. The high precision and recall shows that VDocScan can anticipate vulnerabilities detected in a product's lifetime ahead of time during the Design phase to incorporate necessary security mechanisms. The performance is consistently high for vulnerabilities with the mode of introduction: architecture and design.",Security,Social Order,VDocScan's ability to predict vulnerabilities ensures the order and smooth running of social infrastructures that rely on these software products; aligning with the value item Social Order and the corresponding value Security.,"The ability of VDocScan to predict vulnerabilities in software products directly aligns with the value item of Social Order and its corresponding value of Security. By anticipating vulnerabilities in the design phase, VDocScan ensures that software products function in a secure manner, thus maintaining the order and smooth functioning of social infrastructures that rely on these software products. This aligns with the value item of Social Order, which emphasizes the importance of maintaining stability and harmony within society, and the corresponding value of Security, which highlights the need for protecting individuals and societal systems from potential harm or risks.",Agreed-Clarified,Disagree,reconcile with coder_2,Disagree,"The justification doesn't align with the value item of the Social Order
",
Exp D,Exp I,696,ICSE,Security & Privacy,On Privacy Weaknesses and Vulnerabilities in Software Systems,"In this digital era, our privacy is under constant threat as our personal data and traceable online/offline activities are frequently collected, processed and transferred by many software applications. Privacy attacks are often formed by exploiting vulnerabilities found in those software applications. The Common Weakness Enumeration (CWE) and Common Vulnerabilities and Exposures (CVE) systems are currently the main sources that software engineers rely on for understanding and preventing publicly disclosed software vulnerabilities. However, our study on all 922 weaknesses in the CWE and 156,537 vulnerabilities registered in the CVE to date has found a very small coverage of privacy-related vulnerabilities in both systems, only 4.45% in CWE and 0.1% in CVE. These also cover only a small number of areas of privacy threats that have been raised in existing privacy software engineering research, privacy regulations and frameworks, and relevant reputable organisations. The actionable insights generated from our study led to the introduction of 11 new common privacy weaknesses to supplement the CWE system, making it become a source for both security and privacy vulnerabilities.",Self Direction,Privacy,The 'Paper X' contributions focus on identifying and addressing privacy weaknesses in software applications; enhancing user privacy. This directly aligns with the value item 'Privacy' and its corresponding value 'Self Direction'.,"In 'Paper X', the main contributions revolve around identifying and addressing privacy weaknesses in software applications to enhance user privacy. This aligns with the value item of 'Privacy' from Schwartz's Taxonomy, as privacy is directly addressed and prioritized in the paper's focus. Furthermore, the corresponding value of 'Self Direction' is aligned, as the paper aims to empower users by improving their control and autonomy over their own personal data and traceable online/offline activities. By addressing privacy vulnerabilities, the paper promotes self-directed decision-making and the ability for individuals to choose how their personal information is collected, processed, and transferred.",Agreed-Justified,Agree,,Agree,,
Exp E,Exp J,696,ICSE,Security & Privacy,On Privacy Weaknesses and Vulnerabilities in Software Systems,"In this digital era, our privacy is under constant threat as our personal data and traceable online/offline activities are frequently collected, processed and transferred by many software applications. Privacy attacks are often formed by exploiting vulnerabilities found in those software applications. The Common Weakness Enumeration (CWE) and Common Vulnerabilities and Exposures (CVE) systems are currently the main sources that software engineers rely on for understanding and preventing publicly disclosed software vulnerabilities. However, our study on all 922 weaknesses in the CWE and 156,537 vulnerabilities registered in the CVE to date has found a very small coverage of privacy-related vulnerabilities in both systems, only 4.45% in CWE and 0.1% in CVE. These also cover only a small number of areas of privacy threats that have been raised in existing privacy software engineering research, privacy regulations and frameworks, and relevant reputable organisations. The actionable insights generated from our study led to the introduction of 11 new common privacy weaknesses to supplement the CWE system, making it become a source for both security and privacy vulnerabilities.",Security,Social Order,By identifying and addressing privacy vulnerabilities in software applications; 'Paper X' helps in maintaining social order as privacy breaches often lead to chaos and mistrust in society. This aligns with the value item 'Social Order' and its corresponding value 'Security'.,"By addressing privacy vulnerabilities in software applications, 'Paper X' contributes to maintaining social order by ensuring the security of personal data and preventing chaos and mistrust that can arise from privacy breaches. Social order relies on individuals feeling a sense of security and trust in their interactions, and addressing privacy vulnerabilities directly aligns with this value item and its corresponding value of security within a software context.",Agreed-Justified,Agree,,Agree,,
Exp F,Exp J,696,ICSE,Security & Privacy,On Privacy Weaknesses and Vulnerabilities in Software Systems,"In this digital era, our privacy is under constant threat as our personal data and traceable online/offline activities are frequently collected, processed and transferred by many software applications. Privacy attacks are often formed by exploiting vulnerabilities found in those software applications. The Common Weakness Enumeration (CWE) and Common Vulnerabilities and Exposures (CVE) systems are currently the main sources that software engineers rely on for understanding and preventing publicly disclosed software vulnerabilities. However, our study on all 922 weaknesses in the CWE and 156,537 vulnerabilities registered in the CVE to date has found a very small coverage of privacy-related vulnerabilities in both systems, only 4.45% in CWE and 0.1% in CVE. These also cover only a small number of areas of privacy threats that have been raised in existing privacy software engineering research, privacy regulations and frameworks, and relevant reputable organisations. The actionable insights generated from our study led to the introduction of 11 new common privacy weaknesses to supplement the CWE system, making it become a source for both security and privacy vulnerabilities.",Universalism,Equality,By exposing the limited coverage of privacy-related vulnerabilities in common systems; 'Paper X' emphasizes the need for equitable privacy standards across all software applications. This relates to the value item 'Equality' and its corresponding value 'Universalism'.,"In 'Paper X', the authors highlight the lack of coverage of privacy-related vulnerabilities in common systems, which implies the unequal treatment of privacy in software applications. This directly aligns with the value item Equality, as it emphasizes the need for fair and equitable privacy standards across all software applications. This aligns with the corresponding value Universalism, which promotes a sense of equality, justice, and fairness in the treatment of individuals' privacy. By addressing the limited coverage and introducing new common privacy weaknesses to supplement existing systems, 'Paper X' contributes to the promotion of equality and universal principles in privacy protection within the software context.",Agreed-Clarified,Agree,,Agree,,
Exp G,Exp K,787,ICSE,Software Testing & QA,Mttm: Metamorphic testing for textual content moderation software,"The exponential growth of social media platforms such as Twitter and Facebook has revolutionized textual communication and textual content publication in human society. However, they have been increasingly exploited to propagate toxic content, such as hate speech, malicious advertisement, and pornography, which can lead to highly negative impacts (e.g., harmful effects on teen mental health). Researchers and practitioners have been enthusiastically developing and extensively deploying textual content moderation software to address this problem. However, we find that malicious users can evade moderation by changing only a few words in the toxic content. Moreover, modern content moderation software's performance against malicious inputs remains underexplored. To this end, we propose MTTM, a Metamorphic Testing framework for Textual content Moderation software. Specifically, we conduct a pilot study on 2, 000 text messages collected from real users and summarize eleven metamorphic relations across three perturbation levels: character, word, and sentence. MTTM employs these metamorphic relations on toxic textual contents to generate test cases, which are still toxic yet likely to evade moderation. In our evaluation, we employ MTTM to test three commercial textual content moderation software and two state-of-the-art moderation algorithms against three kinds of toxic content. The results show that MTTM achieves up to 83.9%, 51%, and 82.5% error finding rates (EFR) when testing commercial moderation software provided by Google, Baidu, and Huawei, respectively, and it obtains up to 91.2% EFR when testing the state-of-the-art algorithms from the academy. In addition, we leverage the test cases generated by MTTM to retrain the model we explored, which largely improves model robustness 0% ~ 5.9% EFR) while maintaining the accuracy on the original test set. A demo can be found in this link1.",Benevolence,Responsibility,The paper contributes to the responsibility value by introducing MTTM; a software solution designed to moderate and manage toxic content which can be harmful to the software users. It improves safety and well-being of the users which directly aligns with the value item Responsibility in the Benevolence value category.,"In 'Paper X', the proposed MTTM framework for textual content moderation directly aligns with the value item Responsibility and its corresponding value Benevolence from the perspective of a software user. The paper explicitly states that the framework aims to address the negative impacts of toxic content on users, such as harmful effects on teen mental health. By improving the safety and well-being of users through more effective content moderation, the paper demonstrates a direct alignment with the value item Responsibility, as it takes on the responsibility to protect and care for the user community. This aligns with the Benevolence value, which emphasizes helpfulness, responsibility, and the promotion of a positive and healthy environment for users.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,787,ICSE,Software Testing & QA,Mttm: Metamorphic testing for textual content moderation software,"The exponential growth of social media platforms such as Twitter and Facebook has revolutionized textual communication and textual content publication in human society. However, they have been increasingly exploited to propagate toxic content, such as hate speech, malicious advertisement, and pornography, which can lead to highly negative impacts (e.g., harmful effects on teen mental health). Researchers and practitioners have been enthusiastically developing and extensively deploying textual content moderation software to address this problem. However, we find that malicious users can evade moderation by changing only a few words in the toxic content. Moreover, modern content moderation software's performance against malicious inputs remains underexplored. To this end, we propose MTTM, a Metamorphic Testing framework for Textual content Moderation software. Specifically, we conduct a pilot study on 2, 000 text messages collected from real users and summarize eleven metamorphic relations across three perturbation levels: character, word, and sentence. MTTM employs these metamorphic relations on toxic textual contents to generate test cases, which are still toxic yet likely to evade moderation. In our evaluation, we employ MTTM to test three commercial textual content moderation software and two state-of-the-art moderation algorithms against three kinds of toxic content. The results show that MTTM achieves up to 83.9%, 51%, and 82.5% error finding rates (EFR) when testing commercial moderation software provided by Google, Baidu, and Huawei, respectively, and it obtains up to 91.2% EFR when testing the state-of-the-art algorithms from the academy. In addition, we leverage the test cases generated by MTTM to retrain the model we explored, which largely improves model robustness 0% ~ 5.9% EFR) while maintaining the accuracy on the original test set. A demo can be found in this link1.",Security,Healthy,The paper contributes to the software user's health by working on reducing the potential harmful effects of toxic content on social media; which can be particularly impactful on mental health for teenagers. This aligns with the value item Health and its corresponding value Security.,"In the paper abstract, it is clearly stated that the exponential growth of social media platforms has led to the propagation of toxic content, which can have harmful effects on teen mental health. The main contribution of 'Paper X' is the development of a textual content moderation software to address this problem. By reducing the potential harmful effects of toxic content on social media, the paper aligns with the value item Healthy and its corresponding value Security. This is because ensuring the health and well-being of software users, specifically in relation to mental health, is a fundamental aspect of providing a secure and safe environment for users to engage with online content.",Agreed-Justified,Agree,,Agree,,
Exp B,Exp H,787,ICSE,Software Testing & QA,Mttm: Metamorphic testing for textual content moderation software,"The exponential growth of social media platforms such as Twitter and Facebook has revolutionized textual communication and textual content publication in human society. However, they have been increasingly exploited to propagate toxic content, such as hate speech, malicious advertisement, and pornography, which can lead to highly negative impacts (e.g., harmful effects on teen mental health). Researchers and practitioners have been enthusiastically developing and extensively deploying textual content moderation software to address this problem. However, we find that malicious users can evade moderation by changing only a few words in the toxic content. Moreover, modern content moderation software's performance against malicious inputs remains underexplored. To this end, we propose MTTM, a Metamorphic Testing framework for Textual content Moderation software. Specifically, we conduct a pilot study on 2, 000 text messages collected from real users and summarize eleven metamorphic relations across three perturbation levels: character, word, and sentence. MTTM employs these metamorphic relations on toxic textual contents to generate test cases, which are still toxic yet likely to evade moderation. In our evaluation, we employ MTTM to test three commercial textual content moderation software and two state-of-the-art moderation algorithms against three kinds of toxic content. The results show that MTTM achieves up to 83.9%, 51%, and 82.5% error finding rates (EFR) when testing commercial moderation software provided by Google, Baidu, and Huawei, respectively, and it obtains up to 91.2% EFR when testing the state-of-the-art algorithms from the academy. In addition, we leverage the test cases generated by MTTM to retrain the model we explored, which largely improves model robustness 0% ~ 5.9% EFR) while maintaining the accuracy on the original test set. A demo can be found in this link1.",Universalism,A World at Peace,By controlling toxic content; the software contributes to a healthier; more peaceful online environment. This reduces conflict between users on the platform and contributes to a sense of peace. This directly aligns with the value item A World at Peace in the Universalism value category.,"The justification for aligning 'Paper X' with the value item A World at Peace and its corresponding value Universalism is that the software's ability to control toxic content contributes to creating a healthier and more peaceful online environment. By reducing conflict between users on the platform, the software helps to foster a sense of peace. This aligns directly with the value item A World at Peace in the Universalism value category.",Agreed-Justified,Agree,,Agree,,
Exp C,Exp I,790,ICSE,Security & Privacy,Taintmini: Detecting flow of sensitive data in mini-programs with static taint analysis,"Mini-programs, which are programs running inside mobile super apps such as WeChat, often have access to privacy-sensitive information, such as location data and phone numbers, through APUs provided by the super apps. This access poses a risk of privacy sensitive data leaks, either accidentally from carelessly programmed mini-programs or intentionally from malicious ones. To address this concern, it is crucial to track the flow of sensitive data in mini-programs for either human analysis or automated tools. Although existing taint analysis techniques have been widely studied, they face unique challenges in tracking sensitive data flows in mini-programs, such as cross-language, cross-page, and cross-mini-program data flows. This paper presents a novel framework, Taintmini, which addresses these challenges by using a novel universal data flow graph approach that captures data flows within and across mini-programs. We have evaluated Taintminiwith 238,866 mini-programs and detect 27,184 that contain sensitive data flows. We have also applied Taintminito detect privacy leakage colluding mini-programs and identify 455 such programs from them that clearly violate privacy policy.",Self Direction,Privacy,The paper presents Taintmini; a novel framework that prevents privacy leaks in mobile super apps. This directly aligns with the 'Privacy' value item under the 'Self Direction' value; as it contributes to maintaining the privacy of software users by protecting their sensitive data.,"The main contribution of 'Paper X', Taintmini, directly aligns with the value item Privacy and its corresponding value Self Direction from a ""Software User"" perspective. By specifically addressing the concern of privacy-sensitive data leaks in mobile super apps, Taintmini actively contributes to maintaining the privacy of software users by effectively protecting their sensitive data. This alignment is evident as Taintmini's framework aims to track the flow of sensitive data within and across mini-programs, enabling the detection and prevention of privacy breaches, which aligns with the value of self-direction in terms of individuals having control over their personal information and choosing to protect their privacy.",Agreed-Justified,Agree,,Agree,,
Exp E,Exp J,790,ICSE,Security & Privacy,Taintmini: Detecting flow of sensitive data in mini-programs with static taint analysis,"Mini-programs, which are programs running inside mobile super apps such as WeChat, often have access to privacy-sensitive information, such as location data and phone numbers, through APUs provided by the super apps. This access poses a risk of privacy sensitive data leaks, either accidentally from carelessly programmed mini-programs or intentionally from malicious ones. To address this concern, it is crucial to track the flow of sensitive data in mini-programs for either human analysis or automated tools. Although existing taint analysis techniques have been widely studied, they face unique challenges in tracking sensitive data flows in mini-programs, such as cross-language, cross-page, and cross-mini-program data flows. This paper presents a novel framework, Taintmini, which addresses these challenges by using a novel universal data flow graph approach that captures data flows within and across mini-programs. We have evaluated Taintminiwith 238,866 mini-programs and detect 27,184 that contain sensitive data flows. We have also applied Taintminito detect privacy leakage colluding mini-programs and identify 455 such programs from them that clearly violate privacy policy.",Universalism,Social Justice,By detecting and preventing privacy-sensitive data leaks; this paper contributes to providing software users a just digital environment where their privacy rights are protected. This aligns with the 'Social Justice' value item under the 'Universalism' value; as it focuses on ensuring justice and fairness in the software user's environment.,"The main contribution of 'Paper X' is the development of Taintmini, a framework that tracks sensitive data flows in mini-programs to detect and prevent privacy-sensitive data leaks. This directly aligns with the value item of Social Justice and its corresponding value of Universalism because it focuses on ensuring justice and fairness in the software user's environment by protecting their privacy rights. Detecting and preventing privacy leaks contributes to creating a just digital environment where individuals' privacy is respected and their rights are protected, which is in line with the principles of Social Justice and Universalism.",Agreed-Justified,Agree,,Agree,,
Exp F,Exp J,792,ICSE,AI & Machine Learning,Fairify: Fairness verification of neural networks,"Fairness of machine learning (ML) software has become a major concern in the recent past. Although recent research on testing and improving fairness have demonstrated impact on real-world software, providing fairness guarantee in practice is still lacking. Certification of ML models is challenging because of the complex decision-making process of the models. In this paper, we proposed Fairify, an SMT-based approach to verify individual fairness property in neural network (NN) models. Individual fairness ensures that any two similar individuals get similar treatment irrespective of their protected attributes e.g., race, sex, age. Verifying this fairness property is hard because of the global checking and non-linear computation nodes in NN. We proposed sound approach to make individual fairness verification tractable for the developers. The key idea is that many neurons in the NN always remain inactive when a smaller part of the input domain is considered. So, Fairify leverages white-box access to the models in production and then apply formal analysis based pruning. Our approach adopts input partitioning and then prunes the NN for each partition to provide fairness certification or counterexample. We leveraged interval arithmetic and activation heuristic of the neurons to perform the pruning as necessary. We evaluated Fairify on 25 real-world neural networks collected from four different sources, and demonstrated the effectiveness, scalability and performance over baseline and closely related work. Fairify is also configurable based on the domain and size of the NN. Our novel formulation of the problem can answer targeted verification queries with relaxations and counterexamples, which have practical implications.",Hedonism,Enjoying Life,The paper presents the development of a machine learning fairness verifier; Fairify; which aims to ensure equal and fair treatment of all users; ultimately delivering a more enjoyable software user experience by eliminating biases in decision making. This aligns with the value item Enjoying Life and its corresponding value Hedonism.,"The development of Fairify, a machine learning fairness verifier, aligns with the value item Enjoying Life and its corresponding value Hedonism because it ensures equal and fair treatment of all users, thereby eliminating biases in decision making. This directly contributes to a more enjoyable software user experience as users can trust that they are receiving unbiased and equitable treatment. By aligning with Hedonism, the paper aims to enhance the overall enjoyment and satisfaction of software users by promoting fairness and eliminating discriminatory practices.",Agreed-Justified,Disagree,,Disagree,This paper presents a method for offering equality in my perspective.,
Exp G,Exp K,792,ICSE,AI & Machine Learning,Fairify: Fairness verification of neural networks,"Fairness of machine learning (ML) software has become a major concern in the recent past. Although recent research on testing and improving fairness have demonstrated impact on real-world software, providing fairness guarantee in practice is still lacking. Certification of ML models is challenging because of the complex decision-making process of the models. In this paper, we proposed Fairify, an SMT-based approach to verify individual fairness property in neural network (NN) models. Individual fairness ensures that any two similar individuals get similar treatment irrespective of their protected attributes e.g., race, sex, age. Verifying this fairness property is hard because of the global checking and non-linear computation nodes in NN. We proposed sound approach to make individual fairness verification tractable for the developers. The key idea is that many neurons in the NN always remain inactive when a smaller part of the input domain is considered. So, Fairify leverages white-box access to the models in production and then apply formal analysis based pruning. Our approach adopts input partitioning and then prunes the NN for each partition to provide fairness certification or counterexample. We leveraged interval arithmetic and activation heuristic of the neurons to perform the pruning as necessary. We evaluated Fairify on 25 real-world neural networks collected from four different sources, and demonstrated the effectiveness, scalability and performance over baseline and closely related work. Fairify is also configurable based on the domain and size of the NN. Our novel formulation of the problem can answer targeted verification queries with relaxations and counterexamples, which have practical implications.",Universalism,Equality,The paper's contribution revolves around the development of software that verifies fairness in machine learning models to ensure equal treatment of users thus aligning with the value item Equality and the corresponding value Universalism.,"The justification for aligning 'Paper X' with the value item Equality and its corresponding value Universalism is based on the paper's explicit focus on verifying fairness in machine learning models. By ensuring that individuals receive equal treatment regardless of their protected attributes, the paper's contributions directly aim to promote equality within the software context. This aligns with the value item Equality, which emphasizes equal treatment and opportunities, and the corresponding value Universalism, which emphasizes respecting the welfare and rights of all individuals.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,792,ICSE,AI & Machine Learning,Fairify: Fairness verification of neural networks,"Fairness of machine learning (ML) software has become a major concern in the recent past. Although recent research on testing and improving fairness have demonstrated impact on real-world software, providing fairness guarantee in practice is still lacking. Certification of ML models is challenging because of the complex decision-making process of the models. In this paper, we proposed Fairify, an SMT-based approach to verify individual fairness property in neural network (NN) models. Individual fairness ensures that any two similar individuals get similar treatment irrespective of their protected attributes e.g., race, sex, age. Verifying this fairness property is hard because of the global checking and non-linear computation nodes in NN. We proposed sound approach to make individual fairness verification tractable for the developers. The key idea is that many neurons in the NN always remain inactive when a smaller part of the input domain is considered. So, Fairify leverages white-box access to the models in production and then apply formal analysis based pruning. Our approach adopts input partitioning and then prunes the NN for each partition to provide fairness certification or counterexample. We leveraged interval arithmetic and activation heuristic of the neurons to perform the pruning as necessary. We evaluated Fairify on 25 real-world neural networks collected from four different sources, and demonstrated the effectiveness, scalability and performance over baseline and closely related work. Fairify is also configurable based on the domain and size of the NN. Our novel formulation of the problem can answer targeted verification queries with relaxations and counterexamples, which have practical implications.",Benevolence,A Spiritual Life,The work on Fairify presented in the paper embodies the aspiration of providing a safe and fair environment where users are treated with respect without discrimination; contributing to the users' spiritual life by creating a space of digital ethics. Thus; it directly aligns with the value item A Spiritual Life under the value Benevolence.,"The work on Fairify aligns with the value item A Spiritual Life and its corresponding value Benevolence because it focuses on ensuring individual fairness in machine learning models and treating all individuals equally, regardless of their protected attributes. By promoting fairness and non-discrimination, the paper contributes to creating a digital environment that embodies ethical principles and respects the spiritual well-being of software users.",Agreed-Justified,Agree,,Agree,,
Exp B,Exp H,794,ICSE,Emerging Technologies,MorphQ: Metamorphic testing of the Qiskit quantum computing platform,"As quantum computing is becoming increasingly popular, the underlying quantum computing platforms are growing both in ability and complexity. Unfortunately, testing these platforms is challenging due to the relatively small number of existing quantum programs and because of the oracle problem, i.e., a lack of specifications of the expected behavior of programs. This paper presents MorphQ, the first metamorphic testing approach for quantum computing platforms. Our two key contributions are (i) a program generator that creates a large and diverse set of valid (i.e., non-crashing) quantum programs, and (ii) a set of program transformations that exploit quantum-specific metamorphic relationships to alleviate the oracle problem. Evaluating the approach by testing the popular Qiskit platform shows that the approach creates over 8k program pairs within two days, many of which expose crashes. Inspecting the crashes, we find 13 bugs, nine of which have already been confirmed. MorphQ widens the slim portfolio of testing techniques of quantum computing platforms, helping to create a reliable software stack for this increasingly important field.",Stimulation,Variation in Life,The paper contributes MorphQ; which can generate a large and diverse set of quantum programs; adding variation and thus aligning with the value item Variation in Life and its corresponding value Stimulation.,"My justification for labeling 'Paper X' as aligning with the value item Variation in Life and its corresponding value Stimulation from a ""Software User"" perspective is based on the fact that the paper's contribution of MorphQ, a program generator that creates a large and diverse set of quantum programs, introduces variation in the quantum computing platforms. This variation can lead to a more stimulating software user experience as it allows for the exploration of different program behaviors and possibilities within the quantum computing domain.",Agreed-Clarified,Agree,,Agree,,
Exp D,Exp I,794,ICSE,Emerging Technologies,MorphQ: Metamorphic testing of the Qiskit quantum computing platform,"As quantum computing is becoming increasingly popular, the underlying quantum computing platforms are growing both in ability and complexity. Unfortunately, testing these platforms is challenging due to the relatively small number of existing quantum programs and because of the oracle problem, i.e., a lack of specifications of the expected behavior of programs. This paper presents MorphQ, the first metamorphic testing approach for quantum computing platforms. Our two key contributions are (i) a program generator that creates a large and diverse set of valid (i.e., non-crashing) quantum programs, and (ii) a set of program transformations that exploit quantum-specific metamorphic relationships to alleviate the oracle problem. Evaluating the approach by testing the popular Qiskit platform shows that the approach creates over 8k program pairs within two days, many of which expose crashes. Inspecting the crashes, we find 13 bugs, nine of which have already been confirmed. MorphQ widens the slim portfolio of testing techniques of quantum computing platforms, helping to create a reliable software stack for this increasingly important field.",Achievement,Successful,The paper mentions the detection of bugs in the popular Qiskit platform through their approach; which indirectly contributes to user success by improving the usability of the platform. This is in sync with Successful from the Achievement value.,"The alignment of 'Paper X' with the value item Successful and its corresponding value Achievement is justified based on the paper's mention of detecting bugs in the Qiskit platform through their approach. By detecting and addressing these bugs, the usability and reliability of the platform are improved, ultimately leading to a more successful user experience. This improvement in user success aligns with the value of Achievement, as it pertains to the successful accomplishment of goals and tasks.",Agreed-Justified,Agree,,Agree,,
Exp E,Exp J,798,ICSE,Security & Privacy,Understanding the threats of upstream vulnerabilities to downstream projects in the maven ecosystem,"Modern software systems are increasingly relying on dependencies from the ecosystem. A recent estimation shows that around 35% of an open-source project's code come from its depended libraries. Unfortunately, open-source libraries are often threatened by various vulnerability issues, and the number of disclosed vulnerabilities is increasing steadily over the years. Such vulnerabilities can pose significant security threats to the whole ecosystem, not only to the vulnerable libraries themselves, but also to the corresponding downstream projects. Many Software Composition Analysis (SCA) tools have been proposed, aiming to detect vulnerable libraries or components referring to existing vulnerability databases. However, recent studies report that such tools often generate a large number of false alerts. Particularly, up to 73.3% of the projects depending on vulnerable libraries are actually safe. Aiming to devise more precise tools, understanding the threats of vulnerabilities holistically in the ecosystem is significant, as already performed by a number of existing studies. However, previous researches either analyze at a very coarse granularity (e.g., without analyzing the source code) or are limited by the study scales. This study aims to bridge such gaps. In particular, we collect 44,450 instances of (CVE, upstream, downstream) relations and analyze around 50 million invocations made from downstream to upstream projects to understand the potential threats of upstream vulnerabilities to downstream projects in the Maven ecosystem. Our investigation makes interesting yet significant findings with respect to multiple aspects, including the reach-ability of vulnerabilities, the complexities of the reachable paths as well as how downstream projects and developers perceive upstream vulnerabilities. We believe such findings can not only provide a holistic understanding towards the threats of upstream vulnerabilities in the Maven ecosystem, but also can guide future researches in this field.",Security,Healthy,The paper is focussed on ensuring the safety and security of open-source libraries; minimizing vulnerability issues that could threaten the health of the entire ecosystem,"In 'Paper X', the main contribution is aimed at addressing the vulnerabilities in open-source libraries and their potential security threats to the entire ecosystem. By identifying and analyzing the (CVE, upstream, downstream) relations in the Maven ecosystem, the paper aims to provide a holistic understanding of the threats posed by upstream vulnerabilities. This analysis directly aligns with the value item Healthy and its corresponding value Security from a ""Software User"" perspective because it focuses on ensuring the safety and security of the software ecosystem, which is essential for maintaining the overall health and well-being of the system. By minimizing vulnerability issues and addressing potential security threats, 'Paper X' contributes to creating a more secure and safe software environment for software users.",Agreed-Justified,Agree,,Agree,,
Exp F,Exp J,798,ICSE,Security & Privacy,Understanding the threats of upstream vulnerabilities to downstream projects in the maven ecosystem,"Modern software systems are increasingly relying on dependencies from the ecosystem. A recent estimation shows that around 35% of an open-source project's code come from its depended libraries. Unfortunately, open-source libraries are often threatened by various vulnerability issues, and the number of disclosed vulnerabilities is increasing steadily over the years. Such vulnerabilities can pose significant security threats to the whole ecosystem, not only to the vulnerable libraries themselves, but also to the corresponding downstream projects. Many Software Composition Analysis (SCA) tools have been proposed, aiming to detect vulnerable libraries or components referring to existing vulnerability databases. However, recent studies report that such tools often generate a large number of false alerts. Particularly, up to 73.3% of the projects depending on vulnerable libraries are actually safe. Aiming to devise more precise tools, understanding the threats of vulnerabilities holistically in the ecosystem is significant, as already performed by a number of existing studies. However, previous researches either analyze at a very coarse granularity (e.g., without analyzing the source code) or are limited by the study scales. This study aims to bridge such gaps. In particular, we collect 44,450 instances of (CVE, upstream, downstream) relations and analyze around 50 million invocations made from downstream to upstream projects to understand the potential threats of upstream vulnerabilities to downstream projects in the Maven ecosystem. Our investigation makes interesting yet significant findings with respect to multiple aspects, including the reach-ability of vulnerabilities, the complexities of the reachable paths as well as how downstream projects and developers perceive upstream vulnerabilities. We believe such findings can not only provide a holistic understanding towards the threats of upstream vulnerabilities in the Maven ecosystem, but also can guide future researches in this field.",Security,National Security,The study prioritizes on the aspect of national security by analysing threats posed upon the ecosystems by upstream vulnerabilities. This will strengthen software's reliability; lessening the security concerns of the users,"The justification for the alignment of 'Paper X' with the value item National Security and its corresponding value Security from a ""Software User"" perspective is based on the fact that the study focuses on analyzing and understanding the potential threats of upstream vulnerabilities in the Maven ecosystem. By doing so, the study aims to bridge the gap in knowledge and provide a holistic understanding of these threats, which can ultimately lead to safer software systems and enhance the security and reliability of the software used by the users. This directly aligns with the value of Security, as it addresses the concerns and priorities of software users in terms of protecting their systems and data from potential security threats.",Agreed-Justified,Disagree,I checked again and this is related to the security ,Agree,,
Exp G,Exp K,798,ICSE,Security & Privacy,Understanding the threats of upstream vulnerabilities to downstream projects in the maven ecosystem,"Modern software systems are increasingly relying on dependencies from the ecosystem. A recent estimation shows that around 35% of an open-source project's code come from its depended libraries. Unfortunately, open-source libraries are often threatened by various vulnerability issues, and the number of disclosed vulnerabilities is increasing steadily over the years. Such vulnerabilities can pose significant security threats to the whole ecosystem, not only to the vulnerable libraries themselves, but also to the corresponding downstream projects. Many Software Composition Analysis (SCA) tools have been proposed, aiming to detect vulnerable libraries or components referring to existing vulnerability databases. However, recent studies report that such tools often generate a large number of false alerts. Particularly, up to 73.3% of the projects depending on vulnerable libraries are actually safe. Aiming to devise more precise tools, understanding the threats of vulnerabilities holistically in the ecosystem is significant, as already performed by a number of existing studies. However, previous researches either analyze at a very coarse granularity (e.g., without analyzing the source code) or are limited by the study scales. This study aims to bridge such gaps. In particular, we collect 44,450 instances of (CVE, upstream, downstream) relations and analyze around 50 million invocations made from downstream to upstream projects to understand the potential threats of upstream vulnerabilities to downstream projects in the Maven ecosystem. Our investigation makes interesting yet significant findings with respect to multiple aspects, including the reach-ability of vulnerabilities, the complexities of the reachable paths as well as how downstream projects and developers perceive upstream vulnerabilities. We believe such findings can not only provide a holistic understanding towards the threats of upstream vulnerabilities in the Maven ecosystem, but also can guide future researches in this field.",Self Direction,Self Respect,The paper's initiative to fix the false alert system in the Software Composition Analysis tools emphasizes on self-respect; allowing users to trust the system and not question their competencies because of flawed tool feedback.,"The alignment of 'Paper X' with the value item Self Respect and its corresponding value Self Direction is evident in the paper's aim to address the issue of false alerts in Software Composition Analysis tools. By improving the accuracy of these tools, users can have confidence in their abilities and trust the system's feedback, allowing them to make informed decisions without questioning their competencies. This emphasis on self-respect aligns with the value of self-direction, as it empowers software users to independently navigate the ecosystem and prioritize their own security and decision-making process.",Agreed-Justified,Disagree,"The abstract primarily focuses on privacy and security by enhancing the accuracy of these tools and  is not related to the self respect.






",Disagree,agree with coder_1,
Exp A,Exp H,799,ICSE,Security & Privacy,Bad Snakes: Understanding and Improving Python Package Index Malware Scanning,"Open-source, community-driven package repositories see thousands of malware packages each year, but do not currently run automated malware detection systems. In this work, we explore the security goals of the repository administrators and the requirements for deploying such malware scanners via a case study of the Python ecosystem and PyPI repository, including interviews with administrators and maintainers. Further, we evaluate existing malware detection techniques for deployment in this setting by creating a benchmark dataset and comparing several existing tools: the malware checks implemented in PyPI, Bandit4Mal, and OSSGadget's OSS Detect Backdoor. We find that repository administrators have exacting requirements for such malware detection tools. Specifically, they consider a false positive rate of even 0.1% to be unacceptably high, given the large number of package releases that might trigger false alerts. Measured tools have false positive rates between 15% and 97%; increasing thresholds for detection rules to reduce this rate renders the true positive rate useless. While automated tools are far from reaching these demands, we find that a socio-technical malware detection system has emerged to meet these needs: external security researchers perform repository malware scans, filter for useful results, and report the results to repository administrators. These parties face different incentives and constraints on their time and tooling. We conclude with recommendations for improving detection capabilities and strengthening the collaboration between security researchers and software repository administrators.",Security,Healthy,The main contribution of 'Paper X' is aimed at enhancing the security of software repositories by improving malware detection. This directly aligns with the value of 'Security'; specifically the value item 'Healthy' as it focuses on mitigating threats and maintaining a safe environment for the software users.,"In the context of 'Paper X', the main contribution is centered around enhancing the security of software repositories. This aligns with the value item 'Healthy' from Schwartz's Taxonomy, as it directly addresses the mitigation of threats and the maintenance of a safe environment for software users. By implementing improved malware detection techniques, the paper aims to ensure that the software ecosystem remains healthy and secure, providing users with confidence in the integrity of the packages they download. This alignment with the value item 'Healthy' demonstrates a focus on safeguarding the well-being and protection of software users, emphasizing the importance of security in the software context.",Agreed-Justified,Disagree,reconcile with coder_2,Disagree,"The justification doesn't align with the value item of the Healthy
",
Exp C,Exp I,799,ICSE,Security & Privacy,Bad Snakes: Understanding and Improving Python Package Index Malware Scanning,"Open-source, community-driven package repositories see thousands of malware packages each year, but do not currently run automated malware detection systems. In this work, we explore the security goals of the repository administrators and the requirements for deploying such malware scanners via a case study of the Python ecosystem and PyPI repository, including interviews with administrators and maintainers. Further, we evaluate existing malware detection techniques for deployment in this setting by creating a benchmark dataset and comparing several existing tools: the malware checks implemented in PyPI, Bandit4Mal, and OSSGadget's OSS Detect Backdoor. We find that repository administrators have exacting requirements for such malware detection tools. Specifically, they consider a false positive rate of even 0.1% to be unacceptably high, given the large number of package releases that might trigger false alerts. Measured tools have false positive rates between 15% and 97%; increasing thresholds for detection rules to reduce this rate renders the true positive rate useless. While automated tools are far from reaching these demands, we find that a socio-technical malware detection system has emerged to meet these needs: external security researchers perform repository malware scans, filter for useful results, and report the results to repository administrators. These parties face different incentives and constraints on their time and tooling. We conclude with recommendations for improving detection capabilities and strengthening the collaboration between security researchers and software repository administrators.",Universalism,Social Justice,The paper's contributions include recommendations for improving detection capabilities and fostering collaboration between security researchers and software administrators. This effort embodies 'Universalism'; especially the value-item 'Social Justice'; emphasizing fair and equitable practices in maintaining a secure environment for software users.,"The paper aligns with the value item 'Social Justice' and its corresponding value 'Universalism' because it emphasizes the importance of fair and equitable practices in maintaining a secure environment for software users. The recommendations for improving detection capabilities and strengthening the collaboration between security researchers and software administrators demonstrate a commitment to addressing social inequalities and ensuring equal access to safe software for all users. This aligns with the value of 'Universalism', which values equality, social justice, and a world at peace.",Agreed-Clarified,Agree,"when we talk about security reaechers and SE team, we try to consider both group. So it is a kind of equitability",Agree,Securiy/Healthy,
Exp D,Exp I,803,ICSE,Software Testing & QA,Many-objective reinforcement learning for online testing of dnn-enabled systems,"Deep Neural Networks (DNNs) have been widely used to perform real-world tasks in cyber-physical systems such as Autonomous Driving Systems (ADS). Ensuring the correct behavior of such DNN-Enabled Systems (DES) is a crucial topic. Online testing is one of the promising modes for testing such systems with their application environments (simulated or real) in a closed loop, taking into account the continuous interaction between the systems and their environments. However, the environmental variables (e.g., lighting conditions) that might change during the systems' operation in the real world, causing the DES to violate requirements (safety, functional), are often kept constant during the execution of an online test scenario due to the two major challenges: (1) the space of all possible scenarios to explore would become even larger if they changed and (2) there are typically many requirements to test simultaneously. In this paper, we present MORLOT (Many-Objective Rein-forcement Learning for Online Testing), a novel online testing approach to address these challenges by combining Reinforcement Learning (RL) and many-objective search. MORLOT leverages RL to incrementally generate sequences of environmental changes while relying on many-objective search to determine the changes so that they are more likely to achieve any of the uncovered objectives. We empirically evaluate MORLOT using CARLA, a high-fidelity simulator widely used for autonomous driving research, integrated with Transfuser, a DNN-enabled ADS for end-to-end driving. The evaluation results show that MORLOT is significantly more effective and efficient than alternatives with a large effect size. In other words, MORLOT is a good option to test DES with dynamically changing environments while accounting for multiple safety requirements.",Security,Healthy,"The paper presents MORLOT; a method for online testing of DNN-Enabled Systems in Autonomous Driving Systems; which ensures their correct behavior. This direct contribution aligns with the value item ""Healthy"" as it enhances usability and safety for end users.","The justification for aligning 'Paper X' with the value item Healthy and its corresponding value Security is based on the fact that the paper presents MORLOT, a method for online testing of DNN-Enabled Systems in Autonomous Driving Systems, which ensures their correct behavior. By focusing on online testing and incorporating environmental changes, the method aims to enhance usability and safety for end users. This aligns with the value item Healthy as it addresses the important aspect of system reliability and the value of Security as it aims to provide a secure and safe driving experience.",Agreed-Justified,Agree,,Agree,,
Exp E,Exp J,803,ICSE,Software Testing & QA,Many-objective reinforcement learning for online testing of dnn-enabled systems,"Deep Neural Networks (DNNs) have been widely used to perform real-world tasks in cyber-physical systems such as Autonomous Driving Systems (ADS). Ensuring the correct behavior of such DNN-Enabled Systems (DES) is a crucial topic. Online testing is one of the promising modes for testing such systems with their application environments (simulated or real) in a closed loop, taking into account the continuous interaction between the systems and their environments. However, the environmental variables (e.g., lighting conditions) that might change during the systems' operation in the real world, causing the DES to violate requirements (safety, functional), are often kept constant during the execution of an online test scenario due to the two major challenges: (1) the space of all possible scenarios to explore would become even larger if they changed and (2) there are typically many requirements to test simultaneously. In this paper, we present MORLOT (Many-Objective Rein-forcement Learning for Online Testing), a novel online testing approach to address these challenges by combining Reinforcement Learning (RL) and many-objective search. MORLOT leverages RL to incrementally generate sequences of environmental changes while relying on many-objective search to determine the changes so that they are more likely to achieve any of the uncovered objectives. We empirically evaluate MORLOT using CARLA, a high-fidelity simulator widely used for autonomous driving research, integrated with Transfuser, a DNN-enabled ADS for end-to-end driving. The evaluation results show that MORLOT is significantly more effective and efficient than alternatives with a large effect size. In other words, MORLOT is a good option to test DES with dynamically changing environments while accounting for multiple safety requirements.",Security,Social Order,"MORLOT not only ensures correct behavior of the system but also accounts for multiple safety requirements. This adherence to safety parameters can be related to the value item ""Social Order""; promoting a stable and predictable environment for the users.","In the context of a ""Software User"" perspective, MORLOT's adherence to multiple safety requirements can be directly aligned with the value item ""Social Order"" because it promotes a sense of stability and predictability in the software system. By ensuring the correct behavior of the system and accounting for safety parameters, MORLOT contributes to establishing a structured and organized environment for users, where they can trust that their interactions with the software will be reliable and secure.",Agreed-Clarified,Agree,,Agree,,
Exp F,Exp J,803,ICSE,Software Testing & QA,Many-objective reinforcement learning for online testing of dnn-enabled systems,"Deep Neural Networks (DNNs) have been widely used to perform real-world tasks in cyber-physical systems such as Autonomous Driving Systems (ADS). Ensuring the correct behavior of such DNN-Enabled Systems (DES) is a crucial topic. Online testing is one of the promising modes for testing such systems with their application environments (simulated or real) in a closed loop, taking into account the continuous interaction between the systems and their environments. However, the environmental variables (e.g., lighting conditions) that might change during the systems' operation in the real world, causing the DES to violate requirements (safety, functional), are often kept constant during the execution of an online test scenario due to the two major challenges: (1) the space of all possible scenarios to explore would become even larger if they changed and (2) there are typically many requirements to test simultaneously. In this paper, we present MORLOT (Many-Objective Rein-forcement Learning for Online Testing), a novel online testing approach to address these challenges by combining Reinforcement Learning (RL) and many-objective search. MORLOT leverages RL to incrementally generate sequences of environmental changes while relying on many-objective search to determine the changes so that they are more likely to achieve any of the uncovered objectives. We empirically evaluate MORLOT using CARLA, a high-fidelity simulator widely used for autonomous driving research, integrated with Transfuser, a DNN-enabled ADS for end-to-end driving. The evaluation results show that MORLOT is significantly more effective and efficient than alternatives with a large effect size. In other words, MORLOT is a good option to test DES with dynamically changing environments while accounting for multiple safety requirements.",Achievement,Capable,"By proving that MORLOT is more effective and efficient than alternatives; the paper contributes to the value item ""Capable"" under the value ""Achievement"". It assists the users in accomplishing tasks successfully using the software.","In the paper, MORLOT is presented as a novel approach for online testing in cyber-physical systems. By demonstrating that MORLOT is significantly more effective and efficient compared to alternatives, the paper directly aligns with the value item ""Capable"" under the value ""Achievement"" from a ""Software User"" perspective. This alignment is evident as MORLOT enables users to accomplish tasks successfully using the software, fulfilling their desire for achievement and demonstrating their capability in utilizing the system effectively.",Agreed-Justified,Agree,,Agree,,
Exp G,Exp K,808,ICSE,Code Generation & Analysis,APICAD: Augmenting API Misuse Detection through Specifications from Code and Documents,"Using API should follow its specifications. Otherwise, it can bring security impacts while the functionality is damaged. To detect API misuse, we need to know what its specifications are. In addition to being provided manually, current tools usually mine the majority usage in the existing codebase as specifications, or capture specifications from its relevant texts in human language. However, the former depends on the quality of the codebase itself, while the latter is limited to the irregularity of the text. In this work, we observe that the information carried by code and documents can complement each other. To mitigate the demand for a high-quality codebase and reduce the pressure to capture valid information from texts, we present APICAD to detect API misuse bugs of C/C++ by combining the specifications mined from code and documents. On the one hand, we effectively build the contexts for API invocations and mine specifications from them through a frequency-based method. On the other hand, we acquire the specifications from documents by using lightweight keyword-based and NLP-assisted techniques. Finally, the combined specifications are generated for bug detection. Experiments show that APICAD can handle diverse API usage semantics to deal with different types of API misuse bugs. With the help of APICAD, we report 153 new bugs in Curl, Httpd, OpenSSL and Linux kernel, 145 of which have been confirmed and 126 have applied our patches.",Security,Healthy,The paper contributes a method; APICAD; for detecting API misuse bugs; which can potentially improve the health and performance of software applications - aligning with the value item 'Healthy' and its corresponding value 'Security'.,"In the abstract of 'Paper X', it is stated that the proposed method, APICAD, aims to detect API misuse bugs in order to mitigate security impacts and ensure the functionality of software applications is not damaged. By detecting and resolving API misuse bugs, APICAD contributes to improving the security and stability of software applications, which aligns with the value item 'Healthy' and its corresponding value 'Security' in Schwartz's Taxonomy.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,808,ICSE,Code Generation & Analysis,APICAD: Augmenting API Misuse Detection through Specifications from Code and Documents,"Using API should follow its specifications. Otherwise, it can bring security impacts while the functionality is damaged. To detect API misuse, we need to know what its specifications are. In addition to being provided manually, current tools usually mine the majority usage in the existing codebase as specifications, or capture specifications from its relevant texts in human language. However, the former depends on the quality of the codebase itself, while the latter is limited to the irregularity of the text. In this work, we observe that the information carried by code and documents can complement each other. To mitigate the demand for a high-quality codebase and reduce the pressure to capture valid information from texts, we present APICAD to detect API misuse bugs of C/C++ by combining the specifications mined from code and documents. On the one hand, we effectively build the contexts for API invocations and mine specifications from them through a frequency-based method. On the other hand, we acquire the specifications from documents by using lightweight keyword-based and NLP-assisted techniques. Finally, the combined specifications are generated for bug detection. Experiments show that APICAD can handle diverse API usage semantics to deal with different types of API misuse bugs. With the help of APICAD, we report 153 new bugs in Curl, Httpd, OpenSSL and Linux kernel, 145 of which have been confirmed and 126 have applied our patches.",Security,Social Order,By detecting API misuse bugs; APICAD helps to maintain the correct functionality and order of software applications; aligning with the value item 'Social Order' and its corresponding value 'Security'.,"By detecting API misuse bugs, APICAD significantly contributes to the value item of Social Order and its corresponding value of Security. API misuse can lead to malfunctions and vulnerabilities in software applications, which can disrupt the overall order and stability of the system. By identifying and addressing these bugs, APICAD ensures that software applications function correctly and securely, promoting the maintenance of social order within the software context. Users rely on software to perform tasks efficiently and reliably, and the presence of API misuse bugs can undermine their trust in the software and its ability to maintain a secure and orderly environment. APICAD directly aligns with the value item of Social Order by mitigating the potential risks and maintaining the security and stability of software applications.",Agreed-Justified,Disagree,reconcile with coder_2,Disagree,"The justification doesn't align with the value item of the Social Order
",
Exp B,Exp H,808,ICSE,Code Generation & Analysis,APICAD: Augmenting API Misuse Detection through Specifications from Code and Documents,"Using API should follow its specifications. Otherwise, it can bring security impacts while the functionality is damaged. To detect API misuse, we need to know what its specifications are. In addition to being provided manually, current tools usually mine the majority usage in the existing codebase as specifications, or capture specifications from its relevant texts in human language. However, the former depends on the quality of the codebase itself, while the latter is limited to the irregularity of the text. In this work, we observe that the information carried by code and documents can complement each other. To mitigate the demand for a high-quality codebase and reduce the pressure to capture valid information from texts, we present APICAD to detect API misuse bugs of C/C++ by combining the specifications mined from code and documents. On the one hand, we effectively build the contexts for API invocations and mine specifications from them through a frequency-based method. On the other hand, we acquire the specifications from documents by using lightweight keyword-based and NLP-assisted techniques. Finally, the combined specifications are generated for bug detection. Experiments show that APICAD can handle diverse API usage semantics to deal with different types of API misuse bugs. With the help of APICAD, we report 153 new bugs in Curl, Httpd, OpenSSL and Linux kernel, 145 of which have been confirmed and 126 have applied our patches.",Power,Social Recognition,The paper's method helps reveal bugs and make necessary patches; contributing to a higher performance software; potentially leading to increased social recognition for the user from utilizing a highly efficient tool - aligning with the value item 'Social Recognition' and its corresponding value 'Power'.,"In 'Paper X', the authors propose a method called APICAD that helps detect API misuse bugs in C/C++ code. By combining specifications mined from code and documents, they are able to generate comprehensive specifications for bug detection. This method ultimately leads to the identification and patching of bugs, resulting in higher performance software. From a ""Software User"" perspective, utilizing a highly efficient and bug-free tool can enhance the user's productivity and effectiveness, potentially gaining recognition and power within their software context. Hence, the alignment with the value item 'Social Recognition' and its corresponding value 'Power' is evident and relevant.",Agreed-Justified,Agree,,Agree,,
Exp C,Exp I,818,ICSE,Mobile & IoT,Ex pede Herculem: Augmenting Activity Transition Graph for Apps via Graph Convolution Network,"Mobile apps are indispensable for people's daily life. With the increase of GUI functions, apps have become more complex and diverse. As the Android app is event-driven, Activity Transition Graph (ATG) becomes an important way of app abstract and graphical user interface (GUI) modeling. Although existing works provide static and dynamic analysis to build ATG for applications, the completeness of ATG obtained is poor due to the low coverage of these techniques. To tackle this challenge, we propose a novel approach, ArchiDroid, to automatically augment the ATG via graph convolution network. It models both the semantics of activities and the graph structure of activity transitions to predict the transition between activities based on the seed ATG extracted by static analysis. The evaluation demonstrates that ArchiDroid can achieve 86% precision and 94% recall in predicting the transition between activities for augmenting ATG. We further apply the augmented ATG in two downstream tasks, i.e., guidance in automated GUI testing and assistance in app function design. Results show that the automated GUI testing tool integrated with ArchiDroid achieves 43% more activity coverage and detects 208% more bugs. Besides, ArchiDroid can predict the missing transition with 85% accuracy in real-world apps for assisting the app function design, and an interview case study further demonstrates its usefulness.",Self Direction,Creativity,ArchiDroid is a novel approach that uses the graph convolution network to augment the ATG; allowing for more diverse and creative GUI modeling functionalities which directly aligns with the value item 'Creativity' and its associated value 'Self Direction'.,"The justification for aligning 'Paper X' with the value item Creativity and its corresponding value Self Direction is that ArchiDroid, as described in the abstract, introduces a novel approach that utilizes graph convolution network to augment the Activity Transition Graph (ATG) in Android apps. This augmentation enables more diverse and creative GUI modeling functionalities, allowing software users to have greater control and autonomy in choosing and designing their own app interfaces. This directly aligns with the value item Creativity and its associated value Self Direction, as it empowers users to exercise their own independent thinking and explore unique and personalized app experiences.",Agreed-Clarified,Agree,,Agree,,
Exp D,Exp I,818,ICSE,Mobile & IoT,Ex pede Herculem: Augmenting Activity Transition Graph for Apps via Graph Convolution Network,"Mobile apps are indispensable for people's daily life. With the increase of GUI functions, apps have become more complex and diverse. As the Android app is event-driven, Activity Transition Graph (ATG) becomes an important way of app abstract and graphical user interface (GUI) modeling. Although existing works provide static and dynamic analysis to build ATG for applications, the completeness of ATG obtained is poor due to the low coverage of these techniques. To tackle this challenge, we propose a novel approach, ArchiDroid, to automatically augment the ATG via graph convolution network. It models both the semantics of activities and the graph structure of activity transitions to predict the transition between activities based on the seed ATG extracted by static analysis. The evaluation demonstrates that ArchiDroid can achieve 86% precision and 94% recall in predicting the transition between activities for augmenting ATG. We further apply the augmented ATG in two downstream tasks, i.e., guidance in automated GUI testing and assistance in app function design. Results show that the automated GUI testing tool integrated with ArchiDroid achieves 43% more activity coverage and detects 208% more bugs. Besides, ArchiDroid can predict the missing transition with 85% accuracy in real-world apps for assisting the app function design, and an interview case study further demonstrates its usefulness.",Achievement,Intelligent,ArchiDroid improves the precision and recall rate in predicting the transition between activities for augmenting ATG; thus enhancing the 'Intelligence' and performance of the mobile app which contributes to 'Achievement' value from the software user's perspective.,"ArchiDroid's improvement in the precision and recall rate of predicting activity transitions directly contributes to the value of 'Intelligence' as it enhances the accuracy and efficiency of the mobile app's functionality. This aligns with the value of 'Achievement' from the perspective of a software user, as it enables them to accomplish tasks with greater success and progress, ultimately leading to a sense of accomplishment and fulfillment.",Agreed-Justified,Agree,,Agree,,
Exp E,Exp J,818,ICSE,Mobile & IoT,Ex pede Herculem: Augmenting Activity Transition Graph for Apps via Graph Convolution Network,"Mobile apps are indispensable for people's daily life. With the increase of GUI functions, apps have become more complex and diverse. As the Android app is event-driven, Activity Transition Graph (ATG) becomes an important way of app abstract and graphical user interface (GUI) modeling. Although existing works provide static and dynamic analysis to build ATG for applications, the completeness of ATG obtained is poor due to the low coverage of these techniques. To tackle this challenge, we propose a novel approach, ArchiDroid, to automatically augment the ATG via graph convolution network. It models both the semantics of activities and the graph structure of activity transitions to predict the transition between activities based on the seed ATG extracted by static analysis. The evaluation demonstrates that ArchiDroid can achieve 86% precision and 94% recall in predicting the transition between activities for augmenting ATG. We further apply the augmented ATG in two downstream tasks, i.e., guidance in automated GUI testing and assistance in app function design. Results show that the automated GUI testing tool integrated with ArchiDroid achieves 43% more activity coverage and detects 208% more bugs. Besides, ArchiDroid can predict the missing transition with 85% accuracy in real-world apps for assisting the app function design, and an interview case study further demonstrates its usefulness.",Security,Sense of Belonging,The augmentation of ATG by ArchiDroid provides guidance in automated GUI testing; assistance in app function design; and detects more bugs; improving the software user's sense of 'Belonging' by ensuring a seamless and error-free interaction with the app which aligns with the value 'Security'.,"The augmentation of ATG by ArchiDroid, which improves automated GUI testing, assists in app function design, and detects more bugs, directly aligns with the value item Sense of Belonging and its corresponding value Security. By ensuring a seamless and error-free interaction with the app, ArchiDroid enhances the software user's sense of belonging and provides a secure experience, which is essential for users to feel a sense of security and belonging while using the software.",Agreed-Justified,Agree,,Agree,,
Exp F,Exp J,823,ICSE,Software Testing & QA,Metamorphic shader fusion for testing graphics shader compilers,"Computer graphics are powered by graphics APIs (e.g., OpenGL, Direct3D) and their associated shader compilers, which render high-quality images by compiling and optimizing user-written high-level shader programs into GPU machine code. Graphics rendering is extensively used in production scenarios like virtual reality (VR), gaming, autonomous driving, and robotics. Despite the development by industrial manufacturers such as Intel, Nvidia, and AMD, shader compilers - like traditional software - may produce ill-rendered outputs. In turn, these errors may result in negative results, from poor user experience in entertainment to accidents in driving assistance systems. This paper introduces FSHADER, a metamorphic testing (MT) framework designed specifically for shader compilers to uncover erroneous compilations and optimizations. FSHADER tests shader compilers by mutating input shader programs via four carefully-designed metamorphic relations (MRs). In particular, FSHADER fuses two shader programs via an MR and checks the visual consistency between the image rendered from the fused shader program with the output of fusing individually rendered images. Our study of 12 shader compilers covers five mainstream GPU vendors, including Intel, AMD, Nvidia, ARM, and Apple. We successfully uncover over 16K error-triggering inputs that generate incorrect rendering outputs. We manually locate and characterize buggy optimization places, and developers have confirmed representative bugs.",Stimulation,Excitement in Life,The paper introduces FSHADER; a framework for testing shader compilers specifically designed to uncover issues in rendering and optimize user experience in various applications; such as gaming and VR. This aligns with the value item Excitement in Life because users can have a more immersive and exciting graphics experience through games or VR due to optimized rendering.,"Certainly! The main contribution of 'Paper X' is the introduction of FSHADER, a framework designed to test and uncover errors in shader compilers that may result in poor rendering outputs. By optimizing the rendering process, FSHADER aims to enhance user experience in various software applications, such as gaming and VR. This directly aligns with the value item Excitement in Life and its corresponding value Stimulation, as users can enjoy a more immersive and exciting graphics experience through games or VR, thanks to the optimized rendering provided by FSHADER.",Agreed-Clarified,Agree,,Agree,,
Exp G,Exp K,823,ICSE,Software Testing & QA,Metamorphic shader fusion for testing graphics shader compilers,"Computer graphics are powered by graphics APIs (e.g., OpenGL, Direct3D) and their associated shader compilers, which render high-quality images by compiling and optimizing user-written high-level shader programs into GPU machine code. Graphics rendering is extensively used in production scenarios like virtual reality (VR), gaming, autonomous driving, and robotics. Despite the development by industrial manufacturers such as Intel, Nvidia, and AMD, shader compilers - like traditional software - may produce ill-rendered outputs. In turn, these errors may result in negative results, from poor user experience in entertainment to accidents in driving assistance systems. This paper introduces FSHADER, a metamorphic testing (MT) framework designed specifically for shader compilers to uncover erroneous compilations and optimizations. FSHADER tests shader compilers by mutating input shader programs via four carefully-designed metamorphic relations (MRs). In particular, FSHADER fuses two shader programs via an MR and checks the visual consistency between the image rendered from the fused shader program with the output of fusing individually rendered images. Our study of 12 shader compilers covers five mainstream GPU vendors, including Intel, AMD, Nvidia, ARM, and Apple. We successfully uncover over 16K error-triggering inputs that generate incorrect rendering outputs. We manually locate and characterize buggy optimization places, and developers have confirmed representative bugs.",Power,Wealth,FSHADER tests graphics APIs; used extensively in autonomous driving systems and robotics; and through its effective detection of errors; can contribute to enhancing the quality of such systems. By contributing to wealthier industrial applications; this aligns with the value item Wealth under Power; as it is indirectly facilitating better industrial advancements which could result in increased productivity and economic benefits.,"FSHADER contributes to enhancing the quality of autonomous driving systems and robotics through effective error detection in graphics APIs. By improving the reliability and performance of these systems, FSHADER indirectly facilitates better industrial advancements, which can lead to increased productivity and economic benefits. This aligns with the value item Wealth and its corresponding value Power from a ""Software User"" perspective as it offers the potential for societal and economic gains.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,823,ICSE,Software Testing & QA,Metamorphic shader fusion for testing graphics shader compilers,"Computer graphics are powered by graphics APIs (e.g., OpenGL, Direct3D) and their associated shader compilers, which render high-quality images by compiling and optimizing user-written high-level shader programs into GPU machine code. Graphics rendering is extensively used in production scenarios like virtual reality (VR), gaming, autonomous driving, and robotics. Despite the development by industrial manufacturers such as Intel, Nvidia, and AMD, shader compilers - like traditional software - may produce ill-rendered outputs. In turn, these errors may result in negative results, from poor user experience in entertainment to accidents in driving assistance systems. This paper introduces FSHADER, a metamorphic testing (MT) framework designed specifically for shader compilers to uncover erroneous compilations and optimizations. FSHADER tests shader compilers by mutating input shader programs via four carefully-designed metamorphic relations (MRs). In particular, FSHADER fuses two shader programs via an MR and checks the visual consistency between the image rendered from the fused shader program with the output of fusing individually rendered images. Our study of 12 shader compilers covers five mainstream GPU vendors, including Intel, AMD, Nvidia, ARM, and Apple. We successfully uncover over 16K error-triggering inputs that generate incorrect rendering outputs. We manually locate and characterize buggy optimization places, and developers have confirmed representative bugs.",Security,Healthy,The paper contributes to the testing of graphics rendering vital in scenarios like VR; gaming; autonomous driving; and robotics. This aligns with the value item Healthy within the value of Security because erroneous graphic renderings in applications like autonomous driving could potentially lead to dangerous situations; and the validation and correction of such errors contribute to safer user experiences.,"The justification provided aligns with the value item Healthy and its corresponding value Security because the paper focuses on testing graphics rendering, which is essential for applications like VR, gaming, autonomous driving, and robotics. Errors in graphic renderings, particularly in autonomous driving, can have serious implications on user safety. By uncovering and correcting such errors, the paper contributes to ensuring a safer user experience in these software contexts, thus aligning with the value item Healthy within the value of Security.",Agreed-Justified,Agree,,Agree,,
Exp B,Exp H,824,ICSE,Security & Privacy,PTPDroid: Detecting Violated User Privacy Disclosures to Third-Parties of Android Apps,"Android apps frequently access personal information to provide customized services. Since such information is sensitive in general, regulators require Android app vendors to publish privacy policies that describe what information is collected and why it is collected. Existing work mainly focuses on the types of the collected data but seldom considers the entities that collect user privacy, which could falsely classify problematic declarations about user privacy collected by third-parties into clear disclosures. To address this problem, we propose PTPDroid, a flow-to-policy consistency checking approach and an automated tool, to comprehensively uncover from the privacy policy the violated disclosures to third-parties. Our experiments on real-world apps demonstrate the effectiveness and superiority of PTPDroid, and our empirical study on 1,000 popular real-world apps reveals that violated user privacy disclosures to third-parties are prevalent in practice.",Self Direction,Privacy,The main focus of 'Paper X' is on enhancing user privacy by proposing a tool (PTPDroid) that checks for consistency between data usage and privacy policies; especially data shared with third parties. This directly aligns with the value item 'Privacy' under the value 'Self Direction'.,"In 'Paper X', the main objective is to address the issue of user privacy in Android apps. The proposed tool, PTPDroid, aims to uncover violated user privacy disclosures to third parties by checking the consistency between data usage and privacy policies. By focusing on enhancing user privacy, the paper aligns with the value item 'Privacy' under the value 'Self Direction'. This alignment is evident as the paper explicitly states that it aims to unveil problematic declarations about user privacy collected by third parties, reflecting the importance of self-directed control over personal information. Therefore, the direct alignment between the main contributions of 'Paper X' and the value item 'Privacy' under 'Self Direction' is supported by the paper's focus on protecting user privacy and allowing individuals to make informed choices regarding their personal information.",Agreed-Justified,Agree,,Agree,,
Exp D,Exp I,824,ICSE,Security & Privacy,PTPDroid: Detecting Violated User Privacy Disclosures to Third-Parties of Android Apps,"Android apps frequently access personal information to provide customized services. Since such information is sensitive in general, regulators require Android app vendors to publish privacy policies that describe what information is collected and why it is collected. Existing work mainly focuses on the types of the collected data but seldom considers the entities that collect user privacy, which could falsely classify problematic declarations about user privacy collected by third-parties into clear disclosures. To address this problem, we propose PTPDroid, a flow-to-policy consistency checking approach and an automated tool, to comprehensively uncover from the privacy policy the violated disclosures to third-parties. Our experiments on real-world apps demonstrate the effectiveness and superiority of PTPDroid, and our empirical study on 1,000 popular real-world apps reveals that violated user privacy disclosures to third-parties are prevalent in practice.",Conformity,Self-Discipline,By encouraging a more disciplined usage of user data especially by third parties; 'Paper X' aligns with the value of 'Conformity'; value item 'Self-Discipline'; as the proposed tool calls for higher integrity and stricter adherence to privacy policies.,"In aligning with the value item of Self-Discipline and the corresponding value of Conformity, 'Paper X' addresses the importance of adhering to privacy policies and promoting a more disciplined usage of user data, particularly by third-party entities. By emphasizing flow-to-policy consistency checking and uncovering violated disclosures, the paper highlights the need for higher integrity and stricter adherence to privacy policies, thus aligning with the value of Conformity and the value item of Self-Discipline.",Agreed-Clarified,Agree,,Agree,,
Exp E,Exp J,831,ICSE,Software Testing & QA,Generating Realistic and Diverse Tests for LiDAR-Based Perception Systems,"Autonomous systems rely on a perception component to interpret their surroundings, and when misinterpretations occur, they can and have led to serious and fatal system-level failures. Yet, existing methods for testing perception software remain limited in both their capacity to efficiently generate test data that translates to real-world performance and in their diversity to capture the long tail of rare but safety-critical scenarios. These limitations are particularly evident for perception systems based on LiDAR sensors, which have emerged as a crucial component in modern autonomous systems due to their ability to provide a 3D scan of the world and operate in all lighting conditions. To address these limitations, we introduce a novel approach for testing LiDAR-based perception systems by leveraging existing real-world data as a basis to generate realistic and diverse test cases through mutations that preserve realism invariants while generating inputs rarely found in existing data sets, and automatically crafting oracles that identify potentially safety-critical issues in perception performance. We implemented our approach to assess its ability to identify perception failures, generating over 50,000 test inputs for five state-of-the-art LiDAR-based perception systems. We found that it efficiently generated test cases that yield errors in perception that could result in real consequences if these systems were deployed and does so at a low rate of false positives.",Achievement,Successful,The paper introduces a new approach that improves the perception systems by efficiently generating test data. This leads them to achieve better performance and avoid system level-errors; directly contributing to the success of the Autonomous systems; aligning with the 'Successful' value item and corresponding 'Achievement' value.,"In the paper abstract, it is explicitly mentioned that the novel approach introduced in 'Paper X' efficiently generates test data for perception systems, leading to the identification of perception failures and avoiding system-level errors. This directly contributes to the success of autonomous systems as it improves their performance and ensures their safe deployment. This alignment with the value item Successful and its corresponding value Achievement is evident as the paper's main contributions directly address the goal of achieving better outcomes in the software context.",Agreed-Justified,Agree,,Agree,,
Exp F,Exp J,831,ICSE,Software Testing & QA,Generating Realistic and Diverse Tests for LiDAR-Based Perception Systems,"Autonomous systems rely on a perception component to interpret their surroundings, and when misinterpretations occur, they can and have led to serious and fatal system-level failures. Yet, existing methods for testing perception software remain limited in both their capacity to efficiently generate test data that translates to real-world performance and in their diversity to capture the long tail of rare but safety-critical scenarios. These limitations are particularly evident for perception systems based on LiDAR sensors, which have emerged as a crucial component in modern autonomous systems due to their ability to provide a 3D scan of the world and operate in all lighting conditions. To address these limitations, we introduce a novel approach for testing LiDAR-based perception systems by leveraging existing real-world data as a basis to generate realistic and diverse test cases through mutations that preserve realism invariants while generating inputs rarely found in existing data sets, and automatically crafting oracles that identify potentially safety-critical issues in perception performance. We implemented our approach to assess its ability to identify perception failures, generating over 50,000 test inputs for five state-of-the-art LiDAR-based perception systems. We found that it efficiently generated test cases that yield errors in perception that could result in real consequences if these systems were deployed and does so at a low rate of false positives.",Security,Healthy,Through the improved perception system test methodology; potential failures and errors that could result in real consequences can be identified and hence avoided beforehand; ensuring the healthy and safe functioning of the autonomous systems. This aligns with the 'Healthy' value item and the corresponding 'Security' value.,"By implementing the improved perception system test methodology proposed in 'Paper X', potential failures and errors in autonomous systems can be identified and addressed before deployment, thus ensuring the healthy and safe functioning of these systems. This aligns with the 'Healthy' value item as it focuses on the well-being and proper functioning of the software, and the corresponding value of 'Security' as it aims to prevent any potential harm or negative consequences that may arise from system-level failures.",Agreed-Justified,Agree,,Agree,,
Exp G,Exp K,831,ICSE,Software Testing & QA,Generating Realistic and Diverse Tests for LiDAR-Based Perception Systems,"Autonomous systems rely on a perception component to interpret their surroundings, and when misinterpretations occur, they can and have led to serious and fatal system-level failures. Yet, existing methods for testing perception software remain limited in both their capacity to efficiently generate test data that translates to real-world performance and in their diversity to capture the long tail of rare but safety-critical scenarios. These limitations are particularly evident for perception systems based on LiDAR sensors, which have emerged as a crucial component in modern autonomous systems due to their ability to provide a 3D scan of the world and operate in all lighting conditions. To address these limitations, we introduce a novel approach for testing LiDAR-based perception systems by leveraging existing real-world data as a basis to generate realistic and diverse test cases through mutations that preserve realism invariants while generating inputs rarely found in existing data sets, and automatically crafting oracles that identify potentially safety-critical issues in perception performance. We implemented our approach to assess its ability to identify perception failures, generating over 50,000 test inputs for five state-of-the-art LiDAR-based perception systems. We found that it efficiently generated test cases that yield errors in perception that could result in real consequences if these systems were deployed and does so at a low rate of false positives.",Benevolence,Responsibility,The proposed testing method promotes safer and more responsible use of Autonomous systems; by reducing the risk of system-level failures. This potentially allows users to take on responsibility for the shared use of such systems in a safe manner. This aligns the value item 'Responsibility' under the value 'Benevolence'.,"In 'Paper X', the proposed approach for testing LiDAR-based perception systems directly aligns with the value item Responsibility and its corresponding value Benevolence. By generating realistic and diverse test cases, the approach aims to identify perception failures that could result in real consequences if these systems were deployed. This promotes a safer and more responsible use of autonomous systems, allowing software users to take on responsibility for the shared use of such systems in a benevolent and ethical manner.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,835,ICSE,Software Architecture & Design,Robustification of behavioral designs against environmental deviations,"Modern software systems are deployed in a highly dynamic, uncertain environment. Ideally, a system that is robust should be capable of establishing its most critical requirements even in the presence of possible deviations in the environment. We propose a technique called behavioral robustification, which involves systematically and rigorously improving the robustness of a design against potential deviations. Given behavioral models of a system and its environment, along with a set of user-specified deviations, our robustification method produces a redesign that is capable of satisfying a desired property even when the environment exhibits those deviations. In particular, we describe how the robustification problem can be formulated as a multi-objective optimization problem, where the goal is to restrict the deviating environment from causing a violation of a desired property, while maximizing the amount of existing functionality and minimizing the cost of changes to the original design. We demonstrate the effectiveness of our approach on case studies involving the robustness of an electronic voting machine and safety-critical interfaces.",Achievement,Successful,The paper is proposing a technique for the robustification of system design which aims to increase the success of its functions even in the presence of possible deviations in the environment; aligning with the value item Successful under the value achievement.,"In 'Paper X', the proposed technique of behavioral robustification directly aligns with the value item Successful and its corresponding value Achievement from a ""Software User"" perspective. This alignment is evident as the paper emphasizes improving the robustness of system design, aiming to ensure the system's critical requirements are met even in the presence of deviations in the environment. By achieving this robustness, the system can successfully function and satisfy the desired property specified by the user, thereby aligning with the value item Successful and the value of Achievement.",Agreed-Justified,Agree,,Agree,,
Exp B,Exp H,835,ICSE,Software Architecture & Design,Robustification of behavioral designs against environmental deviations,"Modern software systems are deployed in a highly dynamic, uncertain environment. Ideally, a system that is robust should be capable of establishing its most critical requirements even in the presence of possible deviations in the environment. We propose a technique called behavioral robustification, which involves systematically and rigorously improving the robustness of a design against potential deviations. Given behavioral models of a system and its environment, along with a set of user-specified deviations, our robustification method produces a redesign that is capable of satisfying a desired property even when the environment exhibits those deviations. In particular, we describe how the robustification problem can be formulated as a multi-objective optimization problem, where the goal is to restrict the deviating environment from causing a violation of a desired property, while maximizing the amount of existing functionality and minimizing the cost of changes to the original design. We demonstrate the effectiveness of our approach on case studies involving the robustness of an electronic voting machine and safety-critical interfaces.",Security,Healthy,The method given in the paper ensures that software users can depend on the software system as it successfully and robustly delivers its functions in various environmental conditions; aligning with the value item Healthy under the value Security.,"In the paper, the authors propose a technique called behavioral robustification that aims to improve the robustness of a software system by ensuring that it can satisfy desired properties even in the presence of environmental deviations. This directly aligns with the value item Healthy and its corresponding value Security from a Software User perspective because it ensures that the software system remains dependable and can successfully deliver its functions in various environmental conditions. By addressing possible deviations and minimizing the risk of failures or vulnerabilities, the paper aligns with the value of Security, ultimately promoting a healthy and secure user experience.",Agreed-Clarified,Disagree,reconcile with coder_2,Disagree,"The justification claiming that the paper aligns with the value 'Security' because of 'The method given in the paper ensures that software users can depend on the software system as it successfully and robustly delivers its functions in various environmental conditions. is not supported by the evidence in the abstract, which focuses on 'Modern software systems are deployed in a highly dynamic, uncertain environment.",
Exp D,Exp I,841,ICSE,Security & Privacy,Operand-Variation-Oriented Differential Analysis for Fuzzing Binding Calls in PDF Readers,"Binding calls of embedded scripting engines introduce a serious attack surface in PDF readers. To effectively test binding calls, the knowledge of parameter types is necessary. Unfortunately, due to the absence or incompleteness of documentation and the lack of sufficient samples, automatic type reasoning for binding call parameters is a big challenge. In this paper, we propose a novel operand-variation-oriented differential analysis approach, which automatically extracts features from execution traces as oracles for inferring parameter types. In particular, the parameter types of a binding call are inferred by executing the binding call with different values of different types and investigating which types cause an expected effect on the instruction operands. The inferred type information is used to guide the test generation in fuzzing. Through the evaluation on two popular PDF readers (Adobe Reader and Foxit Reader), we demonstrated the accuracy of our type reasoning method and the effectiveness of the inferred type information for improving fuzzing in both code coverage and vulnerability discovery. We found 38 previously unknown security vulnerabilities, 26 of which were certified with CVE numbers.",Power,Social Recognition,The paper contributes an approach for improving security in PDF readers; which will indirectly result in better social recognition for users worried about their data privacy and security. This aligns with the value item Social Recognition and its corresponding value Power.,"In the context of a software user, the main contribution of 'Paper X' is the improved security in PDF readers. By proposing an approach that can identify and mitigate previously unknown security vulnerabilities, the paper empowers software users to have better control over their data privacy and security. This aligns with the value item Social Recognition, as users who prioritize their privacy and security can gain social recognition and respect for their cautious behavior. Therefore, the alignment with the value item Social Recognition and its corresponding value Power is evident in the paper's focus on enhancing security and protecting users' interests.",Agreed-Clarified,Agree,,Agree,,
Exp E,Exp J,841,ICSE,Security & Privacy,Operand-Variation-Oriented Differential Analysis for Fuzzing Binding Calls in PDF Readers,"Binding calls of embedded scripting engines introduce a serious attack surface in PDF readers. To effectively test binding calls, the knowledge of parameter types is necessary. Unfortunately, due to the absence or incompleteness of documentation and the lack of sufficient samples, automatic type reasoning for binding call parameters is a big challenge. In this paper, we propose a novel operand-variation-oriented differential analysis approach, which automatically extracts features from execution traces as oracles for inferring parameter types. In particular, the parameter types of a binding call are inferred by executing the binding call with different values of different types and investigating which types cause an expected effect on the instruction operands. The inferred type information is used to guide the test generation in fuzzing. Through the evaluation on two popular PDF readers (Adobe Reader and Foxit Reader), we demonstrated the accuracy of our type reasoning method and the effectiveness of the inferred type information for improving fuzzing in both code coverage and vulnerability discovery. We found 38 previously unknown security vulnerabilities, 26 of which were certified with CVE numbers.",Power,Wealth,By improving the security vulnerabilities in PDF readers; users can have a sense of increased wealth in the form of preserved data and avoided potential losses due to security breaches. This aligns with the value item Wealth and its corresponding value Power.,"In 'Paper X', the main contribution is improving the security vulnerabilities in PDF readers. By addressing these vulnerabilities, users can experience a sense of increased wealth in the form of preserved data and avoided potential losses due to security breaches. This aligns with the value item Wealth, as it signifies the accumulation and protection of valuable resources, and its corresponding value Power, as it represents the ability to control and safeguard one's valuable assets.",Agreed-Justified,Agree,,Agree,,
Exp F,Exp J,841,ICSE,Security & Privacy,Operand-Variation-Oriented Differential Analysis for Fuzzing Binding Calls in PDF Readers,"Binding calls of embedded scripting engines introduce a serious attack surface in PDF readers. To effectively test binding calls, the knowledge of parameter types is necessary. Unfortunately, due to the absence or incompleteness of documentation and the lack of sufficient samples, automatic type reasoning for binding call parameters is a big challenge. In this paper, we propose a novel operand-variation-oriented differential analysis approach, which automatically extracts features from execution traces as oracles for inferring parameter types. In particular, the parameter types of a binding call are inferred by executing the binding call with different values of different types and investigating which types cause an expected effect on the instruction operands. The inferred type information is used to guide the test generation in fuzzing. Through the evaluation on two popular PDF readers (Adobe Reader and Foxit Reader), we demonstrated the accuracy of our type reasoning method and the effectiveness of the inferred type information for improving fuzzing in both code coverage and vulnerability discovery. We found 38 previously unknown security vulnerabilities, 26 of which were certified with CVE numbers.",Security,Healthy,The main contribution of the paper is improving the security of PDF readers; which could be interpreted as an attempt to keep the software and; by implication; the user healthy from potential security threats. This aligns with the value item Healthy and its corresponding value Security.,"The main contribution of 'Paper X' is focused on improving the security of PDF readers. By addressing the challenge of binding call parameters and proposing a novel analysis approach, the paper aims to enhance the overall security of the software. This alignment with the value item Healthy and its corresponding value Security is based on the understanding that a healthy software system should be free from potential security threats, providing a secure and protected environment for the software user.",Agreed-Justified,Agree,,Agree,,
Exp G,Exp K,842,ICSE,Security & Privacy,BSHUNTER: Detecting and Tracing Defects of Bitcoin Scripts,"Supporting the most popular cryptocurrency, the Bitcoin platform allows its transactions to be programmable via its scripts. Defects in Bitcoin scripts will make users lose their bitcoins. However, there are few studies on the defects of Bitcoin scripts. In this paper, we conduct the first systematic investigation on the defects of Bitcoin scripts through three steps, including defect definition, defect detection, and exploitation tracing. First, we define six typical defects of scripts in Bitcoin history, namely unbinded-txid, simple-key, useless-sig, uncertain-sig, impossible-key, and never-true. Three are inspired by the community, and three are new from us. Second, we develop a tool to discover Bitcoin scripts with any of typical defects based on symbolic execution and enhanced by historical exact scripts. By analyzing all Bitcoin transactions from Oct. 2009 to Aug. 2022, we find that 383,544 transaction outputs are paid to the Bitcoin scripts with defects. The total amount of them is 3,115.43 BTC, which is around 60 million dollars at present. Third, in order to trace the exploitation of the defects, we instrument the Bitcoin VM to record the traces of the real-world spending transactions of the buggy scripts. We find that 84,130 output scripts are exploited. The implementation and non-harmful datasets are released.",Achievement,Intelligent,The paper enables users to further understand bitcoin transactions; which aligns with the value item 'Intelligent' and its corresponding value 'Achievement'.,"The main contribution of 'Paper X' is the systematic investigation of the defects in Bitcoin scripts, including defining the defects, detecting them, and tracing their exploitation. By enabling users to better understand and detect these defects, the paper empowers them to make more informed decisions and protect their investments in the Bitcoin platform. This aligns with the value item 'Intelligent' as it promotes the acquisition of knowledge and understanding, and the corresponding value 'Achievement' as it enables users to successfully navigate and mitigate risks in the software context of Bitcoin transactions.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,842,ICSE,Security & Privacy,BSHUNTER: Detecting and Tracing Defects of Bitcoin Scripts,"Supporting the most popular cryptocurrency, the Bitcoin platform allows its transactions to be programmable via its scripts. Defects in Bitcoin scripts will make users lose their bitcoins. However, there are few studies on the defects of Bitcoin scripts. In this paper, we conduct the first systematic investigation on the defects of Bitcoin scripts through three steps, including defect definition, defect detection, and exploitation tracing. First, we define six typical defects of scripts in Bitcoin history, namely unbinded-txid, simple-key, useless-sig, uncertain-sig, impossible-key, and never-true. Three are inspired by the community, and three are new from us. Second, we develop a tool to discover Bitcoin scripts with any of typical defects based on symbolic execution and enhanced by historical exact scripts. By analyzing all Bitcoin transactions from Oct. 2009 to Aug. 2022, we find that 383,544 transaction outputs are paid to the Bitcoin scripts with defects. The total amount of them is 3,115.43 BTC, which is around 60 million dollars at present. Third, in order to trace the exploitation of the defects, we instrument the Bitcoin VM to record the traces of the real-world spending transactions of the buggy scripts. We find that 84,130 output scripts are exploited. The implementation and non-harmful datasets are released.",Security,Healthy,By identifying defects in Bitcoin scripts; the paper contributes to the security and healthy usage of the Bitcoin platform. This aligns with the value item 'Healthy' and its corresponding value 'Security'.,"The main contribution of 'Paper X' is its identification of defects in Bitcoin scripts, which ultimately enhances the security and healthy usage of the Bitcoin platform. By addressing these defects, the paper aims to prevent users from losing their bitcoins and ensure a secure environment for transactions. This directly aligns with the value item 'Healthy', as it focuses on creating a safe and reliable software experience for users. Furthermore, ensuring the security of the Bitcoin platform also aligns with the value of 'Security', as it contributes to the protection and trustworthiness of the system, which is crucial for software users.",Agreed-Justified,Disagree,reconcile with coder_2,Disagree,The justification claiming that the paper aligns with the value 'Security' because of 'By identifying defects in Bitcoin scripts; the paper contributes to the security and healthy usage of the Bitcoin platform.  This aligns with the value item 'Healthy' and its corresponding value 'Security'.,
Exp B,Exp H,842,ICSE,Security & Privacy,BSHUNTER: Detecting and Tracing Defects of Bitcoin Scripts,"Supporting the most popular cryptocurrency, the Bitcoin platform allows its transactions to be programmable via its scripts. Defects in Bitcoin scripts will make users lose their bitcoins. However, there are few studies on the defects of Bitcoin scripts. In this paper, we conduct the first systematic investigation on the defects of Bitcoin scripts through three steps, including defect definition, defect detection, and exploitation tracing. First, we define six typical defects of scripts in Bitcoin history, namely unbinded-txid, simple-key, useless-sig, uncertain-sig, impossible-key, and never-true. Three are inspired by the community, and three are new from us. Second, we develop a tool to discover Bitcoin scripts with any of typical defects based on symbolic execution and enhanced by historical exact scripts. By analyzing all Bitcoin transactions from Oct. 2009 to Aug. 2022, we find that 383,544 transaction outputs are paid to the Bitcoin scripts with defects. The total amount of them is 3,115.43 BTC, which is around 60 million dollars at present. Third, in order to trace the exploitation of the defects, we instrument the Bitcoin VM to record the traces of the real-world spending transactions of the buggy scripts. We find that 84,130 output scripts are exploited. The implementation and non-harmful datasets are released.",Security,National Security,The paper's investigation onto Bitcoin scripts contributes to the security of financial transactions; aligning with the value item 'National Security' from the corresponding value 'Security'.,"The investigation conducted by 'Paper X' on the defects of Bitcoin scripts directly aligns with the value item 'National Security' and its corresponding value 'Security' from a ""Software User"" perspective. By addressing and detecting these defects, the paper contributes to enhancing the security of financial transactions, which ultimately promotes the overall security and integrity of the national financial system.",Agreed-Justified,Agree,,Agree,,
Exp C,Exp I,844,ICSE,Security & Privacy,Lejacon: A Lightweight and Efficient Approach to Java Confidential Computing on SGX,"Intel's SGX is a confidential computing technique. It allows key functionalities of C/C++/native applications to be confidentially executed in hardware enclaves. However, numerous cloud applications are written in Java. For supporting their confidential computing, state-of-the-art approaches deploy Java Virtual Machines (JVMs) in enclaves and perform confidential computing on JVMs. Meanwhile, these JVM-in-enclave solutions still suffer from serious limitations, such as heavy overheads of running JVMs in enclaves, large attack surfaces, and deep computation stacks. To mitigate the above limitations, we for-malize a Secure Closed-World (SCW) principle and then propose Lejacon, a lightweight and efficient approach to Java confidential computing. The key idea is, given a Java application, to (1) separately compile its confidential computing tasks into a bundle of Native Confidential Computing (NCC) services; (2) run the NCC services in enclaves on the Trusted Execution Environment (TEE) side, and meanwhile run the non-confidential code on a JVM on the Rich Execution Environment (REE) side. The two sides interact with each other, protecting confidential computing tasks and as well keeping the Trusted Computing Base (TCB) size small. We implement Lejacon and evaluate it against OcclumJ (a state-of-the-art JVM-in-enclave solution) on a set of benchmarks using the BouncyCastle cryptography library. The evaluation results clearly show the strengths of Lejacon: it achieves compet-itive performance in running Java confidential code in enclaves; compared with OcclumJ, Lejacon achieves speedups by up to 16.2x in running confidential code and also reduces the TCB sizes by 90+% on average.",Achievement,Successful,The paper contributes to the development of Lejacon; a lightweight and efficient approach to Java confidential computing which outperforms state-of-the-art techniques on speed and Trusted Computing Base (TCB) size reduction. This helps users to be successful (a value item under Achievement) with their tasks associated with Java confidential computing.,"In the analysis of 'Paper X', it is evident that the main contributions align with the value item Successful and its corresponding value Achievement from a ""Software User"" perspective. The paper explicitly states that Lejacon, the proposed approach, achieves competitive performance in running Java confidential code in enclaves, resulting in speedups and significant reductions in the Trusted Computing Base size. These advancements directly benefit software users by enabling them to successfully accomplish their tasks related to Java confidential computing, thereby aligning with the value item Successful and the value Achievement.",Agreed-Justified,Agree,,Agree,,
Exp D,Exp I,844,ICSE,Security & Privacy,Lejacon: A Lightweight and Efficient Approach to Java Confidential Computing on SGX,"Intel's SGX is a confidential computing technique. It allows key functionalities of C/C++/native applications to be confidentially executed in hardware enclaves. However, numerous cloud applications are written in Java. For supporting their confidential computing, state-of-the-art approaches deploy Java Virtual Machines (JVMs) in enclaves and perform confidential computing on JVMs. Meanwhile, these JVM-in-enclave solutions still suffer from serious limitations, such as heavy overheads of running JVMs in enclaves, large attack surfaces, and deep computation stacks. To mitigate the above limitations, we for-malize a Secure Closed-World (SCW) principle and then propose Lejacon, a lightweight and efficient approach to Java confidential computing. The key idea is, given a Java application, to (1) separately compile its confidential computing tasks into a bundle of Native Confidential Computing (NCC) services; (2) run the NCC services in enclaves on the Trusted Execution Environment (TEE) side, and meanwhile run the non-confidential code on a JVM on the Rich Execution Environment (REE) side. The two sides interact with each other, protecting confidential computing tasks and as well keeping the Trusted Computing Base (TCB) size small. We implement Lejacon and evaluate it against OcclumJ (a state-of-the-art JVM-in-enclave solution) on a set of benchmarks using the BouncyCastle cryptography library. The evaluation results clearly show the strengths of Lejacon: it achieves compet-itive performance in running Java confidential code in enclaves; compared with OcclumJ, Lejacon achieves speedups by up to 16.2x in running confidential code and also reduces the TCB sizes by 90+% on average.",Security,Healthy,The paper contributes to enhancing the protective measures in computing tasks by consistently offering a separate; secure environment for running the confidential computing tasks. This can help users maintain their digital wellbeing by reducing the risk of data breaches or misuse. Hence; it aligns with the value item Healthy under the value category Security.,"Sure, let me clarify my justification. The alignment of 'Paper X' with the value item Healthy and its corresponding value Security from a ""Software User"" perspective stems from the paper's focus on enhancing protective measures for computing tasks. By providing a separate, secure environment for confidential computing tasks, the paper aims to mitigate the risk of data breaches or misuse. This, in turn, contributes to the overall digital wellbeing of software users by providing a sense of security and reducing potential harm to personal or sensitive information. Therefore, the alignment with the value item Healthy is based on the paper's emphasis on creating a secure environment that promotes the security and protection of software users' data.",Agreed-Clarified,Agree,,Agree,,
Exp E,Exp J,844,ICSE,Security & Privacy,Lejacon: A Lightweight and Efficient Approach to Java Confidential Computing on SGX,"Intel's SGX is a confidential computing technique. It allows key functionalities of C/C++/native applications to be confidentially executed in hardware enclaves. However, numerous cloud applications are written in Java. For supporting their confidential computing, state-of-the-art approaches deploy Java Virtual Machines (JVMs) in enclaves and perform confidential computing on JVMs. Meanwhile, these JVM-in-enclave solutions still suffer from serious limitations, such as heavy overheads of running JVMs in enclaves, large attack surfaces, and deep computation stacks. To mitigate the above limitations, we for-malize a Secure Closed-World (SCW) principle and then propose Lejacon, a lightweight and efficient approach to Java confidential computing. The key idea is, given a Java application, to (1) separately compile its confidential computing tasks into a bundle of Native Confidential Computing (NCC) services; (2) run the NCC services in enclaves on the Trusted Execution Environment (TEE) side, and meanwhile run the non-confidential code on a JVM on the Rich Execution Environment (REE) side. The two sides interact with each other, protecting confidential computing tasks and as well keeping the Trusted Computing Base (TCB) size small. We implement Lejacon and evaluate it against OcclumJ (a state-of-the-art JVM-in-enclave solution) on a set of benchmarks using the BouncyCastle cryptography library. The evaluation results clearly show the strengths of Lejacon: it achieves compet-itive performance in running Java confidential code in enclaves; compared with OcclumJ, Lejacon achieves speedups by up to 16.2x in running confidential code and also reduces the TCB sizes by 90+% on average.",Universalism,Protecting the Environment,The paper's focus on supporting 'confidential computing' aligns with the value item Protecting the Environment under the value category Universalism. With separate; secure environments for key functionalities; the risk of unwanted data access or leaks is minimized; thus contributing to a safer digital environment for users.,"I apologize for any confusion caused. Although the alignment may not be immediately apparent, the justification lies in the concept of ""confidential computing"" and its contribution to a safer digital environment. By providing secure environments for key functionalities, the paper aims to minimize the risk of unwanted data access or leaks. This focus on data protection and security can be seen as aligning with the value item of Protecting the Environment under the value category of Universalism, as it promotes the universal welfare and well-being of individuals within a software context.",Agreed-Clarified,Agree,,Agree,,
Exp F,Exp J,846,ICSE,Accessibility & User Experience,Adhere: Automated Detection and Repair of Intrusive Ads,"Today, more than 3 million websites rely on online advertising revenue. Despite the monetary incentives, ads often frustrate users by disrupting their experience, interrupting content, and slowing browsing. To improve ad experiences, leading media associations define Better Ads Standards for ads that are below user expectations. However, little is known about how well websites comply with these standards and whether existing approaches are sufficient for developers to quickly resolve such issues. In this paper, we propose Adhere, a technique that can detect intrusive ads that do not comply with Better Ads Standards and suggest repair proposals. Adhere works by first parsing the initial web page to a DOM tree to search for potential static ads, and then using mutation observers to monitor and detect intrusive (dynamic/static) ads on the fly. To handle ads' volatile nature, Adhere includes two detection algorithms for desktop and mobile ads to identify different ad violations during three phases of page load events. It recursively applies the detection algorithms to resolve nested layers of DOM elements inserted by ad delegations. We evaluate Adhere on Alexa Top 1 Million Websites. The results show that Adhere is effective in detecting violating ads and suggesting repair proposals. Comparing to the current available alternative, Adhere detected intrusive ads on 4,656 more mobile websites and 3,911 more desktop websites, and improved recall by 16.6% and accuracy by 4.2%.",Power,Social Recognition,"Paper X proposes ""Adhere""; a technique that identifies intrusive ads which are not in compliance with standards; giving users a more harmonious browsing experience. This directly aligns with the value item 'Social Recognition' as this tool is recognized for its role in enhancing the quality of online user experiences.","The justification provided aligning 'Paper X' with the value item Social Recognition and its corresponding value Power from a ""Software User"" perspective is that the proposed technique in 'Paper X' (Adhere) addresses the issue of intrusive ads that disrupt user experiences. By detecting and suggesting repair proposals for non-compliant ads, users are provided with a more harmonious browsing experience. This aligns with the value item Social Recognition as the tool is recognized for its role in enhancing the quality of online user experiences, thereby giving users a sense of power and control over their browsing environment.",Agreed-Clarified,Agree,,Agree,,
Exp G,Exp K,846,ICSE,Accessibility & User Experience,Adhere: Automated Detection and Repair of Intrusive Ads,"Today, more than 3 million websites rely on online advertising revenue. Despite the monetary incentives, ads often frustrate users by disrupting their experience, interrupting content, and slowing browsing. To improve ad experiences, leading media associations define Better Ads Standards for ads that are below user expectations. However, little is known about how well websites comply with these standards and whether existing approaches are sufficient for developers to quickly resolve such issues. In this paper, we propose Adhere, a technique that can detect intrusive ads that do not comply with Better Ads Standards and suggest repair proposals. Adhere works by first parsing the initial web page to a DOM tree to search for potential static ads, and then using mutation observers to monitor and detect intrusive (dynamic/static) ads on the fly. To handle ads' volatile nature, Adhere includes two detection algorithms for desktop and mobile ads to identify different ad violations during three phases of page load events. It recursively applies the detection algorithms to resolve nested layers of DOM elements inserted by ad delegations. We evaluate Adhere on Alexa Top 1 Million Websites. The results show that Adhere is effective in detecting violating ads and suggesting repair proposals. Comparing to the current available alternative, Adhere detected intrusive ads on 4,656 more mobile websites and 3,911 more desktop websites, and improved recall by 16.6% and accuracy by 4.2%.",Hedonism,Enjoying Life,"By detecting and suggesting repairs for violating ads; ""Adhere"" increases the overall enjoyment of website browsing for users. This is therefore aligned with the value item 'Enjoying Life' under the 'Hedonism' value.","The main contribution of 'Paper X' is the development of the Adhere technique, which detects intrusive ads that do not comply with Better Ads Standards and suggests repair proposals. By improving ad experiences and reducing the frustration caused by disruptive ads, Adhere contributes to enhancing the overall enjoyment of website browsing for users. This aligns with the value item 'Enjoying Life' under the 'Hedonism' value, as it focuses on maximizing pleasure and minimizing sources of dissatisfaction in the software user's experience.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,846,ICSE,Accessibility & User Experience,Adhere: Automated Detection and Repair of Intrusive Ads,"Today, more than 3 million websites rely on online advertising revenue. Despite the monetary incentives, ads often frustrate users by disrupting their experience, interrupting content, and slowing browsing. To improve ad experiences, leading media associations define Better Ads Standards for ads that are below user expectations. However, little is known about how well websites comply with these standards and whether existing approaches are sufficient for developers to quickly resolve such issues. In this paper, we propose Adhere, a technique that can detect intrusive ads that do not comply with Better Ads Standards and suggest repair proposals. Adhere works by first parsing the initial web page to a DOM tree to search for potential static ads, and then using mutation observers to monitor and detect intrusive (dynamic/static) ads on the fly. To handle ads' volatile nature, Adhere includes two detection algorithms for desktop and mobile ads to identify different ad violations during three phases of page load events. It recursively applies the detection algorithms to resolve nested layers of DOM elements inserted by ad delegations. We evaluate Adhere on Alexa Top 1 Million Websites. The results show that Adhere is effective in detecting violating ads and suggesting repair proposals. Comparing to the current available alternative, Adhere detected intrusive ads on 4,656 more mobile websites and 3,911 more desktop websites, and improved recall by 16.6% and accuracy by 4.2%.",Self Direction,Freedom,"The proposed tool ""Adhere"" allows end users to have a smoother and less intrusive browsing experience by mitigating the interference of unwanted ads. This grants the users more freedom in their online interaction; directly aligning with the value item 'Freedom' under the value 'Self Direction'.","In the context of 'Paper X', the alignment with the value item Freedom and its corresponding value Self Direction can be justified by the fact that the proposed tool, Adhere, aims to improve ad experiences by detecting and suggesting repairs for intrusive ads. By mitigating the interference of unwanted ads, Adhere grants users more control over their online interaction and allows them to freely navigate websites without the disruptions caused by intrusive ads. This directly aligns with the value item Freedom, as users are able to exercise their autonomy and choose how they engage with online content.",Agreed-Justified,Agree,,Agree,,
Exp B,Exp H,849,ICSE,AI & Machine Learning,Lightweight Approaches to DNN Regression Error Reduction: An Uncertainty Alignment Perspective,"Regression errors of Deep Neural Network (DNN) models refer to the case that predictions were correct by the old-version model but wrong by the new-version model. They frequently occur when upgrading DNN models in production systems, causing disproportionate user experience degradation. In this paper, we propose a lightweight regression error reduction approach with two goals: 1) requiring no model retraining and even data, and 2) not sacrificing the accuracy. The proposed approach is built upon the key insight rooted in the unmanaged model uncertainty, which is intrinsic to DNN models, but has not been thoroughly explored especially in the context of quality assurance of DNN models. Specifically, we propose a simple yet effective ensemble strategy that estimates and aligns the two models' uncertainty. We show that a Pareto improvement that reduces the regression errors without compromising the overall accuracy can be guaranteed in theory and largely achieved in practice. Comprehensive experiments with various representative models and datasets confirm that our approaches significantly outperform the state-of-the-art alternatives.",Achievement,Successful,The paper presents a lightweight regression error reduction approach to improve the user experience when upgrading DNN models; aligning with the value item Successful and corresponding value Achievement.,"The paper's proposed approach to reduce regression errors in DNN models without sacrificing accuracy directly contributes to the value item Successful and its corresponding value Achievement from a ""Software User"" perspective. By ensuring that predictions are correct and improving the user experience during model upgrades, the paper aligns with the value of achieving successful outcomes in software usage, which is important for users seeking reliable and accurate results from their software systems.",Agreed-Clarified,Agree,,Agree,,
Exp C,Exp I,849,ICSE,AI & Machine Learning,Lightweight Approaches to DNN Regression Error Reduction: An Uncertainty Alignment Perspective,"Regression errors of Deep Neural Network (DNN) models refer to the case that predictions were correct by the old-version model but wrong by the new-version model. They frequently occur when upgrading DNN models in production systems, causing disproportionate user experience degradation. In this paper, we propose a lightweight regression error reduction approach with two goals: 1) requiring no model retraining and even data, and 2) not sacrificing the accuracy. The proposed approach is built upon the key insight rooted in the unmanaged model uncertainty, which is intrinsic to DNN models, but has not been thoroughly explored especially in the context of quality assurance of DNN models. Specifically, we propose a simple yet effective ensemble strategy that estimates and aligns the two models' uncertainty. We show that a Pareto improvement that reduces the regression errors without compromising the overall accuracy can be guaranteed in theory and largely achieved in practice. Comprehensive experiments with various representative models and datasets confirm that our approaches significantly outperform the state-of-the-art alternatives.",Power,Social Power,The paper's method ensures that the user's experience does not degrade when upgrading DNN models; aligning with the value item Social Power and corresponding value Power.,"Based on the abstract of 'Paper X', the proposed approach aims to reduce regression errors in DNN models without sacrificing overall accuracy, which ensures a better user experience. This aligns with the value item Social Power from Schwartz's Taxonomy, as it focuses on the user's ability to have control and influence in a software context. By minimizing errors and enhancing the accuracy of DNN models, users gain power and confidence in the software system, ultimately leading to a sense of social power.",Agreed-Clarified,Agree,,Agree,,
Exp E,Exp J,851,ICSE,Security & Privacy,BFTDETECTOR: Automatic Detection of Business Flow Tampering for Digital Content Service,"Digital content services provide users with a wide range of content, such as news, articles, or movies, while monetizing their content through various business models and promotional methods. Unfortunately, poorly designed or unpro-tected business logic can be circumvented by malicious users, which is known as business flow tampering. Such flaws can severely harm the businesses of digital content service providers. In this paper, we propose an automated approach that discov-ers business flow tampering flaws. Our technique automatically runs a web service to cover different business flows (e.g., a news website with vs. without a subscription paywall) to collect execution traces. We perform differential analysis on the execution traces to identify divergence points that determine how the business flow begins to differ, and then we test to see if the divergence points can be tampered with. We assess our approach against 352 real-world digital content service providers and discover 315 flaws from 204 websites, including TIME, Fortune, and Forbes. Our evaluation result shows that our technique successfully identifies these flaws with low false-positive and false-negative rates of 0.49% and 1.44%, respectively.",Security,Social Order,The main contribution of the paper is an automated approach to identify flaws in the business flow of digital content services; thereby helping maintain the Social Order of different business flows which aligns with the value item Security.,"The main contribution of ""Paper X"" is the automated approach to identifying flaws in the business flow of digital content services, which aligns with the value item Social Order and its corresponding value Security. By detecting and preventing business flow tampering, the paper aims to maintain the integrity and stability of the digital content services' operations, ensuring security for both the service providers and their users. This directly aligns with the value of Security, as it emphasizes the importance of safeguarding the digital ecosystem and maintaining order in the software context.",Agreed-Clarified,Agree,,Agree,,
Exp F,Exp J,851,ICSE,Security & Privacy,BFTDETECTOR: Automatic Detection of Business Flow Tampering for Digital Content Service,"Digital content services provide users with a wide range of content, such as news, articles, or movies, while monetizing their content through various business models and promotional methods. Unfortunately, poorly designed or unpro-tected business logic can be circumvented by malicious users, which is known as business flow tampering. Such flaws can severely harm the businesses of digital content service providers. In this paper, we propose an automated approach that discov-ers business flow tampering flaws. Our technique automatically runs a web service to cover different business flows (e.g., a news website with vs. without a subscription paywall) to collect execution traces. We perform differential analysis on the execution traces to identify divergence points that determine how the business flow begins to differ, and then we test to see if the divergence points can be tampered with. We assess our approach against 352 real-world digital content service providers and discover 315 flaws from 204 websites, including TIME, Fortune, and Forbes. Our evaluation result shows that our technique successfully identifies these flaws with low false-positive and false-negative rates of 0.49% and 1.44%, respectively.",Security,Sense of Belonging,By discovering flaws and protecting digital content services; software users gain a stronger Sense of Belonging to these platforms; being able to trust in the integrity of their systems. This ties into the value of Security.,"The main contributions of 'Paper X' align with the value item Sense of Belonging and its corresponding value Security by addressing the issue of business flow tampering in digital content services. By identifying and fixing these flaws, the paper aims to provide software users with a sense of belonging and trust in the platforms they use, as their security and the integrity of the systems would be protected. This connection between detecting vulnerabilities and ensuring a secure environment ultimately enhances the sense of belonging for software users.",Agreed-Justified,Agree,,Agree,,
Exp G,Exp K,851,ICSE,Security & Privacy,BFTDETECTOR: Automatic Detection of Business Flow Tampering for Digital Content Service,"Digital content services provide users with a wide range of content, such as news, articles, or movies, while monetizing their content through various business models and promotional methods. Unfortunately, poorly designed or unpro-tected business logic can be circumvented by malicious users, which is known as business flow tampering. Such flaws can severely harm the businesses of digital content service providers. In this paper, we propose an automated approach that discov-ers business flow tampering flaws. Our technique automatically runs a web service to cover different business flows (e.g., a news website with vs. without a subscription paywall) to collect execution traces. We perform differential analysis on the execution traces to identify divergence points that determine how the business flow begins to differ, and then we test to see if the divergence points can be tampered with. We assess our approach against 352 real-world digital content service providers and discover 315 flaws from 204 websites, including TIME, Fortune, and Forbes. Our evaluation result shows that our technique successfully identifies these flaws with low false-positive and false-negative rates of 0.49% and 1.44%, respectively.",Universalism,Protecting the Environment,The paper seeks to Protect the Environment of the digital content service providers by identifying and mitigating potential threats; therefore aligning to the value of Universalism.,"The paper aligns with the value item ""Protecting the Environment"" and its corresponding value ""Universalism"" because it aims to identify and mitigate flaws in the business flow of digital content service providers. By doing so, it helps ensure the sustainability and well-being of the businesses within the digital ecosystem, which can be seen as protecting the overall environment of the software industry. Additionally, the paper's focus on real-world digital content service providers and its success in identifying flaws with low false-positive and false-negative rates demonstrate a universal concern for the preservation of these services and the industry as a whole.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,856,ICSE,AI & Machine Learning,Software Engineering as the Linchpin of Responsible AI,"From humanity's existential risks to safety risks in critical systems to ethical risks, responsible AI, as the saviour, has become a major research challenge with significant real-world consequences. However, achieving responsible AI remains elusive despite the plethora of high-level ethical principles, risk frameworks and progress in algorithmic assurance. In the meantime, software engineering (SE) is being upended by AI, grappling with building system-level quality and alignment from inscrutable machine learning models and code generated from natural language prompts. The upending poses new challenges and opportunities for engineering AI systems responsibly. This talk will share our experiences in helping the industry achieve responsible AI systems by inventing new SE approaches. It will dive into industry challenges (such as risk silos and principle-algorithm gaps) and research challenges (such as lack of requirements, emerging properties and inscrutable systems) and make the point that SE is the linchpin of responsible AI. But SE also requires some fundamental rethinking - shifting from building functions into AI systems to discovering and managing emerging functions from AI systems. Only by doing so can SE take on critical new roles, from understanding human intelligence to building a thriving human-AI symbiosis.",Stimulation,Variation in Life,The paper emphasizes AI evolution which involves constant changes and variation; aligning with the value item 'Variation in Life' from 'Stimulation'.,"In ""Paper X,"" the abstract mentions that responsible AI remains elusive despite the progress in algorithmic assurance, and software engineering is being upended by AI. This implies that the advancements and changes brought by AI in the software context align with the value item ""Variation in Life"" from the perspective of a software user. As AI introduces new challenges and opportunities, it brings excitement and stimulation to the software user's experience, aligning with the value of ""Stimulation"" in Schwartz's Taxonomy.",Agreed-Clarified,Agree,,Agree,,
Exp B,Exp H,856,ICSE,AI & Machine Learning,Software Engineering as the Linchpin of Responsible AI,"From humanity's existential risks to safety risks in critical systems to ethical risks, responsible AI, as the saviour, has become a major research challenge with significant real-world consequences. However, achieving responsible AI remains elusive despite the plethora of high-level ethical principles, risk frameworks and progress in algorithmic assurance. In the meantime, software engineering (SE) is being upended by AI, grappling with building system-level quality and alignment from inscrutable machine learning models and code generated from natural language prompts. The upending poses new challenges and opportunities for engineering AI systems responsibly. This talk will share our experiences in helping the industry achieve responsible AI systems by inventing new SE approaches. It will dive into industry challenges (such as risk silos and principle-algorithm gaps) and research challenges (such as lack of requirements, emerging properties and inscrutable systems) and make the point that SE is the linchpin of responsible AI. But SE also requires some fundamental rethinking - shifting from building functions into AI systems to discovering and managing emerging functions from AI systems. Only by doing so can SE take on critical new roles, from understanding human intelligence to building a thriving human-AI symbiosis.",Achievement,Successful,The paper proposes to help industry achieve responsible AI systems; suggesting successfully implemented new approaches and hence aligns with 'Successful' from 'Achievement'.,"In the abstract of 'Paper X', it is stated that the paper aims to help the industry achieve responsible AI systems by inventing new software engineering (SE) approaches. This implies that the main contribution of the paper is the successful implementation of these new SE approaches to achieve responsible AI. By aligning with the value item Successful and its corresponding value Achievement, the paper highlights the objective of achieving success in developing and implementing responsible AI systems, which is directly relevant to the perspective of a software user within a software context.",Agreed-Clarified,Agree,,Agree,,
Exp C,Exp I,856,ICSE,AI & Machine Learning,Software Engineering as the Linchpin of Responsible AI,"From humanity's existential risks to safety risks in critical systems to ethical risks, responsible AI, as the saviour, has become a major research challenge with significant real-world consequences. However, achieving responsible AI remains elusive despite the plethora of high-level ethical principles, risk frameworks and progress in algorithmic assurance. In the meantime, software engineering (SE) is being upended by AI, grappling with building system-level quality and alignment from inscrutable machine learning models and code generated from natural language prompts. The upending poses new challenges and opportunities for engineering AI systems responsibly. This talk will share our experiences in helping the industry achieve responsible AI systems by inventing new SE approaches. It will dive into industry challenges (such as risk silos and principle-algorithm gaps) and research challenges (such as lack of requirements, emerging properties and inscrutable systems) and make the point that SE is the linchpin of responsible AI. But SE also requires some fundamental rethinking - shifting from building functions into AI systems to discovering and managing emerging functions from AI systems. Only by doing so can SE take on critical new roles, from understanding human intelligence to building a thriving human-AI symbiosis.",Universalism,A World at Peace,The paper proposes building a thriving human-AI symbiosis; denoting a peaceful co-existence of humans and AI; which aligns with 'A World at Peace' under 'Universalism'.,"In the paper, the author mentions building a thriving human-AI symbiosis as a critical new role for software engineering. This concept aligns with the value item A World at Peace under Universalism because it implies a harmonious coexistence and collaboration between humans and AI, which goes beyond the traditional boundaries of software development. By emphasizing the need for a peaceful and cooperative relationship between humans and AI, the paper indirectly supports the values of equality, unity, social justice, and broadmindedness, which are all associated with Universalism.",Agreed-Clarified,Agree,,Agree,,
Exp E,Exp J,857,ICSE,Emerging Technologies,Future Software for Life in Trusted Futures,"How will people, other species, software and hardware live together in as yet unknown futures? How can we work towards trusted and safe futures where human values and the environment are supported by emerging technologies? Research demonstrates that human values and everyday life priorities, ethics, routines and activities will shape our possible futures. I will draw on ethnographic research to outline how people anticipate and imagine everyday life futures with emerging technologies in their homes and neighbourhoods, and how technology workers envisage futures in their professional lives. If, as social science research shows, technologies cannot solve human and societal problems, what roles should they play in future life? What are the implications for future software? What values should underpin its design? Where should it be developed? By and in collaboration with whom? What role can software play in generating the circumstances for trusted futures?",Universalism,A World at Peace,The abstract talks about working towards trusted futures which links to the value item 'A World at Peace' under the value 'Universalism'.,"I selected the value item ""A World at Peace"" and its corresponding value ""Universalism"" from a software user perspective in alignment with the main contribution of 'Paper X' because the abstract explicitly discusses working towards trusted futures, which can be seen as striving for peace and harmony on a global scale. This aligns with the value of Universalism, which emphasizes unity, social justice, and a world at peace. As a software user, the focus on creating software that supports these values is important in shaping future technologies and their impact on society.",Agreed-Clarified,Agree,,Agree,,
Exp F,Exp J,857,ICSE,Emerging Technologies,Future Software for Life in Trusted Futures,"How will people, other species, software and hardware live together in as yet unknown futures? How can we work towards trusted and safe futures where human values and the environment are supported by emerging technologies? Research demonstrates that human values and everyday life priorities, ethics, routines and activities will shape our possible futures. I will draw on ethnographic research to outline how people anticipate and imagine everyday life futures with emerging technologies in their homes and neighbourhoods, and how technology workers envisage futures in their professional lives. If, as social science research shows, technologies cannot solve human and societal problems, what roles should they play in future life? What are the implications for future software? What values should underpin its design? Where should it be developed? By and in collaboration with whom? What role can software play in generating the circumstances for trusted futures?",Benevolence,A Spiritual Life,The abstract discusses ethics which relates to the value item 'A Spiritual Life' under the value 'Benevolence'.,"In the abstract of 'Paper X,' the mention of ethics and the question of how to create trusted and safe futures aligns with the value item 'A Spiritual Life' and its corresponding value 'Benevolence' from a ""Software User"" perspective. Ethics are often associated with moral values and the consideration of the well-being of others, which aligns with benevolence. By exploring how human values and the environment can be supported by emerging technologies, the paper demonstrates a concern for the greater good and the ethical implications of technological development, which connects to the value of benevolence.",Agreed-Clarified,Agree,,Agree,,
Exp G,Exp K,858,ICSE,AI & Machine Learning,The Road Toward Dependable AI Based Systems,"With the advent of deep learning, AI components have achieved unprecedented performance on complex, human competitive tasks, such as image, video, text and audio processing. Hence, they are increasingly integrated into sophisticated software systems, some of which (e.g., autonomous vehicles) are required to deliver certified dependability warranties. In this talk, I will consider the unique features of AI based systems and of the faults possibly affecting them, in order to revise the testing fundamentals and redefine the overall goal of testing, taking a statistical view on the dependability warranties that can be actually delivered. Then, I will consider the key elements of a revised testing process for AI based systems, including the test oracle and the test input generation problems. I will also introduce the notion of runtime supervision, to deal with unexpected error conditions that may occur in the field. Finally, I will identify the future steps that are essential to close the loop from testing to operation, proposing an empirical framework that reconnects the output of testing to its original goals.",Security,Healthy,"The paper discusses ""testing fundamentals"" and ""revised testing process for AI based systems"" to ensure dependable warranties; aligning with the user value item of being Healthy and its corresponding value Security; as these testing processes would ensure the reliability; safety and thus ""healthiness"" of software.","In the paper abstract, the author specifically mentions considering the unique features and faults of AI-based systems, as well as testing processes and runtime supervision for dealing with unexpected error conditions. These discussions directly align with the value item of Healthy and its corresponding value Security, as they aim to ensure the reliability, safety, and overall ""healthiness"" of the software system, which is essential for a software user's security and well-being.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,858,ICSE,AI & Machine Learning,The Road Toward Dependable AI Based Systems,"With the advent of deep learning, AI components have achieved unprecedented performance on complex, human competitive tasks, such as image, video, text and audio processing. Hence, they are increasingly integrated into sophisticated software systems, some of which (e.g., autonomous vehicles) are required to deliver certified dependability warranties. In this talk, I will consider the unique features of AI based systems and of the faults possibly affecting them, in order to revise the testing fundamentals and redefine the overall goal of testing, taking a statistical view on the dependability warranties that can be actually delivered. Then, I will consider the key elements of a revised testing process for AI based systems, including the test oracle and the test input generation problems. I will also introduce the notion of runtime supervision, to deal with unexpected error conditions that may occur in the field. Finally, I will identify the future steps that are essential to close the loop from testing to operation, proposing an empirical framework that reconnects the output of testing to its original goals.",Security,Social Order,"By proposing an ""empirical framework that reconnects the output of testing to its original goals;"" the paper aligns with the value item of Social Order and its corresponding value Security; as such a standardized framework would help in maintaining an ordered functioning of AI-based systems.","The proposed empirical framework in 'Paper X' aligns with the value item of Social Order and its corresponding value Security because it provides a standardized approach to maintain the order and reliability of AI-based systems. By reconnecting the output of testing to its original goals, the framework ensures that these systems operate in a predictable and dependable manner, thus contributing to a sense of security and stability in the software context.",Agreed-Justified,Agree,,Agree,,
Exp B,Exp H,858,ICSE,AI & Machine Learning,The Road Toward Dependable AI Based Systems,"With the advent of deep learning, AI components have achieved unprecedented performance on complex, human competitive tasks, such as image, video, text and audio processing. Hence, they are increasingly integrated into sophisticated software systems, some of which (e.g., autonomous vehicles) are required to deliver certified dependability warranties. In this talk, I will consider the unique features of AI based systems and of the faults possibly affecting them, in order to revise the testing fundamentals and redefine the overall goal of testing, taking a statistical view on the dependability warranties that can be actually delivered. Then, I will consider the key elements of a revised testing process for AI based systems, including the test oracle and the test input generation problems. I will also introduce the notion of runtime supervision, to deal with unexpected error conditions that may occur in the field. Finally, I will identify the future steps that are essential to close the loop from testing to operation, proposing an empirical framework that reconnects the output of testing to its original goals.",Security,National Security,"The paper's mention of ""runtime supervision; to deal with unexpected error conditions that may occur in the field"" signals a concern for National Security (in the sense of security of infrastructure systems); as it ensures these AI-based systems do not malfunction unexpectedly; compromising the secure and safe functioning of software in a broad societal context.","The mention of ""runtime supervision"" in the paper abstract indicates that the authors are concerned about unexpected error conditions that could occur in the field. This implies a focus on ensuring the secure and safe functioning of AI-based systems in a broader societal context, including infrastructure systems. By addressing these error conditions, the paper aligns with the value item of National Security and its corresponding value of Security.",Agreed-Justified,Disagree,reconcile with coder_2,Disagree,"The justification claiming that the paper aligns with the value 'Security' because of 'The paper's mention of ""runtime supervision; to deal with unexpected error conditions that may occur in the field"" signals a concern for National Security (in the sense of security of infrastructure systems); as it ensures these AI-based systems do not malfunction unexpectedly; compromising the secure and safe functioning of software in a broad societal context. ' is not supported by the evidence in the abstract, which focuses on 'With the advent of deep learning, AI components have achieved unprecedented pe",
Exp C,Exp I,860,ICSE,AI & Machine Learning,Jigsaw: Large language models meet program synthesis,"Large pre-trained language models such as GPT-3 [10], Codex [11], and Coogle's language model [7] are now capable of generating code from natural language specifications of programmer intent. We view these developments with a mixture of optimism and caution. On the optimistic side, such large language models have the potential to improve productivity by providing an automated AI pair programmer for every programmer in the world. On the cautionary side, since these large language models do not understand program semantics, they offer no guarantees about quality of the suggested code. In this paper, we present an approach to augment these large language models with post-processing steps based on program analysis and synthesis techniques, that understand the syntax and semantics of programs. Further, we show that such techniques can make use of user feedback and improve with usage. We present our experiences from building and evaluating such a tool Jigsaw, targeted at synthesizing code for using Python Pandas API using multi-modal inputs. Our experience suggests that as these large language models evolve for synthesizing code from intent, Jigsaw has an important role to play in improving the accuracy of the systems.",Self Direction,Freedom,Paper X's main contribution is the development of an AI tool 'Jigsaw' that synthesizes code from programming intent; which simplifies the process of generating code. This allows users more Freedom in defining their program requirements in natural language.,"In 'Paper X', the development of the AI tool 'Jigsaw' aligns with the value item Freedom and its corresponding value Self Direction from a ""Software User"" perspective. This alignment is evident as 'Jigsaw' allows users to define their program requirements in natural language, providing them the freedom to express their intent without the need for specific programming knowledge or syntax. By synthesizing code based on the user's intent, 'Jigsaw' empowers software users to have more control and independence in the development process, enhancing their self-direction and freedom in creating programs.",Agreed-Justified,Agree,,Agree,,
Exp D,Exp I,860,ICSE,AI & Machine Learning,Jigsaw: Large language models meet program synthesis,"Large pre-trained language models such as GPT-3 [10], Codex [11], and Coogle's language model [7] are now capable of generating code from natural language specifications of programmer intent. We view these developments with a mixture of optimism and caution. On the optimistic side, such large language models have the potential to improve productivity by providing an automated AI pair programmer for every programmer in the world. On the cautionary side, since these large language models do not understand program semantics, they offer no guarantees about quality of the suggested code. In this paper, we present an approach to augment these large language models with post-processing steps based on program analysis and synthesis techniques, that understand the syntax and semantics of programs. Further, we show that such techniques can make use of user feedback and improve with usage. We present our experiences from building and evaluating such a tool Jigsaw, targeted at synthesizing code for using Python Pandas API using multi-modal inputs. Our experience suggests that as these large language models evolve for synthesizing code from intent, Jigsaw has an important role to play in improving the accuracy of the systems.",Achievement,Intelligent,The tool developed in Paper X enhances the Intelligent capabilities of users in programming through synthesizing code from specified intent which leads to better outcomes in programming tasks. This aligns with the value item Intelligence and its corresponding value Achievement.,"The tool developed in Paper X aligns with the value item Intelligent and its corresponding value Achievement because it improves the intelligence and capabilities of software users in programming. By synthesizing code from specified intent, it helps users achieve better outcomes in their programming tasks. This aligns with the value of Achievement as it enables users to successfully accomplish their programming goals with enhanced intelligence and efficiency.",Agreed-Justified,Agree,,Agree,,
Exp E,Exp J,860,ICSE,AI & Machine Learning,Jigsaw: Large language models meet program synthesis,"Large pre-trained language models such as GPT-3 [10], Codex [11], and Coogle's language model [7] are now capable of generating code from natural language specifications of programmer intent. We view these developments with a mixture of optimism and caution. On the optimistic side, such large language models have the potential to improve productivity by providing an automated AI pair programmer for every programmer in the world. On the cautionary side, since these large language models do not understand program semantics, they offer no guarantees about quality of the suggested code. In this paper, we present an approach to augment these large language models with post-processing steps based on program analysis and synthesis techniques, that understand the syntax and semantics of programs. Further, we show that such techniques can make use of user feedback and improve with usage. We present our experiences from building and evaluating such a tool Jigsaw, targeted at synthesizing code for using Python Pandas API using multi-modal inputs. Our experience suggests that as these large language models evolve for synthesizing code from intent, Jigsaw has an important role to play in improving the accuracy of the systems.",Security,Healthy,By aiding in accurately synthesizing code; the Jigsaw tool can contribute to making software more reliable and robust thereby indirectly contributing to the Healthy functioning of software used by users. This aligns with the value item Health and its corresponding value Security.,"The Jigsaw tool, by improving the accuracy and reliability of code synthesis, directly contributes to the security and robustness of software used by software users. This alignment with the value item Healthy can be seen in the context of software users relying on software that functions properly without any vulnerabilities or errors, ensuring a secure and optimal user experience.",Agreed-Clarified,Agree,,Agree,,
Exp F,Exp J,865,ICSE,Software Project Management,Did you miss my comment or what? understanding toxicity in open source discussions,"Online toxicity is ubiquitous across the internet and its negative impact on the people and that online communities that it effects has been well documented. However, toxicity manifests differently on various platforms and toxicity in open source communities, while frequently discussed, is not well understood. We take a first stride at understanding the characteristics of open source toxicity to better inform future work on designing effective intervention and detection methods. To this end, we curate a sample of 100 toxic GitHub issue discussions combining multiple search and sampling strategies. We then qualitatively analyze the sample to gain an understanding of the characteristics of open-source toxicity. We find that the pervasive forms of toxicity in open source differ from those observed on other platforms like Reddit or Wikipedia. In our sample, some of the most prevalent forms of toxicity are entitled, demanding, and arrogant comments from project users as well as insults arising from technical disagreements. In addition, not all toxicity was written by people external to the projects; project members were also common authors of toxicity. We also discuss the implications of our findings. Among others we hope that our findings will be useful for future detection work.",Security,Sense of Belonging,The paper examines open-source toxicity in online communities; which directly aligns with the value item Sense of Belonging and its corresponding Security value as it contributes to a positive sense of belonging in the community.,"The paper's examination of open-source toxicity in online communities directly aligns with the value item ""Sense of Belonging"" and its corresponding value ""Security"" from a ""Software User"" perspective. Understanding and addressing toxicity in these communities contributes to a positive sense of belonging by creating a safer and more inclusive environment, enhancing the users' security in participating and contributing to the open-source community.",Agreed-Clarified,Agree,,Agree,,
Exp G,Exp K,865,ICSE,Software Project Management,Did you miss my comment or what? understanding toxicity in open source discussions,"Online toxicity is ubiquitous across the internet and its negative impact on the people and that online communities that it effects has been well documented. However, toxicity manifests differently on various platforms and toxicity in open source communities, while frequently discussed, is not well understood. We take a first stride at understanding the characteristics of open source toxicity to better inform future work on designing effective intervention and detection methods. To this end, we curate a sample of 100 toxic GitHub issue discussions combining multiple search and sampling strategies. We then qualitatively analyze the sample to gain an understanding of the characteristics of open-source toxicity. We find that the pervasive forms of toxicity in open source differ from those observed on other platforms like Reddit or Wikipedia. In our sample, some of the most prevalent forms of toxicity are entitled, demanding, and arrogant comments from project users as well as insults arising from technical disagreements. In addition, not all toxicity was written by people external to the projects; project members were also common authors of toxicity. We also discuss the implications of our findings. Among others we hope that our findings will be useful for future detection work.",Benevolence,Honesty,The paper's aim to identify and characterize the forms of toxicity in open-source communities can be seen as strengthening the value item Honesty and the Benevolence value by enhancing the honesty and transparency in online interactions.,"By analyzing and characterizing the forms of toxicity in open-source communities, 'Paper X' contributes to the value item Honesty and its corresponding value Benevolence by promoting honesty and transparency in online interactions. Understanding the prevalent toxic behaviors, such as entitled or demanding comments, and identifying that toxicity can come from both external users and project members, allows for the development of intervention and detection methods to address these issues. By addressing toxicity, the paper aims to foster an environment where software users can engage with open-source projects in a more respectful and beneficial manner, aligning with the values of honesty and benevolence in online communities.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,865,ICSE,Software Project Management,Did you miss my comment or what? understanding toxicity in open source discussions,"Online toxicity is ubiquitous across the internet and its negative impact on the people and that online communities that it effects has been well documented. However, toxicity manifests differently on various platforms and toxicity in open source communities, while frequently discussed, is not well understood. We take a first stride at understanding the characteristics of open source toxicity to better inform future work on designing effective intervention and detection methods. To this end, we curate a sample of 100 toxic GitHub issue discussions combining multiple search and sampling strategies. We then qualitatively analyze the sample to gain an understanding of the characteristics of open-source toxicity. We find that the pervasive forms of toxicity in open source differ from those observed on other platforms like Reddit or Wikipedia. In our sample, some of the most prevalent forms of toxicity are entitled, demanding, and arrogant comments from project users as well as insults arising from technical disagreements. In addition, not all toxicity was written by people external to the projects; project members were also common authors of toxicity. We also discuss the implications of our findings. Among others we hope that our findings will be useful for future detection work.",Universalism,A World at Peace,The research work performed in the paper to inform future interventions and toxicity detection methods aligns with the value item A World at Peace and the Universalism value as it aims to create a peaceful online community.,"I apologize for any confusion caused. Upon further analysis, 'Paper X' does not directly align with the value item A World at Peace and its corresponding value Universalism. While the paper aims to understand and address toxicity in open source communities, it does not explicitly mention creating a peaceful online community as part of its main contributions. Instead, the paper focuses on analyzing the characteristics of open source toxicity to inform future intervention and detection methods. Therefore, the alignment with A World at Peace and Universalism cannot be clearly evidenced in the abstract.",Agreed-Justified,Agree,,Agree,,
Exp B,Exp H,869,ICSE,Security & Privacy,MVD: memory-related vulnerability detection based on flow-sensitive graph neural networks,"Memory-related vulnerabilities constitute severe threats to the security of modern software. Despite the success of deep learning-based approaches to generic vulnerability detection, they are still limited by the underutilization of flow information when applied for detecting memory-related vulnerabilities, leading to high false positives. In this paper, we propose MVD, a statement-level Memory-related Vulnerability Detection approach based on flow-sensitive graph neural networks (FS-GNN). FS-GNN is employed to jointly embed both unstructured information (i.e., source code) and structured information (i.e., control- and data-flow) to capture implicit memory-related vulnerability patterns. We evaluate MVD on the dataset which contains 4,353 real-world memory-related vulnerabilities, and compare our approach with three state-of-the-art deep learning-based approaches as well as five popular static analysis-based memory detectors. The experiment results show that MVD achieves better detection accuracy, outperforming both state-of-the-art DL-based and static analysis-based approaches. Furthermore, MVD makes a great trade-off between accuracy and efficiency.",Security,Healthy,The paper proposes a method for memory-related vulnerability detection which can make the software user's experience safer; potentially contributing to the user's health by reducing the risk of digital harm.,"The justification to align 'Paper X' with the value item Healthy and its corresponding value Security from a ""Software User"" perspective is that the proposed memory-related vulnerability detection method aims to enhance the security of software, ultimately reducing the risk of digital harm. By mitigating vulnerabilities, the method contributes to a safer software environment for users, potentially protecting their personal information and minimizing the potential negative impacts on their digital well-being. This aligns with the value of Security from Schwartz's Taxonomy, as users value measures that promote their safety and protection in the software context.",Agreed-Justified,Disagree,"Related to Achievement: capable more, as it automatically identifies vulnerabilities",Disagree,"The justification claiming that the paper aligns with the value 'Security' because of 'The paper proposes a method for memory-related vulnerability detection which can make the software user's experience safer; potentially contributing to the user's health by reducing the risk of digital harm. ' is not supported by the evidence in the abstract, which focuses on 'Memory-related vulnerabilities constitute severe threats to the security of modern software.",
Exp D,Exp I,869,ICSE,Security & Privacy,MVD: memory-related vulnerability detection based on flow-sensitive graph neural networks,"Memory-related vulnerabilities constitute severe threats to the security of modern software. Despite the success of deep learning-based approaches to generic vulnerability detection, they are still limited by the underutilization of flow information when applied for detecting memory-related vulnerabilities, leading to high false positives. In this paper, we propose MVD, a statement-level Memory-related Vulnerability Detection approach based on flow-sensitive graph neural networks (FS-GNN). FS-GNN is employed to jointly embed both unstructured information (i.e., source code) and structured information (i.e., control- and data-flow) to capture implicit memory-related vulnerability patterns. We evaluate MVD on the dataset which contains 4,353 real-world memory-related vulnerabilities, and compare our approach with three state-of-the-art deep learning-based approaches as well as five popular static analysis-based memory detectors. The experiment results show that MVD achieves better detection accuracy, outperforming both state-of-the-art DL-based and static analysis-based approaches. Furthermore, MVD makes a great trade-off between accuracy and efficiency.",Achievement,Successful,The improvement in vulnerability detection and reduction of false positives can enhance the success of the software and enable software users to realise their ambitions to a greater extent through the use of more secure software; thus aligning with the value of Achievement and its corresponding value item Successful.,"The justification for aligning 'Paper X' with the value item Successful and its corresponding value Achievement is based on the idea that by improving vulnerability detection and reducing false positives, the software becomes more secure and successful. When software users are able to rely on secure software, they can achieve their goals and ambitions without being hindered by potential vulnerabilities. This alignment with the value of Achievement emphasizes the importance of successful outcomes and the ability for users to achieve their desired objectives with the help of secure software.",Agreed-Justified,Agree,,Agree,,
Exp E,Exp J,872,ICSE,Security & Privacy,Practical automated detection of malicious npm packages,"The npm registry is one of the pillars of the JavaScript and Type-Script ecosystems, hosting over 1.7 million packages ranging from simple utility libraries to complex frameworks and entire applications. Each day, developers publish tens of thousands of updates as well as hundreds of new packages. Due to the overwhelming popularity of npm, it has become a prime target for malicious actors, who publish new packages or compromise existing packages to introduce malware that tampers with or exfiltrates sensitive data from users who install either these packages or any package that (transitively) depends on them. Defending against such attacks is essential to maintaining the integrity of the software supply chain, but the sheer volume of package updates makes comprehensive manual review infeasible. We present Amalfi, a machine-learning based approach for automatically detecting potentially malicious packages comprised of three complementary techniques. We start with classifiers trained on known examples of malicious and benign packages. If a package is flagged as malicious by a classifier, we then check whether it includes metadata about its source repository, and if so whether the package can be reproduced from its source code. Packages that are reproducible from source are not usually malicious, so this step allows us to weed out false positives. Finally, we also employ a simple textual clone-detection technique to identify copies of malicious packages that may have been missed by the classifiers, reducing the number of false negatives. Amalfi improves on the state of the art in that it is lightweight, requiring only a few seconds per package to extract features and run the classifiers, and gives good results in practice: running it on 96287 package versions published over the course of one week, we were able to identify 95 previously unknown malware samples, with a manageable number of false positives.",Power,Social Recognition,The paper contributes to social recognition of the user by developing an automatic detection technique for malicious packages that helps ensure user's trust in the npm ecosystem.,"The paper aligns with Social Recognition and its corresponding value Power as it addresses the need for developers to uphold their reputation and gain recognition by providing users with a trustworthy software supply chain. By developing Amalfi, an automatic detection technique for malicious packages, the paper aims to protect users from potential harm and maintain the integrity of the npm ecosystem. This aligns with the value of Power, as it empowers users by giving them the ability to make informed decisions and protect their sensitive data. Additionally, it aligns with Social Recognition as it contributes to the reputation and recognition of developers who prioritize user security and trust.",Agreed-Clarified,Agree,,Agree,,
Exp F,Exp J,872,ICSE,Security & Privacy,Practical automated detection of malicious npm packages,"The npm registry is one of the pillars of the JavaScript and Type-Script ecosystems, hosting over 1.7 million packages ranging from simple utility libraries to complex frameworks and entire applications. Each day, developers publish tens of thousands of updates as well as hundreds of new packages. Due to the overwhelming popularity of npm, it has become a prime target for malicious actors, who publish new packages or compromise existing packages to introduce malware that tampers with or exfiltrates sensitive data from users who install either these packages or any package that (transitively) depends on them. Defending against such attacks is essential to maintaining the integrity of the software supply chain, but the sheer volume of package updates makes comprehensive manual review infeasible. We present Amalfi, a machine-learning based approach for automatically detecting potentially malicious packages comprised of three complementary techniques. We start with classifiers trained on known examples of malicious and benign packages. If a package is flagged as malicious by a classifier, we then check whether it includes metadata about its source repository, and if so whether the package can be reproduced from its source code. Packages that are reproducible from source are not usually malicious, so this step allows us to weed out false positives. Finally, we also employ a simple textual clone-detection technique to identify copies of malicious packages that may have been missed by the classifiers, reducing the number of false negatives. Amalfi improves on the state of the art in that it is lightweight, requiring only a few seconds per package to extract features and run the classifiers, and gives good results in practice: running it on 96287 package versions published over the course of one week, we were able to identify 95 previously unknown malware samples, with a manageable number of false positives.",Security,Social Order,The paper develops a technique that defends against malicious packages in the npm registry; therefore maintaining the social order in the software user community by ensuring the integrity of the software supply chain.,"In the context of a ""Software User,"" the main contribution of 'Paper X' is to develop a technique, Amalfi, that automatically detects potentially malicious packages in the npm registry. By doing so, it enhances security in the software user community by maintaining the social order through ensuring the integrity of the software supply chain. This aligns with the value item of Social Order and its corresponding value of Security, as the paper's approach directly addresses the need for protection against malicious actors and the potential harm they can cause to the software ecosystem.",Agreed-Clarified,Agree,,Agree,,
Exp G,Exp K,872,ICSE,Security & Privacy,Practical automated detection of malicious npm packages,"The npm registry is one of the pillars of the JavaScript and Type-Script ecosystems, hosting over 1.7 million packages ranging from simple utility libraries to complex frameworks and entire applications. Each day, developers publish tens of thousands of updates as well as hundreds of new packages. Due to the overwhelming popularity of npm, it has become a prime target for malicious actors, who publish new packages or compromise existing packages to introduce malware that tampers with or exfiltrates sensitive data from users who install either these packages or any package that (transitively) depends on them. Defending against such attacks is essential to maintaining the integrity of the software supply chain, but the sheer volume of package updates makes comprehensive manual review infeasible. We present Amalfi, a machine-learning based approach for automatically detecting potentially malicious packages comprised of three complementary techniques. We start with classifiers trained on known examples of malicious and benign packages. If a package is flagged as malicious by a classifier, we then check whether it includes metadata about its source repository, and if so whether the package can be reproduced from its source code. Packages that are reproducible from source are not usually malicious, so this step allows us to weed out false positives. Finally, we also employ a simple textual clone-detection technique to identify copies of malicious packages that may have been missed by the classifiers, reducing the number of false negatives. Amalfi improves on the state of the art in that it is lightweight, requiring only a few seconds per package to extract features and run the classifiers, and gives good results in practice: running it on 96287 package versions published over the course of one week, we were able to identify 95 previously unknown malware samples, with a manageable number of false positives.",Security,Sense of Belonging,The paper ensures a sense of belonging among users by maintaining the integrity of the open-source community and ensuring the safety of users while using npm packages.,"The main contribution of 'Paper X' is focused on defending against malicious attacks on npm packages and maintaining the integrity of the software supply chain. By doing so, it ensures a sense of belonging among software users within the open-source community by creating a secure environment for them to rely on when using npm packages. This aligns with the value item Sense of Belonging and its corresponding value Security, as it emphasizes the importance of protecting users and fostering a community of trust and safety in software usage.",Agreed-Justified,Disagree,This clarification is not related to the Sense of Belonging ,Disagree,"maybe an indirect link, not sure ...",
Exp A,Exp H,876,ICSE,AI & Machine Learning,Ropgen: Towards robust code authorship attribution via automatic coding style transformation,"Source code authorship attribution is an important problem often encountered in applications such as software forensics, bug fixing, and software quality analysis. Recent studies show that current source code authorship attribution methods can be compromised by attackers exploiting adversarial examples and coding style ma-nipulation. This calls for robust solutions to the problem of code authorship attribution. In this paper, we initiate the study on making Deep Learning (DL)-based code authorship attribution robust. We propose an innovative framework called Robust coding style Patterns Generation (RoPGen), which essentially learns authors' unique coding style patterns that are hard for attackers to manip-ulate or imitate. The key idea is to combine data augmentation and gradient augmentation at the adversarial training phase. This effectively increases the diversity of training examples, generates meaningful perturbations to gradients of deep neural networks, and learns diversified representations of coding styles. We evaluate the effectiveness of RoPGen using four datasets of programs written in C, C++, and Java. Experimental results show that RoPGen can significantly improve the robustness of DL-based code authorship attribution, by respectively reducing 22.8% and 41.0% of the success rate of targeted and untargeted attacks on average.",Achievement,Capable,Paper X contributes to code authorship attribution; helping users identify the correct author of a source code; therefore; indirectly enhancing the capability of the user in correctly attributing the authorship hence aligning with the value item Capable under the value Achievement.,"Upon analyzing the abstract of 'Paper X', it is evident that the main contribution is focused on improving code authorship attribution, which can ultimately help software users identify the correct author of a source code. By enhancing the accuracy of this attribution process, users are empowered with the capability to correctly attribute authorship, aligning with the value item Capable. This directly relates to the value of Achievement as users are able to achieve the intended outcome of accurately attributing code authorship.",Agreed-Justified,Agree,,Agree,,
Exp B,Exp H,876,ICSE,AI & Machine Learning,Ropgen: Towards robust code authorship attribution via automatic coding style transformation,"Source code authorship attribution is an important problem often encountered in applications such as software forensics, bug fixing, and software quality analysis. Recent studies show that current source code authorship attribution methods can be compromised by attackers exploiting adversarial examples and coding style ma-nipulation. This calls for robust solutions to the problem of code authorship attribution. In this paper, we initiate the study on making Deep Learning (DL)-based code authorship attribution robust. We propose an innovative framework called Robust coding style Patterns Generation (RoPGen), which essentially learns authors' unique coding style patterns that are hard for attackers to manip-ulate or imitate. The key idea is to combine data augmentation and gradient augmentation at the adversarial training phase. This effectively increases the diversity of training examples, generates meaningful perturbations to gradients of deep neural networks, and learns diversified representations of coding styles. We evaluate the effectiveness of RoPGen using four datasets of programs written in C, C++, and Java. Experimental results show that RoPGen can significantly improve the robustness of DL-based code authorship attribution, by respectively reducing 22.8% and 41.0% of the success rate of targeted and untargeted attacks on average.",Security,Social Order,By enhancing the robustness of DL-based code authorship attribution; Paper X indirectly contributes to maintaining a social order in the coding and software environment by ensuring authors' unique coding styles are recognized and unaltered; thus aligns with the value item Social Order under the value Security.,"In the context of software development, maintaining social order involves ensuring that the coding and software environment is secure and that the work of individual authors is recognized and respected. By enhancing the robustness of Deep Learning-based code authorship attribution, 'Paper X' directly contributes to this goal by making it harder for attackers to manipulate or imitate an author's unique coding style. This aligns with the value item Social Order under the value Security as it helps to protect the integrity and authenticity of individual authors' work, fostering a sense of trust and order in the software development community.",Agreed-Justified,Agree,,Agree,,
Exp C,Exp I,876,ICSE,AI & Machine Learning,Ropgen: Towards robust code authorship attribution via automatic coding style transformation,"Source code authorship attribution is an important problem often encountered in applications such as software forensics, bug fixing, and software quality analysis. Recent studies show that current source code authorship attribution methods can be compromised by attackers exploiting adversarial examples and coding style ma-nipulation. This calls for robust solutions to the problem of code authorship attribution. In this paper, we initiate the study on making Deep Learning (DL)-based code authorship attribution robust. We propose an innovative framework called Robust coding style Patterns Generation (RoPGen), which essentially learns authors' unique coding style patterns that are hard for attackers to manip-ulate or imitate. The key idea is to combine data augmentation and gradient augmentation at the adversarial training phase. This effectively increases the diversity of training examples, generates meaningful perturbations to gradients of deep neural networks, and learns diversified representations of coding styles. We evaluate the effectiveness of RoPGen using four datasets of programs written in C, C++, and Java. Experimental results show that RoPGen can significantly improve the robustness of DL-based code authorship attribution, by respectively reducing 22.8% and 41.0% of the success rate of targeted and untargeted attacks on average.",Universalism,Social Justice,The improved robustness of DL-based code authorship attribution contributed by Paper X can indirectly improve Social Justice by ensuring correct authorship attribution; hence aligning with the value item Social Justice under the value Universalism.,"In 'Paper X', the improved robustness of DL-based code authorship attribution directly aligns with the value item Social Justice under the value Universalism. By accurately attributing code authorship, software users can ensure fairness and accountability in the software development process. This aligns with the value of Social Justice, which promotes equal opportunities, fairness, and inclusivity in the software domain. The improved robustness provided by 'Paper X' indirectly contributes to Social Justice because it helps prevent false attributions and ensures that code is properly credited to its original authors, thus promoting fairness and transparency in software development.",Agreed-Clarified,Agree,,Agree,,
Exp E,Exp J,887,ICSE,AI & Machine Learning,Causality-based neural network repair,"Neural networks have had discernible achievements in a wide range of applications. The wide-spread adoption also raises the concern of their dependability and reliability. Similar to traditional decision-making programs, neural networks can have defects that need to be repaired. The defects may cause unsafe behaviors, raise security concerns or unjust societal impacts. In this work, we address the problem of repairing a neural network for desirable properties such as fairness and the absence of backdoor. The goal is to construct a neural network that satisfies the property by (minimally) adjusting the given neural network's parameters (i.e., weights). Specifically, we propose CARE (CAusality-based REpair), a causality-based neural network repair technique that 1) performs causality-based fault localization to identify the ‚Äö√Ñ√∂‚àö√ë‚àö‚â§guilty‚Äö√Ñ√∂‚àö√ë‚àö¬• neurons and 2) optimizes the parameters of the identified neurons to reduce the misbehavior. We have empirically evaluated CARE on various tasks such as backdoor removal, neural network repair for fairness and safety properties. Our experiment results show that CARE is able to repair all neural networks efficiently and effectively. For fairness repair tasks, CARE successfully improves fairness by 61.91 % on average. For backdoor removal tasks, CARE reduces the attack success rate from over 98% to less than 1 %. For safety property repair tasks, CARE reduces the property violation rate to less than 1 %. Results also show that thanks to the causality-based fault localization, CARE's repair focuses on the misbehavior and preserves the accuracy of the neural networks.",Power,Social Recognition,The paper focuses on addressing defects in neural networks that may cause societal impacts; thereby improving the social recognition of the neural network. This aligns with the value item Social recognition and its corresponding value Power.,"In 'Paper X', the authors propose a neural network repair technique called CARE that aims to improve the social recognition of neural networks by addressing defects that may cause unjust societal impacts. This aligns with the value item Social Recognition and its corresponding value Power because it emphasizes the importance of addressing issues in neural networks that can have social consequences, thereby demonstrating a concern for the recognition and influence of the neural network in society.",Agreed-Clarified,Disagree,the social impacts and neural networks does not seems to be connected,Disagree,"The article is about fault localization and misbehaviour, which is not related to social recognition",
Exp F,Exp J,887,ICSE,AI & Machine Learning,Causality-based neural network repair,"Neural networks have had discernible achievements in a wide range of applications. The wide-spread adoption also raises the concern of their dependability and reliability. Similar to traditional decision-making programs, neural networks can have defects that need to be repaired. The defects may cause unsafe behaviors, raise security concerns or unjust societal impacts. In this work, we address the problem of repairing a neural network for desirable properties such as fairness and the absence of backdoor. The goal is to construct a neural network that satisfies the property by (minimally) adjusting the given neural network's parameters (i.e., weights). Specifically, we propose CARE (CAusality-based REpair), a causality-based neural network repair technique that 1) performs causality-based fault localization to identify the ‚Äö√Ñ√∂‚àö√ë‚àö‚â§guilty‚Äö√Ñ√∂‚àö√ë‚àö¬• neurons and 2) optimizes the parameters of the identified neurons to reduce the misbehavior. We have empirically evaluated CARE on various tasks such as backdoor removal, neural network repair for fairness and safety properties. Our experiment results show that CARE is able to repair all neural networks efficiently and effectively. For fairness repair tasks, CARE successfully improves fairness by 61.91 % on average. For backdoor removal tasks, CARE reduces the attack success rate from over 98% to less than 1 %. For safety property repair tasks, CARE reduces the property violation rate to less than 1 %. Results also show that thanks to the causality-based fault localization, CARE's repair focuses on the misbehavior and preserves the accuracy of the neural networks.",Power,Preserving My Public Image,The paper explicitly states that it works on repairing the neural network for desirable properties such as fairness; which contributes to preserving the public image of the software. This aligns with the value item Preserving My Public Image under the value Power.,"In the paper abstract, 'Paper X' is focused on repairing neural networks for desirable properties such as fairness. The main contribution of the paper is directly aligned with the value item Preserving My Public Image, as ensuring fairness in neural networks can support the software's reputation and public image. This alignment is in line with the value of Power, as having a positive public image can enhance the software's influence, authority, and social recognition.",Agreed-Clarified,Agree,,Agree,,
Exp G,Exp K,888,ICSE,Code Generation & Analysis,Jucify: A step towards android code unification for enhanced static analysis,"Native code is now commonplace within Android app packages where it co-exists and interacts with Dex bytecode through the Java Native Interface to deliver rich app functionalities. Yet, state-of-the-art static analysis approaches have mostly overlooked the presence of such native code, which, however, may implement some key sensitive, or even malicious, parts of the app behavior. This limitation of the state of the art is a severe threat to validity in a large range of static analyses that do not have a complete view of the executable code in apps. To address this issue, we propose a new advance in the ambitious research direction of building a unified model of all code in Android apps. The JUCIFY approach presented in this paper is a significant step towards such a model, where we extract and merge call graphs of native code and bytecode to make the final model readily-usable by a common Android analysis framework: in our implementation, JUCIFY builds on the Soot internal intermediate representation. We performed empirical investigations to highlight how, without the unified model, a significant amount of Java methods called from the native code are ‚Äö√Ñ√∂‚àö√ë‚àö‚à´unreachable‚Äö√Ñ√∂‚àö√ë‚àöœÄ in apps' callgraphs, both in goodware and malware. Using JUCIFY, we were able to enable static analyzers to reveal cases where malware relied on native code to hide invocation of payment library code or of other sensitive code in the Android framework. Additionally, JUCIFY'S model enables state-of-the-art tools to achieve better precision and recall in detecting data leaks through native code. Finally, we show that by using JUCIFY we can find sensitive data leaks that pass through native code.",Power,Authority,The paper develops 'JUCIFY'; a method for improving the security of Android apps through a unified model of all code in Android apps. The ability for static analyzers to reveal cases where malware is hiding in native code aligns with the value item Authority under the Power value as it gives software users the power over their apps by ensuring they are secure and free of threats.,"The alignment of 'Paper X' with the value item Authority and its corresponding value Power is justified because the proposed approach, JUCIFY, empowers software users by enhancing the security of Android apps. It achieves this by enabling static analyzers to uncover cases where malware is hiding in native code, giving users the power to ensure their apps are secure and free of threats. This aligns with the value item Authority as it gives users control and influence over their apps, reinforcing their power in the software context.",Agreed-Clarified,Agree,,Agree,,
Exp A,Exp H,888,ICSE,Code Generation & Analysis,Jucify: A step towards android code unification for enhanced static analysis,"Native code is now commonplace within Android app packages where it co-exists and interacts with Dex bytecode through the Java Native Interface to deliver rich app functionalities. Yet, state-of-the-art static analysis approaches have mostly overlooked the presence of such native code, which, however, may implement some key sensitive, or even malicious, parts of the app behavior. This limitation of the state of the art is a severe threat to validity in a large range of static analyses that do not have a complete view of the executable code in apps. To address this issue, we propose a new advance in the ambitious research direction of building a unified model of all code in Android apps. The JUCIFY approach presented in this paper is a significant step towards such a model, where we extract and merge call graphs of native code and bytecode to make the final model readily-usable by a common Android analysis framework: in our implementation, JUCIFY builds on the Soot internal intermediate representation. We performed empirical investigations to highlight how, without the unified model, a significant amount of Java methods called from the native code are ‚Äö√Ñ√∂‚àö√ë‚àö‚àÇ‚Äö√†√∂‚àö√´‚Äö√†√∂‚Äö√†√á‚Äö√Ñ√∂‚àö‚Ä†‚àö‚àÇ‚Äö√†√∂‚àö¬¥‚Äö√Ñ√∂‚àö‚Ä†‚àö‚àÇ‚Äö√Ñ√∂‚àö‚Ä†¬¨¬•unreachable‚Äö√Ñ√∂‚àö√ë‚àö‚àÇ‚Äö√†√∂‚àö√´‚Äö√†√∂‚Äö√†√á‚Äö√Ñ√∂‚àö‚Ä†‚àö‚àÇ‚Äö√†√∂‚àö¬¥‚Äö√Ñ√∂‚àö‚Ä†‚àö‚àÇ‚âà√¨‚àö√ë in apps' callgraphs, both in goodware and malware. Using JUCIFY, we were able to enable static analyzers to reveal cases where malware relied on native code to hide invocation of payment library code or of other sensitive code in the Android framework. Additionally, JUCIFY'S model enables state-of-the-art tools to achieve better precision and recall in detecting data leaks through native code. Finally, we show that by using JUCIFY we can find sensitive data leaks that pass through native code.",Security,Healthy,By developing a means to detect malware and other malicious behavior hidden in native code on Android apps; the paper directly increases the health of the software; leading to a more secure and reliable application for end users. This aligns with the value item Healthy of Schwartz's value Security.,The development of a means to detect malware and malicious behavior hidden in native code directly contributes to the health of software by ensuring that end users have a more secure and reliable application. This aligns with the value item Healthy of Schwartz's value Security as it focuses on the protection and well-being of the software and its users.,Agreed-Justified,Disagree,reconcile with coder_2,Disagree,The justification claiming that the paper aligns with the value 'Security' because of 'By developing a means to detect malware and other malicious behavior hidden in native code on Android apps; the paper directly increases the health of the software; leading to a more secure and reliable application for end users.  This aligns with the value item Healthy of Schwartz's value Security.,
Exp B,Exp H,888,ICSE,Code Generation & Analysis,Jucify: A step towards android code unification for enhanced static analysis,"Native code is now commonplace within Android app packages where it co-exists and interacts with Dex bytecode through the Java Native Interface to deliver rich app functionalities. Yet, state-of-the-art static analysis approaches have mostly overlooked the presence of such native code, which, however, may implement some key sensitive, or even malicious, parts of the app behavior. This limitation of the state of the art is a severe threat to validity in a large range of static analyses that do not have a complete view of the executable code in apps. To address this issue, we propose a new advance in the ambitious research direction of building a unified model of all code in Android apps. The JUCIFY approach presented in this paper is a significant step towards such a model, where we extract and merge call graphs of native code and bytecode to make the final model readily-usable by a common Android analysis framework: in our implementation, JUCIFY builds on the Soot internal intermediate representation. We performed empirical investigations to highlight how, without the unified model, a significant amount of Java methods called from the native code are ‚Äö√Ñ√∂‚àö√ë‚àö‚àÇ‚Äö√†√∂‚àö√´‚Äö√†√∂‚Äö√†√á‚Äö√Ñ√∂‚àö‚Ä†‚àö‚àÇ‚Äö√†√∂‚àö¬¥‚Äö√Ñ√∂‚àö‚Ä†‚àö‚àÇ‚Äö√Ñ√∂‚àö‚Ä†¬¨¬•unreachable‚Äö√Ñ√∂‚àö√ë‚àö‚àÇ‚Äö√†√∂‚àö√´‚Äö√†√∂‚Äö√†√á‚Äö√Ñ√∂‚àö‚Ä†‚àö‚àÇ‚Äö√†√∂‚àö¬¥‚Äö√Ñ√∂‚àö‚Ä†‚àö‚àÇ‚âà√¨‚àö√ë in apps' callgraphs, both in goodware and malware. Using JUCIFY, we were able to enable static analyzers to reveal cases where malware relied on native code to hide invocation of payment library code or of other sensitive code in the Android framework. Additionally, JUCIFY'S model enables state-of-the-art tools to achieve better precision and recall in detecting data leaks through native code. Finally, we show that by using JUCIFY we can find sensitive data leaks that pass through native code.",Security,National Security,"The paper's contribution to national security is expressed in its aid to the detection and elimination of malware from Android apps. This is a direct alignment with the value item of National Security in Schwartz's taxonomy.""","The justification for the alignment between 'Paper X' and the value item of National Security in Schwartz's taxonomy is based on the fact that the paper's main contribution lies in enabling the detection and elimination of malware in Android apps. Malware poses a significant threat to the security of a software user's device and personal information. By addressing this issue, the paper directly aligns with the value of Security, which encompasses the protection and safety of individuals and their digital assets. National Security is an extension of this value, as malware can also have implications for the broader security of a nation's information systems.",Agreed-Justified,Disagree,reconcile with coder_2,Disagree,The justification claiming that the paper aligns with the value 'Security' because of 'The paper's contribution to national security is expressed in its aid to the detection and elimination of malware from Android apps.  This is a direct alignment with the value item of National Security in Schwartz's taxonomy.,
Exp C,Exp I,890,ICSE,Software Testing & QA,Efficient online testing for DNN-enabled systems using surrogate-assisted and many-objective optimization,"With the recent advances of Deep Neural Networks (DNNs) in real-world applications, such as Automated Driving Systems (ADS) for self-driving cars, ensuring the reliability and safety of such DNN-enabled Systems emerges as a fundamental topic in software testing. One of the essential testing phases of such DNN-enabled systems is online testing, where the system under test is embedded into a specific and often simulated application environment (e.g., a driving environment) and tested in a closed-loop mode in interaction with the environment. However, despite the importance of online testing for detecting safety violations, automatically generating new and diverse test data that lead to safety violations presents the following challenges: (1) there can be many safety requirements to be considered at the same time, (2) running a high-fidelity simulator is often very computationally-intensive, and (3) the space of all possible test data that may trigger safety violations is too large to be exhaustively explored. In this paper, we address the challenges by proposing a novel approach, called SAMOTA (Surrogate-Assisted Many-Objective Testing Approach), extending existing many-objective search algorithms for test suite generation to efficiently utilize surrogate models that mimic the simulator, but are much less expensive to run. Empirical evaluation results on Pylot, an advanced ADS composed of multiple DNNs, using CARLA, a high-fidelity driving simulator, show that SAMOTA is significantly more effective and efficient at detecting unknown safety requirement violations than state-of-the-art many-objective test suite generation algorithms and random search. In other words, SAMOTA appears to be a key enabler technology for online testing in practice.",Security,Healthy,The paper proposes a method; SAMOTA; for improved safety testing of deep neural networks utilized in systems like self-driving cars; directly contributing to the user's health and wellbeing by improving the reliability and safety of these systems.,"The justification for aligning 'Paper X' with the value item Healthy and its corresponding value Security from a ""Software User"" perspective is that by proposing SAMOTA for improved safety testing of deep neural networks in self-driving cars, the paper directly contributes to the user's physical health and overall well-being. The reliable and safe operation of these systems ensures that users are protected from potential accidents or harm, thus promoting their personal security and ensuring their physical health remains intact.",Agreed-Justified,Agree,,Agree,,
Exp D,Exp I,890,ICSE,Software Testing & QA,Efficient online testing for DNN-enabled systems using surrogate-assisted and many-objective optimization,"With the recent advances of Deep Neural Networks (DNNs) in real-world applications, such as Automated Driving Systems (ADS) for self-driving cars, ensuring the reliability and safety of such DNN-enabled Systems emerges as a fundamental topic in software testing. One of the essential testing phases of such DNN-enabled systems is online testing, where the system under test is embedded into a specific and often simulated application environment (e.g., a driving environment) and tested in a closed-loop mode in interaction with the environment. However, despite the importance of online testing for detecting safety violations, automatically generating new and diverse test data that lead to safety violations presents the following challenges: (1) there can be many safety requirements to be considered at the same time, (2) running a high-fidelity simulator is often very computationally-intensive, and (3) the space of all possible test data that may trigger safety violations is too large to be exhaustively explored. In this paper, we address the challenges by proposing a novel approach, called SAMOTA (Surrogate-Assisted Many-Objective Testing Approach), extending existing many-objective search algorithms for test suite generation to efficiently utilize surrogate models that mimic the simulator, but are much less expensive to run. Empirical evaluation results on Pylot, an advanced ADS composed of multiple DNNs, using CARLA, a high-fidelity driving simulator, show that SAMOTA is significantly more effective and efficient at detecting unknown safety requirement violations than state-of-the-art many-objective test suite generation algorithms and random search. In other words, SAMOTA appears to be a key enabler technology for online testing in practice.",Power,Social Recognition,SAMOTA's application in improving automation reliability and safety may increase social recognition for the software user; showing that the user is using a technology that ensures safety.,"Based on the abstract of Paper X, SAMOTA, the proposed approach, aims to improve the reliability and safety of DNN-enabled Systems for self-driving cars. By effectively utilizing surrogate models, SAMOTA can detect unknown safety requirement violations more effectively and efficiently. This aligns with the value item of Social Recognition and its corresponding value of Power because having a technology that ensures safety can enhance the software user's reputation and social standing, as they are using a system that prioritizes the well-being and safety of individuals.",Agreed-Justified,Agree,,Agree,,
Exp E,Exp J,890,ICSE,Software Testing & QA,Efficient online testing for DNN-enabled systems using surrogate-assisted and many-objective optimization,"With the recent advances of Deep Neural Networks (DNNs) in real-world applications, such as Automated Driving Systems (ADS) for self-driving cars, ensuring the reliability and safety of such DNN-enabled Systems emerges as a fundamental topic in software testing. One of the essential testing phases of such DNN-enabled systems is online testing, where the system under test is embedded into a specific and often simulated application environment (e.g., a driving environment) and tested in a closed-loop mode in interaction with the environment. However, despite the importance of online testing for detecting safety violations, automatically generating new and diverse test data that lead to safety violations presents the following challenges: (1) there can be many safety requirements to be considered at the same time, (2) running a high-fidelity simulator is often very computationally-intensive, and (3) the space of all possible test data that may trigger safety violations is too large to be exhaustively explored. In this paper, we address the challenges by proposing a novel approach, called SAMOTA (Surrogate-Assisted Many-Objective Testing Approach), extending existing many-objective search algorithms for test suite generation to efficiently utilize surrogate models that mimic the simulator, but are much less expensive to run. Empirical evaluation results on Pylot, an advanced ADS composed of multiple DNNs, using CARLA, a high-fidelity driving simulator, show that SAMOTA is significantly more effective and efficient at detecting unknown safety requirement violations than state-of-the-art many-objective test suite generation algorithms and random search. In other words, SAMOTA appears to be a key enabler technology for online testing in practice.",Achievement,Successful,By improving reliability and safety; SAMOTA indirectly contributes to user success by preventing any hardships or problems that might occur due to software malfunction in real-world applications like self-driving cars.,"The main contribution of 'Paper X' lies in the development of SAMOTA, an approach that improves the reliability and safety of DNN-enabled systems, specifically in the context of self-driving cars. By effectively detecting safety requirement violations, SAMOTA aims to prevent any hardships or problems that users might face due to software malfunction. This aligns with the value item ""Successful"" and its corresponding value ""Achievement"" from a software user perspective, as it directly contributes to the successful use of the software by ensuring its proper functioning and mitigating potential risks or failures.",Agreed-Justified,Agree,,Agree,,
Exp F,Exp J,897,ICSE,AI & Machine Learning,Decomposing convolutional neural networks into reusable and replaceable modules,"Training from scratch is the most common way to build a Convolutional Neural Network (CNN) based model. What if we can build new CNN models by reusing parts from previously built CNN models? What if we can improve a CNN model by replacing (possibly faulty) parts with other parts? In both cases, instead of training, can we identify the part responsible for each output class (module) in the model(s) and reuse or replace only the desired output classes to build a model? Prior work has proposed decomposing dense-based networks into modules (one for each output class) to enable reusability and replace ability in various scenarios. However, this work is limited to the dense layers and is based on the one-to-one relationship between the nodes in consecutive layers. Due to the shared architecture in the CNN model, prior work cannot be adapted directly. In this paper, we propose to decompose a CNN model used for image classification problems into modules for each output class. These modules can further be reused or replaced to build a new model. We have evaluated our approach with CIFAR-10, CIFAR-100, and ImageNet tiny datasets with three variations of ResNet models and found that enabling decomposition comes with a small cost (1.77% and 0.85% for top-1 and top-5 accuracy, respectively). Also, building a model by reusing or replacing modules can be done with a 2.3% and 0.5% average loss of accuracy. Furthermore, reusing and replacing these modules reduces CO2e emission by ~37 times compared to training the model from scratch.",Universalism,Protecting the Environment,"The paper's main contribution of reusing or replacing modules in a CNN model for image classification problems directly reduces CO2e emission by ~37 times compared to training the model from scratch. This aligns with the value item ""Protecting the Environment"" and its corresponding value ""Universalism.""","The main contribution of 'Paper X' aligns with the value item ""Protecting the Environment"" and its corresponding value ""Universalism"" because it directly reduces CO2e emission by ~37 times compared to training the model from scratch. This reduction in CO2e emission signifies a conscious effort to minimize the impact on the environment, which is a central aspect of the value item ""Protecting the Environment."" Universalism reflects the concern for the welfare of all individuals and the broader environment, and by reducing emissions, 'Paper X' demonstrates a commitment to this value. This alignment shows how the paper's contributions address the environmental aspect from a ""Software User"" perspective within a software context.",Agreed-Clarified,Agree,,Agree,,
Exp G,Exp K,897,ICSE,AI & Machine Learning,Decomposing convolutional neural networks into reusable and replaceable modules,"Training from scratch is the most common way to build a Convolutional Neural Network (CNN) based model. What if we can build new CNN models by reusing parts from previously built CNN models? What if we can improve a CNN model by replacing (possibly faulty) parts with other parts? In both cases, instead of training, can we identify the part responsible for each output class (module) in the model(s) and reuse or replace only the desired output classes to build a model? Prior work has proposed decomposing dense-based networks into modules (one for each output class) to enable reusability and replace ability in various scenarios. However, this work is limited to the dense layers and is based on the one-to-one relationship between the nodes in consecutive layers. Due to the shared architecture in the CNN model, prior work cannot be adapted directly. In this paper, we propose to decompose a CNN model used for image classification problems into modules for each output class. These modules can further be reused or replaced to build a new model. We have evaluated our approach with CIFAR-10, CIFAR-100, and ImageNet tiny datasets with three variations of ResNet models and found that enabling decomposition comes with a small cost (1.77% and 0.85% for top-1 and top-5 accuracy, respectively). Also, building a model by reusing or replacing modules can be done with a 2.3% and 0.5% average loss of accuracy. Furthermore, reusing and replacing these modules reduces CO2e emission by ~37 times compared to training the model from scratch.",Security,Healthy,"The paper presents a method that reduces the computational cost and therefore the hardware requirements for building a CNN model; potentially making the technology more accessible and reducing a health risk from device overheat. This aligns with the value ""Health"" nested under the main value ""Security.""","In the context of a ""Software User"" perspective, the alignment of the value item Healthy and its corresponding value Security with the main contributions of 'Paper X' can be justified by the fact that the paper proposes a method that reduces the computational cost and hardware requirements for building a CNN model. By reducing the computational burden and potential device overheat, the paper indirectly addresses the well-being and safety of the software user, thus aligning with the value of Health nested under the main value Security.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,897,ICSE,AI & Machine Learning,Decomposing convolutional neural networks into reusable and replaceable modules,"Training from scratch is the most common way to build a Convolutional Neural Network (CNN) based model. What if we can build new CNN models by reusing parts from previously built CNN models? What if we can improve a CNN model by replacing (possibly faulty) parts with other parts? In both cases, instead of training, can we identify the part responsible for each output class (module) in the model(s) and reuse or replace only the desired output classes to build a model? Prior work has proposed decomposing dense-based networks into modules (one for each output class) to enable reusability and replace ability in various scenarios. However, this work is limited to the dense layers and is based on the one-to-one relationship between the nodes in consecutive layers. Due to the shared architecture in the CNN model, prior work cannot be adapted directly. In this paper, we propose to decompose a CNN model used for image classification problems into modules for each output class. These modules can further be reused or replaced to build a new model. We have evaluated our approach with CIFAR-10, CIFAR-100, and ImageNet tiny datasets with three variations of ResNet models and found that enabling decomposition comes with a small cost (1.77% and 0.85% for top-1 and top-5 accuracy, respectively). Also, building a model by reusing or replacing modules can be done with a 2.3% and 0.5% average loss of accuracy. Furthermore, reusing and replacing these modules reduces CO2e emission by ~37 times compared to training the model from scratch.",Self Direction,Freedom,"The approach proposed by the paper allows end users to replace or reuse certain parts in the machine learning model; thus giving them more control or ""Freedom"" over the formation of the model. This aligns with the value ""Self Direction.""","In the paper, the authors propose a method that enables end users to replace or reuse certain parts of a machine learning model. This empowers the users with the freedom to customize and control the formation of the model according to their specific needs and preferences. By offering this flexibility, the paper aligns with the value item ""Freedom"" and its corresponding value ""Self Direction"" from Schwartz's Taxonomy.",Agreed-Clarified,Agree,,Agree,,
Exp B,Exp H,899,ICSE,AI & Machine Learning,Improving machine translation systems via isotopic replacement,"Machine translation plays an essential role in people's daily international communication. However, machine translation systems are far from perfect. To tackle this problem, researchers have proposed several approaches to testing machine translation. A promising trend among these approaches is to use word replacement, where only one word in the original sentence is replaced with another word to form a sentence pair. However, precise control of the impact of word replacement remains an outstanding issue in these approaches. To address this issue, we propose CAT, a novel word-replacement-based approach, whose basic idea is to identify word replacement with controlled impact (referred to as isotopic replacement). To achieve this purpose, we use a neural-based language model to encode the sentence context, and design a neural-network-based algorithm to evaluate context-aware semantic similarity between two words. Furthermore, similar to TransRepair, a state-of-the-art word-replacement-based approach, CAT also provides automatic fixing of revealed bugs without model retraining. Our evaluation on Google Translate and Transformer indicates that CAT achieves significant improvements over TransRepair. In particular, 1) CAT detects seven more types of bugs than TransRe-pair; 2) CAT detects 129% more translation bugs than TransRepair; 3) CAT repairs twice more bugs than TransRepair, many of which may bring serious consequences if left unfixed; and 4) CAT has better efficiency than TransRepair in input generation (0.01s v.s. 0.41s) and comparable efficiency with TransRepair in bug repair (1.92s v.s. 1.34s).",Achievement,Successful,Paper X is designed to improve on previous strategies in machine translation quality. This aligns with the value item Successful; indicating that the software users powered by this novel approach will aim for success in their communication efforts.,"The justification for aligning 'Paper X' with the value item Successful and its corresponding value Achievement from a ""Software User"" perspective is based on the fact that the paper seeks to improve machine translation quality. By proposing a novel word-replacement-based approach, the paper aims to achieve significant improvements over existing methods. The success of this approach directly impacts software users who rely on machine translation for their international communication needs. With better translation quality, software users can achieve their communication goals more effectively and successfully. Therefore, aligning 'Paper X' with the value item Successful and its corresponding value Achievement is justified by the direct impact of the paper's contributions on the success of software users in their communication efforts.",Agreed-Clarified,Agree,,Agree,,
Exp C,Exp I,899,ICSE,AI & Machine Learning,Improving machine translation systems via isotopic replacement,"Machine translation plays an essential role in people's daily international communication. However, machine translation systems are far from perfect. To tackle this problem, researchers have proposed several approaches to testing machine translation. A promising trend among these approaches is to use word replacement, where only one word in the original sentence is replaced with another word to form a sentence pair. However, precise control of the impact of word replacement remains an outstanding issue in these approaches. To address this issue, we propose CAT, a novel word-replacement-based approach, whose basic idea is to identify word replacement with controlled impact (referred to as isotopic replacement). To achieve this purpose, we use a neural-based language model to encode the sentence context, and design a neural-network-based algorithm to evaluate context-aware semantic similarity between two words. Furthermore, similar to TransRepair, a state-of-the-art word-replacement-based approach, CAT also provides automatic fixing of revealed bugs without model retraining. Our evaluation on Google Translate and Transformer indicates that CAT achieves significant improvements over TransRepair. In particular, 1) CAT detects seven more types of bugs than TransRe-pair; 2) CAT detects 129% more translation bugs than TransRepair; 3) CAT repairs twice more bugs than TransRepair, many of which may bring serious consequences if left unfixed; and 4) CAT has better efficiency than TransRepair in input generation (0.01s v.s. 0.41s) and comparable efficiency with TransRepair in bug repair (1.92s v.s. 1.34s).",Security,Healthy,By improving the performance of machine translation and addressing bugs and inaccuracies; Paper X contributes to this value item by fostering a 'healthy' and efficient communication environment. This directly resonates with the value of Security.,"The justification for aligning 'Paper X' with the value item Healthy and its corresponding value Security from a ""Software User"" perspective is based on the premise that by improving the performance of machine translation and addressing bugs and inaccuracies, 'Paper X' contributes to creating a more secure and reliable communication environment for users. This alignment stems from the understanding that a ""Software User"" values a secure and efficient communication experience, which can be enhanced by ensuring the accuracy and reliability of machine translation systems, ultimately contributing to the overall sense of security and well-being in their software interactions.",Agreed-Clarified,Agree,"if a software works properly, users are not worried and feel relax when using it",Agree,,
Exp D,Exp I,899,ICSE,AI & Machine Learning,Improving machine translation systems via isotopic replacement,"Machine translation plays an essential role in people's daily international communication. However, machine translation systems are far from perfect. To tackle this problem, researchers have proposed several approaches to testing machine translation. A promising trend among these approaches is to use word replacement, where only one word in the original sentence is replaced with another word to form a sentence pair. However, precise control of the impact of word replacement remains an outstanding issue in these approaches. To address this issue, we propose CAT, a novel word-replacement-based approach, whose basic idea is to identify word replacement with controlled impact (referred to as isotopic replacement). To achieve this purpose, we use a neural-based language model to encode the sentence context, and design a neural-network-based algorithm to evaluate context-aware semantic similarity between two words. Furthermore, similar to TransRepair, a state-of-the-art word-replacement-based approach, CAT also provides automatic fixing of revealed bugs without model retraining. Our evaluation on Google Translate and Transformer indicates that CAT achieves significant improvements over TransRepair. In particular, 1) CAT detects seven more types of bugs than TransRe-pair; 2) CAT detects 129% more translation bugs than TransRepair; 3) CAT repairs twice more bugs than TransRepair, many of which may bring serious consequences if left unfixed; and 4) CAT has better efficiency than TransRepair in input generation (0.01s v.s. 0.41s) and comparable efficiency with TransRepair in bug repair (1.92s v.s. 1.34s).",Benevolence,Honesty,Paper X significantly improves the honesty and accuracy of machine translation by detecting and fixing more translation bugs than the previous approach; TransRepair.,"In 'Paper X', the main contribution of CAT, the proposed word-replacement-based approach, is aligned with the value item of Honesty and its corresponding value of Benevolence from a ""Software User"" perspective. This is because CAT aims to improve the honesty and accuracy of machine translation by detecting and fixing more translation bugs than the previous approach, TransRepair. By addressing the issue of word replacement impact in translation, CAT enhances the trustworthiness and reliability of machine translation systems, ultimately benefiting users who rely on accurate and truthful translations for effective communication.",Agreed-Clarified,Agree,,Agree,,
Exp E,Exp J,906,ICSE,Security & Privacy,Less is more: supporting developers in vulnerability detection during code review,"Reviewing source code from a security perspective has proven to be a difficult task. Indeed, previous research has shown that developers often miss even popular and easy-to-detect vulnerabilities during code review. Initial evidence suggests that a significant cause may lie in the reviewers' mental attitude and common practices. In this study, we investigate whether and how explicitly asking developers to focus on security during a code review affects the detection of vulnerabilities. Furthermore, we evaluate the effect of providing a security checklist to guide the security review. To this aim, we conduct an online experiment with 150 participants, of which 71% report to have three or more years of professional development experience. Our results show that simply asking reviewers to focus on security during the code review increases eight times the probability of vulnerability detection. The presence of a security checklist does not significantly improve the outcome further, even when the checklist is tailored to the change under review and the existing vulnerabilities in the change. These results provide evidence supporting the mental attitude hypothesis and call for further work on security checklists' effectiveness and design. Preprint: https://arxiv.org/abs/2202.04586 Data and materials: https://doi.org/10.5281/zenodo.6026291",Achievement,Successful,"The paper explores methods that increase the chance of detecting vulnerabilities during code reviews; thus aligning with the user value item ""Successful"" under the user value ""Achievement""","My justification for labeling 'Paper X' as aligning with the value item Successful and its corresponding value Achievement from a ""Software User"" perspective is based on the fact that the paper investigates methods to improve the detection of vulnerabilities during code reviews. By addressing this issue, the paper aims to enhance the security of software, thereby achieving a successful outcome for software users who value the achievement of secure and reliable software systems. The focus on vulnerability detection and security aligns with the user value item ""Successful"" under the user value ""Achievement"" as it strives for the successful achievement of secure software.",Agreed-Clarified,Agree,,Agree,,
Exp F,Exp J,906,ICSE,Security & Privacy,Less is more: supporting developers in vulnerability detection during code review,"Reviewing source code from a security perspective has proven to be a difficult task. Indeed, previous research has shown that developers often miss even popular and easy-to-detect vulnerabilities during code review. Initial evidence suggests that a significant cause may lie in the reviewers' mental attitude and common practices. In this study, we investigate whether and how explicitly asking developers to focus on security during a code review affects the detection of vulnerabilities. Furthermore, we evaluate the effect of providing a security checklist to guide the security review. To this aim, we conduct an online experiment with 150 participants, of which 71% report to have three or more years of professional development experience. Our results show that simply asking reviewers to focus on security during the code review increases eight times the probability of vulnerability detection. The presence of a security checklist does not significantly improve the outcome further, even when the checklist is tailored to the change under review and the existing vulnerabilities in the change. These results provide evidence supporting the mental attitude hypothesis and call for further work on security checklists' effectiveness and design. Preprint: https://arxiv.org/abs/2202.04586 Data and materials: https://doi.org/10.5281/zenodo.6026291",Security,Healthy,"By identifying security vulnerabilities; the paper's contributions directly relate to maintaining a ""Healthy"" software environment; a value item under ""Security"".","In the context of 'Paper X', the alignment with the value item Healthy and its corresponding value Security is justified by the paper's focus on detecting vulnerabilities during code review. By identifying and addressing these vulnerabilities, the paper's contributions directly contribute to maintaining a secure software environment, which is an essential aspect of ensuring the overall health and well-being of software users.",Agreed-Clarified,Agree,,Agree,,
Exp G,Exp K,906,ICSE,Security & Privacy,Less is more: supporting developers in vulnerability detection during code review,"Reviewing source code from a security perspective has proven to be a difficult task. Indeed, previous research has shown that developers often miss even popular and easy-to-detect vulnerabilities during code review. Initial evidence suggests that a significant cause may lie in the reviewers' mental attitude and common practices. In this study, we investigate whether and how explicitly asking developers to focus on security during a code review affects the detection of vulnerabilities. Furthermore, we evaluate the effect of providing a security checklist to guide the security review. To this aim, we conduct an online experiment with 150 participants, of which 71% report to have three or more years of professional development experience. Our results show that simply asking reviewers to focus on security during the code review increases eight times the probability of vulnerability detection. The presence of a security checklist does not significantly improve the outcome further, even when the checklist is tailored to the change under review and the existing vulnerabilities in the change. These results provide evidence supporting the mental attitude hypothesis and call for further work on security checklists' effectiveness and design. Preprint: https://arxiv.org/abs/2202.04586 Data and materials: https://doi.org/10.5281/zenodo.6026291",Security,National Security,"The initiative discussed; improves security measures in code review with potential implications on software's ""National Security""; aligning with the same value item under ""Security"".","My justification for aligning 'Paper X' with the value item National Security and its corresponding value Security from a ""Software User"" perspective is based on the fact that the paper focuses on improving code review practices to enhance vulnerability detection. By enhancing security measures in code review, the paper indirectly contributes to the overall security of software systems. National Security can be seen as a broader concept encompassing the security of critical systems and infrastructure, and any improvement in software security can ultimately have implications for national security as well. Therefore, the alignment between 'Paper X' and the value item National Security is justified by the potential impact of the paper's contributions on the overall security landscape.",Agreed-Justified,Agree,,Agree,,
Exp C,Exp I,909,ICSE,AI & Machine Learning,Neuronfair: Interpretable white-box fairness testing through biased neuron identification,"Deep neural networks (DNNs) have demonstrated their outper-formance in various domains. However, it raises a social concern whether DNNs can produce reliable and fair decisions especially when they are applied to sensitive domains involving valuable re-source allocation, such as education, loan, and employment. It is crucial to conduct fairness testing before DNNs are reliably de-ployed to such sensitive domains, i.e., generating as many instances as possible to uncover fairness violations. However, the existing testing methods are still limited from three aspects: interpretabil-ity, performance, and generalizability. To overcome the challenges, we propose NeuronFair, a new DNN fairness testing framework that differs from previous work in several key aspects: (1) inter-pretable - it quantitatively interprets DNNs' fairness violations for the biased decision; (2) effective - it uses the interpretation results to guide the generation of more diverse instances in less time; (3) generic - it can handle both structured and unstructured data. Extensive evaluations across 7 datasets and the corresponding DNNs demonstrate NeuronFair's superior performance. For instance, on structured datasets, it generates much more instances (~ ‚Äö√†√∂‚àö‚â•5.84) and saves more time (with an average speedup of 534.56%) compared with the state-of-the-art methods. Besides, the instances of NeuronFair can also be leveraged to improve the fairness of the biased DNNs, which helps build more fair and trustworthy deep learning systems. The code of NeuronFair is open-sourced at https:/github.com/haibinzheng/NeuronFair.",Security,Social Order,The use of the developed framework in sensitive domains to secure reliable and fair decision-making aligns with the value item 'Social Order' under the value of 'Security'.,"The main contributions of 'Paper X' directly align with the value item 'Social Order' and its corresponding value 'Security' from a ""Software User"" perspective. The development of the NeuronFair framework aims to address fairness and reliability concerns in sensitive domains involving resource allocation, such as education, loan, and employment. By conducting fairness testing and uncovering fairness violations, the framework promotes social order by ensuring that decision-making processes in these domains are secure, unbiased, and aligned with principles of fairness and equality.",Agreed-Clarified,Agree,,Agree,,
Exp D,Exp I,915,ICSE,Software Project Management,GitHub sponsors: exploring a new way to contribute to open source,"GitHub Sponsors, launched in 2019, enables donations to individual open source software (OSS) developers. Financial support for OSS maintainers and developers is a major issue in terms of sustaining OSS projects, and the ability to donate to individuals is expected to support the sustainability of developers, projects, and community. In this work, we conducted a mixed-methods study of GitHub Sponsors, including quantitative and qualitative analyses, to understand the characteristics of developers who are likely to receive donations and what developers think about donations to individuals. We found that: (1) sponsored developers are more active than non-sponsored developers, (2) the possibility to receive donations is related to whether there is someone in their community who is donating, and (3) developers are sponsoring as a new way to contribute to OSS. Our findings are the first step towards data-informed guidance for using GitHub Sponsors, opening up avenues for future work on this new way of financially sustaining the OSS community.",Power,Social Recognition,In 'Paper X'; the sponsors program gives recognition to individual developers who are doing impactful work; aligning with the value item of Social Recognition and its corresponding value of Power.,"In 'Paper X', the GitHub Sponsors program provides a platform for individual developers to receive financial support and recognition for their impactful work in open source software. By enabling donations to individual developers, it acknowledges their contributions and gives them visibility within the software community. This aligns with the value item of Social Recognition from Schwartz's Taxonomy, as it involves being acknowledged and appreciated by others for one's skills and accomplishments. Furthermore, the act of sponsoring in this context can be seen as a demonstration of power, as it allows software users to exert influence and support the developers that they value. Therefore, the alignment of 'Paper X' with the value item Social Recognition and its corresponding value Power is evident in the recognition and support that the GitHub Sponsors program provides to individual developers.",Agreed-Justified,Agree,,Agree,,
Exp E,Exp J,915,ICSE,Software Project Management,GitHub sponsors: exploring a new way to contribute to open source,"GitHub Sponsors, launched in 2019, enables donations to individual open source software (OSS) developers. Financial support for OSS maintainers and developers is a major issue in terms of sustaining OSS projects, and the ability to donate to individuals is expected to support the sustainability of developers, projects, and community. In this work, we conducted a mixed-methods study of GitHub Sponsors, including quantitative and qualitative analyses, to understand the characteristics of developers who are likely to receive donations and what developers think about donations to individuals. We found that: (1) sponsored developers are more active than non-sponsored developers, (2) the possibility to receive donations is related to whether there is someone in their community who is donating, and (3) developers are sponsoring as a new way to contribute to OSS. Our findings are the first step towards data-informed guidance for using GitHub Sponsors, opening up avenues for future work on this new way of financially sustaining the OSS community.",Security,Reciprocation of Favors,The provision for OSS developers to financially support others in the community is like reciprocating favors; which directly aligns with the value item Reciprocation of Favors and the corresponding value of Security.,"In 'Paper X', the provision for OSS developers to financially support others in the community resembles a reciprocal relationship, where developers contribute to the sustainability of the community by donating and, in return, may potentially receive support from others in the future. This reciprocal exchange aligns with the value item Reciprocation of Favors and the corresponding value of Security, as it ensures a sense of security and support within the software community.",Agreed-Clarified,Agree,,Agree,,
Exp F,Exp J,915,ICSE,Software Project Management,GitHub sponsors: exploring a new way to contribute to open source,"GitHub Sponsors, launched in 2019, enables donations to individual open source software (OSS) developers. Financial support for OSS maintainers and developers is a major issue in terms of sustaining OSS projects, and the ability to donate to individuals is expected to support the sustainability of developers, projects, and community. In this work, we conducted a mixed-methods study of GitHub Sponsors, including quantitative and qualitative analyses, to understand the characteristics of developers who are likely to receive donations and what developers think about donations to individuals. We found that: (1) sponsored developers are more active than non-sponsored developers, (2) the possibility to receive donations is related to whether there is someone in their community who is donating, and (3) developers are sponsoring as a new way to contribute to OSS. Our findings are the first step towards data-informed guidance for using GitHub Sponsors, opening up avenues for future work on this new way of financially sustaining the OSS community.",Benevolence,Helpful,The act of developers donating to help sustain the OSS community aligns with being Helpful under the value of Benevolence.,"The act of developers donating to help sustain the OSS community aligns with being Helpful under the value of Benevolence because it demonstrates a selfless and caring attitude towards supporting and benefiting others in the software community. By contributing financially to individual developers, the donors exhibit a willingness to assist and promote the growth and longevity of OSS projects, ultimately benefiting users like myself who rely on these software solutions.",Agreed-Justified,Agree,,Agree,,
Exp G,Exp K,917,ICSE,Software Testing & QA,GIFdroid: automated replay of visual bug reports for Android apps,"Bug reports are vital for software maintenance that allow users to inform developers of the problems encountered while using software. However, it is difficult for non-technical users to write clear descriptions about the bug occurrence. Therefore, more and more users begin to record the screen for reporting bugs as it is easy to be created and contains detailed procedures triggering the bug. But it is still tedious and time-consuming for developers to reproduce the bug due to the length and unclear actions within the recording. To overcome these issues, we propose GIFdroid, a light-weight approach to automatically replay the execution trace from visual bug reports. GIFdroid adopts image processing techniques to extract the keyframes from the recording, map them to states in GUI Transitions Graph, and generate the execution trace of those states to trigger the bug. Our automated experiments and user study demonstrate its accuracy, efficiency, and usefulness of the approach.",Stimulation,Variation in Life,The paper contributes to Variation in Life by developing a tool (GIFdroid) that automatically replays the execution trace from bug reports; thus simplifying and enhancing the user's experience with software bug reporting.,"The paper's contribution to Variation in Life is aligned with the value of Stimulation because it introduces a tool (GIFdroid) that enhances the user's experience with software bug reporting. By automatically replaying the execution trace from bug reports, GIFdroid simplifies and streamlines the process for users, making it more engaging and stimulating. This aligns with the value of Stimulation as it provides users with a more dynamic and interactive way to report bugs, increasing their engagement and enjoyment in the software usage process.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,917,ICSE,Software Testing & QA,GIFdroid: automated replay of visual bug reports for Android apps,"Bug reports are vital for software maintenance that allow users to inform developers of the problems encountered while using software. However, it is difficult for non-technical users to write clear descriptions about the bug occurrence. Therefore, more and more users begin to record the screen for reporting bugs as it is easy to be created and contains detailed procedures triggering the bug. But it is still tedious and time-consuming for developers to reproduce the bug due to the length and unclear actions within the recording. To overcome these issues, we propose GIFdroid, a light-weight approach to automatically replay the execution trace from visual bug reports. GIFdroid adopts image processing techniques to extract the keyframes from the recording, map them to states in GUI Transitions Graph, and generate the execution trace of those states to trigger the bug. Our automated experiments and user study demonstrate its accuracy, efficiency, and usefulness of the approach.",Stimulation,Excitement in Life,By developing GIFdroid; the paper introduces an innovative tool that makes the bug reporting process more efficient and less tedious; thereby bringing Excitement in Life.,"In developing GIFdroid, the paper presents a solution to the tedious and time-consuming process of bug reporting by automatically replaying the execution trace from visual bug reports. This innovative tool improves the efficiency of bug reporting for software users, allowing them to quickly and accurately provide detailed information about the bugs they encounter. By simplifying and streamlining this process, GIFdroid brings excitement to the lives of software users, as they can now actively participate in improving the software they use, leading to a more engaging and dynamic software experience.",Agreed-Clarified,Agree,,Agree,,
Exp B,Exp H,917,ICSE,Software Testing & QA,GIFdroid: automated replay of visual bug reports for Android apps,"Bug reports are vital for software maintenance that allow users to inform developers of the problems encountered while using software. However, it is difficult for non-technical users to write clear descriptions about the bug occurrence. Therefore, more and more users begin to record the screen for reporting bugs as it is easy to be created and contains detailed procedures triggering the bug. But it is still tedious and time-consuming for developers to reproduce the bug due to the length and unclear actions within the recording. To overcome these issues, we propose GIFdroid, a light-weight approach to automatically replay the execution trace from visual bug reports. GIFdroid adopts image processing techniques to extract the keyframes from the recording, map them to states in GUI Transitions Graph, and generate the execution trace of those states to trigger the bug. Our automated experiments and user study demonstrate its accuracy, efficiency, and usefulness of the approach.",Achievement,Intelligent,The paper contributes to making users more Intelligent by proposing a new approach to understanding and reporting bugs; without needing technical in-depth knowledge.,"In the context of a ""Software User,"" the main contribution of 'Paper X' aligns with the value item Intelligent and its corresponding value Achievement because it proposes a new approach, GIFdroid, that enhances the users' ability to effectively report bugs without requiring extensive technical knowledge. By automatically replaying the execution trace from visual bug reports, GIFdroid simplifies and streamlines the bug reproduction process for developers. This approach increases the users' intelligence in bug reporting by empowering them to provide clear and detailed descriptions of the bug occurrence. Consequently, developers can more efficiently address and resolve these issues, leading to improved software performance and user experience, thus aligning with the value of achievement in providing a successful and capable software ecosystem.",Agreed-Justified,Disagree,reconcile with coder_2,Disagree,"The justification claiming that the paper aligns with the value 'Achievement' because of 'The paper contributes to making users more Intelligent by proposing a new approach to understanding and reporting bugs; without needing technical in-depth knowledge. ' is not supported by the evidence in the abstract, which focuses on 'Bug reports are vital for software maintenance that allow users to inform developers of the problems.",
Exp C,Exp I,921,ICSE,Security & Privacy,Automated detection of password leakage from public github repositories,"The prosperity of the GitHub community has raised new concerns about data security in public repositories. Practitioners who manage authentication secrets such as textual passwords and API keys in the source code may accidentally leave these texts in the public repositories, resulting in secret leakage. If such leakage in the source code can be automatically detected in time, potential damage would be avoided. With existing approaches focusing on detecting secrets with distinctive formats (e.g., API keys, cryptographic keys in PEM format), textual passwords, which are ubiquitously used for authentication, fall through the crack. Given that textual passwords could be virtually any strings, a naive detection scheme based on regular expression performs poorly. This paper presents PassFinder, an automated approach to effectively detecting password leakage from public repositories that involve various programming languages on a large scale. PassFinder utilizes deep neural networks to unveil the intrinsic characteristics of textual passwords and understand the semantics of the code snippets that use textual passwords for authentication, i.e., the contextual information of the passwords in the source code. Using this new technique, we performed the first large-scale and longitudinal analysis of password leakage on GitHub. We inspected newly uploaded public code files on GitHub for 75 days and found that password leakage is pervasive, affecting over sixty thousand repositories. Our work contributes to a better understanding of password leakage on GitHub, and we believe our technique could promote the security of the open-source ecosystem.",Security,Healthy,PassFinder; the tool described in the paper would help in automatically detecting password leakage issues such as accidental secret/key leaks in public repositories; ultimately contributing to the better health and security of the platform.,"PassFinder, as described in 'Paper X', directly aligns with the value item of Security and its corresponding value of Healthy from a ""Software User"" perspective. PassFinder helps in automatically detecting password leakage issues, which can contribute to the overall security and healthy functioning of the GitHub platform. By identifying and addressing potential secret and key leaks, PassFinder enhances the security of public repositories, ensuring that sensitive information remains protected. This directly aligns with the value of Security, as it reduces the risk of data breaches and unauthorized access. Additionally, by preventing the accidental leakage of authentication secrets, PassFinder contributes to the healthy functioning of the platform by promoting trust and reliability among users.",Agreed-Justified,Agree,,Agree,,
Exp E,Exp J,921,ICSE,Security & Privacy,Automated detection of password leakage from public github repositories,"The prosperity of the GitHub community has raised new concerns about data security in public repositories. Practitioners who manage authentication secrets such as textual passwords and API keys in the source code may accidentally leave these texts in the public repositories, resulting in secret leakage. If such leakage in the source code can be automatically detected in time, potential damage would be avoided. With existing approaches focusing on detecting secrets with distinctive formats (e.g., API keys, cryptographic keys in PEM format), textual passwords, which are ubiquitously used for authentication, fall through the crack. Given that textual passwords could be virtually any strings, a naive detection scheme based on regular expression performs poorly. This paper presents PassFinder, an automated approach to effectively detecting password leakage from public repositories that involve various programming languages on a large scale. PassFinder utilizes deep neural networks to unveil the intrinsic characteristics of textual passwords and understand the semantics of the code snippets that use textual passwords for authentication, i.e., the contextual information of the passwords in the source code. Using this new technique, we performed the first large-scale and longitudinal analysis of password leakage on GitHub. We inspected newly uploaded public code files on GitHub for 75 days and found that password leakage is pervasive, affecting over sixty thousand repositories. Our work contributes to a better understanding of password leakage on GitHub, and we believe our technique could promote the security of the open-source ecosystem.",Power,Social Recognition,By introducing the novel system of PassFinder; the paper aims to improve the public image of the open-source ecosystem as a safer and more secure environment for users.,"My justification is based on the fact that the PassFinder system, introduced in 'Paper X', aims to effectively detect password leakage in public repositories on GitHub. By preventing such leakage and promoting a higher level of data security, the paper contributes to improving the public image of the open-source ecosystem as a safer and more secure environment for software users. This aligns with the value item of Social Recognition, as it involves recognition and acceptance from others, and the corresponding value of Power, as it pertains to the ability to influence and make a positive impact in the software community.",Agreed-Clarified,Agree,,Agree,,
Exp F,Exp J,922,ICSE,Security & Privacy,Difuzer: Uncovering suspicious hidden sensitive operations in android apps,"One prominent tactic used to keep malicious behavior from being detected during dynamic test campaigns is logic bombs, where malicious operations are triggered only when specific conditions are satisfied. Defusing logic bombs remains an unsolved problem in the literature. In this work, we propose to investigate Suspicious Hidden Sensitive Operations (SHSOs) as a step towards triaging logic bombs. To that end, we develop a novel hybrid approach that combines static analysis and anomaly detection techniques to un-cover SHSOs, which we predict as likely implementations of logic bombs. Concretely, Difuzer identifies SHSO entry-points using an instrumentation engine and an inter-procedural data-flow analysis. Then, it extracts trigger-specific features to characterize SHSOs and leverages One-Class SVM to implement an unsupervised learning model for detecting abnormal triggers. We evaluate our prototype and show that it yields a precision of 99.02% to detect SHSOs among which 29.7% are logic bombs. Difuzer outperforms the state-of-the-art in revealing more logic bombs while yielding less false positives in about one order of magnitude less time. All our artifacts are released to the community.",Security,Healthy,The paper's contribution to an efficient detection of malicious operations like logic bombs undoubtedly contributes to the 'Health' of a 'Software User's' device maintaining its functionality and safety from potential harm.,"The paper's focus on detecting and defusing malicious logic bombs directly aligns with the value item of Healthy and its corresponding value of Security from a ""Software User"" perspective. By effectively identifying and mitigating these threats, the paper ensures that the user's device remains safe and functional, promoting their overall security and well-being.",Agreed-Clarified,Agree,,Agree,,
Exp G,Exp K,922,ICSE,Security & Privacy,Difuzer: Uncovering suspicious hidden sensitive operations in android apps,"One prominent tactic used to keep malicious behavior from being detected during dynamic test campaigns is logic bombs, where malicious operations are triggered only when specific conditions are satisfied. Defusing logic bombs remains an unsolved problem in the literature. In this work, we propose to investigate Suspicious Hidden Sensitive Operations (SHSOs) as a step towards triaging logic bombs. To that end, we develop a novel hybrid approach that combines static analysis and anomaly detection techniques to un-cover SHSOs, which we predict as likely implementations of logic bombs. Concretely, Difuzer identifies SHSO entry-points using an instrumentation engine and an inter-procedural data-flow analysis. Then, it extracts trigger-specific features to characterize SHSOs and leverages One-Class SVM to implement an unsupervised learning model for detecting abnormal triggers. We evaluate our prototype and show that it yields a precision of 99.02% to detect SHSOs among which 29.7% are logic bombs. Difuzer outperforms the state-of-the-art in revealing more logic bombs while yielding less false positives in about one order of magnitude less time. All our artifacts are released to the community.",Security,Social Order,By proposing a tactic for identifying and defusing logic bombs; this paper contributes to maintaining 'social order' in the digital space; as this can help ensure safety and integrity of software applications which plays a part in overall social harmony.,"By proposing a method to identify and defuse logic bombs, 'Paper X' contributes to maintaining 'social order' by ensuring the safety and integrity of software applications. Logic bombs, which are malicious operations triggered under specific conditions, can cause significant harm to the users and the overall digital space. By detecting these suspicious hidden sensitive operations and preventing them from being executed, software users can rely on the security and stability of the software they use. This enhances social harmony by minimizing potential disruptions and threats in the digital environment that could negatively impact individuals and society as a whole.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,922,ICSE,Security & Privacy,Difuzer: Uncovering suspicious hidden sensitive operations in android apps,"One prominent tactic used to keep malicious behavior from being detected during dynamic test campaigns is logic bombs, where malicious operations are triggered only when specific conditions are satisfied. Defusing logic bombs remains an unsolved problem in the literature. In this work, we propose to investigate Suspicious Hidden Sensitive Operations (SHSOs) as a step towards triaging logic bombs. To that end, we develop a novel hybrid approach that combines static analysis and anomaly detection techniques to un-cover SHSOs, which we predict as likely implementations of logic bombs. Concretely, Difuzer identifies SHSO entry-points using an instrumentation engine and an inter-procedural data-flow analysis. Then, it extracts trigger-specific features to characterize SHSOs and leverages One-Class SVM to implement an unsupervised learning model for detecting abnormal triggers. We evaluate our prototype and show that it yields a precision of 99.02% to detect SHSOs among which 29.7% are logic bombs. Difuzer outperforms the state-of-the-art in revealing more logic bombs while yielding less false positives in about one order of magnitude less time. All our artifacts are released to the community.",Security,National Security,Defusing logic bombs contributes to 'National security'. While the abstract does not explicitly outline this; malicious interventions leading to data breaches or other security issues can potentially reach a national level; particularly if they target integral systems or critical infrastructure. This paper; hence; indirectly supports national security through better handling of security threats in software.,"Sure, National Security falls under the value item Security in Schwartz's Taxonomy. While the abstract of 'Paper X' does not explicitly mention National Security, it does highlight the issue of logic bombs, which are malicious operations that can go undetected during dynamic test campaigns. By proposing a hybrid approach to uncover Suspicious Hidden Sensitive Operations (SHSOs) and detect logic bombs, the paper contributes to enhancing the security of software systems. This, in turn, indirectly supports national security by mitigating the risk of security threats that could potentially have significant consequences at a national level, such as data breaches or attacks on critical infrastructure.",Agreed-Justified,Disagree,reconcile with coder_2,Disagree,The justification claiming that the paper aligns with the value 'Security' because of 'Defusing logic bombs contributes to 'National security'.  While the abstract does not explicitly outline this; malicious interventions leading to data breaches or other security issues can potentially reach a national level; particularly if they target integral systems or critical infrastructure.,
Exp B,Exp H,931,ICSE,Security & Privacy,What the fork? finding hidden code clones in npm,"This work presents findings and mitigations on an under-studied issue, which we term shrinkwrapped clones, that is endemic to the npm software package ecosystem. A shrink-wrapped clone is a package which duplicates, or near-duplicates, the code of another package without any indication or refer-ence to the original package. This phenomenon represents a challenge to the hygiene of package ecosystems, as a clone package may siphon interest from the package being cloned, or create hidden duplicates of vulnerable, insecure code which can fly under the radar of audit processes. Motivated by these considerations, we propose UNWRAP-PER, a mechanism to programmatically detect shrinkwrapped clones and match them to their source package. UNWRAP-PER uses a package difference metric based on directory tree similarity, augmented with a prefilter which quickly weeds out packages unlikely to be clones of a target. Overall, our prototype can compare a given package within the entire npm ecosystem (1,716,061 packages with 20,190,452 differ-ent versions) in 72.85 seconds, and it is thus practical for live deployment. Using our tool, we performed an analysis of a subset of npm packages, which resulted in finding up to 6,292 previously unknown shrinkwrapped clones, of which up to 207 carried vulnerabilities from the original package that had already been fixed in the original package. None of such vulnerabilities were discoverable via the standard npm audit process.",Security,Healthy,The paper's contribution involves identifying vulnerabilities within existing software by detecting 'shrinkwrapped clones' which unknowingly harbor such insecurities. This promotes the enhancement of software user health in a digital context.,"The justification for aligning 'Paper X' with the value item Healthy from a ""Software User"" perspective is that by identifying and mitigating vulnerabilities within software, the paper contributes to ensuring the health and well-being of software users in a digital context. It aims to enhance the security and safety of users by detecting and addressing hidden duplicates of insecure code that may go unnoticed by standard audit processes, thereby reducing potential risks and threats associated with using software.",Agreed-Justified,Agree,,Agree,,
Exp D,Exp I,931,ICSE,Security & Privacy,What the fork? finding hidden code clones in npm,"This work presents findings and mitigations on an under-studied issue, which we term shrinkwrapped clones, that is endemic to the npm software package ecosystem. A shrink-wrapped clone is a package which duplicates, or near-duplicates, the code of another package without any indication or refer-ence to the original package. This phenomenon represents a challenge to the hygiene of package ecosystems, as a clone package may siphon interest from the package being cloned, or create hidden duplicates of vulnerable, insecure code which can fly under the radar of audit processes. Motivated by these considerations, we propose UNWRAP-PER, a mechanism to programmatically detect shrinkwrapped clones and match them to their source package. UNWRAP-PER uses a package difference metric based on directory tree similarity, augmented with a prefilter which quickly weeds out packages unlikely to be clones of a target. Overall, our prototype can compare a given package within the entire npm ecosystem (1,716,061 packages with 20,190,452 differ-ent versions) in 72.85 seconds, and it is thus practical for live deployment. Using our tool, we performed an analysis of a subset of npm packages, which resulted in finding up to 6,292 previously unknown shrinkwrapped clones, of which up to 207 carried vulnerabilities from the original package that had already been fixed in the original package. None of such vulnerabilities were discoverable via the standard npm audit process.",Stimulation,Daring,By daring to explore the under-studied issue of 'shrinkwrapped clones' and proposing a mechanism to tackle this; the paper contributes to enhancing the stimulation for software users by pushing boundaries in the field.,"By addressing the issue of 'shrinkwrapped clones', the paper demonstrates a sense of daring as it explores an under-studied problem in the npm software package ecosystem. This daring approach contributes to the stimulation for software users by pushing boundaries in the field and providing innovative solutions to enhance the hygiene and security of package ecosystems.",Agreed-Clarified,Agree,,Agree,,
Exp E,Exp J,933,ICSE,AI & Machine Learning,FairNeuron: improving deep neural network fairness with adversary games on selective neurons,"With Deep Neural Network (DNN) being integrated into a growing number of critical systems with far-reaching impacts on society, there are increasing concerns on their ethical performance, such as fairness. Unfortunately, model fairness and accuracy in many cases are contradictory goals to optimize during model training. To solve this issue, there has been a number of works trying to improve model fairness by formalizing an adversarial game in the model level. This approach introduces an adversary that evaluates the fairness of a model besides its prediction accuracy on the main task, and performs joint-optimization to achieve a balanced result. In this paper, we noticed that when performing backward prop-agation based training, such contradictory phenomenon are also observable on individual neuron level. Based on this observation, we propose Fairneuron, a Dnn model automatic repairing tool, to mitigate fairness concerns and balance the accuracy-fairness trade-off without introducing another model. It works on detecting neurons with contradictory optimization directions from accuracy and fairness training goals, and achieving a trade-off by selective dropout. Comparing with state-of-the-art methods, our approach is lightweight, scaling to large models and more efficient. Our eval-uation on three datasets shows that Fairneuron can effectively improve all models' fairness while maintaining a stable utility.",Hedonism,Enjoying Life,The Fairneuron tool in 'Paper X' is described to effectively improve the fairness of all models while maintaining utility; potentially contributing to the software user's enjoyment in using the software application. This can align with the value item of Enjoying Life and the corresponding Hedonism value.,"In 'Paper X', the Fairneuron tool is specifically designed to improve fairness in models while maintaining utility. This implies that the software user will have a more enjoyable experience using the software application as it addresses concerns regarding fairness, ensuring a balanced and equitable outcome. This aligns with the value item of Enjoying Life and the corresponding Hedonism value, as it directly contributes to the user's satisfaction and pleasure in using the software.",Agreed-Justified,Agree,,Agree,,
Exp F,Exp J,933,ICSE,AI & Machine Learning,FairNeuron: improving deep neural network fairness with adversary games on selective neurons,"With Deep Neural Network (DNN) being integrated into a growing number of critical systems with far-reaching impacts on society, there are increasing concerns on their ethical performance, such as fairness. Unfortunately, model fairness and accuracy in many cases are contradictory goals to optimize during model training. To solve this issue, there has been a number of works trying to improve model fairness by formalizing an adversarial game in the model level. This approach introduces an adversary that evaluates the fairness of a model besides its prediction accuracy on the main task, and performs joint-optimization to achieve a balanced result. In this paper, we noticed that when performing backward prop-agation based training, such contradictory phenomenon are also observable on individual neuron level. Based on this observation, we propose Fairneuron, a Dnn model automatic repairing tool, to mitigate fairness concerns and balance the accuracy-fairness trade-off without introducing another model. It works on detecting neurons with contradictory optimization directions from accuracy and fairness training goals, and achieving a trade-off by selective dropout. Comparing with state-of-the-art methods, our approach is lightweight, scaling to large models and more efficient. Our eval-uation on three datasets shows that Fairneuron can effectively improve all models' fairness while maintaining a stable utility.",Universalism,Equality,The main goal of the proposed Fairneuron tool in 'Paper X' is to improve model fairness in DNNs. This aligns with the value item; Equality; and its corresponding value; Universalism; as fairness in software can be seen as promoting equality among software users.,"The proposed Fairneuron tool in 'Paper X' aims to improve model fairness in DNNs by detecting and repairing neurons with contradictory optimization directions. This promotes equality among software users by ensuring that the model's predictions are not biased towards certain groups. Therefore, aligning with the value item Equality and its corresponding value Universalism.",Agreed-Clarified,Agree,,Agree,,
Exp G,Exp K,933,ICSE,AI & Machine Learning,FairNeuron: improving deep neural network fairness with adversary games on selective neurons,"With Deep Neural Network (DNN) being integrated into a growing number of critical systems with far-reaching impacts on society, there are increasing concerns on their ethical performance, such as fairness. Unfortunately, model fairness and accuracy in many cases are contradictory goals to optimize during model training. To solve this issue, there has been a number of works trying to improve model fairness by formalizing an adversarial game in the model level. This approach introduces an adversary that evaluates the fairness of a model besides its prediction accuracy on the main task, and performs joint-optimization to achieve a balanced result. In this paper, we noticed that when performing backward prop-agation based training, such contradictory phenomenon are also observable on individual neuron level. Based on this observation, we propose Fairneuron, a Dnn model automatic repairing tool, to mitigate fairness concerns and balance the accuracy-fairness trade-off without introducing another model. It works on detecting neurons with contradictory optimization directions from accuracy and fairness training goals, and achieving a trade-off by selective dropout. Comparing with state-of-the-art methods, our approach is lightweight, scaling to large models and more efficient. Our eval-uation on three datasets shows that Fairneuron can effectively improve all models' fairness while maintaining a stable utility.",Universalism,Social Justice,The Fairneuron tool's emphasis on fairness links to Social Justice from Universalism value in Schwartz's taxonomy; as it strives to correct potential unfair practices in algorithmic outcomes.,"Fairneuron, as described in 'Paper X', directly aligns with the value item of Social Justice and its corresponding value of Universalism from Schwartz's Taxonomy. The paper focuses on addressing fairness concerns in deep neural network models by detecting and addressing neurons with contradictory optimization directions. By striving to correct potential unfair practices in algorithmic outcomes, Fairneuron aligns with the value of social justice, which is concerned with promoting equality and fairness in society.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,940,ICSE,Software Testing & QA,Multi-intention-aware configuration selection for performance tuning,"Automatic configuration tuning helps users who intend to improve software performance. However, the auto-tuners are limited by the huge configuration search space. More importantly, they fo-cus only on performance improvement while being unaware of other important user intentions (e.g., reliability, security). To re-duce the search space, researchers mainly focus on pre-selecting performance-related parameters which requires a heavy stage of dynamically running under different configurations to build per-formance models. Given that other important user intentions are not paid attention to, we focus on guiding users in pre-selecting performance-related parameters in general while warning about side-effects on non-performance intentions. We find that the con-figuration document often, if it does not always, contains rich in-formation about the parameters' relationship with diverse user intentions, but documents might also be long and domain-specific. In this paper, we first conduct a comprehensive study on 13 representative software containing 7,349 configuration parame-ters, and derive six types of ways in which configuration parame-ters may affect non-performance intentions. Guided by this study, we design SAFETUNE, a multi-intention-aware method that pre-selects important performance-related parameters and warns about their side-effects on non-performance intentions. Evaluation on target software shows that SAFETUNE correctly identifies 22‚Äö√Ñ√∂‚àö√ë‚àö‚àÇ‚Äö√†√∂‚àö√´‚Äö√†√∂‚Äö√†√á‚Äö√Ñ√∂‚àö‚Ä†‚àö‚àÇ‚Äö√†√∂‚àö¬¥‚Äö√Ñ√∂‚àö‚Ä†‚àö‚àÇ¬¨¬®¬¨√Ü26 performance-related parameters that are missed by state-of-the-art tools but have significant performance impact (up to 14.7x). Furthermore, we illustrate eight representative cases to show that SAFETUNE can effectively prevent real-world and critical side-effects on other user intentions.",Achievement,Successful,The paper develops SAFETUNE; a tool which helps users in pre-selecting performance-related parameters; therefore achieving better software performance. This aligns with the value item Successful and the corresponding value Achievement.,"My justification is that the paper explicitly states that SAFETUNE, the tool developed in the paper, helps users in pre-selecting performance-related parameters to achieve better software performance. This directly aligns with the value item Successful and its corresponding value Achievement because users would consider achieving better performance as a measure of success in their software usage. By providing this tool, the paper facilitates the user's ability to achieve their desired performance outcomes, thus aligning with the value item Successful and the corresponding value Achievement.",Agreed-Clarified,Agree,,Agree,,
Exp B,Exp H,940,ICSE,Software Testing & QA,Multi-intention-aware configuration selection for performance tuning,"Automatic configuration tuning helps users who intend to improve software performance. However, the auto-tuners are limited by the huge configuration search space. More importantly, they fo-cus only on performance improvement while being unaware of other important user intentions (e.g., reliability, security). To re-duce the search space, researchers mainly focus on pre-selecting performance-related parameters which requires a heavy stage of dynamically running under different configurations to build per-formance models. Given that other important user intentions are not paid attention to, we focus on guiding users in pre-selecting performance-related parameters in general while warning about side-effects on non-performance intentions. We find that the con-figuration document often, if it does not always, contains rich in-formation about the parameters' relationship with diverse user intentions, but documents might also be long and domain-specific. In this paper, we first conduct a comprehensive study on 13 representative software containing 7,349 configuration parame-ters, and derive six types of ways in which configuration parame-ters may affect non-performance intentions. Guided by this study, we design SAFETUNE, a multi-intention-aware method that pre-selects important performance-related parameters and warns about their side-effects on non-performance intentions. Evaluation on target software shows that SAFETUNE correctly identifies 22‚Äö√Ñ√∂‚àö√ë‚àö‚àÇ‚Äö√†√∂‚àö√´‚Äö√†√∂‚Äö√†√á‚Äö√Ñ√∂‚àö‚Ä†‚àö‚àÇ‚Äö√†√∂‚àö¬¥‚Äö√Ñ√∂‚àö‚Ä†‚àö‚àÇ¬¨¬®¬¨√Ü26 performance-related parameters that are missed by state-of-the-art tools but have significant performance impact (up to 14.7x). Furthermore, we illustrate eight representative cases to show that SAFETUNE can effectively prevent real-world and critical side-effects on other user intentions.",Security,Healthy,The paper focuses on guiding users in optimizing software configuration; warning about side-effects on non-performance intentions to ensure the software works correctly; contributing to the value item Healthy and the corresponding value Security.,"In 'Paper X', the main contribution is focused on guiding software users in optimizing configuration parameters to improve performance while also considering non-performance intentions. By providing warnings about side-effects and ensuring the software works correctly, the paper aligns with the value item Healthy and its corresponding value Security. This alignment is evident as the paper aims to protect the user's well-being (Healthy) by addressing potential risks and vulnerabilities, ensuring the software operates securely and reliably (Security).",Agreed-Justified,Disagree,reconcile with coder_2,Disagree,"The justification claiming that the paper aligns with the value 'Security' because of 'The paper focuses on guiding users in optimizing software configuration; warning about side-effects on non-performance intentions to ensure the software works correctly; contributing to the value item Healthy and the corresponding value Security. ' is not supported by the evidence in the abstract, which focuses on 'Automatic configuration tuning helps users who intend to improve software performance.",
Exp C,Exp I,940,ICSE,Software Testing & QA,Multi-intention-aware configuration selection for performance tuning,"Automatic configuration tuning helps users who intend to improve software performance. However, the auto-tuners are limited by the huge configuration search space. More importantly, they fo-cus only on performance improvement while being unaware of other important user intentions (e.g., reliability, security). To re-duce the search space, researchers mainly focus on pre-selecting performance-related parameters which requires a heavy stage of dynamically running under different configurations to build per-formance models. Given that other important user intentions are not paid attention to, we focus on guiding users in pre-selecting performance-related parameters in general while warning about side-effects on non-performance intentions. We find that the con-figuration document often, if it does not always, contains rich in-formation about the parameters' relationship with diverse user intentions, but documents might also be long and domain-specific. In this paper, we first conduct a comprehensive study on 13 representative software containing 7,349 configuration parame-ters, and derive six types of ways in which configuration parame-ters may affect non-performance intentions. Guided by this study, we design SAFETUNE, a multi-intention-aware method that pre-selects important performance-related parameters and warns about their side-effects on non-performance intentions. Evaluation on target software shows that SAFETUNE correctly identifies 22‚Äö√Ñ√∂‚àö√ë‚àö¬®26 performance-related parameters that are missed by state-of-the-art tools but have significant performance impact (up to 14.7x). Furthermore, we illustrate eight representative cases to show that SAFETUNE can effectively prevent real-world and critical side-effects on other user intentions.",Stimulation,Excitement in Life,The paper aims to add an element of excitement to the software user's experience by allowing them to improve software performance themselves using SAFETUNE. This directly aligns with the value item Excitement in Life under the corresponding value Stimulation.,"The justification for aligning 'Paper X' with the value item Excitement in Life and its corresponding value Stimulation from a ""Software User"" perspective is based on the fact that the paper introduces SAFETUNE as a method that allows users to actively improve software performance. By empowering users to take control and make performance-related parameter selections, the paper adds an element of excitement to their software experience. This aligns with the value item Excitement in Life, as it brings a sense of novelty, engagement, and enjoyment to the user's interaction with the software. Consequently, it directly corresponds to the value of Stimulation, which prioritizes seeking diverse experiences and excitement in one's life.",Agreed-Justified,Agree,,Agree,,
Exp D,Exp I,941,ICSE,AI & Machine Learning,Type4py: Practical deep similarity learning-based type inference for python,"Dynamic languages, such as Python and Javascript, trade static typing for developer flexibility and productivity. Lack of static typing can cause run-time exceptions and is a major factor for weak IDE support. To alleviate these issues, PEP 484 introduced optional type annotations for Python. As retrofitting types to existing code-bases is error-prone and laborious, machine learning (ML)-based approaches have been proposed to enable automatic type infer-ence based on existing, partially annotated codebases. However, previous ML-based approaches are trained and evaluated on human-provided type annotations, which might not always be sound, and hence this may limit the practicality for real-world usage. In this paper, we present TYPE4Py, a deep similarity learning-based hier-archical neural network model. It learns to discriminate between similar and dissimilar types in a high-dimensional space, which results in clusters of types. Likely types for arguments, variables, and return values can then be inferred through the nearest neigh-bor search. Unlike previous work, we trained and evaluated our model on a type-checked dataset and used mean reciprocal rank (MRR) to reflect the performance perceived by users. The obtained results show that TYPE4Py achieves an MRR of 77.1 %, which is a substantial improvement of 8.1% and 16.7% over the state-of-the-art approaches Typilus and Typewriter, respectively. Finally, to aid developers with retrofitting types, we released a Visual Stu-dio Code extension, which uses TYPE4Py to provide ML-based type auto-completion for Python.",Achievement,Intelligent,"Paper X contributes to improving code typing in Python through a new model; TYPE4Py. The effectiveness of this model reflects upon users being able to use software more effectively; which aligns with the value item ""Intelligence"" and the corresponding value ""Achievement"".","The justification for aligning 'Paper X' with the value item 'Intelligent' and its corresponding value 'Achievement' is based on the fact that the paper presents a new model, TYPE4Py, which improves code typing in Python. This improvement in coding efficiency and accuracy can be seen as a demonstration of intelligence and achievement, as it enables software users to effectively utilize their coding skills and achieve desired outcomes with greater ease and success.",Agreed-Justified,Agree,,Agree,,
Exp E,Exp J,941,ICSE,AI & Machine Learning,Type4py: Practical deep similarity learning-based type inference for python,"Dynamic languages, such as Python and Javascript, trade static typing for developer flexibility and productivity. Lack of static typing can cause run-time exceptions and is a major factor for weak IDE support. To alleviate these issues, PEP 484 introduced optional type annotations for Python. As retrofitting types to existing code-bases is error-prone and laborious, machine learning (ML)-based approaches have been proposed to enable automatic type infer-ence based on existing, partially annotated codebases. However, previous ML-based approaches are trained and evaluated on human-provided type annotations, which might not always be sound, and hence this may limit the practicality for real-world usage. In this paper, we present TYPE4Py, a deep similarity learning-based hier-archical neural network model. It learns to discriminate between similar and dissimilar types in a high-dimensional space, which results in clusters of types. Likely types for arguments, variables, and return values can then be inferred through the nearest neigh-bor search. Unlike previous work, we trained and evaluated our model on a type-checked dataset and used mean reciprocal rank (MRR) to reflect the performance perceived by users. The obtained results show that TYPE4Py achieves an MRR of 77.1 %, which is a substantial improvement of 8.1% and 16.7% over the state-of-the-art approaches Typilus and Typewriter, respectively. Finally, to aid developers with retrofitting types, we released a Visual Stu-dio Code extension, which uses TYPE4Py to provide ML-based type auto-completion for Python.",Achievement,Successful,"By using the model TYPE4Py; users will be able to use software with less run-time exceptions; which can lead to more successful use of software. This aligns with the value item ""Successful"" and its corresponding value ""Achievement"".","The justification for aligning 'Paper X' with the value item Successful and its corresponding value Achievement is based on the fact that the TYPE4Py model aims to alleviate runtime exceptions in software. By reducing these exceptions, the model enables a more successful use of software by users. This aligns with the value item Successful because it contributes to achieving the desired outcome of successful software usage, which is valued by software users.",Agreed-Justified,Agree,,Agree,,
Exp F,Exp J,941,ICSE,AI & Machine Learning,Type4py: Practical deep similarity learning-based type inference for python,"Dynamic languages, such as Python and Javascript, trade static typing for developer flexibility and productivity. Lack of static typing can cause run-time exceptions and is a major factor for weak IDE support. To alleviate these issues, PEP 484 introduced optional type annotations for Python. As retrofitting types to existing code-bases is error-prone and laborious, machine learning (ML)-based approaches have been proposed to enable automatic type infer-ence based on existing, partially annotated codebases. However, previous ML-based approaches are trained and evaluated on human-provided type annotations, which might not always be sound, and hence this may limit the practicality for real-world usage. In this paper, we present TYPE4Py, a deep similarity learning-based hier-archical neural network model. It learns to discriminate between similar and dissimilar types in a high-dimensional space, which results in clusters of types. Likely types for arguments, variables, and return values can then be inferred through the nearest neigh-bor search. Unlike previous work, we trained and evaluated our model on a type-checked dataset and used mean reciprocal rank (MRR) to reflect the performance perceived by users. The obtained results show that TYPE4Py achieves an MRR of 77.1 %, which is a substantial improvement of 8.1% and 16.7% over the state-of-the-art approaches Typilus and Typewriter, respectively. Finally, to aid developers with retrofitting types, we released a Visual Stu-dio Code extension, which uses TYPE4Py to provide ML-based type auto-completion for Python.",Achievement,Capable,"The TYPE4Py proposed in Paper X provides a better convenience to software users by limiting run-time exceptions through automatic type-inference. By allowing users to use software more conveniently; this aligns with value item ""Capable"" under the corresponding value ""Achievement"".","The main contribution of 'Paper X' is the development of TYPE4Py, a machine learning-based model that enables automatic type inference in Python. This directly aligns with the value item Capable and its corresponding value Achievement from a ""Software User"" perspective because it improves the capability and achievement of software users by reducing run-time exceptions and providing a more convenient and efficient experience while using the software.",Agreed-Justified,Agree,,Agree,,
Exp G,Exp K,942,ICSE,AI & Machine Learning,Explanation-guided fairness testing through genetic algorithm,"The fairness characteristic is a critical attribute of trusted AI systems. A plethora of research has proposed diverse methods for individual fairness testing. However, they are suffering from three major limitations, i.e., low efficiency, low effectiveness, and model-specificity. This work proposes ExpGA, an explanation-guided fairness testing approach through a genetic algorithm (GA). ExpGA employs the explanation results generated by interpretable methods to collect high-quality initial seeds, which are prone to derive discriminatory samples by slightly modifying feature values. ExpGA then adopts GA to search discriminatory sample candidates by optimizing a fitness value. Benefiting from this combination of explanation results and GA, ExpGA is both efficient and effective to detect discriminatory individuals. Moreover, ExpGA only requires prediction probabilities of the tested model, resulting in a better generalization capability to various models. Experiments on multiple real-world benchmarks, including tabular and text datasets, show that ExpGA presents higher efficiency and effectiveness than four state-of-the-art approaches.",Universalism,Social Justice,The paper proposes a fairness testing approach that helps identify discriminatory actions in AI systems. This clearly aligns with the value item Social Justice and its corresponding value Universalism.,"In the paper abstract, it is stated that the proposed approach, ExpGA, aims to detect discriminatory individuals in AI systems. Discrimination is often seen as an unjust and unfair practice, and addressing it aligns with the value item Social Justice, which aims to promote fairness and equality. Additionally, the paper mentions that ExpGA has a better generalization capability to various models, indicating that the approach is focused on promoting fairness universally, in line with the value of Universalism. Thus, the main contributions of 'Paper X' directly align with the value item Social Justice and its corresponding value Universalism from a ""Software User"" perspective.",Agreed-Justified,Agree,,Agree,,
Exp C,Exp I,945,ICSE,Security & Privacy,Exploiting input sanitization for regex denial of service,"Web services use server-side input sanitization to guard against harmful input. Some web services publish their sanitization logic to make their client interface more usable, e.g., allowing clients to debug invalid requests locally. However, this usability practice poses a security risk. Specifically, services may share the regexes they use to sanitize input strings - and regex-based denial of service (ReDoS) is an emerging threat. Although prominent service outages caused by ReDoS have spurred interest in this topic, we know little about the degree to which live web services are vulnerable to ReDoS. In this paper, we conduct the first black-box study measuring the extent of ReDoS vulnerabilities in live web services. We apply the Consistent Sanitization Assumption: that client-side sanitization logic, including regexes, is consistent with the sanitization logic on the server-side. We identify a service's regex-based input sanitization in its HTML forms or its API, find vulnerable regexes among these regexes, craft ReDoS probes, and pinpoint vulnerabilities. We analyzed the HTML forms of 1,000 services and the APIs of 475 services. Of these, 355 services publish regexes; 17 services publish unsafe regexes; and 6 services are vulnerable to ReDoS through their APIs (6 domains; 15 subdomains). Both Microsoft and Amazon Web Services patched their web services as a result of our disclosure. Since these vulnerabilities were from API specifications, not HTML forms, we proposed a ReDoS defense for a popular API validation library, and our patch has been merged. To summarize: in client-visible sanitization logic, some web services advertise Re-DoS vulnerabilities in plain sight. Our results motivate short-term patches and long-term fundamental solutions. ‚Äö√Ñ√∂‚àö√ë‚àö‚à´Make measurable what cannot be measured.‚Äö√Ñ√∂‚àö√ë‚àöœÄ -Galileo Galilei",Security,Healthy,The paper contributes to the safety of the software users by preventing harmful server attacks. This aligns with the value item Healthy (referring to a healthy software environment) and its corresponding value Security.,"My justification aligning 'Paper X' with the value item Healthy and its corresponding value Security is based on the fact that the paper aims to prevent harmful server attacks through input sanitization. By addressing vulnerabilities in web services and mitigating the risk of regex-based denial of service (ReDoS) attacks, the paper contributes to creating a safer and more secure software environment for users. This directly aligns with the value item Healthy, as it promotes the well-being and integrity of the software system, and corresponds to the value of Security, ensuring the protection of sensitive data and preventing potential harm to users.",Agreed-Reconciled,Agree,,Agree,,
Exp D,Exp I,945,ICSE,Security & Privacy,Exploiting input sanitization for regex denial of service,"Web services use server-side input sanitization to guard against harmful input. Some web services publish their sanitization logic to make their client interface more usable, e.g., allowing clients to debug invalid requests locally. However, this usability practice poses a security risk. Specifically, services may share the regexes they use to sanitize input strings - and regex-based denial of service (ReDoS) is an emerging threat. Although prominent service outages caused by ReDoS have spurred interest in this topic, we know little about the degree to which live web services are vulnerable to ReDoS. In this paper, we conduct the first black-box study measuring the extent of ReDoS vulnerabilities in live web services. We apply the Consistent Sanitization Assumption: that client-side sanitization logic, including regexes, is consistent with the sanitization logic on the server-side. We identify a service's regex-based input sanitization in its HTML forms or its API, find vulnerable regexes among these regexes, craft ReDoS probes, and pinpoint vulnerabilities. We analyzed the HTML forms of 1,000 services and the APIs of 475 services. Of these, 355 services publish regexes; 17 services publish unsafe regexes; and 6 services are vulnerable to ReDoS through their APIs (6 domains; 15 subdomains). Both Microsoft and Amazon Web Services patched their web services as a result of our disclosure. Since these vulnerabilities were from API specifications, not HTML forms, we proposed a ReDoS defense for a popular API validation library, and our patch has been merged. To summarize: in client-visible sanitization logic, some web services advertise Re-DoS vulnerabilities in plain sight. Our results motivate short-term patches and long-term fundamental solutions. ‚Äö√Ñ√∂‚àö√ë‚àö‚à´Make measurable what cannot be measured.‚Äö√Ñ√∂‚àö√ë‚àöœÄ -Galileo Galilei",Security,Social Order,The contributions of the paper aim to maintain structure and order in the web services by eliminating threat. This correlates with the value item of Social Order under the value of Security.,"The main contributions of 'Paper X' align with the value item of Social Order and its corresponding value of Security from a ""Software User"" perspective because the paper focuses on identifying and mitigating vulnerabilities in web services that could potentially disrupt the intended order and functionality of those services. By addressing regex-based denial of service threats and proposing patches and solutions, the paper seeks to ensure the security and integrity of web services, thereby maintaining the desired social order within the software context.",Agreed-Clarified,Agree,,Agree,,
Exp E,Exp J,945,ICSE,Security & Privacy,Exploiting input sanitization for regex denial of service,"Web services use server-side input sanitization to guard against harmful input. Some web services publish their sanitization logic to make their client interface more usable, e.g., allowing clients to debug invalid requests locally. However, this usability practice poses a security risk. Specifically, services may share the regexes they use to sanitize input strings - and regex-based denial of service (ReDoS) is an emerging threat. Although prominent service outages caused by ReDoS have spurred interest in this topic, we know little about the degree to which live web services are vulnerable to ReDoS. In this paper, we conduct the first black-box study measuring the extent of ReDoS vulnerabilities in live web services. We apply the Consistent Sanitization Assumption: that client-side sanitization logic, including regexes, is consistent with the sanitization logic on the server-side. We identify a service's regex-based input sanitization in its HTML forms or its API, find vulnerable regexes among these regexes, craft ReDoS probes, and pinpoint vulnerabilities. We analyzed the HTML forms of 1,000 services and the APIs of 475 services. Of these, 355 services publish regexes; 17 services publish unsafe regexes; and 6 services are vulnerable to ReDoS through their APIs (6 domains; 15 subdomains). Both Microsoft and Amazon Web Services patched their web services as a result of our disclosure. Since these vulnerabilities were from API specifications, not HTML forms, we proposed a ReDoS defense for a popular API validation library, and our patch has been merged. To summarize: in client-visible sanitization logic, some web services advertise Re-DoS vulnerabilities in plain sight. Our results motivate short-term patches and long-term fundamental solutions. ‚Äö√Ñ√∂‚àö√ë‚àö‚à´Make measurable what cannot be measured.‚Äö√Ñ√∂‚àö√ë‚àöœÄ -Galileo Galilei",Self Direction,Self Respect,By preventing ReDos threats on the services; the paper provides users a secure and reliable platform; thereby fostering self-respect (confident in their choice of reliable software). This corresponds to Self-Direction value.,"The paper, by addressing ReDoS vulnerabilities in live web services, ensures that users have a secure and reliable platform to interact with. This enhances their self-respect as they can trust their choice of using software that protects their data and privacy. This aligns with the value item of Self Respect and its corresponding value of Self Direction, as it empowers users to make independent choices and exercise their freedom by selecting reliable software.",Agreed-Justified,Agree,,Agree,,
Exp F,Exp J,948,ICSE,Software Testing & QA,Adaptive performance anomaly detection for online service systems via pattern sketching,"To ensure the performance of online service systems, their status is closely monitored with various software and system metrics. Performance anomalies represent the performance degradation issues (e.g., slow response) of the service systems. When performing anomaly detection over the metrics, existing methods often lack the merit of interpretability, which is vital for engineers and analysts to take remediation actions. Moreover, they are unable to effectively accommodate the ever-changing services in an online fashion. To address these limitations, in this paper, we propose ADSketch, an interpretable and adaptive performance anomaly detection approach based on pattern sketching. ADSketch achieves interpretability by identifying groups of anomalous metric patterns, which represent particular types of performance issues. The underlying issues can then be immediately recognized if similar patterns emerge again. In addition, an adaptive learning algorithm is designed to embrace unprecedented patterns induced by service updates or user behavior changes. The proposed approach is evaluated with public data as well as industrial data collected from a representative online service system in Huawei Cloud. The experimental results show that ADSketch outperforms state-of-the-art approaches by a significant margin, and demonstrate the effectiveness of the online algorithm in new pattern discovery. Furthermore, our approach has been successfully deployed in industrial practice.",Stimulation,Variation in Life,The paper presents an approach that constantly adapts to new patterns in service performance metrics; which enhances the variation and dynamism in the user experience of the online service systems.,"The justification for aligning 'Paper X' with the value item Variation in Life and its corresponding value Stimulation from a ""Software User"" perspective is that the paper introduces an adaptive performance anomaly detection approach that can accommodate ever-changing services. This adaptability leads to a dynamic and varied user experience since the system can detect and address performance issues in real-time, resulting in a stimulating and constantly evolving online service environment for the users.",Agreed-Clarified,Disagree,,Disagree,Its not about variation in life. it proposes a method for increasing interpretability.,
Exp G,Exp K,950,ICSE,Mobile & IoT,Where is your app frustrating users?,"User reviews of mobile apps provide a communication channel for developers to perceive user satisfaction. Many app features that users have problems with are usually expressed by key phrases such as ‚Äö√Ñ√∂‚àö√ë‚àö‚à´upload pictures‚Äö√Ñ√∂‚àö√ë‚àöœÄ, which could be buried in the review texts. The lack of fine-grained view about problematic features could obscure the developers' understanding of where the app is frustrating users, and postpone the improvement of the apps. Existing pattern-based approaches to extract target phrases suffer from low accuracy due to insufficient semantic understanding of the reviews, thus can only summarize the high-level topics/aspects of the reviews. This paper proposes a semantic-aware, fine-grained app review analysis approach (SIRA) to extract, cluster, and visualize the problematic features of apps. The main component of SIRA is a novel BERT+Attr-CRF model for fine-grained problematic feature extraction, which combines textual descriptions and review attributes to better model the semantics of reviews and boost the performance of the traditional BERT-CRF model. SIRA also clusters the extracted phrases based on their semantic relations and presents a visualization of the summaries. Our evaluation on 3,426 reviews from six apps confirms the effectiveness of SIRA in problematic feature extraction and clustering. We further conduct an empirical study with SIRA on 318,534 reviews of 18 popular apps to explore its potential application and examine its usefulness in real-world practice.",Self Direction,Freedom,The main contribution of 'Paper X' is a new approach for understanding user reviews in a detailed and semantically aware manner; allowing for improvements in the application based on identified problematic features. This aligns with the value item Freedom (v1.2) from the Self Direction value; as it can contribute to improved user experience allowing users freedom to express their satisfaction or frustration with certain app features via user reviews.,"The main contribution of 'Paper X' aligns with the value item Freedom (v1.2) from the Self Direction value because it enables users to freely express their satisfaction or frustration with specific app features through user reviews. This alignment allows for a more detailed and semantically aware analysis of the reviews, leading to a better understanding of the problematic features and potential improvements in the application. By providing users with the freedom to express their experiences and opinions, the paper's approach empowers users and enhances their ability to influence the development and enhancement of software applications.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,950,ICSE,Mobile & IoT,Where is your app frustrating users?,"User reviews of mobile apps provide a communication channel for developers to perceive user satisfaction. Many app features that users have problems with are usually expressed by key phrases such as ‚Äö√Ñ√∂‚àö√ë‚àö‚àÇ‚Äö√†√∂‚àö√´‚Äö√†√∂‚Äö√†√á‚Äö√Ñ√∂‚àö‚Ä†‚àö‚àÇ‚Äö√†√∂‚àö¬¥‚Äö√Ñ√∂‚àö‚Ä†‚àö‚àÇ‚Äö√Ñ√∂‚àö‚Ä†¬¨¬•upload pictures‚Äö√Ñ√∂‚àö√ë‚àö‚àÇ‚Äö√†√∂‚àö√´‚Äö√†√∂‚Äö√†√á‚Äö√Ñ√∂‚àö‚Ä†‚àö‚àÇ‚Äö√†√∂‚àö¬¥‚Äö√Ñ√∂‚àö‚Ä†‚àö‚àÇ‚âà√¨‚àö√ë, which could be buried in the review texts. The lack of fine-grained view about problematic features could obscure the developers' understanding of where the app is frustrating users, and postpone the improvement of the apps. Existing pattern-based approaches to extract target phrases suffer from low accuracy due to insufficient semantic understanding of the reviews, thus can only summarize the high-level topics/aspects of the reviews. This paper proposes a semantic-aware, fine-grained app review analysis approach (SIRA) to extract, cluster, and visualize the problematic features of apps. The main component of SIRA is a novel BERT+Attr-CRF model for fine-grained problematic feature extraction, which combines textual descriptions and review attributes to better model the semantics of reviews and boost the performance of the traditional BERT-CRF model. SIRA also clusters the extracted phrases based on their semantic relations and presents a visualization of the summaries. Our evaluation on 3,426 reviews from six apps confirms the effectiveness of SIRA in problematic feature extraction and clustering. We further conduct an empirical study with SIRA on 318,534 reviews of 18 popular apps to explore its potential application and examine its usefulness in real-world practice.",Achievement,Successful,The proposed approach (SIRA) also aligns with the value item 'Successful' (v4.3) from the Achievement value (v4). This is evidenced by the ability of the approach to effectively extract and cluster problematic feature references; thereby potentially leading to user success with the app after improvements made by developers in response to detected issues.,"The alignment with the value item 'Successful' from the Achievement value is justified by the fact that the proposed approach (SIRA) aims to extract and cluster problematic features of mobile apps based on user reviews. By addressing these identified issues and making improvements to the apps, developers can potentially enhance user satisfaction and ultimately achieve success in providing a better user experience. This alignment reflects the value that users place on being able to successfully use and benefit from software applications.",Agreed-Justified,Agree,,Agree,,
Exp B,Exp H,950,ICSE,Mobile & IoT,Where is your app frustrating users?,"User reviews of mobile apps provide a communication channel for developers to perceive user satisfaction. Many app features that users have problems with are usually expressed by key phrases such as ‚Äö√Ñ√∂‚àö√ë‚àö‚àÇ‚Äö√†√∂‚àö√´‚Äö√†√∂‚Äö√†√á‚Äö√Ñ√∂‚àö‚Ä†‚àö‚àÇ‚Äö√†√∂‚àö¬¥‚Äö√Ñ√∂‚àö‚Ä†‚àö‚àÇ‚Äö√Ñ√∂‚àö‚Ä†¬¨¬•upload pictures‚Äö√Ñ√∂‚àö√ë‚àö‚àÇ‚Äö√†√∂‚àö√´‚Äö√†√∂‚Äö√†√á‚Äö√Ñ√∂‚àö‚Ä†‚àö‚àÇ‚Äö√†√∂‚àö¬¥‚Äö√Ñ√∂‚àö‚Ä†‚àö‚àÇ‚âà√¨‚àö√ë, which could be buried in the review texts. The lack of fine-grained view about problematic features could obscure the developers' understanding of where the app is frustrating users, and postpone the improvement of the apps. Existing pattern-based approaches to extract target phrases suffer from low accuracy due to insufficient semantic understanding of the reviews, thus can only summarize the high-level topics/aspects of the reviews. This paper proposes a semantic-aware, fine-grained app review analysis approach (SIRA) to extract, cluster, and visualize the problematic features of apps. The main component of SIRA is a novel BERT+Attr-CRF model for fine-grained problematic feature extraction, which combines textual descriptions and review attributes to better model the semantics of reviews and boost the performance of the traditional BERT-CRF model. SIRA also clusters the extracted phrases based on their semantic relations and presents a visualization of the summaries. Our evaluation on 3,426 reviews from six apps confirms the effectiveness of SIRA in problematic feature extraction and clustering. We further conduct an empirical study with SIRA on 318,534 reviews of 18 popular apps to explore its potential application and examine its usefulness in real-world practice.",Security,Social Order,Finally; 'Paper X' is aligned with the value item 'Social Order' (v6.3) of the 'Security' value (v6); as its main contribution helps establish a systematic and orderly way for developers to understand the issues identified by users; which would ideally contribute to a smoother and more secure application for users.,"In 'Paper X', the main contribution of the proposed SIRA approach is to extract and visualize the problematic features of mobile apps based on user reviews. This directly aligns with the value item of 'Social Order' (v6.3) and its corresponding value of 'Security' (v6) from a ""Software User"" perspective. By providing developers with a systematic and thorough understanding of user issues, the SIRA approach can contribute to a more secure and orderly app experience for users. This alignment is evident as the paper aims to improve the app based on user feedback, ultimately enhancing the overall security and reliability of the software.",Agreed-Clarified,Agree,,Agree,,
Exp C,Exp K,951,ICSE,Security & Privacy,Analyzing user perspectives on mobile app privacy at scale,"In this paper we present a methodology to analyze users‚Äö√Ñ√∂‚àö√ë‚àö‚â§ con-cerns and perspectives about privacy at scale. We leverage NLP techniques to process millions of mobile app reviews and extract privacy concerns. Our methodology is composed of a binary clas-sifier that distinguishes between privacy and non-privacy related reviews. We use clustering to gather reviews that discuss similar privacy concerns, and employ summarization metrics to extract representative reviews to summarize each cluster. We apply our methods on 287M reviews for about 2M apps across the 29 cate-gories in Google Play to identify top privacy pain points in mobile apps. We identified approximately 440K privacy related reviews. We find that privacy related reviews occur in all 29 categories, with some issues arising across numerous app categories and other issues only surfacing in a small set of app categories. We show empirical evidence that confirms dominant privacy themes - concerns about apps requesting unnecessary permissions, collection of personal information, frustration with privacy controls, tracking and the selling of personal data. As far as we know, this is the first large scale analysis to confirm these findings based on hundreds of thousands of user inputs. We also observe some unexpected findings such as users warning each other not to install an app due to privacy issues, users uninstalling apps due to privacy reasons, as well as positive reviews that reward developers for privacy friendly apps. Finally we discuss the implications of our method and findings for developers and app stores.",Self Direction,Privacy,The methodology presented in the paper explicitly aims to analyze and extract user concerns about privacy from app reviews; a direct alignment with the value item 'Privacy' under the value 'Self Direction'.,"My justification is based on the fact that the methodology presented in 'Paper X' focuses on analyzing and extracting user concerns about privacy from app reviews. This aligns directly with the value item 'Privacy' under the value 'Self Direction' because it acknowledges the importance of individuals having control over their personal information and making informed choices about the apps they use. By addressing privacy concerns, the paper aims to empower software users to exercise their autonomy and make decisions aligned with their own values and preferences.",Agreed-Justified,Agree,,Agree,,
Exp E,Exp J,951,ICSE,Security & Privacy,Analyzing user perspectives on mobile app privacy at scale,"In this paper we present a methodology to analyze users‚Äö√Ñ√∂‚àö√ë‚àö‚â§ con-cerns and perspectives about privacy at scale. We leverage NLP techniques to process millions of mobile app reviews and extract privacy concerns. Our methodology is composed of a binary clas-sifier that distinguishes between privacy and non-privacy related reviews. We use clustering to gather reviews that discuss similar privacy concerns, and employ summarization metrics to extract representative reviews to summarize each cluster. We apply our methods on 287M reviews for about 2M apps across the 29 cate-gories in Google Play to identify top privacy pain points in mobile apps. We identified approximately 440K privacy related reviews. We find that privacy related reviews occur in all 29 categories, with some issues arising across numerous app categories and other issues only surfacing in a small set of app categories. We show empirical evidence that confirms dominant privacy themes - concerns about apps requesting unnecessary permissions, collection of personal information, frustration with privacy controls, tracking and the selling of personal data. As far as we know, this is the first large scale analysis to confirm these findings based on hundreds of thousands of user inputs. We also observe some unexpected findings such as users warning each other not to install an app due to privacy issues, users uninstalling apps due to privacy reasons, as well as positive reviews that reward developers for privacy friendly apps. Finally we discuss the implications of our method and findings for developers and app stores.",Achievement,Successful,The paper addresses user reviews that reward developers for privacy friendly apps; reflecting users' desire for 'Success' in achieving a privacy-oriented application environment - part of the 'Achievement' value.,"The paper's focus on user reviews that reward developers for privacy-friendly apps aligns with the value item ""Successful"" and its corresponding value ""Achievement"" because it demonstrates that users value and desire a successful outcome in achieving a privacy-oriented application environment. This alignment suggests that users see the ability to maintain privacy as a measure of success and accomplishment within the software context.",Agreed-Justified,Agree,,Agree,,
Exp F,Exp J,952,ICSE,Software Testing & QA,Linear-time temporal logic guided greybox fuzzing,"Software model checking as well as runtime verification are verification techniques which are widely used for checking temporal properties of software systems. Even though they are property verification techniques, their common usage in practice is in ‚Äö√Ñ√∂‚àö√ë‚àö‚à´bug finding‚Äö√Ñ√∂‚àö√ë‚àöœÄ, that is, finding violations of temporal properties. Motivated by this observation and leveraging the recent progress in fuzzing, we build a greybox fuzzing framework to find violations of Linear-time Temporal Logic (LTL) properties. Our framework takes as input a sequential program written in C/C++, and an LTL property. It finds violations, or counterexample traces, of the LTL property in stateful software systems; however, it does not achieve verification. Our work substantially extends directed greybox fuzzing to witness arbitrarily complex event or-derings. We note that existing directed greybox fuzzing approaches are limited to witnessing reaching a location or witnessing simple event orderings like use-after-free. At the same time, compared to model checkers, our approach finds the counterexamples faster, thereby finding more counterexamples within a given time budget. Our LTL-FUZZER tool, built on top of the AFL fuzzer, is shown to be effective in detecting bugs in well-known protocol implementations, such as OpenSSL and Telnet. We use LTL-FUZZER to reproduce known vulnerabilities (CVEs), to find 15 zero-day bugs by checking properties extracted from RFCs (for which 12 CVEs have been assigned), and to find violations of both safety as well as liveness properties in real-world protocol implementations. Our work represents a practical advance over software model checkers - while simultaneously representing a conceptual advance over existing greybox fuzzers. Our work thus provides a starting point for understanding the unexplored synergies among software model checking, runtime verification and greybox fuzzing.",Achievement,Intelligent,The provided 'LTL-Fuzzer' tool is designed to be intelligent enough to detect bugs in well-known protocol implementations; hence aligning with the value item 'Intelligent' which is a part of the 'Achievement' value.,"In 'Paper X', the LTL-Fuzzer tool is described as being able to detect bugs in well-known protocol implementations. This ability requires the tool to possess a level of intelligence in order to accurately identify and expose these bugs. By aligning with the value item 'Intelligent' from Schwartz's Taxonomy, the paper demonstrates that the tool achieves a significant level of achievement in terms of its ability to contribute to the software user's experience by effectively detecting bugs and vulnerabilities.",Agreed-Justified,Agree,,Agree,,
Exp G,Exp K,952,ICSE,Software Testing & QA,Linear-time temporal logic guided greybox fuzzing,"Software model checking as well as runtime verification are verification techniques which are widely used for checking temporal properties of software systems. Even though they are property verification techniques, their common usage in practice is in ‚Äö√Ñ√∂‚àö√ë‚àö‚à´bug finding‚Äö√Ñ√∂‚àö√ë‚àöœÄ, that is, finding violations of temporal properties. Motivated by this observation and leveraging the recent progress in fuzzing, we build a greybox fuzzing framework to find violations of Linear-time Temporal Logic (LTL) properties. Our framework takes as input a sequential program written in C/C++, and an LTL property. It finds violations, or counterexample traces, of the LTL property in stateful software systems; however, it does not achieve verification. Our work substantially extends directed greybox fuzzing to witness arbitrarily complex event or-derings. We note that existing directed greybox fuzzing approaches are limited to witnessing reaching a location or witnessing simple event orderings like use-after-free. At the same time, compared to model checkers, our approach finds the counterexamples faster, thereby finding more counterexamples within a given time budget. Our LTL-FUZZER tool, built on top of the AFL fuzzer, is shown to be effective in detecting bugs in well-known protocol implementations, such as OpenSSL and Telnet. We use LTL-FUZZER to reproduce known vulnerabilities (CVEs), to find 15 zero-day bugs by checking properties extracted from RFCs (for which 12 CVEs have been assigned), and to find violations of both safety as well as liveness properties in real-world protocol implementations. Our work represents a practical advance over software model checkers - while simultaneously representing a conceptual advance over existing greybox fuzzers. Our work thus provides a starting point for understanding the unexplored synergies among software model checking, runtime verification and greybox fuzzing.",Security,Healthy,By detecting bugs in software systems; the 'LTL-Fuzzer' can contribute to the well-being and health of the software; aligning with the value item 'Healthy' and its corresponding value 'Security'.,"The contribution of 'Paper X' in detecting bugs in software systems aligns with the value item ""Healthy"" and its corresponding value ""Security"" from a ""Software User"" perspective because by identifying and addressing vulnerabilities in software, it enhances the overall security of the software system, ensuring that it operates in a secure and safe manner. This directly contributes to the well-being and health of the software, aligning with the value item ""Healthy.""",Agreed-Justified,Disagree,Agree with coder_2,Disagree,Value healthy concerns Human health not software health,
Exp A,Exp H,952,ICSE,Software Testing & QA,Linear-time temporal logic guided greybox fuzzing,"Software model checking as well as runtime verification are verification techniques which are widely used for checking temporal properties of software systems. Even though they are property verification techniques, their common usage in practice is in ‚Äö√Ñ√∂‚àö√ë‚àö‚àÇ‚Äö√†√∂‚àö√´‚Äö√†√∂‚Äö√†√á‚Äö√Ñ√∂‚àö‚Ä†‚àö‚àÇ‚Äö√†√∂‚àö¬¥‚Äö√Ñ√∂‚àö‚Ä†‚àö‚àÇ‚Äö√Ñ√∂‚àö‚Ä†¬¨¬•bug finding‚Äö√Ñ√∂‚àö√ë‚àö‚àÇ‚Äö√†√∂‚àö√´‚Äö√†√∂‚Äö√†√á‚Äö√Ñ√∂‚àö‚Ä†‚àö‚àÇ‚Äö√†√∂‚àö¬¥‚Äö√Ñ√∂‚àö‚Ä†‚àö‚àÇ‚âà√¨‚àö√ë, that is, finding violations of temporal properties. Motivated by this observation and leveraging the recent progress in fuzzing, we build a greybox fuzzing framework to find violations of Linear-time Temporal Logic (LTL) properties. Our framework takes as input a sequential program written in C/C++, and an LTL property. It finds violations, or counterexample traces, of the LTL property in stateful software systems; however, it does not achieve verification. Our work substantially extends directed greybox fuzzing to witness arbitrarily complex event or-derings. We note that existing directed greybox fuzzing approaches are limited to witnessing reaching a location or witnessing simple event orderings like use-after-free. At the same time, compared to model checkers, our approach finds the counterexamples faster, thereby finding more counterexamples within a given time budget. Our LTL-FUZZER tool, built on top of the AFL fuzzer, is shown to be effective in detecting bugs in well-known protocol implementations, such as OpenSSL and Telnet. We use LTL-FUZZER to reproduce known vulnerabilities (CVEs), to find 15 zero-day bugs by checking properties extracted from RFCs (for which 12 CVEs have been assigned), and to find violations of both safety as well as liveness properties in real-world protocol implementations. Our work represents a practical advance over software model checkers - while simultaneously representing a conceptual advance over existing greybox fuzzers. Our work thus provides a starting point for understanding the unexplored synergies among software model checking, runtime verification and greybox fuzzing.",Stimulation,Variation in Life,The authors mention that their work represents a practical and conceptual advance; suggesting a variation in established software verification techniques. This aligns with the value item 'Variation in Life' under the value 'Stimulation'.,"In the paper abstract, the authors state that their work represents a practical and conceptual advance over existing software verification techniques. This suggests that their approach introduces a new and different way of verifying software, which aligns with the value item 'Variation in Life' under the value 'Stimulation' in Schwartz's Taxonomy. This alignment implies that the main contribution of 'Paper X' brings a sense of novelty and excitement to the software user, stimulating their experience with different verification methods.",Agreed-Justified,Agree,,Agree,,
Exp B,Exp H,954,ICSE,AI & Machine Learning,Control parameters considered harmful: Detecting range specification bugs in drone configuration modules via learning-guided search,"In order to support a variety of missions and deal with different flight environments, drone control programs typically provide configurable control parameters. However, such a flexibility introduces vulnerabilities. One such vulnerability, referred to as range specification bugs, has been recently identified. The vulnerability originates from the fact that even though each individual parameter receives a value in the recommended value range, certain combinations of parameter values may affect the drone physical stability. In this paper, we develop a novel learning-guided search system to find such combinations, that we refer to as incorrect configurations. Our system applies metaheuristic search algorithms mutating configurations to detect the configuration parameters that have values driving the drone to unstable physical states. To guide the mutations, our system leverages a machine learning based predictor as the fitness evaluator. Finally, by utilizing multi-objective optimization, our system returns the feasible ranges based on the mutation search results. Because in our system the mutations are guided by a predictor, evaluating the parameter configurations does not require realistic/simulation executions. Therefore, our system supports a comprehensive and yet efficient detection of incorrect configurations. We have carried out an experimental evaluation of our system. The evaluation results show that the system successfully reports potentially incorrect configurations, of which over 85% lead to actual unstable physical states.",Stimulation,Variation in Life,The system in the Paper X helps the drone users to deal with different flight environments; varying the usage of the drone control programs. This aligns with the value item Variation in Life and its corresponding value Stimulation.,"The main contribution of 'Paper X' is the development of a learning-guided search system that detects incorrect configurations of drone control parameters, which could lead to unstable physical states. This system helps drone users by providing them with the feasible parameter ranges based on the mutation search results, without the need for realistic/simulation executions. This aligns with the value item Variation in Life and its corresponding value Stimulation because it allows users to have different and exciting flight experiences by enabling them to explore various configurations of the drone control parameters and adapt to different flight environments.",Agreed-Justified,Agree,,Agree,,
Exp C,Exp K,954,ICSE,AI & Machine Learning,Control parameters considered harmful: Detecting range specification bugs in drone configuration modules via learning-guided search,"In order to support a variety of missions and deal with different flight environments, drone control programs typically provide configurable control parameters. However, such a flexibility introduces vulnerabilities. One such vulnerability, referred to as range specification bugs, has been recently identified. The vulnerability originates from the fact that even though each individual parameter receives a value in the recommended value range, certain combinations of parameter values may affect the drone physical stability. In this paper, we develop a novel learning-guided search system to find such combinations, that we refer to as incorrect configurations. Our system applies metaheuristic search algorithms mutating configurations to detect the configuration parameters that have values driving the drone to unstable physical states. To guide the mutations, our system leverages a machine learning based predictor as the fitness evaluator. Finally, by utilizing multi-objective optimization, our system returns the feasible ranges based on the mutation search results. Because in our system the mutations are guided by a predictor, evaluating the parameter configurations does not require realistic/simulation executions. Therefore, our system supports a comprehensive and yet efficient detection of incorrect configurations. We have carried out an experimental evaluation of our system. The evaluation results show that the system successfully reports potentially incorrect configurations, of which over 85% lead to actual unstable physical states.",Security,Healthy,The Paper X contributes to the drone users' safety by detecting the parameter configurations that might drive the drone to unstable physical states. This directly aligns with the value item Healthy and its corresponding value Security.,"In the context of a ""Software User,"" the alignment of 'Paper X' with the value item Healthy and its corresponding value Security is evident because the paper explicitly states that the goal is to detect incorrect configurations that may lead to unstable physical states for the drone. By identifying these configurations, the paper aims to ensure the safety and physical well-being of the drone users, which aligns with the value of security. By promoting safe drone operation, the paper indirectly contributes to the value of health as it helps prevent potential accidents or harm to the users.",Agreed-Reconciled,Agree,,Agree,,
Exp D,Exp K,954,ICSE,AI & Machine Learning,Control parameters considered harmful: Detecting range specification bugs in drone configuration modules via learning-guided search,"In order to support a variety of missions and deal with different flight environments, drone control programs typically provide configurable control parameters. However, such a flexibility introduces vulnerabilities. One such vulnerability, referred to as range specification bugs, has been recently identified. The vulnerability originates from the fact that even though each individual parameter receives a value in the recommended value range, certain combinations of parameter values may affect the drone physical stability. In this paper, we develop a novel learning-guided search system to find such combinations, that we refer to as incorrect configurations. Our system applies metaheuristic search algorithms mutating configurations to detect the configuration parameters that have values driving the drone to unstable physical states. To guide the mutations, our system leverages a machine learning based predictor as the fitness evaluator. Finally, by utilizing multi-objective optimization, our system returns the feasible ranges based on the mutation search results. Because in our system the mutations are guided by a predictor, evaluating the parameter configurations does not require realistic/simulation executions. Therefore, our system supports a comprehensive and yet efficient detection of incorrect configurations. We have carried out an experimental evaluation of our system. The evaluation results show that the system successfully reports potentially incorrect configurations, of which over 85% lead to actual unstable physical states.",Security,National Security,The paper indirectly contributes to national security as the health of drone systems can be critical in several defense scenarios; aligning with the value item National Security and its corresponding value Security.,"My previous justification aligning 'Paper X' with the value item National Security and its corresponding value Security from a ""Software User"" perspective is based on the understanding that the physical stability of drone systems can be crucial in defense scenarios. Ensuring the security and health of drone systems directly contributes to maintaining national security. By identifying incorrect configurations that can potentially lead to unstable physical states, 'Paper X' indirectly addresses the security aspect of drone systems, thus aligning with the value item National Security and its corresponding value Security.",Agreed-Justified,Agree,,Agree,,
Exp E,Exp J,957,ICSE,Software Testing & QA,PerfSig: Extracting performance bug signatures via multi-modality causal analysis,"Diagnosing a performance bug triggered in production cloud environments is notoriously challenging. Extracting performance bug signatures can help cloud operators quickly pinpoint the problem and avoid repeating manual efforts for diagnosing similar performance bugs. In this paper, we present PerfSig, a multi-modality performance bug signature extraction tool which can identify principal anomaly patterns and root cause functions for performance bugs. PerfSig performs fine-grained anomaly detection over various machine data such as system metrics, system logs, and function call traces. We then conduct causal analysis across different machine data using information theory method to pinpoint the root cause function of a performance bug. PerfSig generates bug signatures as the combination of the identified anomaly patterns and root cause functions. We have implemented a prototype of PerfSig and conducted evaluation using 20 real world performance bugs in six commonly used cloud systems. Our experimental results show that PerfSig captures various kinds of fine-grained anomaly patterns from different machine data and successfully identifies the root cause functions through multi-modality causal analysis for 19 out of 20 tested performance bugs.",Security,Healthy,"The paper 'PerfSig'; a tool which is dedicated to diagnosing performance bugs in cloud environments. This contributes to the well-being of the software user by maintaining the performance and functionality of the cloud environments they are using; aligning with the value item ""Healthy"" and its corresponding value ""Security"".","The paper 'PerfSig' directly aligns with the value item ""Healthy"" and its corresponding value ""Security"" from a ""Software User"" perspective because it focuses on diagnosing performance bugs in cloud environments. By efficiently identifying principal anomaly patterns and root cause functions, PerfSig contributes to maintaining the performance and functionality of the cloud environments that software users rely on. This ensures a secure and reliable experience for the users, promoting their overall well-being and aligning with the value of security in the context of software usage.",Agreed-Justified,Agree,,Agree,,
Exp F,Exp J,957,ICSE,Software Testing & QA,PerfSig: Extracting performance bug signatures via multi-modality causal analysis,"Diagnosing a performance bug triggered in production cloud environments is notoriously challenging. Extracting performance bug signatures can help cloud operators quickly pinpoint the problem and avoid repeating manual efforts for diagnosing similar performance bugs. In this paper, we present PerfSig, a multi-modality performance bug signature extraction tool which can identify principal anomaly patterns and root cause functions for performance bugs. PerfSig performs fine-grained anomaly detection over various machine data such as system metrics, system logs, and function call traces. We then conduct causal analysis across different machine data using information theory method to pinpoint the root cause function of a performance bug. PerfSig generates bug signatures as the combination of the identified anomaly patterns and root cause functions. We have implemented a prototype of PerfSig and conducted evaluation using 20 real world performance bugs in six commonly used cloud systems. Our experimental results show that PerfSig captures various kinds of fine-grained anomaly patterns from different machine data and successfully identifies the root cause functions through multi-modality causal analysis for 19 out of 20 tested performance bugs.",Security,Social Order,"By diagnosing performance bugs and subsequently avoiding repeating manual efforts; 'PerfSig' contributes to maintaining a stable and orderly user experience; aligning with the value item ""Social Order"" under the value ""Security"".","The justification for aligning 'Paper X' with the value item Social Order and its corresponding value Security is based on the fact that PerfSig, the tool presented in the paper, helps cloud operators quickly diagnose and pinpoint performance bugs in production cloud environments. By doing so, it avoids the repetition of manual efforts and contributes to a stable and orderly user experience. This directly aligns with the value item Social Order, as a stable and orderly software system promotes a sense of security for software users.",Agreed-Justified,Agree,,Agree,,
Exp G,Exp K,957,ICSE,Software Testing & QA,PerfSig: Extracting performance bug signatures via multi-modality causal analysis,"Diagnosing a performance bug triggered in production cloud environments is notoriously challenging. Extracting performance bug signatures can help cloud operators quickly pinpoint the problem and avoid repeating manual efforts for diagnosing similar performance bugs. In this paper, we present PerfSig, a multi-modality performance bug signature extraction tool which can identify principal anomaly patterns and root cause functions for performance bugs. PerfSig performs fine-grained anomaly detection over various machine data such as system metrics, system logs, and function call traces. We then conduct causal analysis across different machine data using information theory method to pinpoint the root cause function of a performance bug. PerfSig generates bug signatures as the combination of the identified anomaly patterns and root cause functions. We have implemented a prototype of PerfSig and conducted evaluation using 20 real world performance bugs in six commonly used cloud systems. Our experimental results show that PerfSig captures various kinds of fine-grained anomaly patterns from different machine data and successfully identifies the root cause functions through multi-modality causal analysis for 19 out of 20 tested performance bugs.",Security,Sense of Belonging,"The paper contributes to a sense of belonging of the software user by ensuring the continuity and smooth operation of communal cloud environments with the performance bug signature extraction tool; aligning with the value item ""Sense of Belonging"" and its corresponding value ""Security"".","In the paper, PerfSig is described as a tool that helps cloud operators quickly identify and resolve performance bugs in production cloud environments. By accurately pinpointing the root cause functions of these bugs, PerfSig ensures the smooth operation and continuity of communal cloud environments, which directly aligns with the value item ""Sense of Belonging"" as users will feel secure and connected to the cloud community knowing that their performance issues are being addressed effectively. Therefore, the alignment with the value item ""Sense of Belonging"" and its corresponding value ""Security"" is evident in the paper's focus on providing a reliable and stable software experience for users within a cloud environment.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,961,ICSE,AI & Machine Learning,SapientML: synthesizing machine learning pipelines by learning from human-writen solutions,"Automatic machine learning, or AutoML, holds the promise of truly democratizing the use of machine learning (ML), by substantially automating the work of data scientists. However, the huge combinatorial search space of candidate pipelines means that current AutoML techniques, generate sub-optimal pipelines, or none at all, especially on large, complex datasets. In this work we propose an AutoML technique SapientML, that can learn from a corpus of existing datasets and their human-written pipelines, and efficiently generate a high-quality pipeline for a predictive task on a new dataset. To combat the search space explosion of AutoML, SapientML employs a novel divide-and-conquer strategy realized as a three-stage program synthesis approach, that reasons on successively smaller search spaces. The first stage uses meta-learning to predict a set of plausible ML components to constitute a pipeline. In the second stage, this is then refined into a small pool of viable concrete pipelines using a pipeline dataflow model derived from the corpus. Dynamically evaluating these few pipelines, in the third stage, provides the best solution. We instantiate SapientML as part of a fully automated tool-chain that creates a cleaned, labeled learning corpus by mining Kaggle, learns from it, and uses the learned models to then synthesize pipelines for new predictive tasks. We have created a training corpus of 1,094 pipelines spanning 170 datasets, and evaluated SapientML on a set of 41 benchmark datasets, including 10 new, large, real-world datasets from Kaggle, and against 3 state-of-the-art AutoML tools and 4 baselines. Our evaluation shows that SapientML produces the best or comparable accuracy on 27 of the benchmarks while the second best tool fails to even produce a pipeline on 9 of the instances. This difference is amplified on the 10 most challenging benchmarks, where SapientML wins on 9 instances with the other tools failing to produce pipelines on 4 or more benchmarks.",Achievement,Intelligent,SapientML; the AutoML technique developed in this paper; is specifically designed to produce high-quality outcomes. Thus; the paper directly aligns with the value item 'Intelligent' under the value 'Achievement' as it exhibits the aim to accomplish better results using smart automation.,"In the paper, the authors introduce SapientML as an AutoML technique that aims to generate high-quality pipelines for predictive tasks. This directly aligns with the value item 'Intelligent' under the value 'Achievement' from Schwartz's Taxonomy. By automating the work of data scientists and producing better results through smart automation, the paper demonstrates an achievement in using intelligent techniques to accomplish superior outcomes.",Agreed-Clarified,Agree,,Agree,,
Exp B,Exp H,961,ICSE,AI & Machine Learning,SapientML: synthesizing machine learning pipelines by learning from human-writen solutions,"Automatic machine learning, or AutoML, holds the promise of truly democratizing the use of machine learning (ML), by substantially automating the work of data scientists. However, the huge combinatorial search space of candidate pipelines means that current AutoML techniques, generate sub-optimal pipelines, or none at all, especially on large, complex datasets. In this work we propose an AutoML technique SapientML, that can learn from a corpus of existing datasets and their human-written pipelines, and efficiently generate a high-quality pipeline for a predictive task on a new dataset. To combat the search space explosion of AutoML, SapientML employs a novel divide-and-conquer strategy realized as a three-stage program synthesis approach, that reasons on successively smaller search spaces. The first stage uses meta-learning to predict a set of plausible ML components to constitute a pipeline. In the second stage, this is then refined into a small pool of viable concrete pipelines using a pipeline dataflow model derived from the corpus. Dynamically evaluating these few pipelines, in the third stage, provides the best solution. We instantiate SapientML as part of a fully automated tool-chain that creates a cleaned, labeled learning corpus by mining Kaggle, learns from it, and uses the learned models to then synthesize pipelines for new predictive tasks. We have created a training corpus of 1,094 pipelines spanning 170 datasets, and evaluated SapientML on a set of 41 benchmark datasets, including 10 new, large, real-world datasets from Kaggle, and against 3 state-of-the-art AutoML tools and 4 baselines. Our evaluation shows that SapientML produces the best or comparable accuracy on 27 of the benchmarks while the second best tool fails to even produce a pipeline on 9 of the instances. This difference is amplified on the 10 most challenging benchmarks, where SapientML wins on 9 instances with the other tools failing to produce pipelines on 4 or more benchmarks.",Universalism,Equality,In the abstract; the authors mention the goal of democratizing the use of machine learning through automatic machine learning which can be aligned to 'Equality' under the value 'Universalism'; as it intends to provide equal opportunity for all users irrespective of their level of expertise.,"In the abstract, the authors explicitly state that the main goal of 'Paper X' is to democratize the use of machine learning through automatic machine learning. This aligns with the value item Equality and its corresponding value Universalism from a ""Software User"" perspective. By automating the work of data scientists, the paper aims to provide equal opportunities for all users to utilize machine learning, regardless of their level of expertise. This promotes fairness and inclusivity within the software context.",Agreed-Clarified,Agree,,Agree,,
Exp D,Exp K,966,ICSE,Security & Privacy,Large-scale security measurements on the android firmware ecosystem,"Android is the most popular smartphone platform with over 85% market share. Its success is built on openness, and phone vendors can utilize the Android source code to make products with unique software/hardware features. On the other hand, the fragmentation and customization of Android also bring many security risks that have attracted the attention of researchers. Many efforts were put in to investigate the security of customized Android firmware. However, most of the previous work focuses on designing efficient analysis tools or analyzing particular aspects of the firmware. There still lacks a panoramic view of Android firmware ecosystem secu-rity and the corresponding understandings based on large-scale firmware datasets. In this work, we made a large-scale compre-hensive measurement of the Android firmware ecosystem security. Our study is based on 6,261 firmware images from 153 vendors and 602 Android-related CVEs, which is the largest Android firmware dataset ever used for security measurements. In particular, our study followed a series of research questions, covering vulnerabili-ties, patches, security updates, and pre-installed apps. To automate the analysis process, we designed a framework, ANDSCANNER, to complete ROM crawling, ROM parsing, patch analysis, and app analysis. Through massive data analysis and case explorations, sev-eral interesting findings are obtained. For example, the patch delay and missing issues are widespread in Android images, say 24.2% and 6.1 % of all images, respectively. The latest images of several phones still contain vulnerable pre-installed apps, and even the corresponding vulnerabilities have been publicly disclosed. In ad-dition to data measurements, we also explore the causes behind these security threats through case studies and demonstrate that the discovered security threats can be converted into exploitable vulnerabilities via 38 newfound vulnerabilities by our framework, 32 of which have been assigned CVE/CNVD numbers. This study provides much new knowledge of the Android firmware ecosystem with deep understanding of software engineering security practices.",Security,Healthy,Paper X contributes to enhancing android security and reducing vulnerabilities which aligns with the value item Healthy(Software system in this case) and its corresponding value Security.,"In 'Paper X', the authors conduct a comprehensive measurement of the Android firmware ecosystem security, focusing on vulnerabilities, patches, security updates, and pre-installed apps. They discover widespread patch delays and missing patches in Android images, as well as the presence of vulnerable pre-installed apps in the latest images of some phones. These findings directly align with the value item Healthy from Schwartz's Taxonomy, as the paper's contributions aim to enhance the security of the Android software system, reducing vulnerabilities and ensuring a healthier, more secure user experience.",Agreed-Clarified,Agree,,Disagree,Value healthy concerns Human health not software health,
Exp E,Exp J,966,ICSE,Security & Privacy,Large-scale security measurements on the android firmware ecosystem,"Android is the most popular smartphone platform with over 85% market share. Its success is built on openness, and phone vendors can utilize the Android source code to make products with unique software/hardware features. On the other hand, the fragmentation and customization of Android also bring many security risks that have attracted the attention of researchers. Many efforts were put in to investigate the security of customized Android firmware. However, most of the previous work focuses on designing efficient analysis tools or analyzing particular aspects of the firmware. There still lacks a panoramic view of Android firmware ecosystem secu-rity and the corresponding understandings based on large-scale firmware datasets. In this work, we made a large-scale compre-hensive measurement of the Android firmware ecosystem security. Our study is based on 6,261 firmware images from 153 vendors and 602 Android-related CVEs, which is the largest Android firmware dataset ever used for security measurements. In particular, our study followed a series of research questions, covering vulnerabili-ties, patches, security updates, and pre-installed apps. To automate the analysis process, we designed a framework, ANDSCANNER, to complete ROM crawling, ROM parsing, patch analysis, and app analysis. Through massive data analysis and case explorations, sev-eral interesting findings are obtained. For example, the patch delay and missing issues are widespread in Android images, say 24.2% and 6.1 % of all images, respectively. The latest images of several phones still contain vulnerable pre-installed apps, and even the corresponding vulnerabilities have been publicly disclosed. In ad-dition to data measurements, we also explore the causes behind these security threats through case studies and demonstrate that the discovered security threats can be converted into exploitable vulnerabilities via 38 newfound vulnerabilities by our framework, 32 of which have been assigned CVE/CNVD numbers. This study provides much new knowledge of the Android firmware ecosystem with deep understanding of software engineering security practices.",Security,Social Order,The paper‚Äö√Ñ√∂‚àö√ë‚àö¬•s efforts to measure and improve Android firmware ecosystem security align with the value item Social Order(Here it refers to order in software system); and its corresponding value Security.,"In 'Paper X', the authors focus on analyzing the security of the Android firmware ecosystem. This aligns with the value item of Social Order, as it pertains to maintaining order within a software system. By assessing vulnerabilities, patches, security updates, and pre-installed apps, the paper addresses the need for a secure and orderly software environment. This directly corresponds to the value of Security, as it aims to protect the system and ensure its smooth functioning. Thus, the efforts of 'Paper X' align with Social Order and its corresponding value, Security, from a ""Software User"" perspective.",Agreed-Justified,Agree,,Agree,,
Exp F,Exp J,966,ICSE,Security & Privacy,Large-scale security measurements on the android firmware ecosystem,"Android is the most popular smartphone platform with over 85% market share. Its success is built on openness, and phone vendors can utilize the Android source code to make products with unique software/hardware features. On the other hand, the fragmentation and customization of Android also bring many security risks that have attracted the attention of researchers. Many efforts were put in to investigate the security of customized Android firmware. However, most of the previous work focuses on designing efficient analysis tools or analyzing particular aspects of the firmware. There still lacks a panoramic view of Android firmware ecosystem secu-rity and the corresponding understandings based on large-scale firmware datasets. In this work, we made a large-scale compre-hensive measurement of the Android firmware ecosystem security. Our study is based on 6,261 firmware images from 153 vendors and 602 Android-related CVEs, which is the largest Android firmware dataset ever used for security measurements. In particular, our study followed a series of research questions, covering vulnerabili-ties, patches, security updates, and pre-installed apps. To automate the analysis process, we designed a framework, ANDSCANNER, to complete ROM crawling, ROM parsing, patch analysis, and app analysis. Through massive data analysis and case explorations, sev-eral interesting findings are obtained. For example, the patch delay and missing issues are widespread in Android images, say 24.2% and 6.1 % of all images, respectively. The latest images of several phones still contain vulnerable pre-installed apps, and even the corresponding vulnerabilities have been publicly disclosed. In ad-dition to data measurements, we also explore the causes behind these security threats through case studies and demonstrate that the discovered security threats can be converted into exploitable vulnerabilities via 38 newfound vulnerabilities by our framework, 32 of which have been assigned CVE/CNVD numbers. This study provides much new knowledge of the Android firmware ecosystem with deep understanding of software engineering security practices.",Universalism,A World at Peace,Through improving and reinforcing the Android firmware ecosystem security; paper X strives for A World at Peace(In this case; refers to harmonious software environment) which aligns with its corresponding value Universalism.,"In the context of a ""Software User,"" the alignment of 'Paper X' with the value item A World at Peace and its corresponding value Universalism can be justified by the fact that by improving and reinforcing the security of the Android firmware ecosystem, the paper aims to create a harmonious software environment where users can feel safe and secure in their interactions with their smartphones. This aligns with the value of Universalism, which emphasizes equality, justice, and unity, as it strives to create a software ecosystem that promotes peace and fairness for all users.",Agreed-Justified,Agree,,Agree,,
Exp G,Exp K,967,ICSE,AI & Machine Learning,BugListener: identifying and synthesizing bug reports from collaborative live chats,"In community-based software development, developers frequently rely on live-chatting to discuss emergent bugs/errors they encounter in daily development tasks. However, it remains a challenging task to accurately record such knowledge due to the noisy nature of interleaved dialogs in live chat data. In this paper, we first formulate the task of identifying and synthesizing bug reports from commu-nity live chats, and propose a novel approach, named BugListener, to address the challenges. Specifically, BugListener automates three sub-tasks: 1) Disentangle the dialogs from massive chat logs by using a Feed-Forward neural network; 2) Identify the bug-report dialogs from separated dialogs by leveraging the Graph neural net-work to learn the contextual information; 3) Synthesize the bug reports by utilizing Transfer Learning techniques to classify the sentences into: observed behaviors (OB), expected behaviors (EB), and steps to reproduce the bug (SR). BugListener is evaluated on six open source projects. The results show that: for bug report identification, BugListener achieves the average Fl of 77.74%, im-proving the best baseline by 12.96%; and for bug report synthesis task, BugListener could classify the OB, EB, and SR sentences with the F1 of 84.62%, 71.46%, and 73.13%, improving the best baselines by 9.32%,12.21%,10.91%, respectively. A human evaluation study also confirms the effectiveness of Bug Listener in generating relevant and accurate bug reports. These demonstrate the significant potential of applying BugListener in community-based software development, for promoting bug discovery and quality improvement.",Achievement,Successful,The paper presents a technique; BugListener; those main goal it is to deal more efficiently with bugs in the software for better software quality improving users' success in using the software. This aligns with the value item Successful and its corresponding value Achievement.,"The justification for aligning 'Paper X' with the value item Successful and its corresponding value Achievement from a ""Software User"" perspective is based on the primary goal of BugListener, which is to improve the software quality by enhancing bug identification and synthesis. By effectively addressing bugs in the software, BugListener aims to provide a smoother user experience and increase the success rate of users in utilizing the software. This aligns with the value of Achievement as it focuses on the user's ability to successfully achieve their goals and objectives in using the software.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,973,ICSE,Security & Privacy,Rotten apples spoil the bunch: An anatomy of Google Play malware,"This paper provides an in-depth analysis of Android malware that bypassed the strictest defenses of the Google Play application store and penetrated the official Android market between January 2016 and July 2021. We systematically identified 1,238 such malicious applications, grouped them into 134 families, and manually analyzed one application from 105 distinct families. During our manual analysis, we identified malicious payloads the applications execute, conditions guarding execution of the payloads, hiding techniques applications employ to evade detection by the user, and other implementation-level properties relevant for automated malware detection. As most applications in our dataset contain multiple payloads, each triggered via its own complex activation logic, we also contribute a graph-based representation showing activation paths for all application payloads in form of a control- and data-flow graph. Furthermore, we discuss the capabilities of existing malware detection tools, put them in context of the properties observed in the analyzed malware, and identify gaps and future research directions. We believe that our detailed analysis of the recent, evasive malware will be of interest to researchers and practitioners and will help further improve malware detection tools.",Security,Healthy,The paper contributes to enhancing the protection of Android software users by investigating and identifying malware instances and their techniques. This aligns with the value item 'Healthy' and its corresponding value 'Security'.,"The analysis of 'Paper X' directly aligns with the value item Healthy and its corresponding value Security from a ""Software User"" perspective because the paper focuses on identifying and analyzing Android malware and their techniques, which ultimately contributes to the protection and well-being of software users. By understanding and addressing the security vulnerabilities in Android applications, the paper aims to create a safer and more secure software environment for users, ensuring their devices remain healthy and free from potential threats.",Agreed-Justified,Disagree,reconcile with coder_2,Disagree,"The justification claiming that the paper aligns with the value 'Security' because of 'The paper contributes to enhancing the protection of Android software users by investigating and identifying malware instances and their techniques.It doesn't align with the value item of the Healthy.
",
Exp B,Exp H,973,ICSE,Security & Privacy,Rotten apples spoil the bunch: An anatomy of Google Play malware,"This paper provides an in-depth analysis of Android malware that bypassed the strictest defenses of the Google Play application store and penetrated the official Android market between January 2016 and July 2021. We systematically identified 1,238 such malicious applications, grouped them into 134 families, and manually analyzed one application from 105 distinct families. During our manual analysis, we identified malicious payloads the applications execute, conditions guarding execution of the payloads, hiding techniques applications employ to evade detection by the user, and other implementation-level properties relevant for automated malware detection. As most applications in our dataset contain multiple payloads, each triggered via its own complex activation logic, we also contribute a graph-based representation showing activation paths for all application payloads in form of a control- and data-flow graph. Furthermore, we discuss the capabilities of existing malware detection tools, put them in context of the properties observed in the analyzed malware, and identify gaps and future research directions. We believe that our detailed analysis of the recent, evasive malware will be of interest to researchers and practitioners and will help further improve malware detection tools.",Security,National Security,The paper contributes to the broader cybersecurity field; thus promoting a safer digital environment. This can align with the value item 'National Security' from the value 'Security'.,"In analyzing the contributions of 'Paper X', it is evident that the paper's focus on identifying and analyzing Android malware that bypassed the strict defenses of the Google Play store contributes to the overall goal of promoting a safer digital environment. By analyzing the malware's capabilities, activation paths, and hiding techniques, the paper aims to improve malware detection tools. These efforts directly align with the value item of National Security, as the paper's findings can help protect users' devices and strengthen the security measures against malicious applications, thus contributing to the broader goal of ensuring the security and integrity of digital systems and data.",Agreed-Justified,Agree,,Agree,,
Exp C,Exp K,973,ICSE,Security & Privacy,Rotten apples spoil the bunch: An anatomy of Google Play malware,"This paper provides an in-depth analysis of Android malware that bypassed the strictest defenses of the Google Play application store and penetrated the official Android market between January 2016 and July 2021. We systematically identified 1,238 such malicious applications, grouped them into 134 families, and manually analyzed one application from 105 distinct families. During our manual analysis, we identified malicious payloads the applications execute, conditions guarding execution of the payloads, hiding techniques applications employ to evade detection by the user, and other implementation-level properties relevant for automated malware detection. As most applications in our dataset contain multiple payloads, each triggered via its own complex activation logic, we also contribute a graph-based representation showing activation paths for all application payloads in form of a control- and data-flow graph. Furthermore, we discuss the capabilities of existing malware detection tools, put them in context of the properties observed in the analyzed malware, and identify gaps and future research directions. We believe that our detailed analysis of the recent, evasive malware will be of interest to researchers and practitioners and will help further improve malware detection tools.",Achievement,Capable,Through the analysis and identification of malware techniques and properties; the user can potentially grow more capable in dealing with cyber threats which aligns with the value item 'Capable' associated with 'Achievement'.,"In the context of a software user, the alignment of 'Paper X' with the value item Capable and its corresponding value Achievement is justified because the paper's contributions directly focus on identifying and analyzing Android malware, highlighting the malicious payloads, execution conditions, hiding techniques, and other implementation-level properties. By gaining this knowledge and understanding of the malware landscape, users can become more capable in recognizing and dealing with cyber threats, enhancing their ability to achieve a sense of security and success in using software without falling victim to malware.",Agreed-Justified,Agree,,Agree,,
Exp D,Exp K,976,ICSE,Software Project Management,This is damn slick! estimating the impact of tweets on open source project popularity and new contributors,"Twitter is widely used by software developers. But how effective are tweets at promoting open source projects? How could one use Twitter to increase a project's popularity or attract new contributors? In this paper we report on a mixed-methods empirical study of 44,544 tweets containing links to 2,370 open-source GitHub repositories, looking for evidence of causal effects of these tweets on the projects attracting new GitHub stars and contributors, as well as characterizing the high-impact tweets, the people likely being attracted by them, and how they differ from contributors attracted otherwise. Among others, we find that tweets have a statistically significant and practically sizable effect on obtaining new stars and a small average effect on attracting new contributors. The popularity, content of the tweet, as well as the identity of tweet authors all affect the scale of the attraction effect. In addition, our qualitative analysis suggests that forming an active Twitter community for an open source project plays an important role in attracting new committers via tweets. We also report that developers who are new to GitHub or have a long history of Twitter usage but few tweets posted are most likely to be attracted as contributors to the repositories mentioned by tweets. Our work contributes to the literature on open source sustainability.",Achievement,Successful,The paper explores the significance of tweets in attracting new GitHub stars and contributors to open source projects; therefore contributing to a user's Achievement value as it enhances their success in promoting their projects.,"The main contribution of 'Paper X' is examining the impact of tweets on the success and popularity of open source projects. This aligns with the value item Successful and its corresponding value Achievement from a ""Software User"" perspective because it demonstrates how effectively utilizing Twitter can lead to the achievement of goals, such as attracting new contributors and receiving more GitHub stars, ultimately enhancing the user's success in promoting their projects.",Agreed-Justified,Agree,,Agree,,
Exp E,Exp J,976,ICSE,Software Project Management,This is damn slick! estimating the impact of tweets on open source project popularity and new contributors,"Twitter is widely used by software developers. But how effective are tweets at promoting open source projects? How could one use Twitter to increase a project's popularity or attract new contributors? In this paper we report on a mixed-methods empirical study of 44,544 tweets containing links to 2,370 open-source GitHub repositories, looking for evidence of causal effects of these tweets on the projects attracting new GitHub stars and contributors, as well as characterizing the high-impact tweets, the people likely being attracted by them, and how they differ from contributors attracted otherwise. Among others, we find that tweets have a statistically significant and practically sizable effect on obtaining new stars and a small average effect on attracting new contributors. The popularity, content of the tweet, as well as the identity of tweet authors all affect the scale of the attraction effect. In addition, our qualitative analysis suggests that forming an active Twitter community for an open source project plays an important role in attracting new committers via tweets. We also report that developers who are new to GitHub or have a long history of Twitter usage but few tweets posted are most likely to be attracted as contributors to the repositories mentioned by tweets. Our work contributes to the literature on open source sustainability.",Achievement,Influential,The paper identifies and characterizes the high-impact tweets and people likely being attracted by them; thereby developing a user's influential power; which aligns with the value item Influential in Achievement.,"The justification for aligning 'Paper X' with the value item Influential and its corresponding value Achievement from a ""Software User"" perspective is based on the paper's focus on the effectiveness of tweets in promoting open source projects and attracting new contributors. By identifying and characterizing high-impact tweets and the people likely being attracted by them, the paper highlights the potential for users to have an influential role in driving the popularity and growth of open source projects. This aligns with the value item Influential, as users who are able to attract attention and engagement through their tweets can achieve a sense of influence and impact within the software community, which is in line with the value of Achievement.",Agreed-Clarified,Agree,,Agree,,
Exp F,Exp J,976,ICSE,Software Project Management,This is damn slick! estimating the impact of tweets on open source project popularity and new contributors,"Twitter is widely used by software developers. But how effective are tweets at promoting open source projects? How could one use Twitter to increase a project's popularity or attract new contributors? In this paper we report on a mixed-methods empirical study of 44,544 tweets containing links to 2,370 open-source GitHub repositories, looking for evidence of causal effects of these tweets on the projects attracting new GitHub stars and contributors, as well as characterizing the high-impact tweets, the people likely being attracted by them, and how they differ from contributors attracted otherwise. Among others, we find that tweets have a statistically significant and practically sizable effect on obtaining new stars and a small average effect on attracting new contributors. The popularity, content of the tweet, as well as the identity of tweet authors all affect the scale of the attraction effect. In addition, our qualitative analysis suggests that forming an active Twitter community for an open source project plays an important role in attracting new committers via tweets. We also report that developers who are new to GitHub or have a long history of Twitter usage but few tweets posted are most likely to be attracted as contributors to the repositories mentioned by tweets. Our work contributes to the literature on open source sustainability.",Power,Social Recognition,The paper shows that the popularity and content of a tweet; along with the identity of the tweet author; can affect the scale of the attractiveness effect. This aligns with the dissemination of social recognition (value item Social Recognition under the Power value) through the software user's tweets.,"In the paper abstract, it is mentioned that the popularity and content of a tweet, as well as the identity of the tweet author, affect the scale of the attractiveness effect. This aligns with the value item of Social Recognition under the Power value in Schwartz's Taxonomy because the dissemination of social recognition is facilitated by the software user's tweets. When a tweet gains popularity and attention, it can increase the social recognition and influence of the tweet author, thereby aligning with the Power value.",Agreed-Clarified,Agree,,Agree,,
Exp G,Exp K,980,ICSE,Security & Privacy,The extent of orphan vulnerabilities from code reuse in open source software,"Motivation: A key premise of open source software is the ability to copy code to other open source projects (white-box reuse). Such copying accelerates development of new projects, but the code flaws in the original projects, such as vulnerabilities, may also spread even if fixed in the projects from where the code was appropriated. The extent of the spread of vulnerabilities through code reuse, the potential impact of such spread, or avenues for mitigating risk of these secondary vulnerabilities has not been studied in the context of a nearly complete collection of open source code. Aim: We aim to find ways to detect the white-box reuse induced vulnerabilities, determine how prevalent they are, and explore how they may be addressed. Method: We rely on World of Code infrastructure that provides a curated and cross-referenced collection of nearly all open source software to conduct a case study of a few known vulnerabilities. To conduct our case study we develop a tool, VDiOS, to help identify and fix white-box-reuse-induced vulnerabilities that have been already patched in the original projects (orphan vulnerabilities). Results: We find numerous instances of orphan vulnerabilities even in currently active and in highly popular projects (over 1K stars). Even apparently inactive projects are still publicly available for others to use and spread the vulnerability further. The often long delay in fixing orphan vulnerabilities even in highly popular projects increases the chances of it spreading to new projects. We provided patches to a number of project maintainers and found that only a small percentage accepted and applied the patch. We hope that VDiOS will lead to further study and mitigation of risks from orphan vulnerabilities and other orphan code flaws.",Security,Social Order,The paper identifies and tackles the 'orphan vulnerabilities' in open source software which promotes 'Social Order'; a key concern related to 'Security' value by working towards making software safer for users.,"In 'Paper X', the identification and addressing of 'orphan vulnerabilities' in open source software contribute to promoting 'Social Order' as it aims to mitigate the risk of these vulnerabilities spreading and potentially causing harm to users. By focusing on fixing vulnerabilities that have been already patched in the original projects, the paper aligns with the value of 'Security' by working towards making software safer for users.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,980,ICSE,Security & Privacy,The extent of orphan vulnerabilities from code reuse in open source software,"Motivation: A key premise of open source software is the ability to copy code to other open source projects (white-box reuse). Such copying accelerates development of new projects, but the code flaws in the original projects, such as vulnerabilities, may also spread even if fixed in the projects from where the code was appropriated. The extent of the spread of vulnerabilities through code reuse, the potential impact of such spread, or avenues for mitigating risk of these secondary vulnerabilities has not been studied in the context of a nearly complete collection of open source code. Aim: We aim to find ways to detect the white-box reuse induced vulnerabilities, determine how prevalent they are, and explore how they may be addressed. Method: We rely on World of Code infrastructure that provides a curated and cross-referenced collection of nearly all open source software to conduct a case study of a few known vulnerabilities. To conduct our case study we develop a tool, VDiOS, to help identify and fix white-box-reuse-induced vulnerabilities that have been already patched in the original projects (orphan vulnerabilities). Results: We find numerous instances of orphan vulnerabilities even in currently active and in highly popular projects (over 1K stars). Even apparently inactive projects are still publicly available for others to use and spread the vulnerability further. The often long delay in fixing orphan vulnerabilities even in highly popular projects increases the chances of it spreading to new projects. We provided patches to a number of project maintainers and found that only a small percentage accepted and applied the patch. We hope that VDiOS will lead to further study and mitigation of risks from orphan vulnerabilities and other orphan code flaws.",Achievement,Successful,The tool 'VDiOS' in the paper helps users to successfully identify and fix white-box-reuse-induced vulnerabilities; which aligns with the value item 'Successful' falling under the value category 'Achievement'.,"The justification for labeling 'Paper X' as aligning with the value item Successful and its corresponding value Achievement from a ""Software User"" perspective is based on the fact that the tool 'VDiOS' developed in the paper aids users in successfully identifying and fixing white-box-reuse-induced vulnerabilities. This aligns with the value item 'Successful' as users are able to achieve the goal of effectively addressing vulnerabilities in their software, contributing to their sense of accomplishment and advancement in their software usage.",Agreed-Clarified,Agree,,Agree,,
Exp B,Exp H,980,ICSE,Security & Privacy,The extent of orphan vulnerabilities from code reuse in open source software,"Motivation: A key premise of open source software is the ability to copy code to other open source projects (white-box reuse). Such copying accelerates development of new projects, but the code flaws in the original projects, such as vulnerabilities, may also spread even if fixed in the projects from where the code was appropriated. The extent of the spread of vulnerabilities through code reuse, the potential impact of such spread, or avenues for mitigating risk of these secondary vulnerabilities has not been studied in the context of a nearly complete collection of open source code. Aim: We aim to find ways to detect the white-box reuse induced vulnerabilities, determine how prevalent they are, and explore how they may be addressed. Method: We rely on World of Code infrastructure that provides a curated and cross-referenced collection of nearly all open source software to conduct a case study of a few known vulnerabilities. To conduct our case study we develop a tool, VDiOS, to help identify and fix white-box-reuse-induced vulnerabilities that have been already patched in the original projects (orphan vulnerabilities). Results: We find numerous instances of orphan vulnerabilities even in currently active and in highly popular projects (over 1K stars). Even apparently inactive projects are still publicly available for others to use and spread the vulnerability further. The often long delay in fixing orphan vulnerabilities even in highly popular projects increases the chances of it spreading to new projects. We provided patches to a number of project maintainers and found that only a small percentage accepted and applied the patch. We hope that VDiOS will lead to further study and mitigation of risks from orphan vulnerabilities and other orphan code flaws.",Stimulation,Variation in Life,By building a tool to identify and fix vulnerabilities copied from other open source projects; the paper introduces 'Variation in Life' to software users; aligning with the value 'Stimulation'.,"In the context of software development, the introduction of a tool that detects and fixes vulnerabilities copied from other open source projects aligns with the value item ""Variation in Life"" and its corresponding value ""Stimulation"" from a ""Software User"" perspective. This is because the tool brings diversity and novelty to the software development process, providing users with new challenges and opportunities for growth and learning. By addressing vulnerabilities and promoting the reuse of code, software users are encouraged to explore different approaches and solutions, stimulating their creativity and curiosity in the software development domain.",Agreed-Clarified,Agree,,Agree,,
Exp C,Exp K,981,ICSE,Security & Privacy,DescribeCtx: context-aware description synthesis for sensitive behaviors in mobile apps,"While mobile applications (i.e., apps) are becoming capable of handling various needs from users, their increasing access to sensitive data raises privacy concerns. To inform such sensitive behaviors to users, existing techniques propose to automatically identify explanatory sentences from app descriptions; however, many sensitive behaviors are not explained in the corresponding app descriptions. There also exist general techniques that translate code to sentences. However, these techniques lack the vocabulary to explain the uses of sensitive data and fail to consider the context (i.e., the app functionalities) of the sensitive behaviors. To address these limitations, we propose Describectx, a context-aware description synthesis approach that trains a neural machine translation model using a large set of popular apps, and generates app-specific descriptions for sensitive behaviors. Specifically, Describectx encodes three heterogeneous sources as input, i.e., vocabularies provided by privacy policies, behavior summary provided by the call graphs in code, and contextual information provided by GUI texts. Our evaluations on 1,262 Android apps show that, compared with existing baselines, Describectx produces more accurate descriptions (24.96 in BLEU) and achieves higher user ratings with respect to the reference sen-tences manually identified in the app descriptions.",Self Direction,Privacy,"The paper focuses on the synthesis of app-specific descriptions for sensitive behaviors; thus facilitating privacy by making it easier for users to understand how their data is handled. This aligns with the value item ""Privacy"" in ""Self Direction"".","In 'Paper X', the authors propose Describectx, a context-aware description synthesis approach for mobile applications. This approach aims to address privacy concerns by generating app-specific descriptions for sensitive behaviors, providing users with a better understanding of how their data is handled. By aligning with the value item ""Privacy"" in ""Self Direction"", the paper directly contributes to empowering software users to make informed decisions and exercise control over their personal data, promoting autonomy and freedom in their mobile app usage.",Agreed-Justified,Agree,,Agree,,
Exp E,Exp J,981,ICSE,Security & Privacy,DescribeCtx: context-aware description synthesis for sensitive behaviors in mobile apps,"While mobile applications (i.e., apps) are becoming capable of handling various needs from users, their increasing access to sensitive data raises privacy concerns. To inform such sensitive behaviors to users, existing techniques propose to automatically identify explanatory sentences from app descriptions; however, many sensitive behaviors are not explained in the corresponding app descriptions. There also exist general techniques that translate code to sentences. However, these techniques lack the vocabulary to explain the uses of sensitive data and fail to consider the context (i.e., the app functionalities) of the sensitive behaviors. To address these limitations, we propose Describectx, a context-aware description synthesis approach that trains a neural machine translation model using a large set of popular apps, and generates app-specific descriptions for sensitive behaviors. Specifically, Describectx encodes three heterogeneous sources as input, i.e., vocabularies provided by privacy policies, behavior summary provided by the call graphs in code, and contextual information provided by GUI texts. Our evaluations on 1,262 Android apps show that, compared with existing baselines, Describectx produces more accurate descriptions (24.96 in BLEU) and achieves higher user ratings with respect to the reference sen-tences manually identified in the app descriptions.",Universalism,Equality,"By facilitating transparency in how apps handle sensitive data; the paper advocates for fairness and equality in the usage of digital services. This aligns with the value item ""Equality"" under ""Universalism"".","The main contribution of 'Paper X' is the development of a context-aware description synthesis approach that generates app-specific descriptions for sensitive behaviors. By providing this information to users, the paper aims to address privacy concerns and enable users to make informed decisions about their data. This aligns with the value item ""Equality"" under ""Universalism"" as it promotes fairness and equality in the usage of digital services, ensuring that all users have access to transparent information about how their sensitive data is being handled by mobile applications.",Agreed-Justified,Agree,,Agree,,
Exp F,Exp J,984,ICSE,Code Generation & Analysis,DrAsync: identifying and visualizing anti-patterns in asynchronous JavaScript,"Promises and async/await have become popular mechanisms for implementing asynchronous computations in JavaScript, but despite their popularity, programmers have difficulty using them. This paper identifies 8 anti-patterns in promise-based JavaScript code that are prevalent across popular JavaScript repositories. We present a light-weight static analysis for automatically detecting these anti-patterns. This analysis is embedded in an interactive visualization tool that additionally relies on dynamic analysis to visualize promise lifetimes and instances of anti-patterns executed at run time. By enabling the user to navigate between promises in the visualization and the source code fragments that they originate from, problems and optimization opportunities can be identified. We implement this approach in a tool called DrAsync, and found 2.6K static instances of anti-patterns in 20 popular JavaScript repositories. Upon examination of a subset of these, we found that the majority of problematic code reported by DrAsync could be eliminated through refactoring. Further investigation revealed that, in a few cases, the elimination of anti-patterns reduced the time needed to execute the refactored code fragments. Moreover, DrAsync's visualization of promise lifetimes and relationships provides additional insight into the execution behavior of asynchronous programs and helped identify further optimization opportunities.",Self Direction,Independent,The paper contributes to the value 'Self Direction' by implementing an interactive tool that cues users on problems and optimization opportunities enabling them to get independent in understanding and navigation through the problematic code.,"In 'Paper X', the implementation of the interactive visualization tool, DrAsync, aligns with the value item Independent and its corresponding value Self Direction from a ""Software User"" perspective. This alignment is evident as the tool empowers users to independently understand and navigate through the problematic code by providing cues on problems and optimization opportunities. Users are able to explore promises in the visualization and easily trace their origins in the source code fragments, facilitating a sense of autonomy and control in problem-solving and code optimization. By leveraging DrAsync, users can actively engage in the software development process, taking ownership of their work and making independent decisions based on the insights gained from the visualization tool.",Agreed-Clarified,Agree,,Agree,,
Exp G,Exp K,984,ICSE,Code Generation & Analysis,DrAsync: identifying and visualizing anti-patterns in asynchronous JavaScript,"Promises and async/await have become popular mechanisms for implementing asynchronous computations in JavaScript, but despite their popularity, programmers have difficulty using them. This paper identifies 8 anti-patterns in promise-based JavaScript code that are prevalent across popular JavaScript repositories. We present a light-weight static analysis for automatically detecting these anti-patterns. This analysis is embedded in an interactive visualization tool that additionally relies on dynamic analysis to visualize promise lifetimes and instances of anti-patterns executed at run time. By enabling the user to navigate between promises in the visualization and the source code fragments that they originate from, problems and optimization opportunities can be identified. We implement this approach in a tool called DrAsync, and found 2.6K static instances of anti-patterns in 20 popular JavaScript repositories. Upon examination of a subset of these, we found that the majority of problematic code reported by DrAsync could be eliminated through refactoring. Further investigation revealed that, in a few cases, the elimination of anti-patterns reduced the time needed to execute the refactored code fragments. Moreover, DrAsync's visualization of promise lifetimes and relationships provides additional insight into the execution behavior of asynchronous programs and helped identify further optimization opportunities.",Self Direction,Choosing Own Goals,The paper aligns with 'Choosing Own Goals' as it identifies problematic code and optimization opportunities; helping the software user to set their own goals toward eliminating the issues and optimizing the code.,"The paper aligns with 'Choosing Own Goals' as it provides a tool that identifies problematic code and optimization opportunities. By visualizing promise lifetimes and instances of anti-patterns, the software user is empowered to navigate through the code and make informed decisions on how to set their own goals for eliminating the identified issues and optimizing the code. This aligns with the value of self-direction, as the software user has the freedom and autonomy to choose their own goals and make improvements to their software.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,984,ICSE,Code Generation & Analysis,DrAsync: identifying and visualizing anti-patterns in asynchronous JavaScript,"Promises and async/await have become popular mechanisms for implementing asynchronous computations in JavaScript, but despite their popularity, programmers have difficulty using them. This paper identifies 8 anti-patterns in promise-based JavaScript code that are prevalent across popular JavaScript repositories. We present a light-weight static analysis for automatically detecting these anti-patterns. This analysis is embedded in an interactive visualization tool that additionally relies on dynamic analysis to visualize promise lifetimes and instances of anti-patterns executed at run time. By enabling the user to navigate between promises in the visualization and the source code fragments that they originate from, problems and optimization opportunities can be identified. We implement this approach in a tool called DrAsync, and found 2.6K static instances of anti-patterns in 20 popular JavaScript repositories. Upon examination of a subset of these, we found that the majority of problematic code reported by DrAsync could be eliminated through refactoring. Further investigation revealed that, in a few cases, the elimination of anti-patterns reduced the time needed to execute the refactored code fragments. Moreover, DrAsync's visualization of promise lifetimes and relationships provides additional insight into the execution behavior of asynchronous programs and helped identify further optimization opportunities.",Achievement,Intelligent,By providing insights into the execution behavior of asynchronous programs; the paper aids in making the software user intelligent and increases understanding of the system; aligning with the 'Achievement' value under 'Intelligent'.,"In the context of a software user, the alignment of 'Paper X' with the value item Intelligent and its corresponding value Achievement is justified by the fact that the paper provides insights into the execution behavior of asynchronous programs. This understanding and knowledge gained from the paper can contribute to the software user's intelligence and ability to effectively navigate and comprehend the system, thus aligning with the Achievement value under Intelligent.",Agreed-Clarified,Agree,,Agree,,
Exp B,Exp H,985,ICSE,Software Deployment & Operations,Bots for pull requests: The good the bad and the promising,"Software bots automate tasks within Open Source Software (OSS) projects' pull requests and save reviewing time and effort (‚Äö√Ñ√∂‚àö√ë‚àö‚àÇ‚Äö√†√∂‚àö√´‚Äö√†√∂‚Äö√†√á‚Äö√Ñ√∂‚àö‚Ä†‚àö‚àÇ‚Äö√†√∂‚àö¬¥‚Äö√Ñ√∂‚àö‚Ä†‚àö‚àÇ‚Äö√Ñ√∂‚àö‚Ä†¬¨¬•the good‚Äö√Ñ√∂‚àö√ë‚àö‚àÇ‚Äö√†√∂‚àö√´‚Äö√†√∂‚Äö√†√á‚Äö√Ñ√∂‚àö‚Ä†‚àö‚àÇ‚Äö√†√∂‚àö¬¥‚Äö√Ñ√∂‚àö‚Ä†‚àö‚àÇ‚âà√¨‚àö√ë). However, their interactions can be disruptive and noisy and lead to information overload (‚Äö√Ñ√∂‚àö√ë‚àö‚àÇ‚Äö√†√∂‚àö√´‚Äö√†√∂‚Äö√†√á‚Äö√Ñ√∂‚àö‚Ä†‚àö‚àÇ‚Äö√†√∂‚àö¬¥‚Äö√Ñ√∂‚àö‚Ä†‚àö‚àÇ‚Äö√Ñ√∂‚àö‚Ä†¬¨¬•the bad‚Äö√Ñ√∂‚àö√ë‚àö‚àÇ‚Äö√†√∂‚àö√´‚Äö√†√∂‚Äö√†√á‚Äö√Ñ√∂‚àö‚Ä†‚àö‚àÇ‚Äö√†√∂‚àö¬¥‚Äö√Ñ√∂‚àö‚Ä†‚àö‚àÇ‚âà√¨‚àö√ë). To identify strategies to overcome such problems, we applied Design Fiction as a participatory method with 32 practitioners. We elicited 22 design strategies for a bot mediator or the pull request user interface (‚Äö√Ñ√∂‚àö√ë‚àö‚àÇ‚Äö√†√∂‚àö√´‚Äö√†√∂‚Äö√†√á‚Äö√Ñ√∂‚àö‚Ä†‚àö‚àÇ‚Äö√†√∂‚àö¬¥‚Äö√Ñ√∂‚àö‚Ä†‚àö‚àÇ‚Äö√Ñ√∂‚àö‚Ä†¬¨¬•the promising‚Äö√Ñ√∂‚àö√ë‚àö‚àÇ‚Äö√†√∂‚àö√´‚Äö√†√∂‚Äö√†√á‚Äö√Ñ√∂‚àö‚Ä†‚àö‚àÇ‚Äö√†√∂‚àö¬¥‚Äö√Ñ√∂‚àö‚Ä†‚àö‚àÇ‚âà√¨‚àö√ë). Participants envisioned a separate place in the pull request interface for bot interactions and a bot mediator that can summarize and customize other bots' actions to mitigate noise. We also collected participants' perceptions about a prototype implementing the envisioned strategies. Our design strategies can guide the development of future bots and social coding platforms.",Stimulation,Excitement in Life,The paper contributes to excitement in life by introducing design strategies for future bots and social coding platforms; making the software user experience more exhilarating.,"The paper's contributions align with the value item of Excitement in Life and its corresponding value of Stimulation because it introduces design strategies for future bots and social coding platforms, which can enhance the user experience and make it more exhilarating. By incorporating these strategies, software users can have a more dynamic and engaging interaction with the software, leading to a heightened sense of excitement and stimulation in their software-related activities.",Agreed-Justified,Agree,,Agree,,
Exp C,Exp K,985,ICSE,Software Deployment & Operations,Bots for pull requests: The good the bad and the promising,"Software bots automate tasks within Open Source Software (OSS) projects' pull requests and save reviewing time and effort (‚Äö√Ñ√∂‚àö√ë‚àö‚à´the good‚Äö√Ñ√∂‚àö√ë‚àöœÄ). However, their interactions can be disruptive and noisy and lead to information overload (‚Äö√Ñ√∂‚àö√ë‚àö‚à´the bad‚Äö√Ñ√∂‚àö√ë‚àöœÄ). To identify strategies to overcome such problems, we applied Design Fiction as a participatory method with 32 practitioners. We elicited 22 design strategies for a bot mediator or the pull request user interface (‚Äö√Ñ√∂‚àö√ë‚àö‚à´the promising‚Äö√Ñ√∂‚àö√ë‚àöœÄ). Participants envisioned a separate place in the pull request interface for bot interactions and a bot mediator that can summarize and customize other bots' actions to mitigate noise. We also collected participants' perceptions about a prototype implementing the envisioned strategies. Our design strategies can guide the development of future bots and social coding platforms.",Security,Healthy,"The abstract mentions of a prototype that implements the envisioned strategies; which potentially improve the user interface. This aligns with the value item ""Healthy"" under the value ""Security"" as improved user interface can reduce stress and subsequent health issues among software users.","In the context of a software user, the value item ""Healthy"" aligns with the main contributions of 'Paper X' as it mentions the potential improvement of the user interface through the implementation of the envisioned strategies. A better user interface can contribute to reducing stress and potential health issues that may arise from using software. By focusing on enhancing the user experience and reducing potential disruptions and noise caused by software bots, the paper addresses the value of security, which includes aspects related to the well-being and overall health of software users.",Agreed-Clarified,Agree,,Agree,,
Exp D,Exp K,985,ICSE,Software Deployment & Operations,Bots for pull requests: The good the bad and the promising,"Software bots automate tasks within Open Source Software (OSS) projects' pull requests and save reviewing time and effort (‚Äö√Ñ√∂‚àö√ë‚àö‚à´the good‚Äö√Ñ√∂‚àö√ë‚àöœÄ). However, their interactions can be disruptive and noisy and lead to information overload (‚Äö√Ñ√∂‚àö√ë‚àö‚à´the bad‚Äö√Ñ√∂‚àö√ë‚àöœÄ). To identify strategies to overcome such problems, we applied Design Fiction as a participatory method with 32 practitioners. We elicited 22 design strategies for a bot mediator or the pull request user interface (‚Äö√Ñ√∂‚àö√ë‚àö‚à´the promising‚Äö√Ñ√∂‚àö√ë‚àöœÄ). Participants envisioned a separate place in the pull request interface for bot interactions and a bot mediator that can summarize and customize other bots' actions to mitigate noise. We also collected participants' perceptions about a prototype implementing the envisioned strategies. Our design strategies can guide the development of future bots and social coding platforms.",Conformity,Self-Discipline,"The research work suggests the implementation of a bot mediator which can help in managing other bots' actions. This contribution aligns with ""Self-Discipline""; as it supports software users in managing the use of software bots; enhancing their self-regulatory capacity.","The justification for labeling 'Paper X' as aligning with the value item Self-Discipline and its corresponding value Conformity from a ""Software User"" perspective is based on the fact that the research work proposes the implementation of a bot mediator to assist users in managing other bots' actions. By providing a separate place in the pull request interface for bot interactions and summarizing and customizing their actions, the proposed bot mediator promotes self-discipline by allowing users to regulate and control the use of software bots within the software context. This aligns with the value of Conformity, as it emphasizes the adherence to norms and rules set by the software user in managing and organizing the bot-mediated interactions.",Agreed-Justified,Agree,,Agree,,
Exp E,Exp J,988,ICSE,Security & Privacy,A grounded theory based approach to characterize software attack surfaces,"The notion of Attack Surface refers to the critical points on the boundary of a software system which are accessible from outside or contain valuable content for attackers. The ability to identify attack surface components of software system has a significant role in effectiveness of vulnerability analysis approaches. Most prior works focus on vulnerability techniques that use an approximation of attack surfaces and there have not been many attempts to create a comprehensive list of attack surface components. Although limited number of studies have focused on attack surface analysis, they defined attack surface components based on project specific hypotheses to evaluate security risk of specific types of software applications. In this study, we leverage a qualitative analysis approach to empirically identify an extensive list of attack surface components. To this end, we conduct a Grounded Theory (GT) analysis on 1444 previously published vulnerability reports and weaknesses with a team of three software developers and security experts. We extract vulnerability information from two publicly available repositories: 1) Common Vulnerabilities and Exposures (CVE) and 2) Common Weakness Enumeration (CWE). We ask three key questions: where the attacks come from, what they target, and how they emerge, and to help answer these questions we define three core categories for attack surface components: Entry points, Targets, and Mechanisms. We extract attack surface concepts related to each category from collected vulnerability information using the GT analysis and provide a comprehensive categorization that represents attack surface components of software systems from various perspectives. The paper introduces 254 new attack surface components that did not exist in the literature. The comparison of the proposed attack surface model with prior works indicates that only 6.7% of the identified Code level attack surface components are studied before.",Security,Healthy,The paper contributes an extensive list of attack surface components; which could help enhance software security; thereby ensuring the healthy use of software for users.,"Certainly! The paper's extensive list of attack surface components directly aligns with the value item ""Healthy"" and its corresponding value ""Security"" from a ""Software User"" perspective. By identifying critical points in software systems accessible to attackers, the paper helps enhance software security, thereby ensuring the healthy use of software for users. This directly addresses the user's need for a secure and protected software environment, contributing to their overall well-being and peace of mind while using software applications.",Agreed-Justified,Agree,,Agree,,
Exp F,Exp J,988,ICSE,Security & Privacy,A grounded theory based approach to characterize software attack surfaces,"The notion of Attack Surface refers to the critical points on the boundary of a software system which are accessible from outside or contain valuable content for attackers. The ability to identify attack surface components of software system has a significant role in effectiveness of vulnerability analysis approaches. Most prior works focus on vulnerability techniques that use an approximation of attack surfaces and there have not been many attempts to create a comprehensive list of attack surface components. Although limited number of studies have focused on attack surface analysis, they defined attack surface components based on project specific hypotheses to evaluate security risk of specific types of software applications. In this study, we leverage a qualitative analysis approach to empirically identify an extensive list of attack surface components. To this end, we conduct a Grounded Theory (GT) analysis on 1444 previously published vulnerability reports and weaknesses with a team of three software developers and security experts. We extract vulnerability information from two publicly available repositories: 1) Common Vulnerabilities and Exposures (CVE) and 2) Common Weakness Enumeration (CWE). We ask three key questions: where the attacks come from, what they target, and how they emerge, and to help answer these questions we define three core categories for attack surface components: Entry points, Targets, and Mechanisms. We extract attack surface concepts related to each category from collected vulnerability information using the GT analysis and provide a comprehensive categorization that represents attack surface components of software systems from various perspectives. The paper introduces 254 new attack surface components that did not exist in the literature. The comparison of the proposed attack surface model with prior works indicates that only 6.7% of the identified Code level attack surface components are studied before.",Security,Social Order,By identifying the attack surface components; the paper contributes to maintaining the social order of the software user community by preventing potential software vulnerabilities.,"The identification of attack surface components by the paper contributes to maintaining social order within the software user community by preventing potential software vulnerabilities. By understanding where attacks come from, what they target, and how they emerge, the paper helps to enhance the security of software systems, which in turn protects the software user community from potential breaches and unauthorized access. This contributes to maintaining a sense of order and stability within the community, as users can feel confident in the security of the software they are using.",Agreed-Justified,Agree,,Agree,,
Exp G,Exp K,988,ICSE,Security & Privacy,A grounded theory based approach to characterize software attack surfaces,"The notion of Attack Surface refers to the critical points on the boundary of a software system which are accessible from outside or contain valuable content for attackers. The ability to identify attack surface components of software system has a significant role in effectiveness of vulnerability analysis approaches. Most prior works focus on vulnerability techniques that use an approximation of attack surfaces and there have not been many attempts to create a comprehensive list of attack surface components. Although limited number of studies have focused on attack surface analysis, they defined attack surface components based on project specific hypotheses to evaluate security risk of specific types of software applications. In this study, we leverage a qualitative analysis approach to empirically identify an extensive list of attack surface components. To this end, we conduct a Grounded Theory (GT) analysis on 1444 previously published vulnerability reports and weaknesses with a team of three software developers and security experts. We extract vulnerability information from two publicly available repositories: 1) Common Vulnerabilities and Exposures (CVE) and 2) Common Weakness Enumeration (CWE). We ask three key questions: where the attacks come from, what they target, and how they emerge, and to help answer these questions we define three core categories for attack surface components: Entry points, Targets, and Mechanisms. We extract attack surface concepts related to each category from collected vulnerability information using the GT analysis and provide a comprehensive categorization that represents attack surface components of software systems from various perspectives. The paper introduces 254 new attack surface components that did not exist in the literature. The comparison of the proposed attack surface model with prior works indicates that only 6.7% of the identified Code level attack surface components are studied before.",Security,National Security,The identification of attack surface components serves as a measure to ensure national security; preventing malicious attacks via software vulnerabilities.,"The identification and analysis of attack surface components in software systems directly align with the value item National Security and its corresponding value Security from a ""Software User"" perspective. By understanding and identifying these components, software users can take necessary measures to prevent malicious attacks and ensure the security of their systems, protecting sensitive information and contributing to national security efforts.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,989,ICSE,Code Generation & Analysis,Imperative versus declarative collection processing: an RCT on the understandability of traditional loops versus the stream API in Java,"Java introduced in version 8 with the Stream API means to operate on collections using lambda expressions. Since then, this API is an alternative way to handle collections in a more declarative manner instead of the traditional, imperative style using loops. However, whether the Stream API is beneficial in comparison to loops in terms of usability is unclear. The present paper introduces a randomized control trial (RCT) on the understandability of collection operations performed on 20 participants with the dependent variables response time and correctness. As tasks, subjects had to determine the results for collection operations (either defined with the Stream API or with loops). The results indicate that the Stream API has a significant $(\mathrm{p} <. 001)$ and large $(\eta_{p}^{2}=.695;\frac{M_{loop}}{M_{stream}}\ \sim 178\%)$ positive effect on the response times. Furthermore, the usage of the Stream API caused significantly less errors. And finally, the participants perceived their speed with the Stream API higher compared to the loop-based code and the participants considered the code based on the Stream API as more readable. Hence, while existing studies found a negative effect of declarative constructs (in terms of lambda expressions) on the usability of a main stream programming language, the present study found the opposite: the present study gives evidence that declarative code on collections using the Stream API based on lambda expressions has a large, positive effect in comparison to traditional loops.",Achievement,Intelligent,The paper presents a trial that shows the Stream API in Java increases the understandability of collection operations; which align with 'Intelligent' from the 'Achievement' value in Schwartz's Taxonomy for the users.,"The justification for aligning 'Paper X' with the value item Intelligent and its corresponding value Achievement is based on the trial conducted in the paper, which demonstrates that the Stream API in Java enhances the understandability of collection operations. This aligns with the 'Intelligent' value in the 'Achievement' category of Schwartz's Taxonomy as it reflects the users' desire to engage in activities that require intellectual stimulation and the ability to solve complex problems, which is achieved by using the Stream API to process collections in a more declarative and efficient manner.",Agreed-Clarified,Agree,,Agree,,
Exp B,Exp H,989,ICSE,Code Generation & Analysis,Imperative versus declarative collection processing: an RCT on the understandability of traditional loops versus the stream API in Java,"Java introduced in version 8 with the Stream API means to operate on collections using lambda expressions. Since then, this API is an alternative way to handle collections in a more declarative manner instead of the traditional, imperative style using loops. However, whether the Stream API is beneficial in comparison to loops in terms of usability is unclear. The present paper introduces a randomized control trial (RCT) on the understandability of collection operations performed on 20 participants with the dependent variables response time and correctness. As tasks, subjects had to determine the results for collection operations (either defined with the Stream API or with loops). The results indicate that the Stream API has a significant $(\mathrm{p} <. 001)$ and large $(\eta_{p}^{2}=.695;\frac{M_{loop}}{M_{stream}}\ \sim 178\%)$ positive effect on the response times. Furthermore, the usage of the Stream API caused significantly less errors. And finally, the participants perceived their speed with the Stream API higher compared to the loop-based code and the participants considered the code based on the Stream API as more readable. Hence, while existing studies found a negative effect of declarative constructs (in terms of lambda expressions) on the usability of a main stream programming language, the present study found the opposite: the present study gives evidence that declarative code on collections using the Stream API based on lambda expressions has a large, positive effect in comparison to traditional loops.",Achievement,Successful,The paper highlights that Stream API usage reduces errors and improves response times which aligns with 'Successful' from the 'Achievement' value in Schwartz's Taxonomy for the users.,"In the paper abstract, it is explicitly stated that the usage of the Stream API resulted in significantly less errors and improved response times. This aligns with the value item 'Successful' and its corresponding value 'Achievement' from Schwartz's Taxonomy because it indicates that the users of the software, in this case, the participants, were able to successfully achieve their goals of accurately and efficiently performing the collection operations.",Agreed-Justified,Agree,,Agree,,
Exp C,Exp K,989,ICSE,Code Generation & Analysis,Imperative versus declarative collection processing: an RCT on the understandability of traditional loops versus the stream API in Java,"Java introduced in version 8 with the Stream API means to operate on collections using lambda expressions. Since then, this API is an alternative way to handle collections in a more declarative manner instead of the traditional, imperative style using loops. However, whether the Stream API is beneficial in comparison to loops in terms of usability is unclear. The present paper introduces a randomized control trial (RCT) on the understandability of collection operations performed on 20 participants with the dependent variables response time and correctness. As tasks, subjects had to determine the results for collection operations (either defined with the Stream API or with loops). The results indicate that the Stream API has a significant $(\mathrm{p} <. 001)$ and large $(\eta_{p}^{2}=.695;\frac{M_{loop}}{M_{stream}}\ \sim 178\%)$ positive effect on the response times. Furthermore, the usage of the Stream API caused significantly less errors. And finally, the participants perceived their speed with the Stream API higher compared to the loop-based code and the participants considered the code based on the Stream API as more readable. Hence, while existing studies found a negative effect of declarative constructs (in terms of lambda expressions) on the usability of a main stream programming language, the present study found the opposite: the present study gives evidence that declarative code on collections using the Stream API based on lambda expressions has a large, positive effect in comparison to traditional loops.",Self Direction,Freedom,The paper discusses how the Stream API offers an alternative; more declarative way to handle collections; providing 'Freedom' of choice in coding style; which aligns with the 'Self Direction' value in Schwartz's Taxonomy; beneficial to the software user.,"In the paper, it is explicitly stated that the Stream API provides a more declarative way to handle collections, offering freedom of choice in coding style. This aligns with the value item of Freedom and its corresponding value of Self Direction in Schwartz's Taxonomy. By empowering software users with the freedom to choose their coding style, the Stream API enhances their ability to direct their own development process, ultimately benefiting them in terms of usability.",Agreed-Justified,Agree,,Agree,,
Exp D,Exp K,990,ICSE,Security & Privacy,Quantifying permissiveness of access control policies,"Due to ubiquitous use of software services, protecting the confidentiality of private information stored in compute clouds is becoming an increasingly critical problem. Although access control specification languages and libraries provide mechanisms for protecting confidentiality of information, without verification and validation techniques that can assist developers in writing policies, complex policy specifications are likely to have errors that can lead to unintended and unauthorized access to data, possibly with disastrous consequences. In this paper, we present a quantitative and differential policy analysis framework that not only identifies if one policy is more permissive than another policy, but also quantifies the relative permissiveness of access control policies. We quantify permissiveness of policies using a model counting constraint solver. We present a heuristic that transforms constraints extracted from access control policies and significantly improves the model counting performance. We demonstrate the effectiveness of our approach by applying it to policies written in Amazon's AWS Identity and Access Management (IAM) policy language and Microsoft's Azure policy language.",Power,Social Recognition,The paper presents a method of enhancing the security of software services; leading to increased trust and social recognition from software users.,"The paper's quantitative and differential policy analysis framework directly contributes to enhancing the security of software services, which is a critical concern for software users. By providing mechanisms to protect the confidentiality of private information stored in compute clouds, the paper addresses a pressing issue in the software context. The increased security and protection of user information instills trust in the software services, leading to a sense of social recognition from software users.",Agreed-Clarified,Agree,,Agree,,
Exp E,Exp J,990,ICSE,Security & Privacy,Quantifying permissiveness of access control policies,"Due to ubiquitous use of software services, protecting the confidentiality of private information stored in compute clouds is becoming an increasingly critical problem. Although access control specification languages and libraries provide mechanisms for protecting confidentiality of information, without verification and validation techniques that can assist developers in writing policies, complex policy specifications are likely to have errors that can lead to unintended and unauthorized access to data, possibly with disastrous consequences. In this paper, we present a quantitative and differential policy analysis framework that not only identifies if one policy is more permissive than another policy, but also quantifies the relative permissiveness of access control policies. We quantify permissiveness of policies using a model counting constraint solver. We present a heuristic that transforms constraints extracted from access control policies and significantly improves the model counting performance. We demonstrate the effectiveness of our approach by applying it to policies written in Amazon's AWS Identity and Access Management (IAM) policy language and Microsoft's Azure policy language.",Security,Healthy,By developing a method to improve the protection of private information; the paper aligns with user's concern for being healthy in a digital context.,"I apologize for not providing a clear justification earlier. The alignment of 'Paper X' with the value item Healthy and its corresponding value Security stems from the fact that the paper addresses the critical problem of protecting the confidentiality of private information stored in compute clouds. This protection is key to ensuring the well-being and health of users in a digital context, as unauthorized access to sensitive data can have disastrous consequences for individuals and their digital security. Therefore, by developing a method that improves the protection of private information, 'Paper X' directly aligns with the user's concern for being healthy within a software context.",Agreed-Justified,Agree,,Agree,,
Exp F,Exp J,990,ICSE,Security & Privacy,Quantifying permissiveness of access control policies,"Due to ubiquitous use of software services, protecting the confidentiality of private information stored in compute clouds is becoming an increasingly critical problem. Although access control specification languages and libraries provide mechanisms for protecting confidentiality of information, without verification and validation techniques that can assist developers in writing policies, complex policy specifications are likely to have errors that can lead to unintended and unauthorized access to data, possibly with disastrous consequences. In this paper, we present a quantitative and differential policy analysis framework that not only identifies if one policy is more permissive than another policy, but also quantifies the relative permissiveness of access control policies. We quantify permissiveness of policies using a model counting constraint solver. We present a heuristic that transforms constraints extracted from access control policies and significantly improves the model counting performance. We demonstrate the effectiveness of our approach by applying it to policies written in Amazon's AWS Identity and Access Management (IAM) policy language and Microsoft's Azure policy language.",Security,National Security,The paper emphasizes the need for increased confidentiality in compute clouds; aligning with the value item National Security in a digital context.,"In the abstract of 'Paper X', it is stated that protecting the confidentiality of private information stored in compute clouds is a critical problem. This aligns with the value item National Security from Schwartz's Taxonomy, as ensuring data security and preventing unauthorized access to sensitive information is crucial for maintaining national security in the digital context. Therefore, the emphasis on increased confidentiality in the paper directly aligns with the value item National Security and its corresponding value Security from a ""Software User"" perspective.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,992,ICSE,AI & Machine Learning,ShellFusion: answer generation for shell programming tasks via knowledge fusion,"Shell commands are widely used for accomplishing tasks, such as network management and file manipulation, in Unix and Linux platforms. There are a large number of shell commands available. For example, 50,000+ commands are documented in the Ubuntu Manual Pages (MPs). Quite often, programmers feel frustrated when searching and orchestrating appropriate shell commands to accomplish specific tasks. To address the challenge, the shell programming community calls for easy-to-use tutorials for shell commands. However, existing tutorials (e.g., TLDR) only cover a limited number of frequently used commands for shell beginners and provide limited support for users to search for commands by a task. We propose an approach, i.e., ShellFusion, to automatically generate comprehensive answers (including relevant shell commands, scripts, and explanations) for shell programming tasks. Our approach integrates knowledge mined from Q&A posts in Stack Exchange, Ubuntu MPs, and TLDR tutorials. For a query that describes a shell programming task, ShellFusion recommends a list of relevant shell commands. Specifically, ShellFusion retrieves the top-n Q&A posts with questions similar to the query and detects shell commands with options (e.g., ls -t) from the accepted answers of the retrieved posts. Next, ShellFusion filters out irrelevant commands with descriptions in MP and TLDR that share little semantics with the query, and further ranks the candidate commands based on their similarities with the query and the retrieved posts. To help users understand how to achieve the task using a recommended command, ShellFusion generates a comprehensive answer for each command by synthesizing knowledge from Q&A posts, MPs, and TLDR. Our evaluation of 434 shell programming tasks shows that ShellFusion significantly outperforms Magnum (the state-of-the-art natural language-to-Bash command approach) by at least 179.6% in terms of MRR@K and MAP@K. A user study conducted with 20 shell programmers further shows that ShellFusion can help users address programming tasks more efficiently and accurately, compared with Magnum and DeepAns (a recent answer recommendation baseline).",Stimulation,Excitement in Life,The paper offers an exciting new approach to shell programming task accomplishment using an integrated knowledge base; thus bringing excitement in the life of a software user who needs this functionality.,"The justification for aligning 'Paper X' with the value item Excitement in Life and its corresponding value Stimulation from a ""Software User"" perspective is based on the introduction of an innovative approach, ShellFusion, that aims to simplify and enhance shell programming tasks. By integrating knowledge from various sources and automatically generating comprehensive answers, this approach offers a potential solution to the frustration and challenges faced by programmers in searching for and orchestrating appropriate shell commands. This ability to efficiently and effectively accomplish tasks in a more streamlined manner can bring excitement and stimulation to the lives of software users who rely on shell programming for their work.",Agreed-Clarified,Agree,,Agree,,
Exp E,Exp J,993,ICSE,AI & Machine Learning,Using reinforcement learning for load testing of video games,"Different from what happens for most types of software systems, testing video games has largely remained a manual activity per-formed by human testers. This is mostly due to the continuous and intelligent user interaction video games require. Recently, rein-forcement learning (RL) has been exploited to partially automate functional testing. RL enables training smart agents that can even achieve super-human performance in playing games, thus being suitable to explore them looking for bugs. We investigate the pos-sibility of using RL for load testing video games. Indeed, the goal of game testing is not only to identify functional bugs, but also to examine the game's performance, such as its ability to avoid lags and keep a minimum number of frames per second (FPS) when high-demanding 3D scenes are shown on screen. We define a method-ology employing RL to train an agent able to play the game as a human while also trying to identify areas of the game resulting in a drop of FPS. We demonstrate the feasibility of our approach on three games. Two of them are used as proof-of-concept, by injecting artificial performance bugs. The third one is an open-source 3D game that we load test using the trained agent showing its potential to identify areas of the game resulting in lower FPS.",Achievement,Successful,The RL algorithms are capable of achieving 'super-human performance' as in they can surpass human testing capabilities. This reflects the capacity of the software users to achieve higher levels of success via the software. This aligns with Successful under the value Achievement.,"The justification for aligning 'Paper X' with the value item Successful and its corresponding value Achievement is based on the statement in the abstract that reinforcement learning (RL) enables training smart agents that can achieve super-human performance in playing games. This implies that the software users, who interact with the games, can experience a higher level of success and performance through the RL algorithms. Therefore, the alignment with Successful under the value Achievement is evident.",Agreed-Clarified,Agree,,Agree,,
Exp F,Exp J,994,ICSE,Security & Privacy,Hiding critical program components via ambiguous translation,"Software systems may contain critical program components such as patented program logic or sensitive data. When those components are reverse-engineered by adversaries, it can cause significantly damage (e.g., financial loss or operational failures). While protecting critical program components (e.g., code or data) in software systems is of utmost importance, existing approaches, unfortunately, have two major weaknesses: (1) they can be reverse-engineered via various program analysis techniques and (2) when an adversary obtains a legitimate-looking critical program component, he or she can be sure that it is genuine. In this paper, we propose Ambitr, a novel technique that hides critical program components. The core of Ambitr is Ambiguous Translator that can generate the critical program components when the input is a correct secret key. The translator is ambiguous as it can accept any inputs and produces a number of legitimate-looking outputs, making it difficult to know whether an input is correct secret key or not. The executions of the translator when it processes the correct secret key and other inputs are also indistinguishable, making the analysis inconclusive. Our evaluation results show that static, dynamic and symbolic analysis techniques fail to identify the hidden information in Ambitr. We also demonstrate that manual analysis of Ambitr is extremely challenging.",Security,Healthy,The paper presents a novel technique; Ambitr; which aids in protecting critical program components in software systems; this aligns with the value item Healthy within the value of Security as it can prevent potential damages to operational integrity by preventing reverse-engineering.,"The justification for aligning 'Paper X' with the value item Healthy and its corresponding value Security is based on the idea that the novel technique presented in the paper, Ambitr, helps in preventing potential damages to operational integrity by protecting critical program components. By hiding these components and making them difficult to reverse engineer, Ambitr reduces the risk of unauthorized access and manipulation, ultimately contributing to the security and health of the software system.",Agreed-Clarified,Agree,,Agree,,
Exp G,Exp K,994,ICSE,Security & Privacy,Hiding critical program components via ambiguous translation,"Software systems may contain critical program components such as patented program logic or sensitive data. When those components are reverse-engineered by adversaries, it can cause significantly damage (e.g., financial loss or operational failures). While protecting critical program components (e.g., code or data) in software systems is of utmost importance, existing approaches, unfortunately, have two major weaknesses: (1) they can be reverse-engineered via various program analysis techniques and (2) when an adversary obtains a legitimate-looking critical program component, he or she can be sure that it is genuine. In this paper, we propose Ambitr, a novel technique that hides critical program components. The core of Ambitr is Ambiguous Translator that can generate the critical program components when the input is a correct secret key. The translator is ambiguous as it can accept any inputs and produces a number of legitimate-looking outputs, making it difficult to know whether an input is correct secret key or not. The executions of the translator when it processes the correct secret key and other inputs are also indistinguishable, making the analysis inconclusive. Our evaluation results show that static, dynamic and symbolic analysis techniques fail to identify the hidden information in Ambitr. We also demonstrate that manual analysis of Ambitr is extremely challenging.",Security,Family Security,Ambitr's novel technique of preserving software components protects sensitive data and patented program logic; preserving a stable and secure software environment for users; aligning with the value item Family Security within the value of Security.,"Based on the abstract, 'Paper X' proposes a novel technique called Ambitr that hides critical program components to protect sensitive data and patented program logic. This aligns with the value item Family Security and its corresponding value Security as it focuses on preserving a stable and secure software environment for users, ensuring that their personal and sensitive information is safeguarded. By hiding critical components, Ambitr contributes to maintaining the security and integrity of software systems, thereby promoting a sense of Family Security for software users.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,994,ICSE,Security & Privacy,Hiding critical program components via ambiguous translation,"Software systems may contain critical program components such as patented program logic or sensitive data. When those components are reverse-engineered by adversaries, it can cause significantly damage (e.g., financial loss or operational failures). While protecting critical program components (e.g., code or data) in software systems is of utmost importance, existing approaches, unfortunately, have two major weaknesses: (1) they can be reverse-engineered via various program analysis techniques and (2) when an adversary obtains a legitimate-looking critical program component, he or she can be sure that it is genuine. In this paper, we propose Ambitr, a novel technique that hides critical program components. The core of Ambitr is Ambiguous Translator that can generate the critical program components when the input is a correct secret key. The translator is ambiguous as it can accept any inputs and produces a number of legitimate-looking outputs, making it difficult to know whether an input is correct secret key or not. The executions of the translator when it processes the correct secret key and other inputs are also indistinguishable, making the analysis inconclusive. Our evaluation results show that static, dynamic and symbolic analysis techniques fail to identify the hidden information in Ambitr. We also demonstrate that manual analysis of Ambitr is extremely challenging.",Security,Sense of Belonging,Ambitr's ambiguous translator creates a sense of belonging as it shields users from potential threats by making it challenging for adversaries to decode; this aligns with the value item Sense of Belonging within the value of Security.,"Ambitr's ambiguous translator aligns with the value item Sense of Belonging within the value of Security because it creates a sense of belonging for software users by providing them with a secure environment. The ambiguous translator shields users from potential threats by generating multiple legitimate-looking outputs, making it difficult for adversaries to decode and ensuring the security of the critical program components. This sense of belonging arises from the assurance and peace of mind that their sensitive data and patented program logic are protected, contributing to the overall feeling of security for software users.",Agreed-Clarified,Agree,,Agree,,
Exp B,Exp H,997,ICSE,Mobile & IoT,Aper: evolution-aware runtime permission misuse detection for Android apps,"The Android platform introduces the runtime permission model in version 6.0. The new model greatly improves data privacy and user experience, but brings new challenges for app developers. First, it allows users to freely revoke granted permissions. Hence, developers cannot assume that the permissions granted to an app would keep being granted. Instead, they should make their apps carefully check the permission status before invoking dangerous APIs. Second, the permission specification keeps evolving, bringing new types of compatibility issues into the ecosystem. To understand the impact of the challenges, we conducted an empirical study on 13,352 popular Google Play apps. We found that 86.0% apps used dangerous APIs asynchronously after permission management and 61.2% apps used evolving dangerous APIs. If an app does not properly handle permission revocations or platform differences, unexpected runtime issues may happen and even cause app crashes. We call such Android Runtime Permission issues as ARP bugs. Unfortunately, existing runtime permission issue detection tools cannot effectively deal with the ARP bugs induced by asynchronous permission management and permission specification evolution. To fill the gap, we designed a static analyzer, Aper, that performs reaching definition and dominator analysis on Android apps to detect the two types of ARP bugs. To compare Aper with existing tools, we built a benchmark, ARPFIX, from 60 real ARP bugs. Our experiment results show that Aper significantly outperforms two academic tools, ARPDROID and Revdroid, and an industrial tool, Lint, on ARPFIX, with an average improvement of 46.3% on F1-score. In addition, Aper successfully found 34 ARP bugs in 214 open-source Android apps, most of which can result in abnormal app behaviors (such as app crashes) according to our manual validation. We reported these bugs to the app developers. So far, 17 bugs have been confirmed and seven have been fixed.",Security,Healthy,The tool 'Aper' designed in the paper; minimises the chances of app crashes which makes the apps run more smoothly and ensures a healthy software environment for the users.,"The justification for aligning 'Paper X' with the value item Healthy and its corresponding value Security from a ""Software User"" perspective is that the tool 'Aper' aims to detect and prevent Android Runtime Permission issues, specifically the ARP bugs that may cause app crashes. By minimizing the chances of app crashes, 'Paper X' contributes to a healthier software environment for users as they can expect smoother running apps without unexpected interruptions. Ensuring security and stability in software usage aligns with the value of Security, as users can have trust and confidence in the reliability of the apps they are using, promoting a sense of well-being and a healthy software experience.",Agreed-Justified,Agree,,Disagree,The justification doesn't align with the value item of the Healthy,
Exp C,Exp K,997,ICSE,Mobile & IoT,Aper: evolution-aware runtime permission misuse detection for Android apps,"The Android platform introduces the runtime permission model in version 6.0. The new model greatly improves data privacy and user experience, but brings new challenges for app developers. First, it allows users to freely revoke granted permissions. Hence, developers cannot assume that the permissions granted to an app would keep being granted. Instead, they should make their apps carefully check the permission status before invoking dangerous APIs. Second, the permission specification keeps evolving, bringing new types of compatibility issues into the ecosystem. To understand the impact of the challenges, we conducted an empirical study on 13,352 popular Google Play apps. We found that 86.0% apps used dangerous APIs asynchronously after permission management and 61.2% apps used evolving dangerous APIs. If an app does not properly handle permission revocations or platform differences, unexpected runtime issues may happen and even cause app crashes. We call such Android Runtime Permission issues as ARP bugs. Unfortunately, existing runtime permission issue detection tools cannot effectively deal with the ARP bugs induced by asynchronous permission management and permission specification evolution. To fill the gap, we designed a static analyzer, Aper, that performs reaching definition and dominator analysis on Android apps to detect the two types of ARP bugs. To compare Aper with existing tools, we built a benchmark, ARPFIX, from 60 real ARP bugs. Our experiment results show that Aper significantly outperforms two academic tools, ARPDROID and Revdroid, and an industrial tool, Lint, on ARPFIX, with an average improvement of 46.3% on F1-score. In addition, Aper successfully found 34 ARP bugs in 214 open-source Android apps, most of which can result in abnormal app behaviors (such as app crashes) according to our manual validation. We reported these bugs to the app developers. So far, 17 bugs have been confirmed and seven have been fixed.",Self Direction,Privacy,By fixing the Android Runtime Permission (ARP) issues; the paper contributes to the improvement of data privacy in apps; aligning it with the value item Privacy and its corresponding value Self Direction.,"In the paper abstract, it is explicitly stated that the new Android runtime permission model improves data privacy. By addressing the ARP bugs and enabling proper permission management, the paper directly contributes to enhancing the privacy of software users. This alignment with the value item of Privacy is supported by the value of Self Direction, as users are given control over their permissions and the ability to revoke them. Thus, the main contributions of 'Paper X' align with the value item Privacy and its corresponding value Self Direction from a software user perspective.",Agreed-Justified,Agree,,Agree,,
Exp D,Exp K,997,ICSE,Mobile & IoT,Aper: evolution-aware runtime permission misuse detection for Android apps,"The Android platform introduces the runtime permission model in version 6.0. The new model greatly improves data privacy and user experience, but brings new challenges for app developers. First, it allows users to freely revoke granted permissions. Hence, developers cannot assume that the permissions granted to an app would keep being granted. Instead, they should make their apps carefully check the permission status before invoking dangerous APIs. Second, the permission specification keeps evolving, bringing new types of compatibility issues into the ecosystem. To understand the impact of the challenges, we conducted an empirical study on 13,352 popular Google Play apps. We found that 86.0% apps used dangerous APIs asynchronously after permission management and 61.2% apps used evolving dangerous APIs. If an app does not properly handle permission revocations or platform differences, unexpected runtime issues may happen and even cause app crashes. We call such Android Runtime Permission issues as ARP bugs. Unfortunately, existing runtime permission issue detection tools cannot effectively deal with the ARP bugs induced by asynchronous permission management and permission specification evolution. To fill the gap, we designed a static analyzer, Aper, that performs reaching definition and dominator analysis on Android apps to detect the two types of ARP bugs. To compare Aper with existing tools, we built a benchmark, ARPFIX, from 60 real ARP bugs. Our experiment results show that Aper significantly outperforms two academic tools, ARPDROID and Revdroid, and an industrial tool, Lint, on ARPFIX, with an average improvement of 46.3% on F1-score. In addition, Aper successfully found 34 ARP bugs in 214 open-source Android apps, most of which can result in abnormal app behaviors (such as app crashes) according to our manual validation. We reported these bugs to the app developers. So far, 17 bugs have been confirmed and seven have been fixed.",Stimulation,Excitement in Life,By reducing unexpected runtime issues and app crashes; the paper contributes to making the use of apps more exciting and enjoyable for the users; aligning it with the value item Excitement in Life and its corresponding value Stimulation.,"The justification for aligning 'Paper X' with the value item Excitement in Life and its corresponding value Stimulation from a ""Software User"" perspective is that by addressing issues related to unexpected runtime issues and app crashes, the paper improves the overall user experience and makes the use of apps more exciting and enjoyable. This aligns with Stimulation as a value because it enhances the level of excitement and engagement that users experience while using mobile applications, ensuring that their interactions with the technology are stimulating and pleasurable.",Agreed-Justified,Disagree,data privacy is more reletd with security,Agree,,
Exp E,Exp J,1007,ICSE,AI & Machine Learning,Generating and visualizing trace link explanations,"Recent breakthroughs in deep-learning (DL) approaches have resulted in the dynamic generation of trace links that are far more accurate than was previously possible. However, DL-generated links lack clear explanations, and therefore non-experts in the domain can find it difficult to understand the underlying semantics of the link, making it hard for them to evaluate the link's correctness or suitability for a specific software engineering task. In this paper we present a novel NLP pipeline for generating and visualizing trace link explanations. Our approach identifies domain-specific concepts, retrieves a corpus of concept-related sentences, mines concept definitions and usage examples, and identifies relations between cross-artifact concepts in order to explain the links. It applies a post-processing step to prioritize the most likely acronyms and definitions and to eliminate non-relevant ones. We evaluate our approach using project artifacts from three different domains of interstellar telescopes, positive train control, and electronic health-care systems, and then report coverage, correctness, and potential utility of the generated definitions. We design and utilize an explanation interface which leverages concept definitions and relations to visualize and explain trace link rationales, and we report results from a user study that was conducted to evaluate the effectiveness of the explanation interface. Results show that the explanations presented in the interface helped non-experts to understand the underlying semantics of a trace link and improved their ability to vet the correctness of the link.",Stimulation,Variation in Life,The software generates dynamic trace links which provides variation in the interaction with the software. This aligns with value item Variation in Life and its corresponding value Stimulation.,"In the context of a ""Software User,"" the generation of dynamic trace links by the software can be seen as providing a sense of variation in their interaction with the software. This variation can create a stimulating experience for the user as they explore and navigate through the trace links, potentially leading to a better understanding of the underlying semantics of the links and improving their ability to evaluate the correctness of the links. Therefore, aligning 'Paper X' with the value item Variation in Life and its corresponding value Stimulation is supported by the idea that the software's contributions offer a fresh and engaging experience to the user within a software context.",Agreed-Justified,Agree,,Agree,,
Exp F,Exp J,1007,ICSE,AI & Machine Learning,Generating and visualizing trace link explanations,"Recent breakthroughs in deep-learning (DL) approaches have resulted in the dynamic generation of trace links that are far more accurate than was previously possible. However, DL-generated links lack clear explanations, and therefore non-experts in the domain can find it difficult to understand the underlying semantics of the link, making it hard for them to evaluate the link's correctness or suitability for a specific software engineering task. In this paper we present a novel NLP pipeline for generating and visualizing trace link explanations. Our approach identifies domain-specific concepts, retrieves a corpus of concept-related sentences, mines concept definitions and usage examples, and identifies relations between cross-artifact concepts in order to explain the links. It applies a post-processing step to prioritize the most likely acronyms and definitions and to eliminate non-relevant ones. We evaluate our approach using project artifacts from three different domains of interstellar telescopes, positive train control, and electronic health-care systems, and then report coverage, correctness, and potential utility of the generated definitions. We design and utilize an explanation interface which leverages concept definitions and relations to visualize and explain trace link rationales, and we report results from a user study that was conducted to evaluate the effectiveness of the explanation interface. Results show that the explanations presented in the interface helped non-experts to understand the underlying semantics of a trace link and improved their ability to vet the correctness of the link.",Benevolence,Responsibility,The software uses a novel NLP pipeline to improve usability of the software which aligns with the value item Responsibility and its corresponding value Benevolence.,"The justification for labeling 'Paper X' as aligned with the value item Responsibility and its corresponding value Benevolence from a ""Software User"" perspective is that the paper presents a novel NLP pipeline that aims to improve the usability of software. By focusing on generating and visualizing trace link explanations, the paper aims to help non-experts in the domain of software engineering understand the underlying semantics of trace links and evaluate their correctness. This contribution aligns with the responsibility of software developers to create software that is user-friendly and meets the needs of its users, thereby promoting the value of benevolence in software development.",Agreed-Justified,Agree,,Agree,,
Exp G,Exp K,1007,ICSE,AI & Machine Learning,Generating and visualizing trace link explanations,"Recent breakthroughs in deep-learning (DL) approaches have resulted in the dynamic generation of trace links that are far more accurate than was previously possible. However, DL-generated links lack clear explanations, and therefore non-experts in the domain can find it difficult to understand the underlying semantics of the link, making it hard for them to evaluate the link's correctness or suitability for a specific software engineering task. In this paper we present a novel NLP pipeline for generating and visualizing trace link explanations. Our approach identifies domain-specific concepts, retrieves a corpus of concept-related sentences, mines concept definitions and usage examples, and identifies relations between cross-artifact concepts in order to explain the links. It applies a post-processing step to prioritize the most likely acronyms and definitions and to eliminate non-relevant ones. We evaluate our approach using project artifacts from three different domains of interstellar telescopes, positive train control, and electronic health-care systems, and then report coverage, correctness, and potential utility of the generated definitions. We design and utilize an explanation interface which leverages concept definitions and relations to visualize and explain trace link rationales, and we report results from a user study that was conducted to evaluate the effectiveness of the explanation interface. Results show that the explanations presented in the interface helped non-experts to understand the underlying semantics of a trace link and improved their ability to vet the correctness of the link.",Universalism,Broadmindedness,The explanation interface present in the software helps non-expert users to understand the underlying semantics of a trace link; making the software more inclusive and broad-minded. This alignment with the value item Broadmindedness and its corresponding value Universalism.,"The justification for aligning 'Paper X' with the value item Broadmindedness and its corresponding value Universalism is based on the fact that the software's explanation interface aims to help non-expert users understand the underlying semantics of a trace link. This signifies a willingness to embrace different perspectives and promote inclusivity, which aligns with the value of broadmindedness. Additionally, by providing users with a clearer understanding of the link, the software contributes to a more universal and equitable user experience, aligning with the value of universalism.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,1018,ICSE,Software Testing & QA,Repairing brain-computer interfaces with fault-based data acquisition,"Brain-computer interfaces (BCls) decode recorded neural signals from the brain and/or stimulate the brain with encoded neural sig-nals. BCls span both hardware and software and have a wide range of applications in restorative medicine, from restoring movement through prostheses and robotic limbs to restoring sensation and communication through spellers. BCls also have applications in di-agnostic medicine, e.g., providing clinicians with data for detecting seizures, sleep patterns, or emotions. Despite their promise, BCls have not yet been adopted for long-term, day-to-day use because of challenges related to reliability and robustness, which are needed for safe operation in all scenarios. Ensuring safe operation currently requires hours of manual data collection and recalibration, involving both patients and clinicians. However, data collection is not targeted at eliminating specific faults in a BCI. This paper presents a new methodology for char-acterizing, detecting, and localizing faults in BCls. Specifically, it proposes partial test oracles as a method for detecting faults and slice functions as a method for localizing faults to characteristic patterns in the input data or relevant tasks performed by the user. Through targeted data acquisition and retraining, the proposed methodology improves the correctness of BCls. We evaluated the proposed methodology on five BCl applications. The results show that the proposed methodology (1) precisely localizes faults and (2) can significantly reduce the frequency of faults through retraining based on targeted, fault-based data acquisition. These results sug-gest that the proposed methodology is a promising step towards repairing faulty BCls.",Achievement,Intelligent,The paper presents a new methodology for detecting and localizing faults in brain-computer interfaces (BCIs). This contribution can make the BCI applications more intelligent and operate more efficiently; explicitly aligning with the software user value item 'Intelligent' and its corresponding value 'Achievement'.,"The new methodology presented in 'Paper X' for detecting and localizing faults in brain-computer interfaces (BCIs) directly aligns with the value item 'Intelligent' and its corresponding value 'Achievement' from a software user perspective. By improving the correctness and reliability of BCIs through targeted data acquisition and retraining, the proposed methodology enhances the intelligence and efficiency of the BCI applications, leading to the achievement of more successful outcomes for software users.",Agreed-Clarified,Agree,,Agree,,
Exp B,Exp H,1018,ICSE,Software Testing & QA,Repairing brain-computer interfaces with fault-based data acquisition,"Brain-computer interfaces (BCls) decode recorded neural signals from the brain and/or stimulate the brain with encoded neural sig-nals. BCls span both hardware and software and have a wide range of applications in restorative medicine, from restoring movement through prostheses and robotic limbs to restoring sensation and communication through spellers. BCls also have applications in di-agnostic medicine, e.g., providing clinicians with data for detecting seizures, sleep patterns, or emotions. Despite their promise, BCls have not yet been adopted for long-term, day-to-day use because of challenges related to reliability and robustness, which are needed for safe operation in all scenarios. Ensuring safe operation currently requires hours of manual data collection and recalibration, involving both patients and clinicians. However, data collection is not targeted at eliminating specific faults in a BCI. This paper presents a new methodology for char-acterizing, detecting, and localizing faults in BCls. Specifically, it proposes partial test oracles as a method for detecting faults and slice functions as a method for localizing faults to characteristic patterns in the input data or relevant tasks performed by the user. Through targeted data acquisition and retraining, the proposed methodology improves the correctness of BCls. We evaluated the proposed methodology on five BCl applications. The results show that the proposed methodology (1) precisely localizes faults and (2) can significantly reduce the frequency of faults through retraining based on targeted, fault-based data acquisition. These results sug-gest that the proposed methodology is a promising step towards repairing faulty BCls.",Security,Healthy,The work carried out in the paper aims to improve the reliability and robustness of BCIs which are used in restorative and diagnostic medicine. This aligns directly with the user value item 'Healthy' and its corresponding value 'Security'; as it potentially increases the positive impact of the software on the user's health and wellbeing.,"The justification for aligning 'Paper X' with the value item Healthy and its corresponding value Security from a ""Software User"" perspective is based on the fact that the paper focuses on improving the reliability and robustness of BCIs used in restorative and diagnostic medicine. By enhancing the safety of BCIs, the software has the potential to positively impact the user's health and wellbeing, providing them with a sense of security in their usage of the technology.",Agreed-Justified,Agree,,Agree,,
Exp C,Exp K,1018,ICSE,Software Testing & QA,Repairing brain-computer interfaces with fault-based data acquisition,"Brain-computer interfaces (BCls) decode recorded neural signals from the brain and/or stimulate the brain with encoded neural sig-nals. BCls span both hardware and software and have a wide range of applications in restorative medicine, from restoring movement through prostheses and robotic limbs to restoring sensation and communication through spellers. BCls also have applications in di-agnostic medicine, e.g., providing clinicians with data for detecting seizures, sleep patterns, or emotions. Despite their promise, BCls have not yet been adopted for long-term, day-to-day use because of challenges related to reliability and robustness, which are needed for safe operation in all scenarios. Ensuring safe operation currently requires hours of manual data collection and recalibration, involving both patients and clinicians. However, data collection is not targeted at eliminating specific faults in a BCI. This paper presents a new methodology for char-acterizing, detecting, and localizing faults in BCls. Specifically, it proposes partial test oracles as a method for detecting faults and slice functions as a method for localizing faults to characteristic patterns in the input data or relevant tasks performed by the user. Through targeted data acquisition and retraining, the proposed methodology improves the correctness of BCls. We evaluated the proposed methodology on five BCl applications. The results show that the proposed methodology (1) precisely localizes faults and (2) can significantly reduce the frequency of faults through retraining based on targeted, fault-based data acquisition. These results sug-gest that the proposed methodology is a promising step towards repairing faulty BCls.",Security,National Security,By improving the reliability and robustness of BCIs; the proposed methodology can potentially enhance the safety/security aspect of the software; which can ensure a user's 'National Security' while using such interfaces that have crucial health and medical applications. This directly aligns with the value item 'National Security' and corresponding value 'Security'.,"The proposed methodology in 'Paper X' aims to improve the reliability and robustness of BCIs, which are software interfaces used in restorative and diagnostic medicine. By enhancing the safety and security aspect of these interfaces, users can have a sense of security regarding their personal health and well-being, which directly aligns with the value item of 'National Security' and its corresponding value of 'Security'. Ensuring the safe operation of BCIs contributes to the user's overall security, including their well-being and protection of personal health information, which are crucial aspects from a 'Software User' perspective.",Agreed-Reconciled,Agree,,Agree,,
Exp D,Exp K,1106,ICSE,Emerging Technologies,Smartmark: Software Watermarking Scheme for Smart Contracts,"A smart contract is a self-executing program on a blockchain to ensure an immutable and transparent agreement without the involvement of intermediaries. Despite its growing popularity for many blockchain platforms like Ethereum, no technical means is available even when a smart contract requires to be protected from being copied. One promising direction to claim a software ownership is software watermarking. However, applying existing software watermarking techniques is challenging because of the unique properties of a smart contract, such as a code size constraint, non-free execution cost, and no support for dynamic allocation under a virtual machine environment. This paper introduces a novel software watermarking scheme, dubbed Smartmark, aiming to protect the ownership of a smart contract against a pirate activity. Smartmark builds the control flow graph of a target contract runtime bytecode, and locates a collection of bytes that are randomly elected for representing a watermark. We implement a full-fledged prototype for Ethereum, applying Smartmark to 27,824 unique smart contract bytecodes. Our empirical results demonstrate that Smartmark can effectively embed a watermark into a smart contract and verify its presence, meeting the requirements of credibility and imperceptibility while incurring an acceptable performance degradation. Besides, our security analysis shows that Smartmark is resilient against viable watermarking corruption attacks; e.g., a large number of dummy opcodes are needed to disable a watermark effectively, resulting in producing an illegitimate smart contract clone that is not economical.
",Achievement,Intelligent,The paper introduces a scheme (Smartmark) that allows software users to claim ownership of a smart contract. This contributes to software users demonstrating their competence; aligning with the value item 'Intelligent' and its corresponding value 'Achievement'.,"In ""Paper X,"" the introduction of the Smartmark scheme allows software users to demonstrate their competence and achievement by claiming ownership of a smart contract. This aligns with the value item 'Intelligent' as it requires the users to possess knowledge and skills in utilizing the Smartmark scheme effectively. The ability to protect the ownership of a smart contract also contributes to the value of 'Achievement' as software users can successfully assert their rights and achieve a sense of accomplishment in ensuring the integrity and security of their contracts.",Agreed-Justified,Agree,,Agree,,
Exp E,Exp J,1106,ICSE,Emerging Technologies,Smartmark: Software Watermarking Scheme for Smart Contracts,"A smart contract is a self-executing program on a blockchain to ensure an immutable and transparent agreement without the involvement of intermediaries. Despite its growing popularity for many blockchain platforms like Ethereum, no technical means is available even when a smart contract requires to be protected from being copied. One promising direction to claim a software ownership is software watermarking. However, applying existing software watermarking techniques is challenging because of the unique properties of a smart contract, such as a code size constraint, non-free execution cost, and no support for dynamic allocation under a virtual machine environment. This paper introduces a novel software watermarking scheme, dubbed Smartmark, aiming to protect the ownership of a smart contract against a pirate activity. Smartmark builds the control flow graph of a target contract runtime bytecode, and locates a collection of bytes that are randomly elected for representing a watermark. We implement a full-fledged prototype for Ethereum, applying Smartmark to 27,824 unique smart contract bytecodes. Our empirical results demonstrate that Smartmark can effectively embed a watermark into a smart contract and verify its presence, meeting the requirements of credibility and imperceptibility while incurring an acceptable performance degradation. Besides, our security analysis shows that Smartmark is resilient against viable watermarking corruption attacks; e.g., a large number of dummy opcodes are needed to disable a watermark effectively, resulting in producing an illegitimate smart contract clone that is not economical.
",Power,Social Recognition,By enabling a watermaking scheme; the paper provides a mechanism for software users to protect and validate the ownership of their smart contracts; which aligns with the value item 'Social Recognition' and its corresponding value 'Power'.,"The justification is that by implementing a watermarking scheme in smart contracts, the paper provides a means for software users to establish and validate ownership of their contracts. This aligns with the value item of 'Social Recognition,' as software users can assert their rights and establish authority over their creations, demonstrating their power within the software context.",Agreed-Clarified,Agree,,Agree,,
Exp F,Exp J,1106,ICSE,Emerging Technologies,Smartmark: Software Watermarking Scheme for Smart Contracts,"A smart contract is a self-executing program on a blockchain to ensure an immutable and transparent agreement without the involvement of intermediaries. Despite its growing popularity for many blockchain platforms like Ethereum, no technical means is available even when a smart contract requires to be protected from being copied. One promising direction to claim a software ownership is software watermarking. However, applying existing software watermarking techniques is challenging because of the unique properties of a smart contract, such as a code size constraint, non-free execution cost, and no support for dynamic allocation under a virtual machine environment. This paper introduces a novel software watermarking scheme, dubbed Smartmark, aiming to protect the ownership of a smart contract against a pirate activity. Smartmark builds the control flow graph of a target contract runtime bytecode, and locates a collection of bytes that are randomly elected for representing a watermark. We implement a full-fledged prototype for Ethereum, applying Smartmark to 27,824 unique smart contract bytecodes. Our empirical results demonstrate that Smartmark can effectively embed a watermark into a smart contract and verify its presence, meeting the requirements of credibility and imperceptibility while incurring an acceptable performance degradation. Besides, our security analysis shows that Smartmark is resilient against viable watermarking corruption attacks; e.g., a large number of dummy opcodes are needed to disable a watermark effectively, resulting in producing an illegitimate smart contract clone that is not economical.
",Security,Healthy,The abstract mentions that Smartmark is resilient against watermarking corruption attacks; indicating that it ensures the health of smart contracts in a software environment. This aligns with the value item 'Healthy' and its corresponding value 'Security'.,"In the abstract, it is stated that Smartmark is resilient against watermarking corruption attacks, which means it has the capability to protect the integrity and security of smart contracts. This directly aligns with the value item 'Healthy' from Schwartz's Taxonomy, as ensuring the health and security of software systems is a key concern for software users. By embedding a watermark and verifying its presence, Smartmark contributes to maintaining the security and trustworthiness of smart contracts for software users.",Agreed-Clarified,Agree,,Agree,,
Exp G,Exp K,1108,ICSE,Software Project Management,Do I Belong? Modeling Sense of Virtual Community Among Linux Kernel Contributors,"The sense of belonging to a community is a basic human need that impacts an individual's behavior, long-term engagement, and job satisfaction, as revealed by research in disciplines such as psychology, healthcare, and education. Despite much research on how to retain developers in Open Source Software (OSS) projects and other virtual, peer-production communities, there is a paucity of research investigating what might contribute to a sense of belonging in these communities. To that end, we develop a theoretical model that seeks to understand the link between OSS developer motives and a Sense of Virtual Community (SVC). We test the model with a dataset collected in the Linux Kernel developer community (N=225), using structural equation modeling techniques. Our results for this case study show that intrinsic motivations (social or hedonic motives) are positively associated with a sense of virtual community, but living in an authoritative country and being paid to contribute can reduce the sense of virtual community. Based on these results, we offer suggestions for open source projects to foster a sense of virtual community, with a view to retaining contributors and improving projects' sustainability.

",Security,Sense of Belonging,The paper contributes towards understanding what might contribute to a sense of belonging in OSS communities; which aligns with the value item Sense of Belonging and its corresponding value Security.,"The paper's contribution to understanding what contributes to a sense of belonging in OSS communities aligns with the value item Sense of Belonging and its corresponding value Security because a sense of belonging is a basic human need that provides individuals with a feeling of security and social integration. By investigating the factors that contribute to a sense of belonging in virtual communities, the paper aims to enhance the sense of security and connection for software users engaging in open source projects, ultimately improving their satisfaction and long-term engagement. This direct alignment between the paper's objective and the value item highlights the importance of creating a secure and inclusive environment for software users within a software context.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,1108,ICSE,Software Project Management,Do I Belong? Modeling Sense of Virtual Community Among Linux Kernel Contributors,"The sense of belonging to a community is a basic human need that impacts an individual's behavior, long-term engagement, and job satisfaction, as revealed by research in disciplines such as psychology, healthcare, and education. Despite much research on how to retain developers in Open Source Software (OSS) projects and other virtual, peer-production communities, there is a paucity of research investigating what might contribute to a sense of belonging in these communities. To that end, we develop a theoretical model that seeks to understand the link between OSS developer motives and a Sense of Virtual Community (SVC). We test the model with a dataset collected in the Linux Kernel developer community (N=225), using structural equation modeling techniques. Our results for this case study show that intrinsic motivations (social or hedonic motives) are positively associated with a sense of virtual community, but living in an authoritative country and being paid to contribute can reduce the sense of virtual community. Based on these results, we offer suggestions for open source projects to foster a sense of virtual community, with a view to retaining contributors and improving projects' sustainability.

",Benevolence,A Spiritual Life,By discussing intrinsic motivations and suggesting ways to retain contributors and increase projects' sustainability; the paper indirectly contributes to the spiritual life of users; which aligns with the value item A Spiritual Life from the value Benevolence.,"I apologize for the lack of clarity in my previous justification. The main contribution of the paper is to provide suggestions for open source projects to foster a sense of virtual community, leading to improved projects' sustainability. By emphasizing intrinsic motivations and retention of contributors, the paper indirectly promotes a sense of purpose and fulfillment, which aligns with the value item A Spiritual Life from the value Benevolence. This aligns with the perspective of a software user as it highlights the broader impact of a sense of virtual community on the users' overall experience and satisfaction.",Agreed-Clarified,Agree,,Agree,,
Exp B,Exp H,1108,ICSE,Software Project Management,Do I Belong? Modeling Sense of Virtual Community Among Linux Kernel Contributors,"The sense of belonging to a community is a basic human need that impacts an individual's behavior, long-term engagement, and job satisfaction, as revealed by research in disciplines such as psychology, healthcare, and education. Despite much research on how to retain developers in Open Source Software (OSS) projects and other virtual, peer-production communities, there is a paucity of research investigating what might contribute to a sense of belonging in these communities. To that end, we develop a theoretical model that seeks to understand the link between OSS developer motives and a Sense of Virtual Community (SVC). We test the model with a dataset collected in the Linux Kernel developer community (N=225), using structural equation modeling techniques. Our results for this case study show that intrinsic motivations (social or hedonic motives) are positively associated with a sense of virtual community, but living in an authoritative country and being paid to contribute can reduce the sense of virtual community. Based on these results, we offer suggestions for open source projects to foster a sense of virtual community, with a view to retaining contributors and improving projects' sustainability.

",Self Direction,Freedom,The suggestion of the paper that contributors in OSS communities should be allowed to contribute based on their intrinsic motivations aligns with the value item Freedom under the value Self Direction.,"In the abstract of 'Paper X', it is stated that intrinsic motivations, such as social or hedonic motives, are positively associated with a sense of virtual community. This aligns with the value item Freedom under the value Self Direction because it suggests that contributors in OSS communities should have the freedom to choose their own motives for contributing, allowing them to pursue their own interests and goals. This emphasis on intrinsic motivations and freedom aligns with the value of Self Direction, as individuals are encouraged to make choices and act in accordance with their own values and desires.",Agreed-Justified,Agree,,Agree,,
Exp C,Exp K,1113,ICSE,Security & Privacy,Demystifying Exploitable Bugs in Smart Contracts,"Exploitable bugs in smart contracts have caused significant monetary loss. Despite the substantial advances in smart contract bug finding, exploitable bugs and real-world attacks are still trending. In this paper we systematically investigate 516 unique real-world smart contract vulnerabilities in years 2021‚Äö√Ñ√∂‚àö√ë‚àö¬®2022, and study how many can be exploited by malicious users and cannot be detected by existing analysis tools. We further categorize the bugs that cannot be detected by existing tools into seven types and study their root causes, distributions, difficulties to audit, consequences, and repair strategies. For each type, we abstract them to a bug model (if possible), facilitating finding similar bugs in other contracts and future automation. We leverage the findings in auditing real world smart contracts, and so far we have been rewarded with",Security,Social Order,"The paper addresses the issue of bugs in smart contracts and offers solutions for their detection and repair. This aligns directly with the value item ""Social Order"" as it contributes to a more secure and orderly digital society.","The alignment of 'Paper X' with the value item Social Order and its corresponding value Security stems from the fact that the paper specifically addresses the vulnerabilities and bugs in smart contracts, which have caused significant monetary loss. By proposing detection and repair strategies for these bugs, the paper contributes to a more secure and orderly digital society. Ensuring the integrity and reliability of smart contracts promotes trust among users and enhances the overall security and stability of the software ecosystem, which directly aligns with the value item Social Order and its underlying value of Security.",Agreed-Clarified,Agree,,Agree,,
Exp D,Exp K,1113,ICSE,Security & Privacy,Demystifying Exploitable Bugs in Smart Contracts,"Exploitable bugs in smart contracts have caused significant monetary loss. Despite the substantial advances in smart contract bug finding, exploitable bugs and real-world attacks are still trending. In this paper we systematically investigate 516 unique real-world smart contract vulnerabilities in years 2021‚Äö√Ñ√∂‚àö√ë‚àö¬®2022, and study how many can be exploited by malicious users and cannot be detected by existing analysis tools. We further categorize the bugs that cannot be detected by existing tools into seven types and study their root causes, distributions, difficulties to audit, consequences, and repair strategies. For each type, we abstract them to a bug model (if possible), facilitating finding similar bugs in other contracts and future automation. We leverage the findings in auditing real world smart contracts, and so far we have been rewarded with",Security,Healthy,"The study aims to detect and address exploitable bugs in smart contracts; aligning with the value item ""Healthy"" as such bugs may lead to monetary risk; which can be considered as a financial health risk for users.","Certainly, I justify the alignment of 'Paper X' with the value item Healthy and its corresponding value Security from a ""Software User"" perspective because the paper explicitly states that it investigates real-world smart contract vulnerabilities and their consequences. Exploitable bugs in smart contracts can lead to significant monetary loss for users, which can be considered a financial health risk. Therefore, addressing these vulnerabilities and enhancing security contributes to the well-being and protection of users, aligning with the value item Healthy and its corresponding value Security.",Agreed-Clarified,Agree,,Agree,,
Exp E,Exp J,1113,ICSE,Security & Privacy,Demystifying Exploitable Bugs in Smart Contracts,"Exploitable bugs in smart contracts have caused significant monetary loss. Despite the substantial advances in smart contract bug finding, exploitable bugs and real-world attacks are still trending. In this paper we systematically investigate 516 unique real-world smart contract vulnerabilities in years 2021‚Äö√Ñ√∂‚àö√ë‚àö¬®2022, and study how many can be exploited by malicious users and cannot be detected by existing analysis tools. We further categorize the bugs that cannot be detected by existing tools into seven types and study their root causes, distributions, difficulties to audit, consequences, and repair strategies. For each type, we abstract them to a bug model (if possible), facilitating finding similar bugs in other contracts and future automation. We leverage the findings in auditing real world smart contracts, and so far we have been rewarded with",Power,Authority,"By finding ways to detect and avoid the exploitation of smart contract bugs; the paper aligns with the value item ""Authority""; as it helps users gain more control over their digital assets.","In aligning with the value item ""Authority"" and its corresponding value ""Power,"" 'Paper X' contributes to the software user's ability to have more control over their digital assets by detecting and avoiding the exploitation of smart contract bugs. By addressing vulnerabilities and providing strategies for repair, the paper empowers users to assert their authority over their assets, ensuring their security and minimizing potential monetary losses caused by exploitable bugs.",Agreed-Justified,Agree,,Agree,,
Exp F,Exp J,1114,ICSE,Software Testing & QA,Understanding and Detecting On-The-Fly Configuration Bugs,"Software systems introduce an increasing number of configuration options to provide flexibility, and support updating the options on the fly to provide persistent services. This mechanism, however, may affect the system reliability, leading to unexpected results like software crashes or functional errors. In this paper, we refer to the bugs caused by on-the-fly configuration updates as on-the-fly configuration bugs, or OCBugs for short. In this paper, we conducted the first in-depth study on 75 real-world OCBugs from 5 widely used systems to understand the symptoms, root causes, and triggering conditions of OCBugs. Based on our study, we designed and implemented Parachute, an automated testing framework to detect OCBugs. Our key insight is that the value of one configuration option, either loaded at the startup phase or updated on the fly, should have the same effects on the target program. Parachute generates tests for on-the-fly configuration updates by mutating the existing tests and conducts differential analysis to identify OCBugs. We evaluated Parachute on 7 real-world software systems. The results show that Parachute detected 75% (42/56) of the known OCBugs, and reported 13 unknown bugs, 11 of which have been confirmed or fixed by developers until the time of writing.
",Achievement,Intelligent,The paper contributes the Parachute; an automated testing framework that detects OCBugs. It enhances the software user's intelligence by providing accurate and reliable performance.,"The main contribution of 'Paper X' is the development of Parachute, an automated testing framework that detects on-the-fly configuration bugs in software systems. By detecting these bugs and providing accurate and reliable performance, the Parachute framework enhances a software user's intelligence by enabling them to make informed decisions about the configuration options and to avoid unexpected results such as software crashes or functional errors. This aligns with the value item Intelligent and its corresponding value Achievement from the perspective of a software user, as it empowers them to effectively navigate and utilize the software system.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,1114,ICSE,Software Testing & QA,Understanding and Detecting On-The-Fly Configuration Bugs,"Software systems introduce an increasing number of configuration options to provide flexibility, and support updating the options on the fly to provide persistent services. This mechanism, however, may affect the system reliability, leading to unexpected results like software crashes or functional errors. In this paper, we refer to the bugs caused by on-the-fly configuration updates as on-the-fly configuration bugs, or OCBugs for short. In this paper, we conducted the first in-depth study on 75 real-world OCBugs from 5 widely used systems to understand the symptoms, root causes, and triggering conditions of OCBugs. Based on our study, we designed and implemented Parachute, an automated testing framework to detect OCBugs. Our key insight is that the value of one configuration option, either loaded at the startup phase or updated on the fly, should have the same effects on the target program. Parachute generates tests for on-the-fly configuration updates by mutating the existing tests and conducts differential analysis to identify OCBugs. We evaluated Parachute on 7 real-world software systems. The results show that Parachute detected 75% (42/56) of the known OCBugs, and reported 13 unknown bugs, 11 of which have been confirmed or fixed by developers until the time of writing.
",Benevolence,Responsibility,The software user's responsibility is increased by the Parachute which systematically detects bugs in the software. This facilitates a more reliable and accountable use of the software.,"The main contribution of 'Paper X' is the design and implementation of Parachute, an automated testing framework to detect on-the-fly configuration bugs. This aligns with the value item Responsibility from Schwartz's Taxonomy as it highlights the software user's responsibility to ensure the reliability and accountability of the software. By systematically detecting bugs, Parachute empowers the software user to take responsibility for the proper functioning of the software, leading to a more reliable and accountable use of the software.",Agreed-Clarified,Disagree,reconcile with coder_2,Disagree,The justification doesn't align with the value item of the Freedom,
Exp C,Exp K,1115,ICSE,Software Project Management,Is It Enough to Recommend Tasks to Newcomers? Understanding Mentoring on Good First Issues,"Newcomers are critical for the success and continuity of open source software (OSS) projects. To attract newcomers and facilitate their onboarding, many OSS projects recommend tasks for newcomers, such as good first issues (GFIs). Previous studies have preliminarily investigated the effects of GFIs and techniques to identify suitable GFIs. However, it is still unclear whether just recommending tasks is enough and how significant mentoring is for newcomers. To better understand mentoring in OSS communities, we analyze the resolution process of 48,402 GFIs from 964 repositories through a mix-method approach. We investigate the extent, the mentorship structures, the discussed topics, and the relevance of expert involvement. We find that ~70% of GFIs have expert participation, with each GFI usually having one expert who makes two comments. Half of GFIs will receive their first expert comment within 8.5 hours after a newcomer comment. Through analysis of the collaboration networks of newcomers and experts, we observe that community mentorship presents four types of structure: centralized mentoring, decentralized mentoring, collaborative mentoring, and distributed mentoring. As for discussed topics, we identify 14 newcomer challenges and 18 expert mentoring content. By fitting the generalized linear models, we find that expert involvement positively correlates with newcomers' successful contributions but negatively correlates with newcomers' retention. Our study manifests the status and significance of mentoring in the OSS projects, which provides rich practical implications for optimizing the mentoring process and helping newcomers contribute smoothly and successfully.

",Benevolence,Responsibility,The abstract mentions facilitating the successful contributions of newcomers. This aligns with the value item Responsibility under the value Benevolence; as it helps the software users to take responsibility for their contributions.,"The main contributions of 'Paper X' in facilitating the successful contributions of newcomers align with the value item Responsibility and its corresponding value Benevolence from a ""Software User"" perspective because it emphasizes the importance of newcomers taking responsibility for their contributions to open source software projects. By providing mentoring and guidance to newcomers, the paper promotes a sense of responsibility among software users to actively participate and contribute to the community, thereby fostering a benevolent and collaborative environment.",Agreed-Justified,Agree,,Agree,,
Exp D,Exp K,1115,ICSE,Software Project Management,Is It Enough to Recommend Tasks to Newcomers? Understanding Mentoring on Good First Issues,"Newcomers are critical for the success and continuity of open source software (OSS) projects. To attract newcomers and facilitate their onboarding, many OSS projects recommend tasks for newcomers, such as good first issues (GFIs). Previous studies have preliminarily investigated the effects of GFIs and techniques to identify suitable GFIs. However, it is still unclear whether just recommending tasks is enough and how significant mentoring is for newcomers. To better understand mentoring in OSS communities, we analyze the resolution process of 48,402 GFIs from 964 repositories through a mix-method approach. We investigate the extent, the mentorship structures, the discussed topics, and the relevance of expert involvement. We find that ~70% of GFIs have expert participation, with each GFI usually having one expert who makes two comments. Half of GFIs will receive their first expert comment within 8.5 hours after a newcomer comment. Through analysis of the collaboration networks of newcomers and experts, we observe that community mentorship presents four types of structure: centralized mentoring, decentralized mentoring, collaborative mentoring, and distributed mentoring. As for discussed topics, we identify 14 newcomer challenges and 18 expert mentoring content. By fitting the generalized linear models, we find that expert involvement positively correlates with newcomers' successful contributions but negatively correlates with newcomers' retention. Our study manifests the status and significance of mentoring in the OSS projects, which provides rich practical implications for optimizing the mentoring process and helping newcomers contribute smoothly and successfully.

",Stimulation,Daring,The paper discusses the mentoring of newcomers and the importance of expert involvement; which can be considered as Daring under the value Stimulation since it entails taking on challenging tasks.,"I apologize for any confusion in my initial justification. The alignment of 'Paper X' with the value item Daring and its corresponding value Stimulation is based on the idea that mentoring newcomers and encouraging expert involvement can be seen as challenging and stimulating tasks within the context of software development. By actively engaging in mentoring and involving experts, newcomers are exposed to new perspectives, knowledge, and experiences that may push them out of their comfort zones and encourage personal growth. This aligns with the value of Stimulation, as it involves taking on new and potentially exciting challenges.",Agreed-Clarified,Agree,,Agree,,
Exp E,Exp J,1121,ICSE,Security & Privacy,AChecker: Statically Detecting Smart Contract Access Control Vulnerabilities,"As most smart contracts have a financial nature and handle valuable assets, smart contract developers use access control to protect assets managed by smart contracts from being misused by malicious or unauthorized people. Unfortunately, programming languages used for writing smart contracts, such as Solidity, were not designed with a permission-based security model in mind. Therefore, smart contract developers implement access control checks based on their judgment and in an adhoc manner, which results in several vulnerabilities in smart contracts, called access control vulnerabilities. Further, the in-consistency in implementing access control makes it difficult to reason about whether a contract meets access control needs and is free of access control vulnerabilities. In this work, we propose AChecker - an approach for detecting access control vulnerabilities. Unlike prior work, AChecker does not rely on pre-defined patterns or contract transactions history. Instead, it infers access control implemented in smart contracts via static data-flow analysis. Moreover, the approach performs further symbolic-based analysis to distinguish cases when unauthorized people can obtain control of the contract as intended functionality. We evaluated AChecker on three public datasets of real-world smart contracts, including one which consists of contracts with assigned access control CVEs, and compared its effectiveness with eight analysis tools. The evaluation results showed that AChecker outperforms these tools in terms of both precision and recall. In addition, AChecker flagged vulnerabilities in 21 frequently-used contracts on Ethereum blockchain with 90% precision.
",Achievement,Intelligent,The proposed AChecker provides a tool to detect access control vulnerabilities in smart contracts; which implies improvement in user competence in identifying potential threats.,"In aligning with the value item ""Intelligent"" and its corresponding value ""Achievement"" from a ""Software User"" perspective, the main contribution of 'Paper X' lies in the proposed AChecker tool that enhances the user's ability to detect access control vulnerabilities in smart contracts. By improving the user's competence in identifying potential threats, the tool empowers users to achieve a higher level of intelligence in their software usage, thereby aligning with the value of achievement.",Agreed-Clarified,Agree,,Agree,,
Exp F,Exp J,1121,ICSE,Security & Privacy,AChecker: Statically Detecting Smart Contract Access Control Vulnerabilities,"As most smart contracts have a financial nature and handle valuable assets, smart contract developers use access control to protect assets managed by smart contracts from being misused by malicious or unauthorized people. Unfortunately, programming languages used for writing smart contracts, such as Solidity, were not designed with a permission-based security model in mind. Therefore, smart contract developers implement access control checks based on their judgment and in an adhoc manner, which results in several vulnerabilities in smart contracts, called access control vulnerabilities. Further, the in-consistency in implementing access control makes it difficult to reason about whether a contract meets access control needs and is free of access control vulnerabilities. In this work, we propose AChecker - an approach for detecting access control vulnerabilities. Unlike prior work, AChecker does not rely on pre-defined patterns or contract transactions history. Instead, it infers access control implemented in smart contracts via static data-flow analysis. Moreover, the approach performs further symbolic-based analysis to distinguish cases when unauthorized people can obtain control of the contract as intended functionality. We evaluated AChecker on three public datasets of real-world smart contracts, including one which consists of contracts with assigned access control CVEs, and compared its effectiveness with eight analysis tools. The evaluation results showed that AChecker outperforms these tools in terms of both precision and recall. In addition, AChecker flagged vulnerabilities in 21 frequently-used contracts on Ethereum blockchain with 90% precision.
",Security,Healthy,By allowing the detection of vulnerabilities in smart contracts; AChecker may contribute to the psychological well-being (emotional health) of users by reducing stress and anxiety associated with financial risks.,"By allowing the detection of vulnerabilities in smart contracts, AChecker contributes to the value item of Security and its corresponding value of Healthy from a ""Software User"" perspective. The detection of vulnerabilities reduces financial risks for users, which in turn reduces stress and anxiety. This enhances the users' psychological well-being and promotes emotional health.",Agreed-Justified,Agree,,Agree,,
Exp G,Exp K,1121,ICSE,Security & Privacy,AChecker: Statically Detecting Smart Contract Access Control Vulnerabilities,"As most smart contracts have a financial nature and handle valuable assets, smart contract developers use access control to protect assets managed by smart contracts from being misused by malicious or unauthorized people. Unfortunately, programming languages used for writing smart contracts, such as Solidity, were not designed with a permission-based security model in mind. Therefore, smart contract developers implement access control checks based on their judgment and in an adhoc manner, which results in several vulnerabilities in smart contracts, called access control vulnerabilities. Further, the in-consistency in implementing access control makes it difficult to reason about whether a contract meets access control needs and is free of access control vulnerabilities. In this work, we propose AChecker - an approach for detecting access control vulnerabilities. Unlike prior work, AChecker does not rely on pre-defined patterns or contract transactions history. Instead, it infers access control implemented in smart contracts via static data-flow analysis. Moreover, the approach performs further symbolic-based analysis to distinguish cases when unauthorized people can obtain control of the contract as intended functionality. We evaluated AChecker on three public datasets of real-world smart contracts, including one which consists of contracts with assigned access control CVEs, and compared its effectiveness with eight analysis tools. The evaluation results showed that AChecker outperforms these tools in terms of both precision and recall. In addition, AChecker flagged vulnerabilities in 21 frequently-used contracts on Ethereum blockchain with 90% precision.
",Security,National Security,The elimination of access control vulnerabilities through AChecker contributes to improving the security of financial transactions; which are often of national interest; hence the alignment with National Security.,"The justification for aligning 'Paper X' with the value item National Security and its corresponding value Security from a ""Software User"" perspective is that the paper proposes AChecker, an approach for detecting access control vulnerabilities in smart contracts. By eliminating these vulnerabilities, AChecker contributes to improving the security of financial transactions, which are often of national interest due to their potential impact on the stability and integrity of the financial system. This alignment is based on the direct impact of the paper's contributions on enhancing security in a software context, aligning with the value of Security at a national level.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,1122,ICSE,Security & Privacy,Silent Vulnerable Dependency Alert Prediction with Vulnerability Key Aspect Explanation,"Due to convenience, open-source software is widely used. For beneficial reasons, open-source maintainers often fix the vulnerabilities silently, exposing their users unaware of the updates to threats. Previous works all focus on black-box binary detection of the silent dependency alerts that suffer from high false-positive rates. Open-source software users need to analyze and explain AI prediction themselves. Explainable AI becomes remarkable as a complementary of black-box AI models, providing details in various forms to explain AI decisions. Noticing there is still no technique that can discover silent dependency alert on time, in this work, we propose a framework using an encoder-decoder model with a binary detector to provide explainable silent dependency alert prediction. Our model generates 4 types of vulnerability key aspects including vulnerability type, root cause, attack vector, and impact to enhance the trustworthiness and users' acceptance to alert prediction. By experiments with several models and inputs, we confirm CodeBERT with both commit messages and code changes achieves the best results. Our user study shows that explainable alert predictions can help users find silent dependency alert more easily than black-box predictions. To the best of our knowledge, this is the first research work on the application of Explainable AI in silent dependency alert prediction, which opens the door of the related domains.

",Power,Social Recognition,"The paper proposes a framework using explainable AI to alert users about silent dependencies; this directly improves the software's perceived trustworthiness for users and thus aligns with the value item of ""Social Recognition"" and its corresponding value ""Power.""","The paper's proposed framework using explainable AI to alert users about silent dependencies directly aligns with the value item of ""Social Recognition"" and its corresponding value ""Power"" because by providing users with detailed explanations and insights into AI predictions, the software developers increase the users' ability to make informed decisions and protect their systems. This increases the perceived trustworthiness of the software among users, enhances their sense of control and influence, and ultimately grants them a higher level of recognition and power in their ability to mitigate and address potential threats.",Agreed-Clarified,Agree,,Agree,,
Exp B,Exp H,1122,ICSE,Security & Privacy,Silent Vulnerable Dependency Alert Prediction with Vulnerability Key Aspect Explanation,"Due to convenience, open-source software is widely used. For beneficial reasons, open-source maintainers often fix the vulnerabilities silently, exposing their users unaware of the updates to threats. Previous works all focus on black-box binary detection of the silent dependency alerts that suffer from high false-positive rates. Open-source software users need to analyze and explain AI prediction themselves. Explainable AI becomes remarkable as a complementary of black-box AI models, providing details in various forms to explain AI decisions. Noticing there is still no technique that can discover silent dependency alert on time, in this work, we propose a framework using an encoder-decoder model with a binary detector to provide explainable silent dependency alert prediction. Our model generates 4 types of vulnerability key aspects including vulnerability type, root cause, attack vector, and impact to enhance the trustworthiness and users' acceptance to alert prediction. By experiments with several models and inputs, we confirm CodeBERT with both commit messages and code changes achieves the best results. Our user study shows that explainable alert predictions can help users find silent dependency alert more easily than black-box predictions. To the best of our knowledge, this is the first research work on the application of Explainable AI in silent dependency alert prediction, which opens the door of the related domains.

",Achievement,Intelligent,The paper supports software users in understanding AI decisions; thereby promoting their Intelligence; which aligns with the value of Achievement.,"In the paper, the authors propose a framework for providing explainable predictions on silent dependency alerts in open-source software. By generating vulnerability key aspects such as vulnerability type, root cause, attack vector, and impact, the framework enhances the users' understanding of the system and allows them to make informed decisions. This aligns with the value of Intelligence as it supports software users in gaining knowledge and understanding AI decisions, thus promoting their ability to achieve their goals effectively.",Agreed-Clarified,Agree,,Agree,,
Exp C,Exp K,1122,ICSE,Security & Privacy,Silent Vulnerable Dependency Alert Prediction with Vulnerability Key Aspect Explanation,"Due to convenience, open-source software is widely used. For beneficial reasons, open-source maintainers often fix the vulnerabilities silently, exposing their users unaware of the updates to threats. Previous works all focus on black-box binary detection of the silent dependency alerts that suffer from high false-positive rates. Open-source software users need to analyze and explain AI prediction themselves. Explainable AI becomes remarkable as a complementary of black-box AI models, providing details in various forms to explain AI decisions. Noticing there is still no technique that can discover silent dependency alert on time, in this work, we propose a framework using an encoder-decoder model with a binary detector to provide explainable silent dependency alert prediction. Our model generates 4 types of vulnerability key aspects including vulnerability type, root cause, attack vector, and impact to enhance the trustworthiness and users' acceptance to alert prediction. By experiments with several models and inputs, we confirm CodeBERT with both commit messages and code changes achieves the best results. Our user study shows that explainable alert predictions can help users find silent dependency alert more easily than black-box predictions. To the best of our knowledge, this is the first research work on the application of Explainable AI in silent dependency alert prediction, which opens the door of the related domains.

",Universalism,Social Justice,"The paper contributes to alert users about silent dependencies; thereby promoting ""Social Justice"" under the value ""Universalism.""","The paper's contribution of alerting users about silent dependencies can be seen as promoting ""Social Justice"" under the value ""Universalism"" because it aims to provide equal access to information and ensure fairness in the software context. By providing users with explainable AI predictions and vulnerability key aspects, the paper enables users to identify and address potential threats, aligning with the values of equality, broadmindedness, and social justice.",Agreed-Clarified,Agree,,Agree,,
Exp D,Exp K,1125,ICSE,Security & Privacy,SecBench.js: An Executable Security Benchmark Suite for Server-Side JavaScript,"Npm is the largest software ecosystem in the world, offering millions of free, reusable packages. In recent years, various security threats to packages published on npm have been reported, including vulnerabilities that affect millions of users. To continuously improve techniques for detecting vulnerabilities and mitigating attacks that exploit them, a reusable benchmark of vulnerabilities would be highly desirable. Ideally, such a benchmark should be realistic, come with executable exploits, and include fixes of vulnerabilities. Unfortunately, there currently is no such benchmark, forcing researchers to repeatedly develop their own evaluation datasets and making it difficult to compare techniques with each other. This paper presents SecBench.js, the first comprehensive benchmark suite of vulnerabilities and executable exploits for npm. The benchmark comprises 600 vulnerabilities, which cover the five most common vulnerability classes for server-side JavaScript. Each vulnerability comes with a payload that exploits the vulnerability and an oracle that validates successful exploitation. SecBench.js enables various applications, of which we explore three in this paper: (i) crosschecking SecBench.js against existing security advisories reveals 168 vulnerable versions in 19 packages that are mislabeled in the advisories; (ii) applying simple code transformations to the exploits in our suite helps identify flawed fixes of vulnerabilities; (iii) dynamically analyzing calls to common sink APIs, e.g., exec(), yields a ground truth of code locations for evaluating vulnerability detectors. Beyond providing a reusable benchmark to the community, our work identified 20 zero-day vulnerabilities, most of which are already acknowledged by practitioners.
",Achievement,Successful,The paper presents 'SecBench.js'; the first comprehensive benchmark suite of Vulnerabilities and executable exploits for npm; which enables applications to validate successful exploitation. This aligns with the value item 'Successful' and its corresponding value 'Achievement' as it empowers the software users to navigate potential vulnerabilities ensuring their success.,"The alignment of 'Paper X' with the value item Successful and its corresponding value Achievement is justified because the paper introduces a benchmark suite, SecBench.js, that enables software users to validate the successful exploitation of vulnerabilities. By providing a comprehensive set of vulnerabilities and executable exploits, the paper empowers software users to assess and navigate potential vulnerabilities effectively, ensuring the achievement of successful outcomes in terms of security and protection.",Agreed-Justified,Agree,,Agree,,
Exp E,Exp J,1125,ICSE,Security & Privacy,SecBench.js: An Executable Security Benchmark Suite for Server-Side JavaScript,"Npm is the largest software ecosystem in the world, offering millions of free, reusable packages. In recent years, various security threats to packages published on npm have been reported, including vulnerabilities that affect millions of users. To continuously improve techniques for detecting vulnerabilities and mitigating attacks that exploit them, a reusable benchmark of vulnerabilities would be highly desirable. Ideally, such a benchmark should be realistic, come with executable exploits, and include fixes of vulnerabilities. Unfortunately, there currently is no such benchmark, forcing researchers to repeatedly develop their own evaluation datasets and making it difficult to compare techniques with each other. This paper presents SecBench.js, the first comprehensive benchmark suite of vulnerabilities and executable exploits for npm. The benchmark comprises 600 vulnerabilities, which cover the five most common vulnerability classes for server-side JavaScript. Each vulnerability comes with a payload that exploits the vulnerability and an oracle that validates successful exploitation. SecBench.js enables various applications, of which we explore three in this paper: (i) crosschecking SecBench.js against existing security advisories reveals 168 vulnerable versions in 19 packages that are mislabeled in the advisories; (ii) applying simple code transformations to the exploits in our suite helps identify flawed fixes of vulnerabilities; (iii) dynamically analyzing calls to common sink APIs, e.g., exec(), yields a ground truth of code locations for evaluating vulnerability detectors. Beyond providing a reusable benchmark to the community, our work identified 20 zero-day vulnerabilities, most of which are already acknowledged by practitioners.
",Security,Healthy,By presenting a benchmark suite that includes fixes of vulnerabilities and provides a ground truth of code locations for evaluating vulnerability detectors; 'SecBench.js' contributes to the value item 'Healthy' under 'Security'. The tool helps to maintain a clean; safe (or healthy) software environment for users.,"I apologize for any confusion. The justification for aligning 'Paper X' with the value item Healthy and its corresponding value Security is based on the fact that the benchmark suite presented, SecBench.js, includes fixes for vulnerabilities and provides a ground truth of code locations for evaluating vulnerability detectors. By addressing and mitigating vulnerabilities, the tool contributes to creating a secure and healthy software environment for users, ensuring the safety of their data and minimizing potential risks.",Agreed-Clarified,Agree,,Agree,,
Exp F,Exp J,1125,ICSE,Security & Privacy,SecBench.js: An Executable Security Benchmark Suite for Server-Side JavaScript,"Npm is the largest software ecosystem in the world, offering millions of free, reusable packages. In recent years, various security threats to packages published on npm have been reported, including vulnerabilities that affect millions of users. To continuously improve techniques for detecting vulnerabilities and mitigating attacks that exploit them, a reusable benchmark of vulnerabilities would be highly desirable. Ideally, such a benchmark should be realistic, come with executable exploits, and include fixes of vulnerabilities. Unfortunately, there currently is no such benchmark, forcing researchers to repeatedly develop their own evaluation datasets and making it difficult to compare techniques with each other. This paper presents SecBench.js, the first comprehensive benchmark suite of vulnerabilities and executable exploits for npm. The benchmark comprises 600 vulnerabilities, which cover the five most common vulnerability classes for server-side JavaScript. Each vulnerability comes with a payload that exploits the vulnerability and an oracle that validates successful exploitation. SecBench.js enables various applications, of which we explore three in this paper: (i) crosschecking SecBench.js against existing security advisories reveals 168 vulnerable versions in 19 packages that are mislabeled in the advisories; (ii) applying simple code transformations to the exploits in our suite helps identify flawed fixes of vulnerabilities; (iii) dynamically analyzing calls to common sink APIs, e.g., exec(), yields a ground truth of code locations for evaluating vulnerability detectors. Beyond providing a reusable benchmark to the community, our work identified 20 zero-day vulnerabilities, most of which are already acknowledged by practitioners.
",Power,Authority,SecBench.js' also identifies 20 zero-day vulnerabilities; which developers have acknowledged. This establishes an authority in the npm ecosystem contributing to the value of 'Authority' in 'Power'. It asserts its dominance and decision-making capacity to identify and rectify potential threats for users.,"In the abstract, 'Paper X' states that SecBench.js identifies 20 zero-day vulnerabilities, which are already acknowledged by practitioners. This demonstrates the authority and expertise of the benchmark in identifying and addressing potential threats within the npm ecosystem. By providing this valuable information, 'Paper X' contributes to the value of 'Authority' in terms of 'Power' for software users, as it establishes a sense of trust and confidence in the benchmark's ability to protect users from vulnerabilities.",Agreed-Justified,Agree,,Agree,,
Exp G,Exp K,1126,ICSE,Data Management & Processing,Detecting Isolation Bugs via Transaction Oracle Construction,"Transactions are used to maintain the data integrity of databases, and have become an indispensable feature in modern Database Management Systems (DBMSs). Despite extensive efforts in testing DBMSs and verifying transaction processing mechanisms, isolation bugs still exist in widely-used DBMSs when these DBMSs violate their claimed transaction isolation levels. Isolation bugs can cause severe consequences, eg, incorrect query results and database states. In this paper, we propose a novel transaction testing approach, Transaction oracle construction (Troc), to automatically detect isolation bugs in DBMSs. The core idea of Troc is to decouple a transaction into independent statements, and execute them on their own database views, which are constructed under the guidance of the claimed transaction isolation level. Any divergence between the actual transaction execution and the independent statement execution indicates an isolation bug. We implement and evaluate Troc on three widely-used DBMSs, ie, MySQL, MariaDB, and TiDB. We have detected 5 previouslyunknown isolation bugs in the latest versions of these DBMSs.
",Security,Healthy,"The paper proposes a new transaction testing approach to detect bugs and maintain data integrity; contributing to the value item ""Healthy"" of the value ""Security"" from a software context due to the overall software health improvement.","The justification for labeling 'Paper X' as aligning with the value item Healthy and its corresponding value Security is that the paper's proposed transaction testing approach aims to detect isolation bugs in DBMSs, which can cause severe consequences, such as incorrect query results and database states. By addressing these bugs and improving data integrity, the paper indirectly contributes to the overall health and security of the software, which is important from a software user perspective.",Agreed-Justified,Disagree,Agree with coder_2,Disagree,Software health is not necessarily human health,
Exp A,Exp H,1126,ICSE,Data Management & Processing,Detecting Isolation Bugs via Transaction Oracle Construction,"Transactions are used to maintain the data integrity of databases, and have become an indispensable feature in modern Database Management Systems (DBMSs). Despite extensive efforts in testing DBMSs and verifying transaction processing mechanisms, isolation bugs still exist in widely-used DBMSs when these DBMSs violate their claimed transaction isolation levels. Isolation bugs can cause severe consequences, eg, incorrect query results and database states. In this paper, we propose a novel transaction testing approach, Transaction oracle construction (Troc), to automatically detect isolation bugs in DBMSs. The core idea of Troc is to decouple a transaction into independent statements, and execute them on their own database views, which are constructed under the guidance of the claimed transaction isolation level. Any divergence between the actual transaction execution and the independent statement execution indicates an isolation bug. We implement and evaluate Troc on three widely-used DBMSs, ie, MySQL, MariaDB, and TiDB. We have detected 5 previouslyunknown isolation bugs in the latest versions of these DBMSs.
",Conformity,Self-Discipline,"The proposed Transaction oracle construction (Troc) adheres to a disciplined approach by decoupling transaction into independent statements and executing each on their own database views; aligning with the value item ""Self-Discipline"" and its corresponding value ""Conformity"".","In the context of software users, the value item ""Self-Discipline"" and its corresponding value ""Conformity"" align with the proposed approach of Transaction oracle construction (Troc) in 'Paper X' because it emphasizes adhering to a disciplined and structured method of decoupling transactions into independent statements and executing them on separate database views. This demonstrates a conformity to specific guidelines and practices in order to ensure accurate and reliable transaction processing, which is aligned with the values of self-discipline and conforming to established standards within the software context.",Agreed-Clarified,Agree,,Agree,,
Exp B,Exp H,1126,ICSE,Data Management & Processing,Detecting Isolation Bugs via Transaction Oracle Construction,"Transactions are used to maintain the data integrity of databases, and have become an indispensable feature in modern Database Management Systems (DBMSs). Despite extensive efforts in testing DBMSs and verifying transaction processing mechanisms, isolation bugs still exist in widely-used DBMSs when these DBMSs violate their claimed transaction isolation levels. Isolation bugs can cause severe consequences, eg, incorrect query results and database states. In this paper, we propose a novel transaction testing approach, Transaction oracle construction (Troc), to automatically detect isolation bugs in DBMSs. The core idea of Troc is to decouple a transaction into independent statements, and execute them on their own database views, which are constructed under the guidance of the claimed transaction isolation level. Any divergence between the actual transaction execution and the independent statement execution indicates an isolation bug. We implement and evaluate Troc on three widely-used DBMSs, ie, MySQL, MariaDB, and TiDB. We have detected 5 previouslyunknown isolation bugs in the latest versions of these DBMSs.
",Security,Social Order,"The paper's main contribution to providing a method to detect and correct isolation bugs in databases automatically contributes to the establishment of ""Social Order"" under the ""Security"" values; as it ensures a stable and well-functioning software environment for users.","The main contribution of 'Paper X' aligns with the value item Social Order and its corresponding value Security because by detecting and correcting isolation bugs in databases, it ensures that the software environment is stable and well-functioning for users. This contributes to maintaining social order as it prevents potential disruptions and vulnerabilities in the software system, leading to a secure and reliable user experience.",Agreed-Justified,Disagree,"Related to Achievement: capable more, as it automatically identifies bugs",Disagree,The justification doesn't align with the value item of the Social order,
Exp C,Exp K,1128,ICSE,Software Engineering Practices,STILL AROUND: Experiences and Survival Strategies of Veteran Women Software Developers,"The intersection of ageism and sexism can create a hostile environment for veteran software developers belonging to marginalized genders. In this study, we conducted 14 interviews to examine the experiences of people at this intersection, primarily women, in order to discover the strategies they employed in order to successfully remain in the field. We identified 283 codes, which fell into three main categories: Strategies, Experiences, and Perception. Several strategies we identified, such as (Deliberately) Not Trying to Look Younger, were not previously described in the software engineering literature. We found that, in some companies, older women developers are recognized as having particular value, further strengthening the known benefits of diversity in the workforce. Based on the experiences and strategies, we suggest organizations employing software developers to consider the benefits of hiring veteran women software developers. For example, companies can draw upon the life experiences of older women developers in order to better understand the needs of customers from a similar demographic. While we recognize that many of the strategies employed by our study participants are a response to systemic issues, we still consider that, in the short-term, there is benefit in describing these strategies for developers who are experiencing such issues today.
",Power,Social Recognition,The abstract mentions that older women developers in some companies are recognized for their particular value. This recognition aligns with the value item Social Recognition and its corresponding value Power.,"In the abstract of 'Paper X', it is explicitly stated that older women developers in certain companies are recognized for their particular value. This recognition implies that they hold a certain level of authority and influence in the software development field, aligning with the value item Social Recognition and its corresponding value Power. As a ""Software User,"" this alignment highlights the importance of recognizing and valuing the expertise and contributions of veteran women software developers, as they can bring unique perspectives and insights based on their life experiences, ultimately benefiting the software user's experience.",Agreed-Clarified,Agree,,Agree,,
Exp D,Exp K,1128,ICSE,Software Engineering Practices,STILL AROUND: Experiences and Survival Strategies of Veteran Women Software Developers,"The intersection of ageism and sexism can create a hostile environment for veteran software developers belonging to marginalized genders. In this study, we conducted 14 interviews to examine the experiences of people at this intersection, primarily women, in order to discover the strategies they employed in order to successfully remain in the field. We identified 283 codes, which fell into three main categories: Strategies, Experiences, and Perception. Several strategies we identified, such as (Deliberately) Not Trying to Look Younger, were not previously described in the software engineering literature. We found that, in some companies, older women developers are recognized as having particular value, further strengthening the known benefits of diversity in the workforce. Based on the experiences and strategies, we suggest organizations employing software developers to consider the benefits of hiring veteran women software developers. For example, companies can draw upon the life experiences of older women developers in order to better understand the needs of customers from a similar demographic. While we recognize that many of the strategies employed by our study participants are a response to systemic issues, we still consider that, in the short-term, there is benefit in describing these strategies for developers who are experiencing such issues today.
",Benevolence,Loyalty,The abstract suggests organizations to consider the benefits of hiring veteran women software developers; implying the responsibility of organizations to foster diversity. This aligns with the value item Responsibility and its corresponding value Benevolence.,"The abstract of 'Paper X' highlights the suggestion for organizations to consider the benefits of hiring veteran women software developers, which implies a sense of responsibility for fostering diversity. This aligns with the value item Loyalty, as organizations are encouraged to prioritize the well-being and success of their employees by recognizing the value of their experiences and contributions. This also aligns with the corresponding value of Benevolence, as organizations are encouraged to show kindness and consideration towards marginalized genders by creating an inclusive and supportive environment.",Agreed-Justified,Agree,,Agree,,
Exp F,Exp J,1140,ICSE,AI & Machine Learning,Information-Theoretic Testing and Debugging of Fairness Defects in Deep Neural Networks,"The deep feedforward neural networks (DNNs) are increasingly deployed in socioeconomic critical decision support software systems. DNNs are exceptionally good at finding minimal, sufficient statistical patterns within their training data. Consequently, DNNs may learn to encode decisions -- amplifying existing biases or introducing new ones -- that may disadvantage protected individuals/groups and may stand to violate legal protections. While the existing search based software testing approaches have been effective in discovering fairness defects, they do not supplement these defects with debugging aids -- such as severity and causal explanations -- crucial to help developers triage and decide on the next course of action. Can we measure the severity of fairness defects in DNNs? Are these defects symptomatic of improper training or they merely reflect biases present in the training data? To answer such questions, we present DICE: an information-theoretic testing and debugging framework to discover and localize fairness defects in DNNs. The key goal of DICE is to assist software developers in triaging fairness defects by ordering them by their severity. Towards this goal, we quantify fairness in terms of protected information (in bits) used in decision making. A quantitative view of fairness defects not only helps in ordering these defects, our empirical evaluation shows that it improves the search efficiency due to resulting smoothness of the search space. Guided by the quantitative fairness, we present a causal debugging framework to localize inadequately trained layers and neurons responsible for fairness defects. Our experiments over ten DNNs, developed for socially critical tasks, show that DICE efficiently characterizes the amounts of discrimination, effectively generates discriminatory instances, and localizes layers/neurons with significant biases.
",Security,Sense of Belonging,The paper contributes DICE an information-theoretic testing and debugging framework to discover and localize fairness defects and this can enhance a sense of security and belonging by ensuring that the users won't possibly be disadvantaged by encoded unfair decisions in the software system.,"In the context of a software system, the main contribution of 'Paper X' aligns with the value item Sense of Belonging and its corresponding value Security from a ""Software User"" perspective because the DICE framework helps in identifying and localizing fairness defects in decision-making processes. By ensuring that users are not disadvantaged by unfair decisions encoded in the software system, the framework promotes a sense of belonging and security, as users can trust that their interests and rights are protected within the system.",Agreed-Clarified,Agree,,Agree,,
Exp A,Exp H,1140,ICSE,AI & Machine Learning,Information-Theoretic Testing and Debugging of Fairness Defects in Deep Neural Networks,"The deep feedforward neural networks (DNNs) are increasingly deployed in socioeconomic critical decision support software systems. DNNs are exceptionally good at finding minimal, sufficient statistical patterns within their training data. Consequently, DNNs may learn to encode decisions -- amplifying existing biases or introducing new ones -- that may disadvantage protected individuals/groups and may stand to violate legal protections. While the existing search based software testing approaches have been effective in discovering fairness defects, they do not supplement these defects with debugging aids -- such as severity and causal explanations -- crucial to help developers triage and decide on the next course of action. Can we measure the severity of fairness defects in DNNs? Are these defects symptomatic of improper training or they merely reflect biases present in the training data? To answer such questions, we present DICE: an information-theoretic testing and debugging framework to discover and localize fairness defects in DNNs. The key goal of DICE is to assist software developers in triaging fairness defects by ordering them by their severity. Towards this goal, we quantify fairness in terms of protected information (in bits) used in decision making. A quantitative view of fairness defects not only helps in ordering these defects, our empirical evaluation shows that it improves the search efficiency due to resulting smoothness of the search space. Guided by the quantitative fairness, we present a causal debugging framework to localize inadequately trained layers and neurons responsible for fairness defects. Our experiments over ten DNNs, developed for socially critical tasks, show that DICE efficiently characterizes the amounts of discrimination, effectively generates discriminatory instances, and localizes layers/neurons with significant biases.
",Universalism,Social Justice,By addressing the issue of fairness defects and potential biases in decision support software systems; the paper contributes to the user value item of 'Social Justice'; aimed at reducing discrimination and ensuring equality in the use of such systems. This aligns with the value of Universalism.,"The main contributions of 'Paper X' in addressing fairness defects and biases in decision support software systems align with the value item of Social Justice because it focuses on reducing discrimination and ensuring equality in the use of such systems. This aligns with the value of Universalism, which emphasizes broadmindedness, equality, and social justice. By providing a framework to quantify and triage fairness defects, the paper directly contributes to promoting social justice and aligns with the values of Universalism.",Agreed-Justified,Agree,,Agree,,
Exp B,Exp H,1142,ICSE,AI & Machine Learning,Heterogeneous Anomaly Detection for Software Systems via Semi-supervised Cross-modal Attention,"Prompt and accurate detection of system anomalies is essential to ensure the reliability of software systems. Unlike manual efforts that exploit all available run-time information, existing approaches usually leverage only a single type of monitoring data (often logs or metrics) or fail to make effective use of the joint information among different types of data. Consequently, many false predictions occur. To better understand the manifestations of system anomalies, we conduct a systematical study on a large amount of heterogeneous data, i.e., logs and metrics. Our study demonstrates that logs and metrics can manifest system anomalies collaboratively and complementarily, and neither of them only is sufficient. Thus, integrating heterogeneous data can help recover the complete picture of a system's health status. In this context, we propose Hades, the first end-to-end semi-supervised approach to effectively identify system anomalies based on heterogeneous data. Our approach employs a hierarchical architecture to learn a global representation of the system status by fusing log semantics and metric patterns. It captures discriminative features and meaningful interactions from heterogeneous data via a cross-modal attention module, trained in a semi-supervised manner. We evaluate Hades extensively on large-scale simulated data and datasets from Huawei Cloud. The experimental results present the effectiveness of our model in detecting system anomalies. We also release the code and the annotated dataset for replication and future research.
",Achievement,Intelligent,The abstract mentions the development of an end-to-end semi-supervised approach named Hades. This could potentially lead software users to feel intelligent as they interact with a system that is advanced and leverages heterogenous data.,"By developing an end-to-end semi-supervised approach named Hades that effectively identifies system anomalies based on heterogeneous data, 'Paper X' aligns with the value item Intelligent and its corresponding value Achievement from a ""Software User"" perspective. This alignment is based on the idea that users interacting with a system equipped with Hades would perceive it as intelligent, as it demonstrates advanced capabilities in accurately detecting anomalies by leveraging diverse types of data. This perception of intelligence can provide users with a sense of achievement, as they would feel empowered to trust and rely on a system that demonstrates advanced problem-solving abilities.",Agreed-Clarified,Agree,,Disagree,The justification doesn't align with the value item of the Intelligent,
Exp E,Exp J,1145,ICSE,AI & Machine Learning,Reliability Assurance for Deep Neural Network Architectures Against Numerical Defects,"With the widespread deployment of deep neural networks (DNNs), ensuring the reliability of DNN-based systems is of great importance. Serious reliability issues such as system failures can be caused by numerical defects, one of the most frequent defects in DNNs. To assure high reliability against numerical defects, in this paper, we propose the RANUM approach including novel techniques for three reliability assurance tasks: detection of potential numerical defects, confirmation of potential-defect feasibility, and suggestion of defect fixes. To the best of our knowledge, RANUM is the first approach that confirms potential-defect feasibility with failure-exhibiting tests and suggests fixes automatically. Extensive experiments on the benchmarks of 63 real-world DNN architectures show that RANUM outperforms state-of-the-art approaches across the three reliability assurance tasks. In addition, when the RANUM-generated fixes are compared with developers' fixes on open-source projects, in 37 out of 40 cases, RANUM-generated fixes are equivalent to or even better than human fixes.
",Achievement,Capable,The paper introduces an approach to increase the reliability of DNN-based systems; enabling system users to easily achieve the objective of high system reliability. This aligns with the value item Capable and its corresponding value Achievement.,"The justification for aligning 'Paper X' with the value item Capable and its corresponding value Achievement is that the paper introduces the RANUM approach, which aims to ensure high reliability of DNN-based systems. By proposing novel techniques for reliability assurance tasks, such as detecting potential numerical defects and suggesting defect fixes, the paper enables system users to achieve the objective of high system reliability. This alignment directly aligns with the value item Capable, as it empowers users to effectively address reliability issues, and the value Achievement, as it allows users to accomplish the goal of reliable DNN-based systems.",Agreed-Justified,Agree,,Agree,,
Exp F,Exp J,1145,ICSE,AI & Machine Learning,Reliability Assurance for Deep Neural Network Architectures Against Numerical Defects,"With the widespread deployment of deep neural networks (DNNs), ensuring the reliability of DNN-based systems is of great importance. Serious reliability issues such as system failures can be caused by numerical defects, one of the most frequent defects in DNNs. To assure high reliability against numerical defects, in this paper, we propose the RANUM approach including novel techniques for three reliability assurance tasks: detection of potential numerical defects, confirmation of potential-defect feasibility, and suggestion of defect fixes. To the best of our knowledge, RANUM is the first approach that confirms potential-defect feasibility with failure-exhibiting tests and suggests fixes automatically. Extensive experiments on the benchmarks of 63 real-world DNN architectures show that RANUM outperforms state-of-the-art approaches across the three reliability assurance tasks. In addition, when the RANUM-generated fixes are compared with developers' fixes on open-source projects, in 37 out of 40 cases, RANUM-generated fixes are equivalent to or even better than human fixes.
",Security,Healthy,By addressing system failures caused by numerical defects in DNNs; the paper provides a solution that ensures the healthy operation of DNN-based systems. This aligns with the value item Healthy and its corresponding value Security.,"The justification for aligning 'Paper X' with the value item Healthy and its corresponding value Security is based on the fact that the paper addresses reliability issues in DNN-based systems that can lead to system failures. By proposing the RANUM approach, the paper aims to detect and confirm potential numerical defects, as well as suggest fixes. This directly contributes to ensuring the healthy operation of DNN-based systems, as system failures can be detrimental to the security and overall well-being of software users relying on these systems.",Agreed-Justified,Agree,,Agree,,
Exp G,Exp K,1145,ICSE,AI & Machine Learning,Reliability Assurance for Deep Neural Network Architectures Against Numerical Defects,"With the widespread deployment of deep neural networks (DNNs), ensuring the reliability of DNN-based systems is of great importance. Serious reliability issues such as system failures can be caused by numerical defects, one of the most frequent defects in DNNs. To assure high reliability against numerical defects, in this paper, we propose the RANUM approach including novel techniques for three reliability assurance tasks: detection of potential numerical defects, confirmation of potential-defect feasibility, and suggestion of defect fixes. To the best of our knowledge, RANUM is the first approach that confirms potential-defect feasibility with failure-exhibiting tests and suggests fixes automatically. Extensive experiments on the benchmarks of 63 real-world DNN architectures show that RANUM outperforms state-of-the-art approaches across the three reliability assurance tasks. In addition, when the RANUM-generated fixes are compared with developers' fixes on open-source projects, in 37 out of 40 cases, RANUM-generated fixes are equivalent to or even better than human fixes.
",Security,Family Security,The RANUM approach introduced in the paper assures high reliability; hence providing family security as families increasingly rely on technologies based on DNNs for various purposes. This aligns with the value item Family Security and its corresponding value Security.,"The justification for aligning 'Paper X' with the value item Family Security and its corresponding value Security is based on the fact that the RANUM approach proposed in the paper aims to ensure high reliability of DNN-based systems. This reliability assurance directly addresses numerical defects in DNNs, which can lead to serious failures in these systems. By addressing these defects and providing reliable DNN systems, the paper contributes to enhancing the security of families who rely on such technologies. As families increasingly incorporate DNN-based systems into their daily lives for various purposes, ensuring the security and reliability of these systems becomes vital, making the alignment with Family Security and Security value items significant.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,1163,ICSE,Software Testing & QA,Coverage Guided Fault Injection for Cloud Systems,"To support high reliability and availability, modern cloud systems are designed to be resilient to node crashes and reboots. That is, a cloud system should gracefully recover from node crashes/reboots and continue to function. However, node crashes/reboots that occur under special timing can trigger crash recovery bugs that lie in incorrect crash recovery protocols and their implementations. To ensure that a cloud system is free from crash recovery bugs, some fault injection approaches have been proposed to test whether a cloud system can correctly recover from various crash scenarios. These approaches are not effective in exploring the huge crash scenario space without developers‚Äö√Ñ√∂‚àö√ë‚àö‚àÇ‚Äö√†√∂‚àö√´‚Äö√†√∂‚Äö√†√á‚Äö√Ñ√∂‚àö‚Ä†‚àö‚àÇ‚Äö√†√∂‚àö¬¥‚Äö√Ñ√∂‚àö‚Ä†‚àö‚àÇ¬¨¬®‚Äö√Ñ¬¢ knowledge.
In this paper, we propose CrashFuzz, a fault injection testing approach that can effectively test crash recovery behaviors and reveal crash recovery bugs in cloud systems. CrashFuzz mutates the combinations of possible node crashes and reboots according to runtime feedbacks, and prioritizes the combinations that are prone to increase code coverage and trigger crash recovery bugs for smart exploration. We have implemented CrashFuzz and evaluated it on three popular open-source cloud systems, ie, ZooKeeper, HDFS and HBase. CrashFuzz has detected 4 unknown bugs and 1 known bug. Compared with other fault injection approaches, CrashFuzz can detect more crash recovery bugs and achieve higher code coverage.
",Achievement,Intelligent,The paper contributes the 'CrashFuzz'; a fault injection testing approach that ensures high reliability and availability of cloud systems. This can help users to have a better digital experience thus empowering their intelligence. This aligns with the value item Intelligent and its corresponding value Achievement.,"The justification for labeling 'Paper X' with the value item Intelligent and its corresponding value Achievement is that the proposed approach, CrashFuzz, aims to improve the reliability and availability of cloud systems. By effectively testing crash recovery behaviors and revealing crash recovery bugs, the approach helps users to have a better digital experience. This aligns with the value item Intelligent, as it empowers users by enabling them to make informed decisions and achieve their goals in using cloud systems. Additionally, the focus on enhancing the performance and functionality of cloud systems aligns with the value of Achievement, as it aims to provide users with successful and influential experiences in their software usage.",Agreed-Justified,Agree,,Agree,,
Exp B,Exp H,1163,ICSE,Software Testing & QA,Coverage Guided Fault Injection for Cloud Systems,"To support high reliability and availability, modern cloud systems are designed to be resilient to node crashes and reboots. That is, a cloud system should gracefully recover from node crashes/reboots and continue to function. However, node crashes/reboots that occur under special timing can trigger crash recovery bugs that lie in incorrect crash recovery protocols and their implementations. To ensure that a cloud system is free from crash recovery bugs, some fault injection approaches have been proposed to test whether a cloud system can correctly recover from various crash scenarios. These approaches are not effective in exploring the huge crash scenario space without developers‚Äö√Ñ√∂‚àö√ë‚àö‚àÇ‚Äö√†√∂‚àö√´‚Äö√†√∂‚Äö√†√á‚Äö√Ñ√∂‚àö‚Ä†‚àö‚àÇ‚Äö√†√∂‚àö¬¥‚Äö√Ñ√∂‚àö‚Ä†‚àö‚àÇ¬¨¬®‚Äö√Ñ¬¢ knowledge.
In this paper, we propose CrashFuzz, a fault injection testing approach that can effectively test crash recovery behaviors and reveal crash recovery bugs in cloud systems. CrashFuzz mutates the combinations of possible node crashes and reboots according to runtime feedbacks, and prioritizes the combinations that are prone to increase code coverage and trigger crash recovery bugs for smart exploration. We have implemented CrashFuzz and evaluated it on three popular open-source cloud systems, ie, ZooKeeper, HDFS and HBase. CrashFuzz has detected 4 unknown bugs and 1 known bug. Compared with other fault injection approaches, CrashFuzz can detect more crash recovery bugs and achieve higher code coverage.
",Security,Healthy,The 'CrashFuzz' ensures the system's resilience against node crashes/reboots and maintains its functionality. This aligns with the value item Healthy and its corresponding value Security.,"In the context of a software user, the value item Healthy and its corresponding value Security align with the main contributions of 'Paper X' because the proposed approach, CrashFuzz, aims to ensure the resilience and availability of cloud systems by testing their crash recovery behaviors. By effectively testing and revealing crash recovery bugs, CrashFuzz helps to maintain the healthy functioning of the system and provides a sense of security to the software users, ensuring that the system can gracefully recover from node crashes and continue to function as expected.",Agreed-Justified,Disagree,reconcile with coder_2,Disagree,"The justification doesn't align with the value item of the Healthy
",
Exp D,Exp K,1165,ICSE,Software Deployment & Operations,Incident-aware Duplicate Ticket Aggregation for Cloud Systems,"In cloud systems, incidents are potential threats to customer satisfaction and business revenue. When customers are affected by incidents, they often request customer support service (CSS) from the cloud provider by submitting a support ticket. Many tickets could be duplicate as they are reported in a distributed and uncoordinated manner. Thus, aggregating such duplicate tickets is essential for efficient ticket management. Previous studies mainly rely on tickets' textual similarity to detect duplication; however, duplicate tickets in a cloud system could carry semantically different descriptions due to the complex service dependency of the cloud system. To tackle this problem, we propose iPACK, an incident-aware method for aggregating duplicate tickets by fusing the failure information between the customer side (i.e., tickets) and the cloud side (i.e., incidents). We extensively evaluate iPACK on three datasets collected from the production environment of a large-scale cloud platform, Azure. The experimental results show that iPACK can precisely and comprehensively aggregate duplicate tickets, achieving an F1 score of 0.871 ~ 0.935 and outperforming state-of-the-art methods by 12.4% ~ 31.2%.
",Achievement,Intelligent,The paper presents the development and testing of a method that improves incident detection in cloud systems; demonstrating users' intelligence in identifying a systemic issue and addressing it.,"The justification for aligning 'Paper X' with the value item Intelligent and its corresponding value Achievement from a ""Software User"" perspective is based on the fact that the paper presents a method that improves incident detection in cloud systems, which requires users to possess intelligence in identifying the systemic issue and addressing it. By developing and testing this method, the paper demonstrates the users' ability to achieve success in resolving incidents and ensuring the smooth functioning of the cloud system.",Agreed-Justified,Agree,,Agree,,
Exp E,Exp J,1165,ICSE,Software Deployment & Operations,Incident-aware Duplicate Ticket Aggregation for Cloud Systems,"In cloud systems, incidents are potential threats to customer satisfaction and business revenue. When customers are affected by incidents, they often request customer support service (CSS) from the cloud provider by submitting a support ticket. Many tickets could be duplicate as they are reported in a distributed and uncoordinated manner. Thus, aggregating such duplicate tickets is essential for efficient ticket management. Previous studies mainly rely on tickets' textual similarity to detect duplication; however, duplicate tickets in a cloud system could carry semantically different descriptions due to the complex service dependency of the cloud system. To tackle this problem, we propose iPACK, an incident-aware method for aggregating duplicate tickets by fusing the failure information between the customer side (i.e., tickets) and the cloud side (i.e., incidents). We extensively evaluate iPACK on three datasets collected from the production environment of a large-scale cloud platform, Azure. The experimental results show that iPACK can precisely and comprehensively aggregate duplicate tickets, achieving an F1 score of 0.871 ~ 0.935 and outperforming state-of-the-art methods by 12.4% ~ 31.2%.
",Security,Family Security,By addressing incidents in cloud systems; the paper's contributions directly enhance the security and protection of the systems used by families and businesses thus directly aligning with Family Security.,"By specifically addressing incidents in cloud systems and proposing a method to aggregate duplicate support tickets, 'Paper X' directly contributes to enhancing the security and protection of the systems accessed by families and businesses. This aligns with the value item of Family Security from Schwartz's Taxonomy, as ensuring a secure and reliable software system is crucial for the safety and well-being of users and their families. The paper's focus on efficient ticket management and preventing duplicate incidents directly addresses the security concerns of users, providing them with a more secure software environment.",Agreed-Justified,Agree,,Agree,,
Exp G,Exp K,1171,ICSE,AI & Machine Learning,An Empirical Study of Pre-Trained Model Reuse in the Hugging Face Deep Learning Model Registry,"Deep Neural Networks (DNNs) are being adopted as components in software systems. Creating and specializing DNNs from scratch has grown increasingly difficult as state-of-the-art architectures grow more complex. Following the path of traditional software engineering, machine learning engineers have begun to reuse large-scale pre-trained models (PTMs) and fine-tune these models for downstream tasks. Prior works have studied reuse practices for traditional software packages to guide software engineers towards better package maintenance and dependency management. We lack a similar foundation of knowledge to guide behaviors in pre-trained model ecosystems.

In this work, we present the first empirical investigation of PTM reuse. We interviewed 12 practitioners from the most popular PTM ecosystem, Hugging Face, to learn the practices and challenges of PTM reuse. From this data, we model the decision-making process for PTM reuse. Based on the identified practices, we describe useful attributes for model reuse, including provenance, reproducibility, and portability. Three challenges for PTM reuse are missing attributes, discrepancies between claimed and actual performance, and model risks. We substantiate these identified challenges with systematic measurements in the Hugging Face ecosystem. Our work informs future directions on optimizing deep learning ecosystems by automated measuring useful attributes and potential attacks, and envision future research on infrastructure and standardization for model registries.

",Achievement,Intelligent,The paper investigates practices and challenges of pre-trained model reuse; focusing on providing users with useful attributes for model reuse; including provenance; reproducibility; and portability. This aligns with the value item Intelligent and its corresponding value Achievement; because it aims to enhance users' ability to navigate and utilize pre-trained models intelligently.,"The justification for aligning 'Paper X' with the value item Intelligent and its corresponding value Achievement is based on the fact that the paper focuses on providing users with useful attributes for model reuse, such as provenance, reproducibility, and portability. By enhancing users' ability to navigate and utilize pre-trained models intelligently, the paper aligns with the value item Intelligent as it promotes users' capacity to make informed decisions and achieve their goals effectively. This contributes to the value of Achievement as users are empowered to optimize their usage of pre-trained models and achieve successful outcomes in their software context.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,1171,ICSE,AI & Machine Learning,An Empirical Study of Pre-Trained Model Reuse in the Hugging Face Deep Learning Model Registry,"Deep Neural Networks (DNNs) are being adopted as components in software systems. Creating and specializing DNNs from scratch has grown increasingly difficult as state-of-the-art architectures grow more complex. Following the path of traditional software engineering, machine learning engineers have begun to reuse large-scale pre-trained models (PTMs) and fine-tune these models for downstream tasks. Prior works have studied reuse practices for traditional software packages to guide software engineers towards better package maintenance and dependency management. We lack a similar foundation of knowledge to guide behaviors in pre-trained model ecosystems.

In this work, we present the first empirical investigation of PTM reuse. We interviewed 12 practitioners from the most popular PTM ecosystem, Hugging Face, to learn the practices and challenges of PTM reuse. From this data, we model the decision-making process for PTM reuse. Based on the identified practices, we describe useful attributes for model reuse, including provenance, reproducibility, and portability. Three challenges for PTM reuse are missing attributes, discrepancies between claimed and actual performance, and model risks. We substantiate these identified challenges with systematic measurements in the Hugging Face ecosystem. Our work informs future directions on optimizing deep learning ecosystems by automated measuring useful attributes and potential attacks, and envision future research on infrastructure and standardization for model registries.

",Security,Reciprocation of Favors,The paper identifies challenges for pre-trained model reuse; such as missing attributes and discrepancies between claimed and actual performance. The solution proposed by the paper involves automated measuring of useful attributes. This helps encourage users to acknowledge and reciprocate the benefits they receive from using the software; aligning with the value item Reciprocation of Favors and its corresponding value Security.,"In the context of the paper, the alignment of the value item Reciprocation of Favors and its corresponding value Security is evident as the paper addresses the challenges of pre-trained model reuse, which involves users benefiting from the software. By proposing automated measuring of useful attributes, the paper encourages software users to acknowledge and reciprocate these benefits, thereby promoting a sense of security in knowing that their use of the software is based on a fair exchange of favors.",Agreed-Clarified,Disagree,reconcile with coder_2,Disagree,The justification doesn't align with the value item of the Reciprocation of Favors,
Exp B,Exp H,1171,ICSE,AI & Machine Learning,An Empirical Study of Pre-Trained Model Reuse in the Hugging Face Deep Learning Model Registry,"Deep Neural Networks (DNNs) are being adopted as components in software systems. Creating and specializing DNNs from scratch has grown increasingly difficult as state-of-the-art architectures grow more complex. Following the path of traditional software engineering, machine learning engineers have begun to reuse large-scale pre-trained models (PTMs) and fine-tune these models for downstream tasks. Prior works have studied reuse practices for traditional software packages to guide software engineers towards better package maintenance and dependency management. We lack a similar foundation of knowledge to guide behaviors in pre-trained model ecosystems.

In this work, we present the first empirical investigation of PTM reuse. We interviewed 12 practitioners from the most popular PTM ecosystem, Hugging Face, to learn the practices and challenges of PTM reuse. From this data, we model the decision-making process for PTM reuse. Based on the identified practices, we describe useful attributes for model reuse, including provenance, reproducibility, and portability. Three challenges for PTM reuse are missing attributes, discrepancies between claimed and actual performance, and model risks. We substantiate these identified challenges with systematic measurements in the Hugging Face ecosystem. Our work informs future directions on optimizing deep learning ecosystems by automated measuring useful attributes and potential attacks, and envision future research on infrastructure and standardization for model registries.

",Universalism,Wisdom,The paper presents the first empirical investigation of pre-trained model reuse within the Hugging Face ecosystem. The insights and recommendations provided in the paper can guide users to make wise decisions when utilizing pre-trained models; aligning with the value item Wisdom and its corresponding value Universalism.,"In the paper, the authors conduct an empirical investigation of pre-trained model reuse and provide insights and recommendations for users within the Hugging Face ecosystem. This aligns with the value item Wisdom, as the paper offers knowledge and guidance to users, enabling them to make wise decisions when utilizing pre-trained models. Additionally, this aligns with the corresponding value Universalism, as the paper aims to optimize deep learning ecosystems and envision future research on infrastructure and standardization, emphasizing the importance of considering the broader context and impacts of model reuse.",Agreed-Clarified,Agree,,Agree,,
Exp C,Exp K,1173,ICSE,Security & Privacy,Compatible Remediation on Vulnerabilities from Third-Party Libraries for Java Projects,"With the increasing disclosure of vulnerabilities in open-source software, software composition analysis (SCA) has been widely applied to reveal third-party libraries and the associated vulnerabilities in software projects. Beyond the revelation, SCA tools adopt various remediation strategies to fix vulnerabilities, the quality of which varies substantially. However, ineffective remediation could induce side effects, such as compilation failures, which impede acceptance by users. According to our studies, existing SCA tools could not correctly handle the concerns of users regarding the compatibility of remediated projects. To this end, we propose Compatible Remediation of Third-party libraries (CORAL) for Maven projects to fix vulnerabilities without breaking the projects. The evaluation proved that CORAL not only fixed 87.56% of vulnerabilities which outperformed other tools (best 75.32%) and achieved a 98.67% successful compilation rate and a 92.96% successful unit test rate. Furthermore, we found that 78.45% of vulnerabilities in popular Maven projects could be fixed without breaking the compilation, and the rest of the vulnerabilities (21.55%) could either be fixed by upgrades that break the compilations or even be impossible to fix by upgrading.
",Achievement,Successful,The paper presents a new tool; CORAL; that successfully fixes a significant percentage of vulnerabilities in open-source software; which aligns with the value item Successful and its corresponding value Achievement.,"The justification for aligning 'Paper X' with the value item Successful and its corresponding value Achievement from a ""Software User"" perspective is based on the fact that the paper introduces a new tool, CORAL, which effectively fixes a high percentage of vulnerabilities in open-source software. This achievement is significant because it demonstrates the success of the tool in addressing security concerns and improving the overall reliability and quality of software projects, which is something that software users value and appreciate.",Agreed-Justified,Agree,,Agree,,
Exp E,Exp J,1173,ICSE,Security & Privacy,Compatible Remediation on Vulnerabilities from Third-Party Libraries for Java Projects,"With the increasing disclosure of vulnerabilities in open-source software, software composition analysis (SCA) has been widely applied to reveal third-party libraries and the associated vulnerabilities in software projects. Beyond the revelation, SCA tools adopt various remediation strategies to fix vulnerabilities, the quality of which varies substantially. However, ineffective remediation could induce side effects, such as compilation failures, which impede acceptance by users. According to our studies, existing SCA tools could not correctly handle the concerns of users regarding the compatibility of remediated projects. To this end, we propose Compatible Remediation of Third-party libraries (CORAL) for Maven projects to fix vulnerabilities without breaking the projects. The evaluation proved that CORAL not only fixed 87.56% of vulnerabilities which outperformed other tools (best 75.32%) and achieved a 98.67% successful compilation rate and a 92.96% successful unit test rate. Furthermore, we found that 78.45% of vulnerabilities in popular Maven projects could be fixed without breaking the compilation, and the rest of the vulnerabilities (21.55%) could either be fixed by upgrades that break the compilations or even be impossible to fix by upgrading.
",Universalism,Social Justice,The tool proposed in the paper enhances the safety and security of the software by addressing vulnerabilities; thereby promoting fair play and equality in the access and usage of software systems. This aligns with the value item Social Justice under the value Universalism.,"In 'Paper X', the proposed tool CORAL addresses vulnerabilities in software projects, ensuring the safety and security of the software for all users. This aligns with the value item Social Justice under the value Universalism because it promotes fairness and equality in the access and usage of software systems, ensuring that all users have equal opportunities to benefit from secure and reliable software.",Agreed-Clarified,Agree,,Agree,,
Exp F,Exp J,1174,ICSE,Security & Privacy,Automated Black-Box Testing of Mass Assignment Vulnerabilities in RESTful APIs,"Mass assignment is one of the most prominent vulnerabilities in RESTful APIs. This vulnerability originates from a misconfiguration in common web frameworks, such that naming convention and automatic binding can be exploited by an attacker to craft malicious requests writing confidential resources and (massively) overriding data, that should be read-only and/or confidential. In this paper, we adopt a black-box testing perspective to automatically detect mass assignment vulnerabilities in RESTful APIs. Execution scenarios are generated purely based on the OpenAPI specification, that lists the available operations and their message format. Clustering is used to group similar operations and reveal read-only fields, the latter are candidate for mass assignment. Then, interaction sequences are automatically generated by instantiating abstract testing templates, trying to exploit the potential vulnerabilities. Finally, test cases are run, and their execution is assessed by a specific oracle, in order to reveal whether the vulnerability could be successfully exploited. The proposed novel approach has been implemented and evaluated on a set of case studies written in different programming languages. The evaluation highlights that the approach is quite effective in detecting seeded vulnerabilities, with a remarkably high accuracy.
",Security,Healthy,The paper's main focus is on detecting and addressing vulnerabilities in RESTful APIs; which are critical to ensuring the software remains functional and secure for users. This aligns with the value item Healthy and its corresponding value Security.,"The alignment of 'Paper X' with the value item Healthy and its corresponding value Security from a ""Software User"" perspective is justified because the paper explicitly states that it aims to detect and address vulnerabilities in RESTful APIs. These vulnerabilities, if exploited, can compromise the security of users' data and their overall user experience, which directly relates to their sense of security and trust in the software. Therefore, ensuring the secure functioning of the software aligns with the value item Healthy and its corresponding value Security, as it prioritizes the well-being and protection of the users.",Agreed-Justified,Agree,,Agree,,
Exp G,Exp K,1174,ICSE,Security & Privacy,Automated Black-Box Testing of Mass Assignment Vulnerabilities in RESTful APIs,"Mass assignment is one of the most prominent vulnerabilities in RESTful APIs. This vulnerability originates from a misconfiguration in common web frameworks, such that naming convention and automatic binding can be exploited by an attacker to craft malicious requests writing confidential resources and (massively) overriding data, that should be read-only and/or confidential. In this paper, we adopt a black-box testing perspective to automatically detect mass assignment vulnerabilities in RESTful APIs. Execution scenarios are generated purely based on the OpenAPI specification, that lists the available operations and their message format. Clustering is used to group similar operations and reveal read-only fields, the latter are candidate for mass assignment. Then, interaction sequences are automatically generated by instantiating abstract testing templates, trying to exploit the potential vulnerabilities. Finally, test cases are run, and their execution is assessed by a specific oracle, in order to reveal whether the vulnerability could be successfully exploited. The proposed novel approach has been implemented and evaluated on a set of case studies written in different programming languages. The evaluation highlights that the approach is quite effective in detecting seeded vulnerabilities, with a remarkably high accuracy.
",Security,Social Order,"The paper's approach to automatically detect mass assignment vulnerabilities in RESTful APIs contributes to maintaining order and consistency in the software thus aligns to the value item ""Social Order"" under the value ""Security"".","The approach proposed in 'Paper X' contributes to maintaining social order and consistency in the software by automatically detecting mass assignment vulnerabilities in RESTful APIs. By identifying and addressing these vulnerabilities, it helps ensure that data remains secure and confidential, aligning with the value item of ""Social Order"" under the value of ""Security.""",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,1174,ICSE,Security & Privacy,Automated Black-Box Testing of Mass Assignment Vulnerabilities in RESTful APIs,"Mass assignment is one of the most prominent vulnerabilities in RESTful APIs. This vulnerability originates from a misconfiguration in common web frameworks, such that naming convention and automatic binding can be exploited by an attacker to craft malicious requests writing confidential resources and (massively) overriding data, that should be read-only and/or confidential. In this paper, we adopt a black-box testing perspective to automatically detect mass assignment vulnerabilities in RESTful APIs. Execution scenarios are generated purely based on the OpenAPI specification, that lists the available operations and their message format. Clustering is used to group similar operations and reveal read-only fields, the latter are candidate for mass assignment. Then, interaction sequences are automatically generated by instantiating abstract testing templates, trying to exploit the potential vulnerabilities. Finally, test cases are run, and their execution is assessed by a specific oracle, in order to reveal whether the vulnerability could be successfully exploited. The proposed novel approach has been implemented and evaluated on a set of case studies written in different programming languages. The evaluation highlights that the approach is quite effective in detecting seeded vulnerabilities, with a remarkably high accuracy.
",Security,National Security,"The emphasis on vulnerabilities that could lead to overriding confidential data implies a focus on securing data on a larger; perhaps even national scale. This reflects an alignment with the value item ""National Security"" within the value ""Security"".","In the paper abstract, it is stated that the vulnerability being addressed is the mass assignment vulnerability, which can be exploited to write confidential resources and override data that should be read-only and/or confidential. This emphasis on protecting and securing data aligns with the value item of ""National Security"" within the broader value of ""Security."" By addressing this vulnerability, the paper's main contributions directly align with the software user's perspective of prioritizing the security and protection of data, which is in line with the value item ""National Security.""",Agreed-Justified,Agree,,Agree,,
Exp B,Exp H,1175,ICSE,Security & Privacy,CoLeFunDa: Explainable Silent Vulnerability Fix Identification,"It is common practice for OSS users to leverage and monitor security advisories to discover newly disclosed OSS vulnerabilities and their corresponding patches for vulnerability remediation. It is common for vulnerability fixes to be publicly available one week earlier than their disclosure. This gap in time provides an opportunity for attackers to exploit the vulnerability. Hence, OSS users need to sense the fix as early as possible so that the vulnerability can be remediated before it is exploited. However, it is common for OSS to adopt a vulnerability disclosure policy which causes the majority of vulnerabilities to be fixed silently, meaning the commit with the fix does not indicate any vulnerability information. In this case even if a fix is identified, it is hard for OSS users to understand the vulnerability and evaluate its potential impact. To improve early sensing of vulnerabilities, the identification of silent fixes and their corresponding explanations (eg, the corresponding common weakness enumeration (CWE) and exploitability rating) are equally important.
However, it is challenging to identify silent fixes and provide explanations due to the limited and diverse data. To tackle this challenge, we propose CoLeFunDa: a framework consisting of a Contrastive Learner and FunDa, which is a novel approach for Function change Data augmentation. FunDa first increases the fix data (ie, code changes) at the function level with unsupervised and supervised strategies. Then the contrastive learner leverages contrastive learning to effectively train a function change encoder, FCBERT, from diverse fix data. Finally, we leverage FCBERT to further fine-tune three downstream tasks, ie, silent fix identification, CWE category classification, and exploitability rating classification, respectively. Our result shows that CoLeFunDa outperforms all the state-of-art baselines in all downstream tasks. We also conduct a survey to verify the effectiveness of CoLeFunDa in practical usage. The result shows that CoLeFunDa can categorize 62.5%(25 out of 40) CVEs with correct CWE categories within the top 2 recommendations.
",Security,Healthy,"The main aim of 'Paper X' is to enhance the early sensing of vulnerabilities; helping users to handle possible security issues before they are exploited. This aligns to the value item ""Healthy"" under the value ""Security""; as 'Paper X' primarily focusses on protecting the software user's environment against potential threats.","The main contribution of 'Paper X' is to improve early sensing of vulnerabilities in order to remediate them before they are exploited. By focusing on protecting the software user's environment against potential threats, the paper aligns with the value item ""Healthy"" under the value ""Security"" in Schwartz's Taxonomy. This alignment can be clearly evidenced in the abstract as the goal of the paper is to help OSS users handle security issues and ensure the security and well-being of their software systems.",Agreed-Justified,Agree,,Disagree,The justification doesn't align with the value item of the Healthy,
Exp C,Exp K,1175,ICSE,Security & Privacy,CoLeFunDa: Explainable Silent Vulnerability Fix Identification,"It is common practice for OSS users to leverage and monitor security advisories to discover newly disclosed OSS vulnerabilities and their corresponding patches for vulnerability remediation. It is common for vulnerability fixes to be publicly available one week earlier than their disclosure. This gap in time provides an opportunity for attackers to exploit the vulnerability. Hence, OSS users need to sense the fix as early as possible so that the vulnerability can be remediated before it is exploited. However, it is common for OSS to adopt a vulnerability disclosure policy which causes the majority of vulnerabilities to be fixed silently, meaning the commit with the fix does not indicate any vulnerability information. In this case even if a fix is identified, it is hard for OSS users to understand the vulnerability and evaluate its potential impact. To improve early sensing of vulnerabilities, the identification of silent fixes and their corresponding explanations (eg, the corresponding common weakness enumeration (CWE) and exploitability rating) are equally important.
However, it is challenging to identify silent fixes and provide explanations due to the limited and diverse data. To tackle this challenge, we propose CoLeFunDa: a framework consisting of a Contrastive Learner and FunDa, which is a novel approach for Function change Data augmentation. FunDa first increases the fix data (ie, code changes) at the function level with unsupervised and supervised strategies. Then the contrastive learner leverages contrastive learning to effectively train a function change encoder, FCBERT, from diverse fix data. Finally, we leverage FCBERT to further fine-tune three downstream tasks, ie, silent fix identification, CWE category classification, and exploitability rating classification, respectively. Our result shows that CoLeFunDa outperforms all the state-of-art baselines in all downstream tasks. We also conduct a survey to verify the effectiveness of CoLeFunDa in practical usage. The result shows that CoLeFunDa can categorize 62.5%(25 out of 40) CVEs with correct CWE categories within the top 2 recommendations.
",Security,Social Order,"The paper introduces CoLeFunDa that offers a robust mechanism to identify silent fixes; classify the vulnerability category (CWE); and classify the exploitability rating. This strengthens the ""Social Order"" under the value ""Security"" as it ensures a structured way of identifying vulnerabilities and fixes to prevent users from cyber threats.","The main contributions of 'Paper X' directly align with the value item Social Order and its corresponding value Security from a ""Software User"" perspective. By introducing CoLeFunDa, the paper provides a framework that enables the thorough identification of silent fixes, classification of vulnerability categories (CWE), and assessment of exploitability ratings. This promotes a sense of security and order within the software context by ensuring a structured and efficient process for identifying and addressing vulnerabilities, thereby mitigating potential cyber threats that software users may encounter.",Agreed-Clarified,Agree,,Agree,,
Exp D,Exp K,1175,ICSE,Security & Privacy,CoLeFunDa: Explainable Silent Vulnerability Fix Identification,"It is common practice for OSS users to leverage and monitor security advisories to discover newly disclosed OSS vulnerabilities and their corresponding patches for vulnerability remediation. It is common for vulnerability fixes to be publicly available one week earlier than their disclosure. This gap in time provides an opportunity for attackers to exploit the vulnerability. Hence, OSS users need to sense the fix as early as possible so that the vulnerability can be remediated before it is exploited. However, it is common for OSS to adopt a vulnerability disclosure policy which causes the majority of vulnerabilities to be fixed silently, meaning the commit with the fix does not indicate any vulnerability information. In this case even if a fix is identified, it is hard for OSS users to understand the vulnerability and evaluate its potential impact. To improve early sensing of vulnerabilities, the identification of silent fixes and their corresponding explanations (eg, the corresponding common weakness enumeration (CWE) and exploitability rating) are equally important.
However, it is challenging to identify silent fixes and provide explanations due to the limited and diverse data. To tackle this challenge, we propose CoLeFunDa: a framework consisting of a Contrastive Learner and FunDa, which is a novel approach for Function change Data augmentation. FunDa first increases the fix data (ie, code changes) at the function level with unsupervised and supervised strategies. Then the contrastive learner leverages contrastive learning to effectively train a function change encoder, FCBERT, from diverse fix data. Finally, we leverage FCBERT to further fine-tune three downstream tasks, ie, silent fix identification, CWE category classification, and exploitability rating classification, respectively. Our result shows that CoLeFunDa outperforms all the state-of-art baselines in all downstream tasks. We also conduct a survey to verify the effectiveness of CoLeFunDa in practical usage. The result shows that CoLeFunDa can categorize 62.5%(25 out of 40) CVEs with correct CWE categories within the top 2 recommendations.
",Achievement,Intelligent,"The output of the presented framework; CoLeFunDa; provides users with detailed information about vulnerabilities and fixes; enhancing their understanding of the potential impact. This aligns with the value item ""Intelligent"" under the value ""Achievement""; as it equips the software user with knowledge and information; enabling them to make informed decisions about the safety of their software environment.","The main contributions of 'Paper X' directly align with the value item Intelligent and its corresponding value Achievement from a ""Software User"" perspective. The framework proposed in the paper, CoLeFunDa, enhances the understanding of vulnerabilities and their potential impact by providing users with detailed information about vulnerabilities and fixes. This aligns with the value item ""Intelligent"" as it equips software users with knowledge and information, enabling them to make informed decisions about the safety of their software environment. By being able to assess and address vulnerabilities effectively, software users achieve a sense of accomplishment and success in maintaining the security and integrity of their software systems.",Agreed-Justified,Agree,,Agree,,
Exp E,Exp J,1176,ICSE,Software Testing & QA,Finding Causally Different Tests for an Industrial Control System,"Industrial control systems (ICSs) are types of cyber-physical systems in which programs, written in languages such as ladder logic or structured text, control industrial processes through sensing and actuating. Given the use of ICSs in critical infrastructure, it is important to test their resilience against manipulations of sensor/actuator inputs. Unfortunately, existing methods fail to test them comprehensively, as they typically focus on finding the simplest-to-craft manipulations for a testing goal, and are also unable to determine when a test is simply a minor permutation of another, i.e. based on the same causal events. In this work, we propose a guided fuzzing approach for finding 'meaningfully different' tests for an ICS via a general formalisation of sensor/actuator-manipulation strategies. Our algorithm identifies the causal events in a test, generalises them to an equivalence class, and then updates the fuzzing strategy so as to find new tests that are causally different from those already identified. An evaluation of our approach on a real-world water treatment system shows that it is able to find 106% more causally different tests than the most comparable fuzzer. While we focus on diversifying the test suite of an ICS, our formalisation may be useful for other fuzzers that intercept communication channels.

",Achievement,Intelligent,The paper develops a novel approach to improving the effectiveness of industrial control systems (ICS) testing; demonstrating the software user's intelligence in addressing critical systems challenges.,"The justification for aligning 'Paper X' with the value item Intelligent and its corresponding value Achievement from a ""Software User"" perspective is based on the fact that the paper proposes a guided fuzzing approach that aims to comprehensively test the resilience of industrial control systems (ICS). By developing a new and improved method for testing ICS, the paper demonstrates the intelligence of the software user in addressing critical system challenges, as achieving robustness and resilience in ICS requires a high level of intelligence and capability.",Agreed-Clarified,Agree,,Agree,,
Exp F,Exp J,1176,ICSE,Software Testing & QA,Finding Causally Different Tests for an Industrial Control System,"Industrial control systems (ICSs) are types of cyber-physical systems in which programs, written in languages such as ladder logic or structured text, control industrial processes through sensing and actuating. Given the use of ICSs in critical infrastructure, it is important to test their resilience against manipulations of sensor/actuator inputs. Unfortunately, existing methods fail to test them comprehensively, as they typically focus on finding the simplest-to-craft manipulations for a testing goal, and are also unable to determine when a test is simply a minor permutation of another, i.e. based on the same causal events. In this work, we propose a guided fuzzing approach for finding 'meaningfully different' tests for an ICS via a general formalisation of sensor/actuator-manipulation strategies. Our algorithm identifies the causal events in a test, generalises them to an equivalence class, and then updates the fuzzing strategy so as to find new tests that are causally different from those already identified. An evaluation of our approach on a real-world water treatment system shows that it is able to find 106% more causally different tests than the most comparable fuzzer. While we focus on diversifying the test suite of an ICS, our formalisation may be useful for other fuzzers that intercept communication channels.

",Security,National Security,The paper emphasizes on improving the resilience of critical infrastructure by proposing better testing methods. This aligns with the value item National Security and its corresponding value Security.,"The main contribution of 'Paper X' is to propose a guided fuzzing approach that improves the resilience of industrial control systems used in critical infrastructure. By identifying and generating ""meaningfully different"" tests, the paper aims to test the systems comprehensively and ensure their security against manipulations. This aligns with the value item National Security and its corresponding value Security, as it directly addresses the need for robustness and protection of critical infrastructure systems, thus enhancing the overall security and stability of a software user's environment.",Agreed-Clarified,Disagree,I don't think if this really related to security,Agree,,
Exp G,Exp K,1176,ICSE,Software Testing & QA,Finding Causally Different Tests for an Industrial Control System,"Industrial control systems (ICSs) are types of cyber-physical systems in which programs, written in languages such as ladder logic or structured text, control industrial processes through sensing and actuating. Given the use of ICSs in critical infrastructure, it is important to test their resilience against manipulations of sensor/actuator inputs. Unfortunately, existing methods fail to test them comprehensively, as they typically focus on finding the simplest-to-craft manipulations for a testing goal, and are also unable to determine when a test is simply a minor permutation of another, i.e. based on the same causal events. In this work, we propose a guided fuzzing approach for finding 'meaningfully different' tests for an ICS via a general formalisation of sensor/actuator-manipulation strategies. Our algorithm identifies the causal events in a test, generalises them to an equivalence class, and then updates the fuzzing strategy so as to find new tests that are causally different from those already identified. An evaluation of our approach on a real-world water treatment system shows that it is able to find 106% more causally different tests than the most comparable fuzzer. While we focus on diversifying the test suite of an ICS, our formalisation may be useful for other fuzzers that intercept communication channels.

",Benevolence,Honesty,The paper highlights a specific area of truth and honesty in its scientific approach by exposing the manipulation in sensors/actuators within industrial control systems. This aligns with the value item Honesty and its corresponding value Benevolence.,"In the paper abstract, 'Paper X' explicitly states that it aims to test the resilience of industrial control systems against manipulations of sensor/actuator inputs. By uncovering these manipulations and focusing on finding 'meaningfully different' tests, the paper contributes to the field by emphasizing the truth and honesty in its scientific approach. This aligns with the value item Honesty and its corresponding value Benevolence as it seeks to ensure the integrity and reliability of industrial control systems for the benefit of users and society.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,1177,ICSE,Software Testing & QA,Doppelg‚Äö√†√∂‚àö‚àè‚Äö√Ñ√∂‚àö‚Ä†‚àö¬¥¬¨¬®¬¨¬Æ¬¨¬®¬¨¬£‚Äö√†√∂‚àö‚àè¬¨¬®¬¨¬¢¬¨¬®¬¨¬Æ¬¨¬®‚àö√ºnger Test Generation for Revealing Bugs in Autonomous Driving Software,"Vehicles controlled by autonomous driving software (ADS) are expected to bring many social and economic benefits, but at the current stage not being broadly used due to concerns with regard to their safety. Virtual tests, where autonomous vehicles are tested in software simulation, are common practices because they are more efficient and safer compared to field operational tests. Specifically, search-based approaches are used to find particularly critical situations. These approaches provide an opportunity to automatically generate tests; however, system-atically producing bug-revealing tests for ADS remains a major challenge. To address this challenge, we introduce DoppelTest, a test generation approach for ADSes that utilizes a genetic algorithm to discover bug-revealing violations by generating scenarios with multiple autonomous vehicles that account for traffic control (e.g., traffic signals and stop signs). Our extensive evaluation shows that DoppelTest can efficiently discover 123 bug-revealing violations for a production-grade ADS (Baidu Apollo) which we then classify into 8 unique bug categories.
",Security,Healthy,"The paper introduces 'DoppelTest'; an efficient test generation approach for autonomous driving software; which aims to improve the safety of autonomous vehicles. This contributes to the value item ""Healthy"" under the value ""Security.""","The main contribution of 'Paper X' is the introduction of 'DoppelTest', a test generation approach for autonomous driving software that aims to improve the safety of autonomous vehicles. This directly aligns with the value item ""Healthy"" and its corresponding value ""Security"" from a ""Software User"" perspective because enhancing the safety of autonomous vehicles promotes a sense of security in using these vehicles, thereby ensuring the user's physical well-being and contributing to their overall sense of security and trust in the software.",Agreed-Justified,Agree,,Agree,,
Exp B,Exp H,1177,ICSE,Software Testing & QA,Doppelg‚Äö√†√∂‚àö‚àè‚Äö√Ñ√∂‚àö‚Ä†‚àö¬¥¬¨¬®¬¨¬Æ¬¨¬®¬¨¬£‚Äö√†√∂‚àö‚àè¬¨¬®¬¨¬¢¬¨¬®¬¨¬Æ¬¨¬®‚àö√ºnger Test Generation for Revealing Bugs in Autonomous Driving Software,"Vehicles controlled by autonomous driving software (ADS) are expected to bring many social and economic benefits, but at the current stage not being broadly used due to concerns with regard to their safety. Virtual tests, where autonomous vehicles are tested in software simulation, are common practices because they are more efficient and safer compared to field operational tests. Specifically, search-based approaches are used to find particularly critical situations. These approaches provide an opportunity to automatically generate tests; however, system-atically producing bug-revealing tests for ADS remains a major challenge. To address this challenge, we introduce DoppelTest, a test generation approach for ADSes that utilizes a genetic algorithm to discover bug-revealing violations by generating scenarios with multiple autonomous vehicles that account for traffic control (e.g., traffic signals and stop signs). Our extensive evaluation shows that DoppelTest can efficiently discover 123 bug-revealing violations for a production-grade ADS (Baidu Apollo) which we then classify into 8 unique bug categories.
",Security,National Security,"The paper aims to improve the safety of autonomous vehicles; which can greatly enhance the safety of society. This contributes to the value item ""National Security"" under the value ""Security.""","In aligning 'Paper X' with the value item National Security and its corresponding value Security, the paper's focus on improving the safety of autonomous vehicles directly contributes to the broader aspect of security within society. By addressing concerns related to the safety of autonomous vehicles, the paper's contributions can contribute to ensuring the overall security and well-being of individuals and communities. Enhancing the safety of these vehicles can potentially prevent accidents and incidents that may pose threats to public safety, thereby aligning with the value item National Security.",Agreed-Justified,Agree,reconcile with coder_2,Agree,,
Exp C,Exp K,1177,ICSE,Software Testing & QA,Doppelg√ø‚àë¬¨¬£√ø¬¢¬¨¬ßnger Test Generation for Revealing Bugs in Autonomous Driving Software,"Vehicles controlled by autonomous driving software (ADS) are expected to bring many social and economic benefits, but at the current stage not being broadly used due to concerns with regard to their safety. Virtual tests, where autonomous vehicles are tested in software simulation, are common practices because they are more efficient and safer compared to field operational tests. Specifically, search-based approaches are used to find particularly critical situations. These approaches provide an opportunity to automatically generate tests; however, system-atically producing bug-revealing tests for ADS remains a major challenge. To address this challenge, we introduce DoppelTest, a test generation approach for ADSes that utilizes a genetic algorithm to discover bug-revealing violations by generating scenarios with multiple autonomous vehicles that account for traffic control (e.g., traffic signals and stop signs). Our extensive evaluation shows that DoppelTest can efficiently discover 123 bug-revealing violations for a production-grade ADS (Baidu Apollo) which we then classify into 8 unique bug categories.
",Achievement,Intelligent,"Through introducing and evaluating 'DoppelTest'; the software users gain the capability to generate reliable tests and ensure safer operation of autonomous vehicles. This contributes to the value item ""Intelligent"" under the value ""Achievement.""","The main contribution of 'Paper X' is the introduction of DoppelTest, a test generation approach for autonomous driving software. By utilizing a genetic algorithm, DoppelTest can efficiently discover bug-revealing violations, leading to the safer operation of autonomous vehicles. This directly aligns with the value item ""Intelligent"" under the value ""Achievement"" because it showcases the capability of software users to generate reliable tests, demonstrating their competence and ability to achieve desired outcomes in the software context.",Agreed-Justified,Agree,,Agree,,
Exp D,Exp K,1178,ICSE,Software Project Management,Rules of Engagement: Why and How Companies Participate in OSS,"Company engagement in open source (OSS) is now the new norm. From large technology companies to startups, companies are participating in the OSS ecosystem by open-sourcing their technology, sponsoring projects through funding or paid developer time. However, our understanding of the OSS ecosystem is rooted in the 'old world' model where individual contributors sustain OSS projects. In this work, we create a more comprehensive understanding of the hybrid OSS landscape by investigating what motivates companies to contribute and how they contribute to OSS. We conducted interviews with 20 participants who have different roles (e.g., CEO, OSPO Lead, Ecosystem Strategist) at 17 different companies of different sizes from large companies (e.g. Microsoft, RedHat, Google, Spotify) to startups. Data from semi-structured interviews reveal that company motivations can be categorized into four levels (Founders' Vision, Reputation, Business Advantage, and Reciprocity) and companies participate through different mechanisms (e.g., Developers' Time, Mentoring Time, Advocacy & Promotion Time), each of which tie to the different types of motivations. We hope our findings nudge more companies to participate in the OSS ecosystem, helping make it robust, diverse, and sustainable.
",Achievement,Influential,The paper demonstrates how companies contribute to Open Source Software (OSS) for reputation and business advantage which aligns with the value item Influential; demonstrating the user's aspiration to be acknowledged for their skills and accomplishments.,"In the context of a ""Software User,"" the alignment of 'Paper X' with the value item Influential and its corresponding value Achievement can be justified by the paper's focus on companies' motivations for contributing to Open Source Software (OSS) for reputation and business advantage. The user, as a software consumer, seeks influential companies that contribute to OSS as it reflects their expertise and success in the industry. By showcasing how companies participate and contribute to OSS projects, the paper highlights their influential role, which resonates with the user's aspiration to be associated with reputable and accomplished entities in the software ecosystem.",Agreed-Justified,Agree,,Agree,,
Exp E,Exp J,1178,ICSE,Software Project Management,Rules of Engagement: Why and How Companies Participate in OSS,"Company engagement in open source (OSS) is now the new norm. From large technology companies to startups, companies are participating in the OSS ecosystem by open-sourcing their technology, sponsoring projects through funding or paid developer time. However, our understanding of the OSS ecosystem is rooted in the 'old world' model where individual contributors sustain OSS projects. In this work, we create a more comprehensive understanding of the hybrid OSS landscape by investigating what motivates companies to contribute and how they contribute to OSS. We conducted interviews with 20 participants who have different roles (e.g., CEO, OSPO Lead, Ecosystem Strategist) at 17 different companies of different sizes from large companies (e.g. Microsoft, RedHat, Google, Spotify) to startups. Data from semi-structured interviews reveal that company motivations can be categorized into four levels (Founders' Vision, Reputation, Business Advantage, and Reciprocity) and companies participate through different mechanisms (e.g., Developers' Time, Mentoring Time, Advocacy & Promotion Time), each of which tie to the different types of motivations. We hope our findings nudge more companies to participate in the OSS ecosystem, helping make it robust, diverse, and sustainable.
",Security,Reciprocation of Favors,The abstract mentions Reciprocity as a company motivation which ties to the value item Reciprocity of Favors; corresponding to the value Security - ensuring the well-being of the community.,"In the abstract, it is mentioned that one of the company motivations for participating in the OSS ecosystem is reciprocity, which aligns with the value item Reciprocation of Favors in Schwartz's Taxonomy. This indicates that companies contribute to OSS as a way of reciprocating the benefits they receive from the community. By contributing, companies ensure the well-being and security of the community. This alignment between company motivations and the value item Security from a ""Software User"" perspective suggests that the main contributions of 'Paper X' focus on fostering a sense of security and well-being within the software community through the concept of reciprocity.",Agreed-Clarified,Agree,,Agree,,
Exp F,Exp J,1178,ICSE,Software Project Management,Rules of Engagement: Why and How Companies Participate in OSS,"Company engagement in open source (OSS) is now the new norm. From large technology companies to startups, companies are participating in the OSS ecosystem by open-sourcing their technology, sponsoring projects through funding or paid developer time. However, our understanding of the OSS ecosystem is rooted in the 'old world' model where individual contributors sustain OSS projects. In this work, we create a more comprehensive understanding of the hybrid OSS landscape by investigating what motivates companies to contribute and how they contribute to OSS. We conducted interviews with 20 participants who have different roles (e.g., CEO, OSPO Lead, Ecosystem Strategist) at 17 different companies of different sizes from large companies (e.g. Microsoft, RedHat, Google, Spotify) to startups. Data from semi-structured interviews reveal that company motivations can be categorized into four levels (Founders' Vision, Reputation, Business Advantage, and Reciprocity) and companies participate through different mechanisms (e.g., Developers' Time, Mentoring Time, Advocacy & Promotion Time), each of which tie to the different types of motivations. We hope our findings nudge more companies to participate in the OSS ecosystem, helping make it robust, diverse, and sustainable.
",Benevolence,Responsibility,The paper promotes responsibility as companies take part in OSS through developers' time; mentoring and advocacy. This aligns with the value item Responsibility under the value Benevolence.,"The justification for aligning 'Paper X' with the value item Responsibility and its corresponding value Benevolence from a ""Software User"" perspective is based on the understanding that the paper highlights how companies contribute to the OSS ecosystem by investing their resources (developers' time), sharing knowledge (mentoring), and promoting the use and benefits of OSS (advocacy). These actions demonstrate a sense of responsibility towards the larger software community, aligning with the value of Benevolence which encompasses acts that benefit others and promote a sense of societal responsibility.",Agreed-Clarified,Agree,,Agree,,
Exp G,Exp K,1179,ICSE,Security & Privacy,An Empirical Study on Software Bill of Materials: Where We Stand and the Road Ahead,"The rapid growth of software supply chain attacks has attracted considerable attention to software bill of materials (SBOM). SBOMs are a crucial building block to ensure the transparency of software supply chains that helps improve software supply chain security. Although there are significant efforts from academia and industry to facilitate SBOM development, it is still unclear how practitioners perceive SBOMs and what are the challenges of adopting SBOMs in practice. Furthermore, existing SBOM-related studies tend to be ad-hoc and lack software engineering focuses. To bridge this gap, we conducted the first empirical study to interview and survey SBOM practitioners. We applied a mixed qualitative and quantitative method for gathering data from 17 interviewees and 65 survey respondents from 15 countries across five continents to understand how practitioners perceive the SBOM field. We summarized 26 statements and grouped them into four topics on SBOM's states of practice. Based on the study results, we derived a goal model and highlighted future directions where practitioners can put in their effort.
",Achievement,Successful,The paper presents an empirical study aimed at understanding the perception and challenges of SBOMs in practice. This understanding can contribute to the software users' feeling of succesful use of software supply chains with SBOMs.,"The paper's empirical study specifically addresses the perception and challenges of SBOMs in practice, which is directly relevant to the software user's perspective. By gaining insights into the practical aspects of SBOMs, users can make informed decisions and successfully navigate software supply chains. Therefore, aligning with the value item Successful and its corresponding value Achievement, the contributions of 'Paper X' enable software users to achieve successful outcomes in utilizing SBOMs for software supply chain security.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,1179,ICSE,Security & Privacy,An Empirical Study on Software Bill of Materials: Where We Stand and the Road Ahead,"The rapid growth of software supply chain attacks has attracted considerable attention to software bill of materials (SBOM). SBOMs are a crucial building block to ensure the transparency of software supply chains that helps improve software supply chain security. Although there are significant efforts from academia and industry to facilitate SBOM development, it is still unclear how practitioners perceive SBOMs and what are the challenges of adopting SBOMs in practice. Furthermore, existing SBOM-related studies tend to be ad-hoc and lack software engineering focuses. To bridge this gap, we conducted the first empirical study to interview and survey SBOM practitioners. We applied a mixed qualitative and quantitative method for gathering data from 17 interviewees and 65 survey respondents from 15 countries across five continents to understand how practitioners perceive the SBOM field. We summarized 26 statements and grouped them into four topics on SBOM's states of practice. Based on the study results, we derived a goal model and highlighted future directions where practitioners can put in their effort.
",Power,Social Recognition,By improving the transparency of the software supply chain; the paper contributes to the community recognizing the value of the software and of its users.,"The paper's contribution to improving the transparency of the software supply chain aligns with the value item of Social Recognition and its corresponding value of Power because it highlights the importance of recognizing the value of software and its users within the community. By promoting transparency, the paper aims to empower software users and acknowledge their role in software supply chains, thereby enhancing their social recognition and elevating their power in the software ecosystem.",Agreed-Justified,Disagree,reconcile with coder_2,Disagree,The justification doesn't align with the value item of the Social Recognition,
Exp B,Exp H,1179,ICSE,Security & Privacy,An Empirical Study on Software Bill of Materials: Where We Stand and the Road Ahead,"The rapid growth of software supply chain attacks has attracted considerable attention to software bill of materials (SBOM). SBOMs are a crucial building block to ensure the transparency of software supply chains that helps improve software supply chain security. Although there are significant efforts from academia and industry to facilitate SBOM development, it is still unclear how practitioners perceive SBOMs and what are the challenges of adopting SBOMs in practice. Furthermore, existing SBOM-related studies tend to be ad-hoc and lack software engineering focuses. To bridge this gap, we conducted the first empirical study to interview and survey SBOM practitioners. We applied a mixed qualitative and quantitative method for gathering data from 17 interviewees and 65 survey respondents from 15 countries across five continents to understand how practitioners perceive the SBOM field. We summarized 26 statements and grouped them into four topics on SBOM's states of practice. Based on the study results, we derived a goal model and highlighted future directions where practitioners can put in their effort.
",Security,Social Order,The work on improving the security of software supply chain can help contribute to maintaining a social order in the digital space and it ensures the users' experience is secure.,"The main contribution of 'Paper X' is the empirical study conducted to understand practitioners' perception of SBOMs and the challenges of adopting them in practice. By improving the security of software supply chains, SBOMs contribute to maintaining social order in the digital space. This is because a secure software supply chain ensures that software users have a reliable and safe experience, without the risk of cyberattacks, data breaches, or other security issues that could disrupt the functioning of the digital ecosystem. Thus, aligning with the value item of Social Order and its corresponding value of Security, 'Paper X' addresses the need for a secure and stable digital environment for software users.",Agreed-Justified,Agree,,Agree,,
Exp C,Exp K,1180,ESEC/FSE,AI & Machine Learning,"AI-assisted programming: applications, user experiences, and neuro-symbolic techniques","AI can enhance programming experiences for a diverse set of programmers: from professional developers and data scientists (proficient programmers) who need help in software engineering and data wrangling, all the way to spreadsheet users (low-code programmers) who need help in authoring formulas, and students (novice programmers) who seek hints when stuck with their programming homework. To communicate their need to AI, users can express their intent explicitly‚Äö√Ñ√∂‚àö√ë‚àö√Üas input-output examples or natural-language specification‚Äö√Ñ√∂‚àö√ë‚àö√Üor implicitly‚Äö√Ñ√∂‚àö√ë‚àö√Üwhere they encounter a bug (and expect AI to suggest a fix), or simply allow AI to observe their last few lines of code or edits (to have it suggest the next steps).
The task of synthesizing an intended program snippet from the user‚Äö√Ñ√∂‚àö√ë‚àö¬•s intent is both a search and a ranking problem. Search is required to discover candidate programs that correspond to the (often ambiguous) intent, and ranking is required to pick the best program from multiple plausible alternatives. This creates a fertile playground for combining symbolic-reasoning techniques, which model the semantics of programming operators, and machine-learning techniques, which can model human preferences in programming. Recent advances in large language models like Codex offer further promise to advance such neuro-symbolic techniques.
Finally, a few critical requirements in AI-assisted programming are usability, precision, and trust; and they create opportunities for innovative user experiences and interactivity paradigms. In this talk, I will explain these concepts using some existing successes, including the Flash Fill feature in Excel, Data Connectors in PowerQuery, and IntelliCode/CoPilot in Visual Studio. I will also describe several new opportunities in AI-assisted programming, which can drive the next set of foundational neuro-symbolic advances.
",Stimulation,Daring,The paper contributes to AI-enhanced programming experiences; which requires users to be daring and experiment with AI to communicate their needs. This aligns with the value item Daring and its corresponding value Stimulation.,"In the paper, it is mentioned that AI can enhance programming experiences by allowing users to express their intent explicitly or implicitly and by suggesting the next steps. This implies that users need to be daring and willing to experiment with AI in order to effectively communicate their needs and make the most out of the AI-assisted programming experience. This aligns with the value item Daring, as being daring involves taking risks and being open to new experiences, and its corresponding value Stimulation, as experimenting with AI can be exciting and stimulating for users.",Agreed-Clarified,Agree,,Agree,,
Exp D,Exp K,1180,ESEC/FSE,AI & Machine Learning,"AI-assisted programming: applications, user experiences, and neuro-symbolic techniques","AI can enhance programming experiences for a diverse set of programmers: from professional developers and data scientists (proficient programmers) who need help in software engineering and data wrangling, all the way to spreadsheet users (low-code programmers) who need help in authoring formulas, and students (novice programmers) who seek hints when stuck with their programming homework. To communicate their need to AI, users can express their intent explicitly‚Äö√Ñ√∂‚àö√ë‚àö√Üas input-output examples or natural-language specification‚Äö√Ñ√∂‚àö√ë‚àö√Üor implicitly‚Äö√Ñ√∂‚àö√ë‚àö√Üwhere they encounter a bug (and expect AI to suggest a fix), or simply allow AI to observe their last few lines of code or edits (to have it suggest the next steps).
The task of synthesizing an intended program snippet from the user‚Äö√Ñ√∂‚àö√ë‚àö¬•s intent is both a search and a ranking problem. Search is required to discover candidate programs that correspond to the (often ambiguous) intent, and ranking is required to pick the best program from multiple plausible alternatives. This creates a fertile playground for combining symbolic-reasoning techniques, which model the semantics of programming operators, and machine-learning techniques, which can model human preferences in programming. Recent advances in large language models like Codex offer further promise to advance such neuro-symbolic techniques.
Finally, a few critical requirements in AI-assisted programming are usability, precision, and trust; and they create opportunities for innovative user experiences and interactivity paradigms. In this talk, I will explain these concepts using some existing successes, including the Flash Fill feature in Excel, Data Connectors in PowerQuery, and IntelliCode/CoPilot in Visual Studio. I will also describe several new opportunities in AI-assisted programming, which can drive the next set of foundational neuro-symbolic advances.
",Achievement,Successful,The AI-assisted programming mentioned in the paper can help users become successful and excel in their tasks; aligning with the value item Successful and its corresponding value Achievement.,"In the paper abstract, it is explicitly mentioned that AI-assisted programming can enhance the programming experiences of diverse users, including spreadsheet users and students who need help with their programming tasks. By providing support and suggestions for their coding needs, the AI system can help these users achieve success and excel in their tasks. This aligns with the value item Successful and its corresponding value Achievement because the main contributions of 'Paper X' directly address the goal of assisting software users in their programming endeavors and helping them achieve their desired outcomes.",Agreed-Justified,Agree,,Agree,,
Exp E,Exp J,1180,ESEC/FSE,AI & Machine Learning,"AI-assisted programming: applications, user experiences, and neuro-symbolic techniques","AI can enhance programming experiences for a diverse set of programmers: from professional developers and data scientists (proficient programmers) who need help in software engineering and data wrangling, all the way to spreadsheet users (low-code programmers) who need help in authoring formulas, and students (novice programmers) who seek hints when stuck with their programming homework. To communicate their need to AI, users can express their intent explicitly‚Äö√Ñ√∂‚àö√ë‚àö√Üas input-output examples or natural-language specification‚Äö√Ñ√∂‚àö√ë‚àö√Üor implicitly‚Äö√Ñ√∂‚àö√ë‚àö√Üwhere they encounter a bug (and expect AI to suggest a fix), or simply allow AI to observe their last few lines of code or edits (to have it suggest the next steps).
The task of synthesizing an intended program snippet from the user‚Äö√Ñ√∂‚àö√ë‚àö¬•s intent is both a search and a ranking problem. Search is required to discover candidate programs that correspond to the (often ambiguous) intent, and ranking is required to pick the best program from multiple plausible alternatives. This creates a fertile playground for combining symbolic-reasoning techniques, which model the semantics of programming operators, and machine-learning techniques, which can model human preferences in programming. Recent advances in large language models like Codex offer further promise to advance such neuro-symbolic techniques.
Finally, a few critical requirements in AI-assisted programming are usability, precision, and trust; and they create opportunities for innovative user experiences and interactivity paradigms. In this talk, I will explain these concepts using some existing successes, including the Flash Fill feature in Excel, Data Connectors in PowerQuery, and IntelliCode/CoPilot in Visual Studio. I will also describe several new opportunities in AI-assisted programming, which can drive the next set of foundational neuro-symbolic advances.
",Power,Social Recognition,The AI-enhanced programming experiences; which include using powerful tools like Codex; could lead to social recognition for users. This aligns with the value item Social Recognition and its corresponding value Power.,"The AI-enhanced programming experiences mentioned in 'Paper X', particularly the use of powerful tools like Codex, have the potential to empower software users and enhance their abilities in programming. By being able to rely on AI assistance to improve their programming skills and solve complex tasks, software users can gain a sense of competence and mastery, which can lead to social recognition and a sense of power within the software context. This alignment with the value item Social Recognition and its corresponding value Power highlights the potential positive impact of 'Paper X' on the software user's sense of accomplishment and recognition within their professional or educational communities.",Agreed-Justified,Agree,,Agree,,
Exp F,Exp J,1181,ESEC/FSE,Software Testing & QA,"On safety, assurance, and reliability: a software engineering perspective","From financial services platforms to social networks to vehicle control, software has come to mediate many activities of daily life. Governing bodies and standards organizations have responded to this trend by creating regulations and standards to address issues such as safety, security and privacy. In this environment, the compliance of software development to standards and regulations has emerged as a key requirement. Compliance claims and arguments are often captured in assurance cases, with linked evidence of compliance. Evidence can come from test cases, verification proofs, human judgement, or a combination of these. That is, we try to build (safety-critical) systems carefully according to well justified methods and articulate these justifications in an assurance case that is ultimately judged by a human.
Building safety arguments for traditional software systems is difficult ‚Äö√Ñ√∂‚àö√ë‚àö√Ü they are lengthy and expensive to maintain, especially as software undergoes change. Safety is also notoriously noncompositional ‚Äö√Ñ√∂‚àö√ë‚àö√Ü each subsystem might be safe but together they may create unsafe behaviors. It is also easy to miss cases, which in the simplest case would mean developing an argument for when a condition is true but missing arguing for a false condition. Furthermore, many ML-based systems are becoming safety-critical. For example, recent Tesla self-driving cars misclassified emergency vehicles and caused multiple crashes. ML-based systems typically do not have precisely specified and machine-verifiable requirements. While some safety requirements can be stated clearly: ‚Äö√Ñ√∂‚àö√ë‚àö‚à´the system should detect all pedestrians at a crossing‚Äö√Ñ√∂‚àö√ë‚àöœÄ, these requirements are for the entire system, making them too high-level for safety analysis of individual components. Thus, systems with ML components (MLCs) add a significant layer of complexity for safety assurance.
I argue that safety assurance should be an integral part of building safe and reliable software systems, but this process needs support from advanced software engineering and software analysis. In this talk, I outline a few approaches for development of principled, tool-supported methodologies for creating and managing assurance arguments. I then describe some of the recent work on specifying and verifying reliability requirements for machine-learned components in safety-critical domains.
",Security,Healthy,The paper emphasises the importance of software safety; which has direct implications for protecting the health and well-being of software users.,"In the paper abstract, it is explicitly mentioned that safety is a key requirement in software development and that compliance with regulations and standards is necessary to address issues such as security and privacy. This emphasis on safety aligns with the value item Healthy from Schwartz's Taxonomy, as ensuring software safety directly contributes to the well-being and protection of software users. As a software user, one can benefit from software systems that prioritize safety and security, thereby enhancing their sense of security and overall well-being.",Agreed-Justified,Agree,,Agree,,
Exp G,Exp K,1181,ESEC/FSE,Software Testing & QA,"On safety, assurance, and reliability: a software engineering perspective","From financial services platforms to social networks to vehicle control, software has come to mediate many activities of daily life. Governing bodies and standards organizations have responded to this trend by creating regulations and standards to address issues such as safety, security and privacy. In this environment, the compliance of software development to standards and regulations has emerged as a key requirement. Compliance claims and arguments are often captured in assurance cases, with linked evidence of compliance. Evidence can come from test cases, verification proofs, human judgement, or a combination of these. That is, we try to build (safety-critical) systems carefully according to well justified methods and articulate these justifications in an assurance case that is ultimately judged by a human.
Building safety arguments for traditional software systems is difficult ‚Äö√Ñ√∂‚àö√ë‚àö√Ü they are lengthy and expensive to maintain, especially as software undergoes change. Safety is also notoriously noncompositional ‚Äö√Ñ√∂‚àö√ë‚àö√Ü each subsystem might be safe but together they may create unsafe behaviors. It is also easy to miss cases, which in the simplest case would mean developing an argument for when a condition is true but missing arguing for a false condition. Furthermore, many ML-based systems are becoming safety-critical. For example, recent Tesla self-driving cars misclassified emergency vehicles and caused multiple crashes. ML-based systems typically do not have precisely specified and machine-verifiable requirements. While some safety requirements can be stated clearly: ‚Äö√Ñ√∂‚àö√ë‚àö‚à´the system should detect all pedestrians at a crossing‚Äö√Ñ√∂‚àö√ë‚àöœÄ, these requirements are for the entire system, making them too high-level for safety analysis of individual components. Thus, systems with ML components (MLCs) add a significant layer of complexity for safety assurance.
I argue that safety assurance should be an integral part of building safe and reliable software systems, but this process needs support from advanced software engineering and software analysis. In this talk, I outline a few approaches for development of principled, tool-supported methodologies for creating and managing assurance arguments. I then describe some of the recent work on specifying and verifying reliability requirements for machine-learned components in safety-critical domains.
",Security,Social Order,The paper's focus on compliance to standards and regulations contributes to maintaining social order by ensuring software operates within established legal and societal guidelines.,"In the paper abstract, it is stated that compliance to standards and regulations is a key requirement for software development. Compliance ensures that software operates within established legal and societal guidelines, which in turn contributes to maintaining social order. By aligning with the value item of Social Order and its corresponding value of Security, 'Paper X' emphasizes the importance of ensuring software systems function in a way that promotes stability, adherence to rules, and the overall well-being of individuals and society.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,1181,ESEC/FSE,Software Testing & QA,"On safety, assurance, and reliability: a software engineering perspective","From financial services platforms to social networks to vehicle control, software has come to mediate many activities of daily life. Governing bodies and standards organizations have responded to this trend by creating regulations and standards to address issues such as safety, security and privacy. In this environment, the compliance of software development to standards and regulations has emerged as a key requirement. Compliance claims and arguments are often captured in assurance cases, with linked evidence of compliance. Evidence can come from test cases, verification proofs, human judgement, or a combination of these. That is, we try to build (safety-critical) systems carefully according to well justified methods and articulate these justifications in an assurance case that is ultimately judged by a human.
Building safety arguments for traditional software systems is difficult ‚Äö√Ñ√∂‚àö√ë‚àö‚àÇ‚Äö√†√∂‚àö√´‚Äö√†√∂‚Äö√†√á‚Äö√Ñ√∂‚àö‚Ä†‚àö‚àÇ‚Äö√†√∂‚àö¬¥‚Äö√Ñ√∂‚àö‚Ä†‚àö‚àÇ‚Äö√†√∂‚àö√∫ they are lengthy and expensive to maintain, especially as software undergoes change. Safety is also notoriously noncompositional ‚Äö√Ñ√∂‚àö√ë‚àö‚àÇ‚Äö√†√∂‚àö√´‚Äö√†√∂‚Äö√†√á‚Äö√Ñ√∂‚àö‚Ä†‚àö‚àÇ‚Äö√†√∂‚àö¬¥‚Äö√Ñ√∂‚àö‚Ä†‚àö‚àÇ‚Äö√†√∂‚àö√∫ each subsystem might be safe but together they may create unsafe behaviors. It is also easy to miss cases, which in the simplest case would mean developing an argument for when a condition is true but missing arguing for a false condition. Furthermore, many ML-based systems are becoming safety-critical. For example, recent Tesla self-driving cars misclassified emergency vehicles and caused multiple crashes. ML-based systems typically do not have precisely specified and machine-verifiable requirements. While some safety requirements can be stated clearly: ‚Äö√Ñ√∂‚àö√ë‚àö‚àÇ‚Äö√†√∂‚àö√´‚Äö√†√∂‚Äö√†√á‚Äö√Ñ√∂‚àö‚Ä†‚àö‚àÇ‚Äö√†√∂‚àö¬¥‚Äö√Ñ√∂‚àö‚Ä†‚àö‚àÇ‚Äö√Ñ√∂‚àö‚Ä†¬¨¬•the system should detect all pedestrians at a crossing‚Äö√Ñ√∂‚àö√ë‚àö‚àÇ‚Äö√†√∂‚àö√´‚Äö√†√∂‚Äö√†√á‚Äö√Ñ√∂‚àö‚Ä†‚àö‚àÇ‚Äö√†√∂‚àö¬¥‚Äö√Ñ√∂‚àö‚Ä†‚àö‚àÇ‚âà√¨‚àö√ë, these requirements are for the entire system, making them too high-level for safety analysis of individual components. Thus, systems with ML components (MLCs) add a significant layer of complexity for safety assurance.
I argue that safety assurance should be an integral part of building safe and reliable software systems, but this process needs support from advanced software engineering and software analysis. In this talk, I outline a few approaches for development of principled, tool-supported methodologies for creating and managing assurance arguments. I then describe some of the recent work on specifying and verifying reliability requirements for machine-learned components in safety-critical domains.
",Stimulation,Variation in Life,"By promoting the development of reliable software systems; the paper indirectly contributes to the stimulation value item of ""Variation in Life"" by enabling diverse and reliable digital experiences.","In the context of a software user, the main contribution of 'Paper X' is the development of reliable software systems. By ensuring the safety and reliability of software, users are able to engage in diverse digital experiences without the fear of system failures or breaches. This enables variation in life by providing users with the freedom to explore different software applications and platforms, leading to stimulation and excitement in their daily interactions with technology.",Agreed-Justified,Agree,,Agree,,
Exp B,Exp H,1184,ESEC/FSE,Software Project Management,Asynchronous technical interviews: reducing the effect of supervised think-aloud on communication ability,"Software engineers often face a critical test before landing a job‚Äö√Ñ√∂‚àö√ë‚àö‚àÇ‚Äö√†√∂‚àö√´‚Äö√†√∂‚Äö√†√á‚Äö√Ñ√∂‚àö‚Ä†‚àö‚àÇ‚Äö√†√∂‚àö¬¥‚Äö√Ñ√∂‚àö‚Ä†‚àö‚àÇ‚Äö√†√∂‚àö√∫passing a technical interview. During these sessions, candidates must write code while thinking aloud as they work toward a solution to a problem under the watchful eye of an interviewer. While thinking aloud during technical interviews gives interviewers a picture of candidates‚Äö√Ñ√∂‚àö√ë‚àö‚àÇ‚Äö√†√∂‚àö√´‚Äö√†√∂‚Äö√†√á‚Äö√Ñ√∂‚àö‚Ä†‚àö‚àÇ‚Äö√†√∂‚àö¬¥‚Äö√Ñ√∂‚àö‚Ä†‚àö‚àÇ¬¨¬®‚Äö√Ñ¬¢ problem-solving ability, surprisingly, these types of interviews often prevent candidates from communicating their thought process effectively. To understand if poor performance related to interviewer presence can be reduced while preserving communication and technical skills, we introduce asynchronous technical interviews‚Äö√Ñ√∂‚àö√ë‚àö‚àÇ‚Äö√†√∂‚àö√´‚Äö√†√∂‚Äö√†√á‚Äö√Ñ√∂‚àö‚Ä†‚àö‚àÇ‚Äö√†√∂‚àö¬¥‚Äö√Ñ√∂‚àö‚Ä†‚àö‚àÇ‚Äö√†√∂‚àö√∫where candidates submit recordings of think-aloud and coding. We compare this approach to traditional whiteboard interviews and find that, by eliminating interviewer supervision, asynchronicity significantly improved the clarity of think-aloud via increased informativeness and reduced stress. Moreover, we discovered asynchronous technical interviews preserved, and in some cases even enhanced, technical problem-solving strategies and code quality. This work offers insight into asynchronous technical interviews as a design for supporting communication during interviews, and discusses trade-offs and guidelines for implementing this approach in software engineering hiring practices.
",Achievement,Intelligent,The paper discusses improving the methods of technical interviews; specifically highlighting the enhancement of technical problem-solving strategies and code quality; which can be seen as alligning with the value item 'Intelligent' within the 'Achievement' category,"In the paper, the authors introduce asynchronous technical interviews, which aim to improve the clarity of candidate's problem-solving abilities and reduce stress by eliminating interviewer supervision. This innovation in the hiring process aligns with the value item 'Intelligent' within the 'Achievement' category because it focuses on enhancing technical problem-solving strategies and code quality, which are indicators of intelligence and achievement. By implementing this approach, software users can benefit from a more effective hiring process that prioritizes intelligence in technical skills.",Agreed-Justified,Agree,,Agree,,
Exp C,Exp K,1184,ESEC/FSE,Software Project Management,Asynchronous technical interviews: reducing the effect of supervised think-aloud on communication ability,"Software engineers often face a critical test before landing a job‚Äö√Ñ√∂‚àö√ë‚àö√Üpassing a technical interview. During these sessions, candidates must write code while thinking aloud as they work toward a solution to a problem under the watchful eye of an interviewer. While thinking aloud during technical interviews gives interviewers a picture of candidates‚Äö√Ñ√∂‚àö√ë‚àö¬• problem-solving ability, surprisingly, these types of interviews often prevent candidates from communicating their thought process effectively. To understand if poor performance related to interviewer presence can be reduced while preserving communication and technical skills, we introduce asynchronous technical interviews‚Äö√Ñ√∂‚àö√ë‚àö√Üwhere candidates submit recordings of think-aloud and coding. We compare this approach to traditional whiteboard interviews and find that, by eliminating interviewer supervision, asynchronicity significantly improved the clarity of think-aloud via increased informativeness and reduced stress. Moreover, we discovered asynchronous technical interviews preserved, and in some cases even enhanced, technical problem-solving strategies and code quality. This work offers insight into asynchronous technical interviews as a design for supporting communication during interviews, and discusses trade-offs and guidelines for implementing this approach in software engineering hiring practices.
",Achievement,Successful,The innovation of asynchronous technical interviews is aimed at enhancing the performance of software users during interviews; signifying an alignment with the value item 'Successful' under the 'Achievement' value.,"The innovation of asynchronous technical interviews in 'Paper X' aligns with the value item 'Successful' and its corresponding value of 'Achievement' because it introduces a new approach that improves the clarity of candidates' thinking process and reduces stress in technical interviews. By providing a more effective means of communication and problem-solving during interviews, this innovation enhances the performance of software users, allowing them to achieve successful outcomes in the hiring process.",Agreed-Justified,Agree,,Agree,,
Exp D,Exp K,1184,ESEC/FSE,Software Project Management,Asynchronous technical interviews: reducing the effect of supervised think-aloud on communication ability,"Software engineers often face a critical test before landing a job‚Äö√Ñ√∂‚àö√ë‚àö√Üpassing a technical interview. During these sessions, candidates must write code while thinking aloud as they work toward a solution to a problem under the watchful eye of an interviewer. While thinking aloud during technical interviews gives interviewers a picture of candidates‚Äö√Ñ√∂‚àö√ë‚àö¬• problem-solving ability, surprisingly, these types of interviews often prevent candidates from communicating their thought process effectively. To understand if poor performance related to interviewer presence can be reduced while preserving communication and technical skills, we introduce asynchronous technical interviews‚Äö√Ñ√∂‚àö√ë‚àö√Üwhere candidates submit recordings of think-aloud and coding. We compare this approach to traditional whiteboard interviews and find that, by eliminating interviewer supervision, asynchronicity significantly improved the clarity of think-aloud via increased informativeness and reduced stress. Moreover, we discovered asynchronous technical interviews preserved, and in some cases even enhanced, technical problem-solving strategies and code quality. This work offers insight into asynchronous technical interviews as a design for supporting communication during interviews, and discusses trade-offs and guidelines for implementing this approach in software engineering hiring practices.
",Self Direction,Self Respect,The paper suggests a new approach to technical interviews - asynchronous technical interviews - which helps candidates communicate their thought process effectively and reduces stress; which could be interpreted as promoting 'Self Respect;' a value item under the 'Self Direction' value.,"The introduction of asynchronous technical interviews in 'Paper X' aligns with the value item Self Respect and its corresponding value Self Direction from a ""Software User"" perspective because it allows candidates to communicate their thought process effectively without the pressure of being observed by an interviewer. This promotes candidates' autonomy and freedom to express their ideas, choices, and preferences in problem-solving, which is an essential aspect of self-direction and self-respect. By providing a more comfortable and stress-free environment for candidates, the paper's approach facilitates their ability to assert themselves and confidently showcase their skills, ultimately promoting a sense of self-respect and self-direction in the software engineering hiring process.",Agreed-Clarified,Agree,,Agree,,
Exp E,Exp J,1187,ESEC/FSE,Code Generation & Analysis,Quantitative relational modelling with QAlloy,"Alloy is a popular language and tool for formal software design. A key factor to this popularity is its relational logic, an elegant specification language with a minimal syntax and semantics. However, many software problems nowadays involve both structural and quantitative requirements, and Alloy's relational logic is not well suited to reason about the latter. This paper introduces QAlloy, an extension of Alloy with quantitative relations that add integer quantities to associations between domain elements. Having integers internalised in relations, instead of being explicit domain elements like in standard Alloy, allows quantitative requirements to be specified in QAlloy with a similar elegance to structural requirements, with the side-effect of providing basic dimensional analysis support via the type system. The QAlloy Analyzer also implements an SMT-based engine that enables quantities to be unbounded, thus avoiding many problems that may arise with the current bounded integer semantics of Alloy.
",Self Direction,Independent,The paper introduces QAlloy; which is an extension of Alloy; increases the user's independence in managing both structural and quantitative requirements through its elegant specification language.,"The justification for labeling 'Paper X' as aligning with the value item Independent and its corresponding value Self Direction from a ""Software User"" perspective is based on the fact that QAlloy, as an extension of Alloy, allows users to independently manage both structural and quantitative requirements through its elegant specification language. The introduction of quantitative relations and the ability to have integers internalized in relations provide users with more freedom and autonomy in specifying and reasoning about software problems, enhancing their ability to make independent choices and decisions in the software development process.",Agreed-Justified,Agree,,Agree,,
Exp F,Exp J,1187,ESEC/FSE,Code Generation & Analysis,Quantitative relational modelling with QAlloy,"Alloy is a popular language and tool for formal software design. A key factor to this popularity is its relational logic, an elegant specification language with a minimal syntax and semantics. However, many software problems nowadays involve both structural and quantitative requirements, and Alloy's relational logic is not well suited to reason about the latter. This paper introduces QAlloy, an extension of Alloy with quantitative relations that add integer quantities to associations between domain elements. Having integers internalised in relations, instead of being explicit domain elements like in standard Alloy, allows quantitative requirements to be specified in QAlloy with a similar elegance to structural requirements, with the side-effect of providing basic dimensional analysis support via the type system. The QAlloy Analyzer also implements an SMT-based engine that enables quantities to be unbounded, thus avoiding many problems that may arise with the current bounded integer semantics of Alloy.
",Self Direction,Freedom,The QAlloy allows quantities to be unbounded; providing more freedom for users in terms of managing and assessing software requirements.,"The introduction of QAlloy, an extension of Alloy with quantitative relations, allows for unbounded quantities in software requirements. This enhanced flexibility and freedom in managing and assessing software requirements aligns with the value item Freedom and its corresponding value Self Direction from a ""Software User"" perspective. Users are empowered to have greater control and independence in defining and meeting their software needs.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,1190,ESEC/FSE,AI & Machine Learning,Improving ML-based information retrieval software with user-driven functional testing and defect class analysis,"Machine Learning (ML) has become the cornerstone of information retrieval (IR) software, as it can drive better user experience by leveraging information-rich data and complex models. However, evaluating the emergent behavior of ML-based IR software can be challenging with traditional software testing approaches: when developers modify the software, they cannot often extract useful information from individual test instances; rather, they seek to holistically verify whether‚Äö√Ñ√∂‚àö√ë‚àö‚àÇ‚Äö√†√∂‚àö√´‚Äö√†√∂‚Äö√†√á‚Äö√Ñ√∂‚àö‚Ä†‚àö‚àÇ‚Äö√†√∂‚àö¬¥‚Äö√Ñ√∂‚àö‚Ä†‚àö‚àÇ‚Äö√†√∂‚àö√∫and where‚Äö√Ñ√∂‚àö√ë‚àö‚àÇ‚Äö√†√∂‚àö√´‚Äö√†√∂‚Äö√†√á‚Äö√Ñ√∂‚àö‚Ä†‚àö‚àÇ‚Äö√†√∂‚àö¬¥‚Äö√Ñ√∂‚àö‚Ä†‚àö‚àÇ‚Äö√†√∂‚àö√∫their modifications caused significant regressions or improvements at scale. In this paper, we introduce not only such a holistic approach to evaluate the system-level behavior of the software, but also the concept of a defect class, which represents a partition of the input space on which the ML-based software does measurably worse for an existing feature or on which the ML task is more challenging for a new feature. We leverage large volumes of functional test cases, automatically obtained, to derive these defect classes, and propose new ways to improve the IR software from an end-user‚Äö√Ñ√∂‚àö√ë‚àö‚àÇ‚Äö√†√∂‚àö√´‚Äö√†√∂‚Äö√†√á‚Äö√Ñ√∂‚àö‚Ä†‚àö‚àÇ‚Äö√†√∂‚àö¬¥‚Äö√Ñ√∂‚àö‚Ä†‚àö‚àÇ¬¨¬®‚Äö√Ñ¬¢s perspective. Applying our approach on a real production Search-AutoComplete system that contains a query interpretation ML component, we demonstrate that (1) our holistic metrics successfully identified two regressions and one improvement, where all 3 were independently verified with retrospective A/B experiments, (2) the automatically obtained defect classes provided actionable insights during early-stage ML development, and (3) we also detected defect classes at the finer sub-component level for which there were significant regressions, which we blocked prior to different releases.
",Achievement,Intelligent,The paper introduces an approach to evaluate the system-level behavior of the software contributing to an intelligently designed software. This aligns with the value item Intelligent and its corresponding value Achievement.,"The main contribution of ""Paper X"" is introducing a holistic approach to evaluate the system-level behavior of ML-based software. This approach focuses on verifying whether and where modifications to the software have caused significant regressions or improvements at scale. By emphasizing the importance of system-level behavior and large volumes of functional test cases, the paper aligns with the value item Intelligent and its corresponding value Achievement. Through this alignment, the paper promotes the development and implementation of intelligent software systems that are capable of achieving their intended goals effectively and efficiently.",Agreed-Justified,Agree,,Agree,,
Exp B,Exp H,1190,ESEC/FSE,AI & Machine Learning,Improving ML-based information retrieval software with user-driven functional testing and defect class analysis,"Machine Learning (ML) has become the cornerstone of information retrieval (IR) software, as it can drive better user experience by leveraging information-rich data and complex models. However, evaluating the emergent behavior of ML-based IR software can be challenging with traditional software testing approaches: when developers modify the software, they cannot often extract useful information from individual test instances; rather, they seek to holistically verify whether‚Äö√Ñ√∂‚àö√ë‚àö‚àÇ‚Äö√†√∂‚àö√´‚Äö√†√∂‚Äö√†√á‚Äö√Ñ√∂‚àö‚Ä†‚àö‚àÇ‚Äö√†√∂‚àö¬¥‚Äö√Ñ√∂‚àö‚Ä†‚àö‚àÇ‚Äö√†√∂‚àö√∫and where‚Äö√Ñ√∂‚àö√ë‚àö‚àÇ‚Äö√†√∂‚àö√´‚Äö√†√∂‚Äö√†√á‚Äö√Ñ√∂‚àö‚Ä†‚àö‚àÇ‚Äö√†√∂‚àö¬¥‚Äö√Ñ√∂‚àö‚Ä†‚àö‚àÇ‚Äö√†√∂‚àö√∫their modifications caused significant regressions or improvements at scale. In this paper, we introduce not only such a holistic approach to evaluate the system-level behavior of the software, but also the concept of a defect class, which represents a partition of the input space on which the ML-based software does measurably worse for an existing feature or on which the ML task is more challenging for a new feature. We leverage large volumes of functional test cases, automatically obtained, to derive these defect classes, and propose new ways to improve the IR software from an end-user‚Äö√Ñ√∂‚àö√ë‚àö‚àÇ‚Äö√†√∂‚àö√´‚Äö√†√∂‚Äö√†√á‚Äö√Ñ√∂‚àö‚Ä†‚àö‚àÇ‚Äö√†√∂‚àö¬¥‚Äö√Ñ√∂‚àö‚Ä†‚àö‚àÇ¬¨¬®‚Äö√Ñ¬¢s perspective. Applying our approach on a real production Search-AutoComplete system that contains a query interpretation ML component, we demonstrate that (1) our holistic metrics successfully identified two regressions and one improvement, where all 3 were independently verified with retrospective A/B experiments, (2) the automatically obtained defect classes provided actionable insights during early-stage ML development, and (3) we also detected defect classes at the finer sub-component level for which there were significant regressions, which we blocked prior to different releases.
",Achievement,Successful,The paper demonstrates successful identification of regressions and improvements through the holistic approach; aligning with the value item Successful and its corresponding value Achievement.,"In the paper, the authors introduce a holistic approach to evaluate the system-level behavior of ML-based software and successfully identify regressions and improvements in a real production Search-AutoComplete system. This demonstrates the achievement of successfully improving the software's performance, aligning with the value item Successful and its corresponding value Achievement. By leveraging large volumes of functional test cases and detecting defect classes, the paper provides actionable insights for early-stage ML development, further reinforcing the alignment with the value item Successful and its corresponding value Achievement.",Agreed-Clarified,Agree,,Agree,,
Exp E,Exp J,1199,ESEC/FSE,Mobile & IoT,eGEN: an energy-saving modeling language and code generator for location-sensing of mobile apps,"Given the limited tool support for energy-saving strategies during the design phase of android applications, developing battery-aware, location-based android applications is a non-trivial task for developers. To this end, we propose eGEN, consisting of (1) a Domain-Specific Modeling Language (DSML) and (2) a code generator to specify and create native battery-aware, location-based mobile applications. We evaluated eGEN by instrumenting the generated battery-aware code in five location-based, open-source android applications and compared the energy consumption with non-eGEN versions. The experimental results show 188 mA (8.34% of battery per hour) of average reduction in battery consumption while showing only 97 meters of degradation in location accuracy over three kilometers of a cycling path. Hence, we see this tool as a first step in helping developers write battery-aware code in location-based android applications. The GitHub repository with source code and all artifacts is available at https://github.com/Kowndinya2000/egen, and the tool demo video at https://youtu.be/Iadfh4cCw8I.
",Self Direction,Choosing Own Goals,Through the use of eGen; users have the opportunity to choose which version of the application (battery-aware or non eGen versions) they want to operate based on their battery usage needs. This empowers the user with the freedom of choice and hence aligns with the value; 'Self Direction';particularly its value item; 'Choosing Own Goals'.,"In 'Paper X', the use of eGEN allows users to have control over their battery consumption by choosing between the battery-aware version and the non-eGEN version of the application. This aligns with the value item ""Choosing Own Goals"" and its corresponding value ""Self Direction"" as users are able to independently determine their own goals and make decisions based on their battery usage needs. This provides them with the freedom to choose and control their experience with the application, thus demonstrating a direct alignment with the values of self-direction and choosing own goals from the perspective of a software user.",Agreed-Justified,Agree,,Agree,,
Exp F,Exp J,1199,ESEC/FSE,Mobile & IoT,eGEN: an energy-saving modeling language and code generator for location-sensing of mobile apps,"Given the limited tool support for energy-saving strategies during the design phase of android applications, developing battery-aware, location-based android applications is a non-trivial task for developers. To this end, we propose eGEN, consisting of (1) a Domain-Specific Modeling Language (DSML) and (2) a code generator to specify and create native battery-aware, location-based mobile applications. We evaluated eGEN by instrumenting the generated battery-aware code in five location-based, open-source android applications and compared the energy consumption with non-eGEN versions. The experimental results show 188 mA (8.34% of battery per hour) of average reduction in battery consumption while showing only 97 meters of degradation in location accuracy over three kilometers of a cycling path. Hence, we see this tool as a first step in helping developers write battery-aware code in location-based android applications. The GitHub repository with source code and all artifacts is available at https://github.com/Kowndinya2000/egen, and the tool demo video at https://youtu.be/Iadfh4cCw8I.
",Stimulation,Daring,The use of eGen by software users implies a certain level of adaptability and acceptance of new ways of improving battery usage which could be considered daring given that software users are often hesitant towards application changes . This aligns with the value; 'Stimulation'; and its value item; 'Daring'.,"In the context of software users, the alignment of 'Paper X' with the value item Daring and its corresponding value Stimulation is justified based on the fact that the adoption of eGEN requires software users to embrace and adapt to new ways of improving battery usage in Android applications. This aspect can be seen as daring because software users are often reluctant or hesitant towards changes in applications. By considering this alignment, 'Paper X' aims to stimulate software users to explore and embrace innovative approaches for better battery-awareness in location-based Android applications.",Agreed-Clarified,Agree,,Agree,,
Exp G,Exp K,1203,ESEC/FSE,AI & Machine Learning,Blackbox adversarial attacks and explanations for automatic speech recognition,"Automatic speech recognition (ASR) models are used widely in applications for voice navigation and voice control of domestic appliances. The computational core of ASRs are Deep Neural Networks (DNNs) that have been shown to be susceptible to adversarial perturbations and exhibit unwanted biases and ethical issues. To assess the security of ASRs, we propose techniques that generate blackbox (agnostic to the DNN) adversarial attacks that are portable across ASRs. This is in contrast to existing work that focuses on whitebox attacks that are time consuming and lack portability. Apart from that, to figure out why ASRs(always blackbox) are easily attacked, we provide explanation methods on ASRs that help increase our understanding of the system and ultimately help build trust in the system.
",Achievement,Intelligent,"The paper focuses on improving understanding of Automatic Speech Recognition (ASR) systems; which may enhance the ability of the software user to effectively use the system; aligning with the value item ""Intelligent"" and its corresponding value ""Achievement"".","The alignment with the value item Intelligent and its corresponding value Achievement is justified based on the fact that the main contribution of 'Paper X' is to provide explanation methods on ASRs that help increase the understanding of the system. By enhancing the user's understanding of the ASR system, the software user can then use the system more effectively, which can be seen as an achievement in their ability to interact intelligently with the technology.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,1203,ESEC/FSE,AI & Machine Learning,Blackbox adversarial attacks and explanations for automatic speech recognition,"Automatic speech recognition (ASR) models are used widely in applications for voice navigation and voice control of domestic appliances. The computational core of ASRs are Deep Neural Networks (DNNs) that have been shown to be susceptible to adversarial perturbations and exhibit unwanted biases and ethical issues. To assess the security of ASRs, we propose techniques that generate blackbox (agnostic to the DNN) adversarial attacks that are portable across ASRs. This is in contrast to existing work that focuses on whitebox attacks that are time consuming and lack portability. Apart from that, to figure out why ASRs(always blackbox) are easily attacked, we provide explanation methods on ASRs that help increase our understanding of the system and ultimately help build trust in the system.
",Security,Healthy,"The paper proposes techniques to assess the security of ASR models which directly caters to the value item ""Healthy"" under the corresponding value ""Security"" signaling a safe and secure software user experience.","In 'Paper X', the proposed techniques to assess the security of ASR models directly align with the value item ""Healthy"" and its corresponding value ""Security"" from a ""Software User"" perspective. By addressing the susceptibility of ASRs to adversarial attacks and providing explanations for these vulnerabilities, the paper aims to ensure a safe and secure software user experience. The focus on identifying and mitigating biases and ethical issues in ASRs contributes to a sense of security and trust, which are fundamental aspects of the value item ""Healthy"" in the context of software usage.",Agreed-Justified,Disagree,reconcile with coder_2,Disagree,"The justification claims that the paper aligns with the value 'Security' because 'The paper proposes techniques to assess the security of ASR models, but The justification doesn't align with the value item of Healthy in the abstract.
",
Exp B,Exp H,1203,ESEC/FSE,AI & Machine Learning,Blackbox adversarial attacks and explanations for automatic speech recognition,"Automatic speech recognition (ASR) models are used widely in applications for voice navigation and voice control of domestic appliances. The computational core of ASRs are Deep Neural Networks (DNNs) that have been shown to be susceptible to adversarial perturbations and exhibit unwanted biases and ethical issues. To assess the security of ASRs, we propose techniques that generate blackbox (agnostic to the DNN) adversarial attacks that are portable across ASRs. This is in contrast to existing work that focuses on whitebox attacks that are time consuming and lack portability. Apart from that, to figure out why ASRs(always blackbox) are easily attacked, we provide explanation methods on ASRs that help increase our understanding of the system and ultimately help build trust in the system.
",Benevolence,Honesty,"The paper presents an explanation of why ASR models are easily attacked; thus promoting honesty and transparency about the system's functionality and possible vulnerabilities. This aligns with the value item ""Honesty"" and its corresponding value ""Benevolence"".","In the paper, the authors propose techniques to generate blackbox adversarial attacks that are portable across ASRs, in order to assess the security of the systems. By providing explanations on ASRs and increasing our understanding of the system's vulnerabilities, the paper promotes transparency and openness, as well as a sense of responsibility towards the software users. This aligns with the value item ""Honesty"" and its corresponding value ""Benevolence"" as it aims to protect and benefit the users by addressing the potential risks and ethical issues associated with ASR models.",Agreed-Clarified,Agree,,Agree,,
Exp C,Exp K,1212,ESEC/FSE,Software Testing & QA,Getting Outside the Bug Boxes (Keynote),"Sometimes, we humans find ourselves a bit slow to abandon the comfort of sitting aEURoeinside the boxaEUR, and this can detract from our ability to innovate. In this talk, IaEUR(tm)ll share some outside-the-box perspectives, gleaned from decades of software engineering work, on boxes IaEUR(tm)ve seen when thinking about bugs aEUR"" from failures to faults, from finding to fixing, and from traditional to very non-traditional notions of aEURoewhat countsaEUR as a bug. IaEUR(tm)ll consider the intellectually freeing perspectives that can come from moving outside the aEURoemechanismsaEUR box to policies; the enhancement to applicability from moving outside sub-sub-area boxes to the whole software lifecycle; the differences revealed when moving outside the aEURoetypical developeraEUR box to diverse humans; and the plethora of possibilities arising from moving outside the aEURoebuggy codeaEUR box to a wide range of bug types.",Self Direction,Curiosity,The abstract talks about ‚Äö√Ñ√∂‚àö√ë‚àö‚â§outside-the-box perspectives;‚Äö√Ñ√∂‚àö√ë‚àö¬• which aligns with the value item 'Curiosity' under the value 'Self Direction' because it implies a pursuit of novel and innovative ideas.,"The mention of ""outside-the-box perspectives"" in the abstract implies a willingness to explore new ideas and approaches, which aligns with the value item of Curiosity under the value of Self Direction. This alignment suggests that the main contribution of 'Paper X' is in promoting a sense of exploration and innovation within the software context, catering to the curiosity and desire for new experiences of software users.",Agreed-Clarified,Agree,,Agree,,
Exp E,Exp J,1212,ESEC/FSE,Software Testing & QA,Getting Outside the Bug Boxes (Keynote),"Sometimes, we humans find ourselves a bit slow to abandon the comfort of sitting aEURoeinside the boxaEUR, and this can detract from our ability to innovate. In this talk, IaEUR(tm)ll share some outside-the-box perspectives, gleaned from decades of software engineering work, on boxes IaEUR(tm)ve seen when thinking about bugs aEUR"" from failures to faults, from finding to fixing, and from traditional to very non-traditional notions of aEURoewhat countsaEUR as a bug. IaEUR(tm)ll consider the intellectually freeing perspectives that can come from moving outside the aEURoemechanismsaEUR box to policies; the enhancement to applicability from moving outside sub-sub-area boxes to the whole software lifecycle; the differences revealed when moving outside the aEURoetypical developeraEUR box to diverse humans; and the plethora of possibilities arising from moving outside the aEURoebuggy codeaEUR box to a wide range of bug types.",Achievement,Intelligent,The abstract discusses exploring diverse perspectives and wide range of bug types; contributing to 'Intelligent' which is a value item under 'Achievement' because bolder exploration of diverse perspectives and bugs types might increase software user success by developing a richer understanding of bug occurrence and management.,"In ""Paper X,"" the abstract mentions the exploration of diverse perspectives and a wide range of bug types. These actions contribute to the value item ""Intelligent"" under the value of ""Achievement"" because by actively seeking out different perspectives and understanding various bug types, software users can develop a deeper understanding of bug occurrence and management. This intelligence and understanding can ultimately lead to more successful software usage and achievement of desired outcomes by the users.",Agreed-Justified,Agree,,Agree,,
Exp F,Exp J,1213,ESEC/FSE,Software Project Management,A Four-Year Study of Student Contributions to OSS vs. OSS4SG with a Lightweight Intervention,"Modern software engineering practice and training increasingly rely on Open Source Software (OSS). The recent growth in demand for professional software engineers has led to increased contributions to, and usage of, OSS. However, there is limited understanding of the factors affecting how developers, and how new or student developers in particular, decide which OSS projects to contribute to, a process critical to OSS sustainability, access, adoption, and growth. To better understand OSS contributions from the developers of tomorrow, we conducted a four-year study with 1,361 students investigating the life cycle of their contributions (from project selection to pull request acceptance). During the study, we also delivered a lightweight intervention to promote the awareness of open source projects for social good (OSS4SG), OSS projects that have positive impacts in other domains. Using both quantitative and qualitative methods, we analyze student experience reports and the pull requests they submit. Compared to general OSS projects, we find significant differences in project selection (an' &lt; 0.0001, effect size = 0.84), student motivation (an' &lt; 0.01, effect size = 0.13), and increased pull-request acceptance rates for OSS4SG contributions. We also find that our intervention correlates with increased student contributions to OSS4SG (an' &lt; 0.0001, effect size = 0.38). Finally, we analyze correlations of factors such as gender or working with a partner. Our findings may help improve the experience for new developers participating in OSS4SG and the quality of their contributions. We also hope our work helps educators, project leaders, and contributors to build a mutually-beneficial framework for the future growth of OSS4SG.",Universalism,Social Justice,The paper's main contribution focuses on promoting the awareness of Open Source Software for Social Good (OSS4SG) projects and understanding how valuable this can be for software users. This aligns directly with the value item 'Social Justice' and its corresponding value 'Universalism' as OSS4SG projects positively impact other domains.,"In the paper, the authors specifically mention that they conducted a study to investigate the life cycle of OSS contributions and analyzed the experiences and pull requests of student developers. They also delivered an intervention to promote awareness of OSS projects for social good. From these statements, it is evident that the main focus of the paper is on the impact of OSS projects on various domains, emphasizing the social good aspect. This directly aligns with the value item 'Social Justice', as promoting awareness and contributing to OSS projects for social good demonstrates a concern for fairness and equity in society. Additionally, it aligns with the corresponding value 'Universalism' as OSS4SG projects aim to benefit and have a positive impact on a wide range of domains, highlighting a broadminded and inclusive perspective, which is consistent with the value of 'Universalism'.",Agreed-Justified,Disagree,I couldn't find a connection to social justice ,Disagree,This paper main focus is social good not social justice.,
Exp A,Exp H,1213,ESEC/FSE,Software Project Management,A Four-Year Study of Student Contributions to OSS vs. OSS4SG with a Lightweight Intervention,"Modern software engineering practice and training increasingly rely on Open Source Software (OSS). The recent growth in demand for professional software engineers has led to increased contributions to, and usage of, OSS. However, there is limited understanding of the factors affecting how developers, and how new or student developers in particular, decide which OSS projects to contribute to, a process critical to OSS sustainability, access, adoption, and growth. To better understand OSS contributions from the developers of tomorrow, we conducted a four-year study with 1,361 students investigating the life cycle of their contributions (from project selection to pull request acceptance). During the study, we also delivered a lightweight intervention to promote the awareness of open source projects for social good (OSS4SG), OSS projects that have positive impacts in other domains. Using both quantitative and qualitative methods, we analyze student experience reports and the pull requests they submit. Compared to general OSS projects, we find significant differences in project selection (an' &lt; 0.0001, effect size = 0.84), student motivation (an' &lt; 0.01, effect size = 0.13), and increased pull-request acceptance rates for OSS4SG contributions. We also find that our intervention correlates with increased student contributions to OSS4SG (an' &lt; 0.0001, effect size = 0.38). Finally, we analyze correlations of factors such as gender or working with a partner. Our findings may help improve the experience for new developers participating in OSS4SG and the quality of their contributions. We also hope our work helps educators, project leaders, and contributors to build a mutually-beneficial framework for the future growth of OSS4SG.",Benevolence,Helpful,By choosing to contribute to OSS4SG; the users of the software are being helpful towards other users or towards others in the wider society. This aligns with the value item 'Helpful' and its corresponding value 'Benevolence'.,"In 'Paper X', the authors conducted a study on student contributions to open source software projects for social good (OSS4SG). By actively choosing to contribute to OSS4SG, the software users are embodying the value of 'Helpful' and its corresponding value 'Benevolence'. Their contributions are driven by a desire to make a positive impact and help others in society through the use of open source software. This aligns directly with the value item and value, as the users' actions demonstrate a selfless willingness to aid and support others, thereby aligning with the value of Benevolence.",Agreed-Clarified,Agree,,Agree,,
Exp B,Exp H,1252,ESEC/FSE,AI & Machine Learning,Fix Fairness DonaEUR(tm)t Ruin Accuracy: Performance Aware Fairness Repair using AutoML,"Machine learning (ML) is increasingly being used in critical decision-making software, but incidents have raised questions about the fairness of ML predictions. To address this issue, new tools and methods are needed to mitigate bias in ML-based software. Previous studies have proposed bias mitigation algorithms that only work in specific situations and often result in a loss of accuracy. Our proposed solution is a novel approach that utilizes automated machine learning (AutoML) techniques to mitigate bias. Our approach includes two key innovations: a novel optimization function and a fairness-aware search space. By improving the default optimization function of AutoML and incorporating fairness objectives, we are able to mitigate bias with little to no loss of accuracy. Additionally, we propose a fairness-aware search space pruning method for AutoML to reduce computational cost and repair time. Our approach, built on the state-of-the-art Auto-Sklearn tool, is designed to reduce bias in real-world scenarios. In order to demonstrate the effectiveness of our approach, we evaluated our approach on four fairness problems and 16 different ML models, and our results show a significant improvement over the baseline and existing bias mitigation techniques. Our approach, Fair-AutoML, successfully repaired 60 out of 64 buggy cases, while existing bias mitigation techniques only repaired up to 44 out of 64 cases.",Achievement,Intelligent,The paper contributes to mitigating bias in Machine Learning-based software; which aligns with the value item Intelligent as improvements in AutoML techniques is an embodiment of software user's competence in creating fair ML models.,"The alignment of 'Paper X' with the value item Intelligent and its corresponding value Achievement is justified because the paper's contribution to mitigating bias in Machine Learning-based software demonstrates the software user's competence and capability in improving AutoML techniques, which directly aligns with the value item Intelligent. By actively working towards creating fair ML models, the software user showcases their ability to achieve success and demonstrate intelligence in their decision-making process.",Agreed-Clarified,Agree,,Agree,,
Exp C,Exp K,1252,ESEC/FSE,AI & Machine Learning,Fix Fairness DonaEUR(tm)t Ruin Accuracy: Performance Aware Fairness Repair using AutoML,"Machine learning (ML) is increasingly being used in critical decision-making software, but incidents have raised questions about the fairness of ML predictions. To address this issue, new tools and methods are needed to mitigate bias in ML-based software. Previous studies have proposed bias mitigation algorithms that only work in specific situations and often result in a loss of accuracy. Our proposed solution is a novel approach that utilizes automated machine learning (AutoML) techniques to mitigate bias. Our approach includes two key innovations: a novel optimization function and a fairness-aware search space. By improving the default optimization function of AutoML and incorporating fairness objectives, we are able to mitigate bias with little to no loss of accuracy. Additionally, we propose a fairness-aware search space pruning method for AutoML to reduce computational cost and repair time. Our approach, built on the state-of-the-art Auto-Sklearn tool, is designed to reduce bias in real-world scenarios. In order to demonstrate the effectiveness of our approach, we evaluated our approach on four fairness problems and 16 different ML models, and our results show a significant improvement over the baseline and existing bias mitigation techniques. Our approach, Fair-AutoML, successfully repaired 60 out of 64 buggy cases, while existing bias mitigation techniques only repaired up to 44 out of 64 cases.",Achievement,Successful,The approach proposed in the paper offers improved results in bias mitigation and repairs more cases than existing techniques; denoting success in achieving fairness in ML; aligning with the value item Successful.,"The justification for the alignment of 'Paper X' with the value item Successful and its corresponding value Achievement from a ""Software User"" perspective is based on the fact that the proposed approach in the paper successfully mitigates bias in ML predictions and outperforms existing techniques by significantly repairing a higher number of cases. This demonstrates the achievement of a successful outcome in terms of fairness, which aligns with the value item Successful. The focus is on the direct impact on software users, who benefit from improved accuracy and reduced bias in critical decision-making software.",Agreed-Justified,Agree,,Agree,,
Exp D,Exp K,1252,ESEC/FSE,AI & Machine Learning,Fix Fairness DonaEUR(tm)t Ruin Accuracy: Performance Aware Fairness Repair using AutoML,"Machine learning (ML) is increasingly being used in critical decision-making software, but incidents have raised questions about the fairness of ML predictions. To address this issue, new tools and methods are needed to mitigate bias in ML-based software. Previous studies have proposed bias mitigation algorithms that only work in specific situations and often result in a loss of accuracy. Our proposed solution is a novel approach that utilizes automated machine learning (AutoML) techniques to mitigate bias. Our approach includes two key innovations: a novel optimization function and a fairness-aware search space. By improving the default optimization function of AutoML and incorporating fairness objectives, we are able to mitigate bias with little to no loss of accuracy. Additionally, we propose a fairness-aware search space pruning method for AutoML to reduce computational cost and repair time. Our approach, built on the state-of-the-art Auto-Sklearn tool, is designed to reduce bias in real-world scenarios. In order to demonstrate the effectiveness of our approach, we evaluated our approach on four fairness problems and 16 different ML models, and our results show a significant improvement over the baseline and existing bias mitigation techniques. Our approach, Fair-AutoML, successfully repaired 60 out of 64 buggy cases, while existing bias mitigation techniques only repaired up to 44 out of 64 cases.",Universalism,Social Justice,By mitigating bias in ML predictions; the paper contributes to Social Justice; as it allows all groups to be treated fairly and equally by the software; aligning with the value item Social Justice in the Universalism value.,"The justification for aligning 'Paper X' with the value item Social Justice and its corresponding value Universalism from a ""Software User"" perspective is based on the fact that by mitigating bias in ML predictions, the paper ensures fair and equal treatment for all groups when interacting with the software. This directly aligns with the value item Social Justice within the Universalism value, as it promotes the idea of treating all individuals with fairness and equity.",Agreed-Justified,Agree,,Agree,,
Exp F,Exp J,1253,ESEC/FSE,AI & Machine Learning,BiasAsker: Measuring the Bias in Conversational AI System,"Powered by advanced Artificial Intelligence (AI) techniques, conversational AI systems, such as ChatGPT, and digital assistants like Siri, have been widely deployed in daily life. However, such systems may still produce content containing biases and stereotypes, causing potential social problems. Due to modern AI techniquesaEUR(tm) data-driven, black-box nature, comprehensively identifying and measuring biases in conversational systems remains challenging. Particularly, it is hard to generate inputs that can comprehensively trigger potential bias due to the lack of data containing both social groups and biased properties. In addition, modern conversational systems can produce diverse responses (e.g., chatting and explanation), which makes existing bias detection methods based solely on sentiment and toxicity hardly being adopted. In this paper, we propose BiasAsker, an automated framework to identify and measure social bias in conversational AI systems. To obtain social groups and biased properties, we construct a comprehensive social bias dataset containing a total of 841 groups and 5,021 biased properties. Given the dataset, BiasAsker automatically generates questions and adopts a novel method based on existence measurement to identify two types of biases (i.e., absolute bias and related bias) in conversational systems. Extensive experiments on eight commercial systems and two famous research models, such as ChatGPT and GPT-3, show that 32.83\% of the questions generated by BiasAsker can trigger biased behaviors in these widely deployed conversational systems. All the code, data, and experimental results have been released to facilitate future research.",Universalism,Social Justice,The proposal of BiasAsker is an effort to identify social biases; which speaks to the software user's value of Social Justice; looking for fairness within the software context.,"The main contribution of 'Paper X' is the automated framework BiasAsker, which is aimed at identifying and measuring social bias in conversational AI systems. This aligns with the value item of Social Justice from Schwartz's Taxonomy because the proposed framework addresses the software user's concern for fairness and equality within the software context. By detecting and measuring biases, BiasAsker promotes a more just and equitable use of conversational AI systems, which directly aligns with the value of Social Justice in Schwartz's Taxonomy.",Agreed-Justified,Agree,,Agree,,
Exp G,Exp K,1253,ESEC/FSE,AI & Machine Learning,BiasAsker: Measuring the Bias in Conversational AI System,"Powered by advanced Artificial Intelligence (AI) techniques, conversational AI systems, such as ChatGPT, and digital assistants like Siri, have been widely deployed in daily life. However, such systems may still produce content containing biases and stereotypes, causing potential social problems. Due to modern AI techniquesaEUR(tm) data-driven, black-box nature, comprehensively identifying and measuring biases in conversational systems remains challenging. Particularly, it is hard to generate inputs that can comprehensively trigger potential bias due to the lack of data containing both social groups and biased properties. In addition, modern conversational systems can produce diverse responses (e.g., chatting and explanation), which makes existing bias detection methods based solely on sentiment and toxicity hardly being adopted. In this paper, we propose BiasAsker, an automated framework to identify and measure social bias in conversational AI systems. To obtain social groups and biased properties, we construct a comprehensive social bias dataset containing a total of 841 groups and 5,021 biased properties. Given the dataset, BiasAsker automatically generates questions and adopts a novel method based on existence measurement to identify two types of biases (i.e., absolute bias and related bias) in conversational systems. Extensive experiments on eight commercial systems and two famous research models, such as ChatGPT and GPT-3, show that 32.83\% of the questions generated by BiasAsker can trigger biased behaviors in these widely deployed conversational systems. All the code, data, and experimental results have been released to facilitate future research.",Security,Social Order,By trying to identify and address biases in conversational AI systems; the paper's main contribution aligns with a software user's value of Social Order; ensuring a regulated system free from bias and discrimination.,"In the context of a ""Software User,"" the value item of Social Order in Schwartz's Taxonomy corresponds to the value of Security. The main contribution of 'Paper X' aligns with this value as it aims to identify and address biases in conversational AI systems. By ensuring a regulated system free from bias and discrimination, the paper directly aligns with the software user's value of Security, contributing to the maintenance of social order within the software context.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,1260,ESEC/FSE,Software Testing & QA,A Practical Human Labeling Method for Online Just-in-Time Software Defect Prediction,"Just-in-Time Software Defect Prediction (JIT-SDP) can be seen as an online learning problem where additional software changes produced over time may be labeled and used to create training examples. These training examples form a data stream that can be used to update JIT-SDP models in an attempt to avoid models becoming obsolete and poorly performing. However, labeling procedures adopted in existing online JIT-SDP studies implicitly assume that practitioners would not inspect software changes upon a defect-inducing prediction, delaying the production of training examples. This is inconsistent with a real-world scenario where practitioners would adopt JIT-SDP models and inspect certain software changes predicted as defect-inducing to check whether they really induce defects. Such inspection means that some software changes would be labeled much earlier than assumed in existing work, potentially leading to different JIT-SDP models and performance results. This paper aims at formulating a more practical human labeling procedure that takes into account the adoption of JIT-SDP models during the software development process. It then analyses whether and to what extent it would impact the predictive performance of JIT-SDP models. We also propose a new method to target the labeling of software changes with the aim of saving human inspection effort. Experiments based on 14 GitHub projects revealed that adopting a more realistic labeling procedure led to significantly higher predictive performance than when delaying the labeling process, meaning that existing work may have been underestimating the performance of JIT-SDP. In addition, our proposed method to target the labeling process was able to reduce human effort while maintaining predictive performance by recommending practitioners to inspect software changes that are more likely to induce defects. We encourage the adoption of more realistic human labeling methods in research studies to obtain an evaluation of JIT-SDP predictive performance that is closer to reality.",Achievement,Successful,The paper proposes and verifies methods to improve the predictive performance of JIT-SDP models hence; contributing to software user success under the value Achievement.,"In the paper, 'Paper X' aims to formulate a more practical human labeling procedure for Just-in-Time Software Defect Prediction (JIT-SDP) models and assess the impact on their predictive performance. By improving the labeling process and targeting the inspection of software changes more effectively, the paper contributes to enhancing the predictive performance of JIT-SDP models. This improvement aligns with the value item Successful and its corresponding value Achievement from a ""Software User"" perspective because it enhances the success of software users by improving the accuracy and reliability of defect prediction models, ultimately leading to more successful software development outcomes.",Agreed-Justified,Agree,,Agree,,
Exp E,Exp J,1262,ESEC/FSE,Software Project Management,Building and Sustaining Ethnically Racially and Gender Diverse Software Engineering Teams: A Study at Google,"Teams that build software are largely demographically homogeneous. Without diversity, homogeneous perspectives dominate how, why, and for whom software is designed. To understand how teams can successfully build and sustain diversity, we interviewed 11 engineers and 9 managers from some of the most gender and racially diverse teams at Google, a large software company. Qualitatively analyzing the interviews, we found shared approaches to recruiting, hiring, and promoting an inclusive environment, all of which create a positive feedback loop. Our findings produce actionable practices that every member of the team can take to increase diversity by fostering a more inclusive software engineering environment.",Conformity,Politeness,"The research addresses the need for ""Politeness""(v8.2) in the software engineering environment. The environment is described as inclusive; respectful and courteous which is an essential part of fostering diversity.","In the context of 'Paper X', the alignment with the value item of Politeness (v8.2) and its corresponding value of Conformity from a ""Software User"" perspective can be justified by the focus on creating an inclusive and respectful environment. Politeness, as a value, emphasizes the importance of being respectful and courteous towards others, which is directly related to fostering diversity in the software engineering environment. By promoting polite and respectful interactions, the paper suggests that teams can create an inclusive atmosphere where diverse perspectives are valued and respected, ultimately leading to the increased diversity in software development.",Agreed-Clarified,Agree,,Agree,,
Exp F,Exp J,1262,ESEC/FSE,Software Project Management,Building and Sustaining Ethnically Racially and Gender Diverse Software Engineering Teams: A Study at Google,"Teams that build software are largely demographically homogeneous. Without diversity, homogeneous perspectives dominate how, why, and for whom software is designed. To understand how teams can successfully build and sustain diversity, we interviewed 11 engineers and 9 managers from some of the most gender and racially diverse teams at Google, a large software company. Qualitatively analyzing the interviews, we found shared approaches to recruiting, hiring, and promoting an inclusive environment, all of which create a positive feedback loop. Our findings produce actionable practices that every member of the team can take to increase diversity by fostering a more inclusive software engineering environment.",Benevolence,Forgiving,"The paper's focus on promoting an inclusive environment hints at the fostering of ""Forgiveness"" (v9.4). In an inclusive environment; differences are accepted; which could lead to a greater tendency among users to be forgiving.","In the context of software engineering and the promotion of diversity, creating an inclusive environment that values differences can lead to a greater tendency among users to be forgiving. When individuals feel accepted and respected within a software environment, they are more likely to overlook minor shortcomings or mistakes and exhibit forgiving behavior. This alignment with the value item ""Forgiving"" (v9.4) from Schwartz's Taxonomy demonstrates the potential impact of the paper's contributions on software users' attitudes and behaviors.",Agreed-Clarified,Agree,,Agree,,
Exp G,Exp K,1263,ESEC/FSE,Software Engineering Practices,Towards Automated Detection of Unethical Behavior in Open-Source Software Projects,"Given the rapid growth of Open-Source Software (OSS) projects, ethical considerations are becoming more important. Past studies focused on specific ethical issues (e.g., gender bias and fairness in OSS). There is little to no study on the different types of unethical behavior in OSS projects. We present the first study of unethical behavior in OSS projects from the stakeholdersaEUR(tm) perspective. Our study of 316 GitHub issues provides a taxonomy of 15 types of unethical behavior guided by six ethical principles (e.g., autonomy). Examples of new unethical behavior include soft forking (copying a repository without forking) and self-promotion (promoting a repository without self-identifying as contributor to the repository). We also identify 18 types of software artifacts affected by the unethical behavior. The diverse types of unethical behavior identified in our study (1) call for attentions of developers and researchers when making contributions in GitHub, and (2) point to future research on automated detection of unethical behavior in OSS projects. From our study, we propose Etor, an approach that can automatically detect six types of unethical behavior by using ontological engineering and Semantic Web Rule Language (SWRL) rules to model GitHub attributes and software artifacts. Our evaluation on 195,621 GitHub issues (1,765 GitHub repositories) shows that Etor can automatically detect 548 unethical behavior with 74.8\% average true positive rate (up to 100\% true positive rate). This shows the feasibility of automated detection of unethical behavior in OSS projects.",Tradition,Respect for Tradition,The paper emphasizes ethical considerations in Open-Source Software projects; aligning with the value item Respect for Tradition which involves generally accepted societal and ethical standards.,"The main contributions of 'Paper X' in studying unethical behavior in OSS projects align with the value item Respect for Tradition and its corresponding value Tradition from a ""Software User"" perspective. The paper explicitly states that the study is guided by six ethical principles and identifies 15 types of unethical behavior, highlighting the importance of ethical standards in the OSS community. This aligns with the value item Respect for Tradition, as it involves adhering to generally accepted societal and ethical standards, which are essential for maintaining trust and integrity in software development.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,1263,ESEC/FSE,Software Engineering Practices,Towards Automated Detection of Unethical Behavior in Open-Source Software Projects,"Given the rapid growth of Open-Source Software (OSS) projects, ethical considerations are becoming more important. Past studies focused on specific ethical issues (e.g., gender bias and fairness in OSS). There is little to no study on the different types of unethical behavior in OSS projects. We present the first study of unethical behavior in OSS projects from the stakeholdersaEUR(tm) perspective. Our study of 316 GitHub issues provides a taxonomy of 15 types of unethical behavior guided by six ethical principles (e.g., autonomy). Examples of new unethical behavior include soft forking (copying a repository without forking) and self-promotion (promoting a repository without self-identifying as contributor to the repository). We also identify 18 types of software artifacts affected by the unethical behavior. The diverse types of unethical behavior identified in our study (1) call for attentions of developers and researchers when making contributions in GitHub, and (2) point to future research on automated detection of unethical behavior in OSS projects. From our study, we propose Etor, an approach that can automatically detect six types of unethical behavior by using ontological engineering and Semantic Web Rule Language (SWRL) rules to model GitHub attributes and software artifacts. Our evaluation on 195,621 GitHub issues (1,765 GitHub repositories) shows that Etor can automatically detect 548 unethical behavior with 74.8\% average true positive rate (up to 100\% true positive rate). This shows the feasibility of automated detection of unethical behavior in OSS projects.",Universalism,Social Justice,The paper's main contribution is aimed towards addressing unfair and unethical behaviors in OSS projects; which corresponds with Social Justice; a value item under Universalism.,"The main contribution of 'Paper X' is the identification and taxonomy of different types of unethical behavior in Open-Source Software (OSS) projects. By addressing these issues, the paper aims to promote fairness and equity within the OSS community, which aligns with the value item of Social Justice under the value of Universalism. This is because Social Justice emphasizes the importance of promoting equality and fairness in society, and by addressing unethical behavior, the paper aligns with these values and seeks to create a more just and inclusive OSS community.",Agreed-Justified,Agree,,Agree,,
Exp B,Exp H,1263,ESEC/FSE,Software Engineering Practices,Towards Automated Detection of Unethical Behavior in Open-Source Software Projects,"Given the rapid growth of Open-Source Software (OSS) projects, ethical considerations are becoming more important. Past studies focused on specific ethical issues (e.g., gender bias and fairness in OSS). There is little to no study on the different types of unethical behavior in OSS projects. We present the first study of unethical behavior in OSS projects from the stakeholdersaEUR(tm) perspective. Our study of 316 GitHub issues provides a taxonomy of 15 types of unethical behavior guided by six ethical principles (e.g., autonomy). Examples of new unethical behavior include soft forking (copying a repository without forking) and self-promotion (promoting a repository without self-identifying as contributor to the repository). We also identify 18 types of software artifacts affected by the unethical behavior. The diverse types of unethical behavior identified in our study (1) call for attentions of developers and researchers when making contributions in GitHub, and (2) point to future research on automated detection of unethical behavior in OSS projects. From our study, we propose Etor, an approach that can automatically detect six types of unethical behavior by using ontological engineering and Semantic Web Rule Language (SWRL) rules to model GitHub attributes and software artifacts. Our evaluation on 195,621 GitHub issues (1,765 GitHub repositories) shows that Etor can automatically detect 548 unethical behavior with 74.8\% average true positive rate (up to 100\% true positive rate). This shows the feasibility of automated detection of unethical behavior in OSS projects.",Security,Healthy,By proposing an approach to detect unethical behavior; the paper indirectly contributes towards promoting a healthier software community environment; 'Healthy' being a value item under Security.,"In the context of software, security is an important value for users as it ensures the protection of personal data and safeguards against potential risks. The paper contributes to security by proposing an approach to detect unethical behavior in open-source software projects, which can ultimately promote a healthier software community environment. Unethical behavior, such as soft forking or self-promotion without proper identification, can potentially compromise the security and trust within the software community. Therefore, by addressing and raising awareness about unethical behavior, the paper aligns with the value item Healthy and its corresponding value Security, as it aims to create a more secure and trustworthy software environment for users.",Agreed-Justified,Agree,reconcile with coder_2,Agree,,
Exp C,Exp K,1264,ESEC/FSE,AI & Machine Learning,NeuRI: Diversifying DNN Generation via Inductive Rule Inference,"Deep Learning (DL) is prevalently used in various industries to improve decision-making and automate processes, driven by the ever-evolving DL libraries and compilers. The correctness of DL systems is crucial for trust in DL applications.  
As such, the recent wave of research has been studying the automated synthesis of test-cases (i.e., DNN models and their inputs) for fuzzing DL systems. However, existing model generators only subsume a limited number of operators, lacking the ability to pervasively model operator constraints.  
To address this challenge, we propose NeuRI, a fully automated approach for generating valid and diverse DL models composed of hundreds of types of operators. NeuRI adopts a three-step process:  
(i) collecting valid and invalid API traces from various sources;  
(ii) applying inductive program synthesis over the traces to infer the constraints for constructing valid models; and  
(iii) using hybrid model generation which incorporates both symbolic and concrete operators.  
Our evaluation shows that NeuRI improves branch coverage of TensorFlow and PyTorch by 24\% and 15\% over the state-of-the-art model-level fuzzers. NeuRI finds 100 new bugs for PyTorch and TensorFlow in four months, with 81 already fixed or confirmed. Of these, 9 bugs are labelled as high priority or security vulnerability, constituting 10\% of all high-priority bugs of the period.  
Open-source developers regard error-inducing tests reported by us as ""high-quality"" and ""common in practice"".",Achievement,Capable,The paper contributes to the development of NeuRI which improves the capability of Deep Learning systems; aligning with the value item Capable and its corresponding value Achievement.,"In the context of a ""Software User,"" the main contribution of 'Paper X' is the development of NeuRI, a fully automated approach for generating valid and diverse DL models. By improving the capability of Deep Learning systems through the generation of valid models, 'Paper X' aligns with the value item Capable and its corresponding value Achievement. This aligns with the user's desire for software that is capable, efficient, and successful in its intended purpose.",Agreed-Justified,Agree,,Agree,,
Exp D,Exp K,1264,ESEC/FSE,AI & Machine Learning,NeuRI: Diversifying DNN Generation via Inductive Rule Inference,"Deep Learning (DL) is prevalently used in various industries to improve decision-making and automate processes, driven by the ever-evolving DL libraries and compilers. The correctness of DL systems is crucial for trust in DL applications.  
As such, the recent wave of research has been studying the automated synthesis of test-cases (i.e., DNN models and their inputs) for fuzzing DL systems. However, existing model generators only subsume a limited number of operators, lacking the ability to pervasively model operator constraints.  
To address this challenge, we propose NeuRI, a fully automated approach for generating valid and diverse DL models composed of hundreds of types of operators. NeuRI adopts a three-step process:  
(i) collecting valid and invalid API traces from various sources;  
(ii) applying inductive program synthesis over the traces to infer the constraints for constructing valid models; and  
(iii) using hybrid model generation which incorporates both symbolic and concrete operators.  
Our evaluation shows that NeuRI improves branch coverage of TensorFlow and PyTorch by 24\% and 15\% over the state-of-the-art model-level fuzzers. NeuRI finds 100 new bugs for PyTorch and TensorFlow in four months, with 81 already fixed or confirmed. Of these, 9 bugs are labelled as high priority or security vulnerability, constituting 10\% of all high-priority bugs of the period.  
Open-source developers regard error-inducing tests reported by us as ""high-quality"" and ""common in practice"".",Achievement,Successful,The paper demonstrates a successful implementation of NeuRI; which increases the branch coverage of TensorFlow and PyTorch and finds new bugs. This aligns with the value item Successful and its corresponding value Achievement.,"In the paper, the authors propose NeuRI, an automated approach for generating valid and diverse DL models that improves branch coverage and identifies new bugs in TensorFlow and PyTorch. By achieving these results, the paper demonstrates the successful implementation of NeuRI in enhancing the performance and reliability of DL systems, which directly aligns with the value item Successful and its corresponding value Achievement.",Agreed-Justified,Agree,,Agree,,
Exp E,Exp J,1264,ESEC/FSE,AI & Machine Learning,NeuRI: Diversifying DNN Generation via Inductive Rule Inference,"Deep Learning (DL) is prevalently used in various industries to improve decision-making and automate processes, driven by the ever-evolving DL libraries and compilers. The correctness of DL systems is crucial for trust in DL applications.  
As such, the recent wave of research has been studying the automated synthesis of test-cases (i.e., DNN models and their inputs) for fuzzing DL systems. However, existing model generators only subsume a limited number of operators, lacking the ability to pervasively model operator constraints.  
To address this challenge, we propose NeuRI, a fully automated approach for generating valid and diverse DL models composed of hundreds of types of operators. NeuRI adopts a three-step process:  
(i) collecting valid and invalid API traces from various sources;  
(ii) applying inductive program synthesis over the traces to infer the constraints for constructing valid models; and  
(iii) using hybrid model generation which incorporates both symbolic and concrete operators.  
Our evaluation shows that NeuRI improves branch coverage of TensorFlow and PyTorch by 24\% and 15\% over the state-of-the-art model-level fuzzers. NeuRI finds 100 new bugs for PyTorch and TensorFlow in four months, with 81 already fixed or confirmed. Of these, 9 bugs are labelled as high priority or security vulnerability, constituting 10\% of all high-priority bugs of the period.  
Open-source developers regard error-inducing tests reported by us as ""high-quality"" and ""common in practice"".",Security,Healthy,The paper contributes to improving the health of Deep Learning systems by fixing bugs and vulnerabilities; aligning with the value item Healthy and its corresponding value Security.,"The paper's contribution of identifying and fixing bugs and vulnerabilities in Deep Learning systems directly aligns with the value item Healthy and its corresponding value Security from a ""Software User"" perspective. By improving the correctness and trustworthiness of DL systems, the paper aims to ensure that these systems operate in a secure and reliable manner, promoting the overall health of the systems and assuring users of their safety.",Agreed-Justified,Agree,,Agree,,
Exp F,Exp J,1266,ESEC/FSE,Software Deployment & Operations,Outage-Watch: Early Prediction of Outages using Extreme Event Regularizer,"Cloud services are omnipresent and critical cloud service failure is a fact of life. In order to retain customers and prevent revenue loss, it is important to provide high reliability guarantees for these services. One way to do this is by predicting outages in advance, which can help in reducing the severity as well as time to recovery. It is difficult to forecast critical failures due to the rarity of these events. Moreover, critical failures are ill-defined in terms of observable data. Our proposed method, Outage-Watch, defines critical service outages as deteriorations in the Quality of Service (QoS) captured by a set of metrics. Outage-Watch detects such outages in advance by using current system state to predict whether the QoS metrics will cross a threshold and initiate an extreme event. A mixture of Gaussian is used to model the distribution of the QoS metrics for flexibility and an extreme event regularizer helps in improving learning in tail of the distribution. An outage is predicted if the probability of any one of the QoS metrics crossing threshold changes significantly. Our evaluation on a real-world SaaS company dataset shows that Outage-Watch significantly outperforms traditional methods with an average AUC of 0.98. Additionally, Outage-Watch detects all the outages exhibiting a change in service metrics and reduces the Mean Time To Detection (MTTD) of outages by up to 88\% when deployed in an enterprise cloud-service system, demonstrating efficacy of our proposed method.",Achievement,Intelligent,The paper focuses on predicting and preventing outages by analyzing system data patterns. This research effort contributes to making cloud services and platforms function more intelligently; aligning with the value item Intelligent and its corresponding value Achievement.,"The paper's main contribution lies in its approach of predicting and preventing outages in cloud services by analyzing system data patterns. By utilizing this approach, cloud services and platforms can operate more intelligently and efficiently, enabling them to achieve higher levels of reliability and performance. This alignment with the value item Intelligent and its corresponding value Achievement is evident as the paper aims to enhance the intelligent functioning of cloud services, ultimately leading to improved achievement in terms of service reliability and customer satisfaction.",Agreed-Justified,Agree,,Agree,,
Exp G,Exp K,1266,ESEC/FSE,Software Deployment & Operations,Outage-Watch: Early Prediction of Outages using Extreme Event Regularizer,"Cloud services are omnipresent and critical cloud service failure is a fact of life. In order to retain customers and prevent revenue loss, it is important to provide high reliability guarantees for these services. One way to do this is by predicting outages in advance, which can help in reducing the severity as well as time to recovery. It is difficult to forecast critical failures due to the rarity of these events. Moreover, critical failures are ill-defined in terms of observable data. Our proposed method, Outage-Watch, defines critical service outages as deteriorations in the Quality of Service (QoS) captured by a set of metrics. Outage-Watch detects such outages in advance by using current system state to predict whether the QoS metrics will cross a threshold and initiate an extreme event. A mixture of Gaussian is used to model the distribution of the QoS metrics for flexibility and an extreme event regularizer helps in improving learning in tail of the distribution. An outage is predicted if the probability of any one of the QoS metrics crossing threshold changes significantly. Our evaluation on a real-world SaaS company dataset shows that Outage-Watch significantly outperforms traditional methods with an average AUC of 0.98. Additionally, Outage-Watch detects all the outages exhibiting a change in service metrics and reduces the Mean Time To Detection (MTTD) of outages by up to 88\% when deployed in an enterprise cloud-service system, demonstrating efficacy of our proposed method.",Security,Healthy,By predicting critical failures; the proposed method directly contributes to prevention; therefore helping to maintain healthy system states that ensure consistent and continuous functionality; aligning with the value item Healthy and its corresponding value Security.,"By predicting critical failures in advance, the proposed method in 'Paper X' enables proactive measures to prevent outages and maintain a healthy system state. This directly aligns with the value item Healthy and its corresponding value Security because it ensures the software user that the system will consistently provide reliable and uninterrupted functionality, minimizing disruptions and potential risks to their security.",Agreed-Justified,Disagree,Agree with coder_2,Disagree,This is system health not human health. The link cannot be seen. ,
Exp A,Exp H,1266,ESEC/FSE,Software Deployment & Operations,Outage-Watch: Early Prediction of Outages using Extreme Event Regularizer,"Cloud services are omnipresent and critical cloud service failure is a fact of life. In order to retain customers and prevent revenue loss, it is important to provide high reliability guarantees for these services. One way to do this is by predicting outages in advance, which can help in reducing the severity as well as time to recovery. It is difficult to forecast critical failures due to the rarity of these events. Moreover, critical failures are ill-defined in terms of observable data. Our proposed method, Outage-Watch, defines critical service outages as deteriorations in the Quality of Service (QoS) captured by a set of metrics. Outage-Watch detects such outages in advance by using current system state to predict whether the QoS metrics will cross a threshold and initiate an extreme event. A mixture of Gaussian is used to model the distribution of the QoS metrics for flexibility and an extreme event regularizer helps in improving learning in tail of the distribution. An outage is predicted if the probability of any one of the QoS metrics crossing threshold changes significantly. Our evaluation on a real-world SaaS company dataset shows that Outage-Watch significantly outperforms traditional methods with an average AUC of 0.98. Additionally, Outage-Watch detects all the outages exhibiting a change in service metrics and reduces the Mean Time To Detection (MTTD) of outages by up to 88\% when deployed in an enterprise cloud-service system, demonstrating efficacy of our proposed method.",Security,Social Order,Through early detection and reduction in severity of outages; Outage-Watch contributes to creating and maintaining an orderly and predictable technological environment. This aligns with the value item Social Order and its corresponding value Security.,"Through early detection and reduction of outages, Outage-Watch aligns with the value item Social Order and its corresponding value Security because it contributes to creating a stable and reliable technological environment. By predicting outages in advance and taking preventive measures, the software user can have a sense of security knowing that the cloud services they rely on will have minimal disruptions, allowing for a more orderly and predictable experience. This directly addresses the value of Security, as users can have confidence in the system's ability to provide consistent and uninterrupted service, ultimately fostering a sense of social order within the software context.",Agreed-Clarified,Agree,,Agree,,
Exp B,Exp H,1268,ESEC/FSE,AI & Machine Learning,Knowledge-Based Version Incompatibility Detection for Deep Learning,"Version incompatibility issues are rampant when reusing or reproducing deep learning models and applications. Existing techniques are limited to library dependency specifications declared in PyPI. Therefore, these techniques cannot detect version issues due to undocumented version constraints or issues involving hardware drivers or OS. To address this challenge, we propose to leverage the abundant discussions of DL version issues from Stack Overflow to facilitate version incompatibility detection. We reformulate the problem of knowledge extraction as a Question-Answering (QA) problem and use a pre-trained QA model to extract version compatibility knowledge from online discussions. The extracted knowledge is further consolidated into a weighted knowledge graph to detect potential version incompatibilities when reusing a DL project. Our evaluation results show that (1) our approach can accurately extract version knowledge with 84\% accuracy, and (2) our approach can accurately identify 65\% of known version issues in 10 popular DL projects with a high precision (92\%), while two state-of-the-art approaches can only detect 29\% and 6\% of these issues with 33\% and 17\% precision respectively.",Achievement,Successful,The paper introduces a technique that improves the accuracy of detecting version compatibility; therefore contributing to the Success value item within the Achievement value. This allows software users to more successfully use software without encountering version incompatibility issues.,"The main contribution of 'Paper X' is the proposal of a technique to detect version incompatibility issues in deep learning projects. By accurately extracting version knowledge and identifying potential issues, the technique enables software users to successfully use software without encountering version compatibility problems. This aligns with the value item Successful and its corresponding value Achievement because it helps software users achieve their goals of effectively utilizing deep learning models and applications without being hindered by version issues.",Agreed-Justified,Agree,,Agree,,
Exp C,Exp K,1268,ESEC/FSE,AI & Machine Learning,Knowledge-Based Version Incompatibility Detection for Deep Learning,"Version incompatibility issues are rampant when reusing or reproducing deep learning models and applications. Existing techniques are limited to library dependency specifications declared in PyPI. Therefore, these techniques cannot detect version issues due to undocumented version constraints or issues involving hardware drivers or OS. To address this challenge, we propose to leverage the abundant discussions of DL version issues from Stack Overflow to facilitate version incompatibility detection. We reformulate the problem of knowledge extraction as a Question-Answering (QA) problem and use a pre-trained QA model to extract version compatibility knowledge from online discussions. The extracted knowledge is further consolidated into a weighted knowledge graph to detect potential version incompatibilities when reusing a DL project. Our evaluation results show that (1) our approach can accurately extract version knowledge with 84\% accuracy, and (2) our approach can accurately identify 65\% of known version issues in 10 popular DL projects with a high precision (92\%), while two state-of-the-art approaches can only detect 29\% and 6\% of these issues with 33\% and 17\% precision respectively.",Security,Social Order,By creating a system that uses collected knowledge to detect potential version incompatibilities; the paper contributes to the Social Order value item within the Security value by providing more reliable and predictable software use for the software users.,"The justification for aligning 'Paper X' with the value item Social Order and its corresponding value Security from a ""Software User"" perspective is based on the fact that the paper proposes a system to detect potential version incompatibilities. By addressing these issues, the paper contributes to a more reliable and predictable software use, which aligns with the value of Security within Social Order. This ensures that software users can have a sense of stability and order in their software applications, enhancing their overall experience and trust in the software system.",Agreed-Clarified,Agree,,Agree,,
Exp E,Exp J,1281,ESEC/FSE,AI & Machine Learning,Benchmarking Robustness of AI-Enabled Multi-sensor Fusion Systems: Challenges and Opportunities,"Multi-Sensor Fusion (MSF) based perception systems have been the foundation in supporting many industrial applications and domains, such as self-driving cars, robotic arms, and unmanned aerial vehicles. Over the past few years, the fast progress in datadriven artificial intelligence (AI) has brought a fast-increasing trend to empower MSF systems by deep learning techniques to further improve performance, especially on intelligent systems and their perception systems. Although quite a few AI-enabled MSF perception systems and techniques have been proposed, up to the present, limited benchmarks that focus on MSF perception are publicly available. Given that many intelligent systems such as self-driving cars are operated in safety-critical contexts where perception systems play an important role, there comes an urgent need for a more in-depth understanding of the performance and reliability of these MSF systems.  

To bridge this gap, we initiate an early step in this direction and construct a public benchmark of AI-enabled MSF-based perception systems including three commonly adopted tasks (i.e., object detection, object tracking, and depth completion). Based on this, to comprehensively understand MSF systemsaEUR(tm) robustness and reliability, we design 14 common and realistic corruption patterns to synthesize large-scale corrupted datasets. We further perform a systematic evaluation of these systems through our large-scale evaluation and identify the following key findings: (1) existing AI-enabled MSF systems are not robust enough against corrupted sensor signals; (2) small synchronization and calibration errors can lead to a crash of AI-enabled MSF systems; (3) existing AI-enabled MSF systems are usually tightly-coupled in which bugs/errors from an individual sensor could result in a system crash; (4) the robustness of MSF systems can be enhanced by improving fusion mechanisms. Our results reveal the vulnerability of the current AI-enabled MSF perception systems, calling for researchers and practitioners to take robustness and reliability into account when designing AI-enabled MSF. Our benchmark, code, and detailed evaluation results are publicly available at https://sites.google.com/view/ai-msf-benchmark.",Security,Healthy,The paper's main contribution is improving performance and reliability of MSF systems which especially emphasizes on the safety of the user in application areas like self-driving cars; indicating a clear alignment with the value item 'Healthy'.,"I apologize for any confusion caused by my previous justification. Upon reevaluating the abstract, I recognize that the alignment between 'Paper X' and the value item Healthy may not be directly evident as the abstract does not explicitly mention the safety of the user in relation to the performance and reliability improvements of MSF systems. Thus, I concede that this alignment may not be the most obvious choice based solely on the information provided in the abstract.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,1282,ESEC/FSE,AI & Machine Learning,Automated Testing and Improvement of Named Entity Recognition Systems,"Named entity recognition (NER) systems have seen rapid progress in recent years due to the development of deep neural networks. These systems are widely used in various natural language processing applications, such as information extraction, question answering, and sentiment analysis. However, the complexity and intractability of deep neural networks can make NER systems unreliable in certain circumstances, resulting in incorrect predictions. For example, NER systems may misidentify female names as chemicals or fail to recognize the names of minority groups, leading to user dissatisfaction. To tackle this problem, we introduce TIN, a novel, widely applicable approach for automatically testing and repairing various NER systems. The key idea for automated testing is that the NER predictions of the same named entities under similar contexts should be identical. The core idea for automated repairing is that similar named entities should have the same NER prediction under the same context. We use TIN to test two SOTA NER models and two commercial NER APIs, i.e., Azure NER and AWS NER. We manually verify 784 of the suspicious issues reported by TIN and find that 702 are erroneous issues, leading to high precision (85.0\%-93.4\%) across four categories of NER errors: omission, over-labeling, incorrect category, and range error. For automated repairing, TIN achieves a high error reduction rate (26.8\%-50.6\%) over the four systems under test, which successfully repairs 1,056 out of the 1,877 reported NER errors.",Security,Healthy,The paper presents a method that will help improve the functionality of Named Entity Recognition (NER) systems; ensuring a healthier software performance by reducing errors. The software user's health in terms of usability is improved.,"The alignment of 'Paper X' with the value item Healthy and its corresponding value Security is justified because the paper introduces a novel approach, TIN, for automatically testing and repairing NER systems. By improving the accuracy and reliability of these systems, user dissatisfaction caused by incorrect predictions is reduced, leading to a healthier software performance. This directly aligns with the value of Security as it provides users with a more reliable and trustworthy software experience. Furthermore, a software user's health in terms of usability is improved as they can depend on the NER system to accurately identify named entities, enhancing the overall functionality and efficiency of the software.",Agreed-Justified,Agree,,Disagree,"this justification is not supported by the evidence in the abstract, which focuses on 'While lots of research has explored how to prevent maintainers from abandoning the open-source project it is not related to Healthy.
",
Exp B,Exp H,1282,ESEC/FSE,AI & Machine Learning,Automated Testing and Improvement of Named Entity Recognition Systems,"Named entity recognition (NER) systems have seen rapid progress in recent years due to the development of deep neural networks. These systems are widely used in various natural language processing applications, such as information extraction, question answering, and sentiment analysis. However, the complexity and intractability of deep neural networks can make NER systems unreliable in certain circumstances, resulting in incorrect predictions. For example, NER systems may misidentify female names as chemicals or fail to recognize the names of minority groups, leading to user dissatisfaction. To tackle this problem, we introduce TIN, a novel, widely applicable approach for automatically testing and repairing various NER systems. The key idea for automated testing is that the NER predictions of the same named entities under similar contexts should be identical. The core idea for automated repairing is that similar named entities should have the same NER prediction under the same context. We use TIN to test two SOTA NER models and two commercial NER APIs, i.e., Azure NER and AWS NER. We manually verify 784 of the suspicious issues reported by TIN and find that 702 are erroneous issues, leading to high precision (85.0\%-93.4\%) across four categories of NER errors: omission, over-labeling, incorrect category, and range error. For automated repairing, TIN achieves a high error reduction rate (26.8\%-50.6\%) over the four systems under test, which successfully repairs 1,056 out of the 1,877 reported NER errors.",Achievement,Successful,The paper presents TIN; which improves the success rate of the NER systems by testing; identifying; and addressing the errors. This aligns with the users' successful interaction with the software.,"The main contribution of 'Paper X' is the introduction of TIN, a novel approach for testing and repairing NER systems. By leveraging TIN, users can achieve a higher success rate when interacting with NER systems by reducing and addressing errors. This aligns with the value item Successful and its corresponding value Achievement from a ""Software User"" perspective. Users value the ability to successfully use software systems without facing significant errors or issues, and 'Paper X' directly addresses this by proposing a method to improve the reliability and accuracy of NER systems, ultimately leading to a more successful user experience.",Agreed-Justified,Agree,,Agree,,
Exp C,Exp K,1282,ESEC/FSE,AI & Machine Learning,Automated Testing and Improvement of Named Entity Recognition Systems,"Named entity recognition (NER) systems have seen rapid progress in recent years due to the development of deep neural networks. These systems are widely used in various natural language processing applications, such as information extraction, question answering, and sentiment analysis. However, the complexity and intractability of deep neural networks can make NER systems unreliable in certain circumstances, resulting in incorrect predictions. For example, NER systems may misidentify female names as chemicals or fail to recognize the names of minority groups, leading to user dissatisfaction. To tackle this problem, we introduce TIN, a novel, widely applicable approach for automatically testing and repairing various NER systems. The key idea for automated testing is that the NER predictions of the same named entities under similar contexts should be identical. The core idea for automated repairing is that similar named entities should have the same NER prediction under the same context. We use TIN to test two SOTA NER models and two commercial NER APIs, i.e., Azure NER and AWS NER. We manually verify 784 of the suspicious issues reported by TIN and find that 702 are erroneous issues, leading to high precision (85.0\%-93.4\%) across four categories of NER errors: omission, over-labeling, incorrect category, and range error. For automated repairing, TIN achieves a high error reduction rate (26.8\%-50.6\%) over the four systems under test, which successfully repairs 1,056 out of the 1,877 reported NER errors.",Stimulation,Excitement in Life,With an improved NER system; made possible by TIN; users will experience excitement in the sense of improved user experience as a result of fewer errors. This is a stimulation of their interaction with the software.,"With the improved NER system introduced in 'Paper X' and the use of TIN for testing and repairing, users will experience excitement in their interaction with the software because they will benefit from a more accurate and reliable system that makes fewer errors. This heightened stimulation is a direct result of the paper's contribution to enhancing the user experience and minimizing frustrations caused by misidentification or failure to recognize certain named entities.",Agreed-Clarified,Agree,,Agree,,
Exp D,Exp K,1293,ESEC/FSE,Security & Privacy,Commit-Level Neural Vulnerability Detection and Assessment,"Software Vulnerabilities (SVs) are security flaws that are exploitable in cyber-attacks. Delay in the detection and assessment of SVs might cause serious consequences due to the unknown impacts on the attacked systems. The state-of-the-art approaches have been proposed to work directly on the committed code changes for early detection. However, none of them could provide both commit-level vulnerability detection and assessment at once. Moreover, the assessment approaches still suffer low accuracy due to limited representations for code changes and surrounding contexts.  

We propose a Context-aware, Graph-based, Commit-level Vulnerability Detection and Assessment Model, VDA, that evaluates a code change, detects any vulnerability and provides the CVSS assessment grades. To build VDA, we have key novel components. First, we design a novel context-aware, graph-based, representation learning model to learn the contextualized embeddings for the code changes that integrate program dependencies and the surrounding contexts of code changes, facilitating the automated vulnerability detection and assessment. Second, VDA considers the mutual impact of learning to detect vulnerability and learning to assess each vulnerability assessment type. To do so, it leverages multi-task learning among the vulnerability detection and vulnerability assessment tasks, improving all the tasks at the same time. Our empirical evaluation shows that on a C vulnerability dataset, VDA achieves 25.5\% and 26.9\% relatively higher than the baselines in vulnerability assessment regarding F-score and MCC, respectively. In a Java dataset, it achieves 31\% and 33.3\% relatively higher than the baselines in F-score and MCC, respectively. VDA also relatively improves the vulnerability detection over the baselines from 13.4aEUR""322\% in F-score.",Security,Healthy,The paper proposes a model for early detection of software vulnerabilities; contributing to the health of the software system. This aligns with the value item Healthy and its corresponding value Security.,"The justification for aligning 'Paper X' with the value item Healthy and its corresponding value Security is that the paper specifically addresses the detection and assessment of software vulnerabilities. By providing a model that can detect vulnerabilities early on, the paper contributes to the security and overall health of the software system. A secure software system is essential for a software user as it ensures the protection of their data, privacy, and overall system stability.",Agreed-Clarified,Agree,,Disagree,Value healthy concerns Human health not software health,
Exp E,Exp J,1293,ESEC/FSE,Security & Privacy,Commit-Level Neural Vulnerability Detection and Assessment,"Software Vulnerabilities (SVs) are security flaws that are exploitable in cyber-attacks. Delay in the detection and assessment of SVs might cause serious consequences due to the unknown impacts on the attacked systems. The state-of-the-art approaches have been proposed to work directly on the committed code changes for early detection. However, none of them could provide both commit-level vulnerability detection and assessment at once. Moreover, the assessment approaches still suffer low accuracy due to limited representations for code changes and surrounding contexts.  

We propose a Context-aware, Graph-based, Commit-level Vulnerability Detection and Assessment Model, VDA, that evaluates a code change, detects any vulnerability and provides the CVSS assessment grades. To build VDA, we have key novel components. First, we design a novel context-aware, graph-based, representation learning model to learn the contextualized embeddings for the code changes that integrate program dependencies and the surrounding contexts of code changes, facilitating the automated vulnerability detection and assessment. Second, VDA considers the mutual impact of learning to detect vulnerability and learning to assess each vulnerability assessment type. To do so, it leverages multi-task learning among the vulnerability detection and vulnerability assessment tasks, improving all the tasks at the same time. Our empirical evaluation shows that on a C vulnerability dataset, VDA achieves 25.5\% and 26.9\% relatively higher than the baselines in vulnerability assessment regarding F-score and MCC, respectively. In a Java dataset, it achieves 31\% and 33.3\% relatively higher than the baselines in F-score and MCC, respectively. VDA also relatively improves the vulnerability detection over the baselines from 13.4aEUR""322\% in F-score.",Security,Social Order,By detecting software vulnerabilities at the code-commit level; the paper contributes to the maintenance of order within the software system. This aligns with the value item Social Order and its corresponding value Security.,"The main contribution of 'Paper X' to the value item Social Order and its corresponding value Security is that it detects software vulnerabilities at the code-commit level. This is significant because vulnerabilities in software systems can disrupt the order and stability of the system, potentially leading to security breaches and negative consequences for the software user. By detecting and addressing these vulnerabilities, 'Paper X' helps to ensure the security and stability of the software system, aligning with the value item Social Order and its corresponding value Security from the perspective of a software user.",Agreed-Justified,Agree,,Agree,,
Exp F,Exp J,1293,ESEC/FSE,Security & Privacy,Commit-Level Neural Vulnerability Detection and Assessment,"Software Vulnerabilities (SVs) are security flaws that are exploitable in cyber-attacks. Delay in the detection and assessment of SVs might cause serious consequences due to the unknown impacts on the attacked systems. The state-of-the-art approaches have been proposed to work directly on the committed code changes for early detection. However, none of them could provide both commit-level vulnerability detection and assessment at once. Moreover, the assessment approaches still suffer low accuracy due to limited representations for code changes and surrounding contexts.  

We propose a Context-aware, Graph-based, Commit-level Vulnerability Detection and Assessment Model, VDA, that evaluates a code change, detects any vulnerability and provides the CVSS assessment grades. To build VDA, we have key novel components. First, we design a novel context-aware, graph-based, representation learning model to learn the contextualized embeddings for the code changes that integrate program dependencies and the surrounding contexts of code changes, facilitating the automated vulnerability detection and assessment. Second, VDA considers the mutual impact of learning to detect vulnerability and learning to assess each vulnerability assessment type. To do so, it leverages multi-task learning among the vulnerability detection and vulnerability assessment tasks, improving all the tasks at the same time. Our empirical evaluation shows that on a C vulnerability dataset, VDA achieves 25.5\% and 26.9\% relatively higher than the baselines in vulnerability assessment regarding F-score and MCC, respectively. In a Java dataset, it achieves 31\% and 33.3\% relatively higher than the baselines in F-score and MCC, respectively. VDA also relatively improves the vulnerability detection over the baselines from 13.4aEUR""322\% in F-score.",Achievement,Successful,The model proposed in the paper helps achieve successful detection and assessment of vulnerabilities; contributing to the successful operation of the software. This aligns with the value item Successful and its corresponding value Achievement.,"In the paper, the proposed model, VDA, aims to provide both commit-level vulnerability detection and assessment, which contributes to the successful operation of the software. By effectively detecting and assessing vulnerabilities, the model enhances the security and reliability of the software, leading to the achievement of successful outcomes for the software user. This aligns with the value item Successful and its corresponding value Achievement as it emphasizes achieving positive and favorable results in the software context.",Agreed-Clarified,Agree,,Agree,,
Exp G,Exp K,1297,ESEC/FSE,Software Testing & QA,Metamong: Detecting Render-Update Bugs in Web Browsers through Fuzzing,"A render-update bug arises when a web browser produces an  
erroneous rendering output due to incorrect rendering updates.  
Such render-update bugs seriously harm the usability and reliability  
of web browsers. However, we find that detecting render-update bugs  
is challenging because the render-update bug is a semantic bug -  
given a rendering result, it is difficult to determine if it is  
correct due to the complex rendering specification of DOM and CSS.  
Thus, unlike memory corruption bugs, the incorrect rendering output  
does not raise the violation or crash. In practice, render-update  
bug detection relies on the time-prohibitive manual analysis of domain  
experts to determine the bug.  

This paper proposes Metamong, an automated framework to detect  
render-update bugs without false positive issues via differential fuzz  
testing. Metamong features two key components: (i) page mutator, and  
(ii) render-update oracle. The page mutator generates render-update  
operations, which change the content of the web page, to trigger a  
render-update bug. The render-update oracle exploits an HTML standard  
rule, so-called yielding, to produce the correct rendering result of  
a given web page. Combining these components, Metamong creates two  
HTML files where each constructs the same web page, but only one of  
them induces the render-update. It then uses differential testing to  
compare their rendering outputs to determine a bug. We implemented  
a prototype of Metamong, which performs differential fuzz testing on  
popular browsers, Chrome and Firefox. By far, Metamong identified 19  
new render-update bugs, 17 in Chrome and two in Firefox. All of those  
have been confirmed by each browser vendor and five are already fixed,  
demonstrating the practical effectiveness of Metamong in identifying  
render-update bugs.",Achievement,Intelligent,The paper proposes Metamong; an intelligent framework that automatically detects render-update bugs. This aligns with the value item Intelligent and its corresponding value Achievement.,"My justification is that the paper's main contribution is the development of Metamong, an intelligent framework that automates the detection of render-update bugs. The use of the term ""intelligent"" suggests that the framework is capable of performing complex tasks and making insightful decisions, which aligns with the value item Intelligent. Furthermore, the development and implementation of such a framework can be seen as achieving a significant milestone in the field of web browser usability and reliability, which aligns with the value of Achievement.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,1297,ESEC/FSE,Software Testing & QA,Metamong: Detecting Render-Update Bugs in Web Browsers through Fuzzing,"A render-update bug arises when a web browser produces an  
erroneous rendering output due to incorrect rendering updates.  
Such render-update bugs seriously harm the usability and reliability  
of web browsers. However, we find that detecting render-update bugs  
is challenging because the render-update bug is a semantic bug -  
given a rendering result, it is difficult to determine if it is  
correct due to the complex rendering specification of DOM and CSS.  
Thus, unlike memory corruption bugs, the incorrect rendering output  
does not raise the violation or crash. In practice, render-update  
bug detection relies on the time-prohibitive manual analysis of domain  
experts to determine the bug.  

This paper proposes Metamong, an automated framework to detect  
render-update bugs without false positive issues via differential fuzz  
testing. Metamong features two key components: (i) page mutator, and  
(ii) render-update oracle. The page mutator generates render-update  
operations, which change the content of the web page, to trigger a  
render-update bug. The render-update oracle exploits an HTML standard  
rule, so-called yielding, to produce the correct rendering result of  
a given web page. Combining these components, Metamong creates two  
HTML files where each constructs the same web page, but only one of  
them induces the render-update. It then uses differential testing to  
compare their rendering outputs to determine a bug. We implemented  
a prototype of Metamong, which performs differential fuzz testing on  
popular browsers, Chrome and Firefox. By far, Metamong identified 19  
new render-update bugs, 17 in Chrome and two in Firefox. All of those  
have been confirmed by each browser vendor and five are already fixed,  
demonstrating the practical effectiveness of Metamong in identifying  
render-update bugs.",Achievement,Successful,By using Metamong; 19 new render-update bugs were successfully identified and confirmed; and five of them are already fixed. This aligns with the value item Successful and its corresponding value Achievement.,"The main contribution of 'Paper X' aligns with the value item Successful and its corresponding value Achievement because the automated framework proposed, Metamong, successfully identified and confirmed 19 new render-update bugs, with five of them already fixed. This demonstrates the achievement of successfully detecting and addressing these bugs, which aligns with the value of achieving success in improving the usability and reliability of web browsers for software users.",Agreed-Justified,Agree,,Agree,,
Exp B,Exp H,1297,ESEC/FSE,Software Testing & QA,Metamong: Detecting Render-Update Bugs in Web Browsers through Fuzzing,"A render-update bug arises when a web browser produces an  
erroneous rendering output due to incorrect rendering updates.  
Such render-update bugs seriously harm the usability and reliability  
of web browsers. However, we find that detecting render-update bugs  
is challenging because the render-update bug is a semantic bug -  
given a rendering result, it is difficult to determine if it is  
correct due to the complex rendering specification of DOM and CSS.  
Thus, unlike memory corruption bugs, the incorrect rendering output  
does not raise the violation or crash. In practice, render-update  
bug detection relies on the time-prohibitive manual analysis of domain  
experts to determine the bug.  

This paper proposes Metamong, an automated framework to detect  
render-update bugs without false positive issues via differential fuzz  
testing. Metamong features two key components: (i) page mutator, and  
(ii) render-update oracle. The page mutator generates render-update  
operations, which change the content of the web page, to trigger a  
render-update bug. The render-update oracle exploits an HTML standard  
rule, so-called yielding, to produce the correct rendering result of  
a given web page. Combining these components, Metamong creates two  
HTML files where each constructs the same web page, but only one of  
them induces the render-update. It then uses differential testing to  
compare their rendering outputs to determine a bug. We implemented  
a prototype of Metamong, which performs differential fuzz testing on  
popular browsers, Chrome and Firefox. By far, Metamong identified 19  
new render-update bugs, 17 in Chrome and two in Firefox. All of those  
have been confirmed by each browser vendor and five are already fixed,  
demonstrating the practical effectiveness of Metamong in identifying  
render-update bugs.",Security,Healthy,By identifying and correcting render-update bugs; Metamong contributes to the improved reliability (health) of web browser software. This aligns with the value item Healthy and its corresponding value Security.,"By detecting and resolving render-update bugs, Metamong ensures that the web browser software functions correctly and reliably. This directly aligns with the value item Healthy, as it contributes to the overall health and well-being of the software by eliminating detrimental bugs that may harm the usability and reliability of the browser. Additionally, by improving the reliability of the software, it enhances the sense of Security for the software user, who can trust that their browsing experience will be stable and free from errors.",Agreed-Justified,Agree,,Agree,,
Exp C,Exp K,1308,ESEC/FSE,Code Generation & Analysis,Program Repair Guided by Datalog-Defined Static Analysis,"Automated program repair relying on static analysis complements test-driven repair, since it does not require failing tests to repair a bug, and it avoids test-overfitting by considering program properties.  
Due to the rich variety and complexity of program analyses, existing static program repair techniques are tied to specific analysers, and thus repair only narrow classes of defects. To develop a general-purpose static program repair framework that targets a wide range of properties and programming languages, we propose to integrate program repair with Datalog-based analysis. Datalog solvers are programmable fixed point engines which can be used to encode many program analysis problems in a modular fashion. The program under analysis is encoded as Datalog facts, while the fixed point equations of the program analysis are expressed as recursive Datalog rules. In this context, we view repairing the program as modifying the corresponding Datalog facts. This is accomplished by a novel technique, symbolic execution of Datalog, that evaluates Datalog queries over a symbolic database of facts, instead of a concrete set of facts. The result of symbolic query evaluation allows us to infer what changes to a given set of Datalog facts repair the program so that it meets the desired analysis goals. We developed a symbolic executor for Datalog called Symlog, on top of which we built a repair tool SymlogRepair. We show the versatility of our  
approach on several analysis problems --- repairing null pointer exceptions in Java programs, repairing data leaks in Python notebooks, and repairing four types of security vulnerabilities in Solidity smart contracts.",Power,Social Recognition,The 'Paper X' proposes a general-purpose static program repair framework that supports a wide range of properties and programming languages; which helps the software to function better and with less bugs; indirectly resulting in greater social recognition of software user‚Äö√Ñ√∂‚àö√ë‚àö¬•s proficiency in using the software.,"The justification for the alignment of 'Paper X' with the value item Social Recognition and its corresponding value Power is based on the fact that the proposed general-purpose static program repair framework improves the functionality and reduces the number of bugs in software. This, in turn, enhances the competency and proficiency of software users, leading to greater social recognition for their ability to effectively utilize the software.",Agreed-Justified,Agree,,Agree,,
Exp D,Exp K,1308,ESEC/FSE,Code Generation & Analysis,Program Repair Guided by Datalog-Defined Static Analysis,"Automated program repair relying on static analysis complements test-driven repair, since it does not require failing tests to repair a bug, and it avoids test-overfitting by considering program properties.  
Due to the rich variety and complexity of program analyses, existing static program repair techniques are tied to specific analysers, and thus repair only narrow classes of defects. To develop a general-purpose static program repair framework that targets a wide range of properties and programming languages, we propose to integrate program repair with Datalog-based analysis. Datalog solvers are programmable fixed point engines which can be used to encode many program analysis problems in a modular fashion. The program under analysis is encoded as Datalog facts, while the fixed point equations of the program analysis are expressed as recursive Datalog rules. In this context, we view repairing the program as modifying the corresponding Datalog facts. This is accomplished by a novel technique, symbolic execution of Datalog, that evaluates Datalog queries over a symbolic database of facts, instead of a concrete set of facts. The result of symbolic query evaluation allows us to infer what changes to a given set of Datalog facts repair the program so that it meets the desired analysis goals. We developed a symbolic executor for Datalog called Symlog, on top of which we built a repair tool SymlogRepair. We show the versatility of our  
approach on several analysis problems --- repairing null pointer exceptions in Java programs, repairing data leaks in Python notebooks, and repairing four types of security vulnerabilities in Solidity smart contracts.",Security,Reciprocation of Favors,The paper contributes to the value of Security through the reciprocal behavior of users; by proposing a method to repair data leaks in Python notebooks; which could instill an unwritten obligation among users to reciprocate by maintaining a secure software environment.,"I apologize for the confusion. The justification for aligning 'Paper X' with the value item Reciprocation of Favors and its corresponding value Security from a ""Software User"" perspective is based on the premise that the proposed method to repair data leaks in Python notebooks contributes to creating a more secure software environment. By repairing the data leaks, users are given the opportunity to reciprocate by taking appropriate actions to maintain the security and integrity of their software. This aligns with the value of Security as it emphasizes the importance of reciprocal behavior in safeguarding the software environment.",Agreed-Clarified,Agree,,Agree,,
Exp E,Exp J,1308,ESEC/FSE,Code Generation & Analysis,Program Repair Guided by Datalog-Defined Static Analysis,"Automated program repair relying on static analysis complements test-driven repair, since it does not require failing tests to repair a bug, and it avoids test-overfitting by considering program properties.  
Due to the rich variety and complexity of program analyses, existing static program repair techniques are tied to specific analysers, and thus repair only narrow classes of defects. To develop a general-purpose static program repair framework that targets a wide range of properties and programming languages, we propose to integrate program repair with Datalog-based analysis. Datalog solvers are programmable fixed point engines which can be used to encode many program analysis problems in a modular fashion. The program under analysis is encoded as Datalog facts, while the fixed point equations of the program analysis are expressed as recursive Datalog rules. In this context, we view repairing the program as modifying the corresponding Datalog facts. This is accomplished by a novel technique, symbolic execution of Datalog, that evaluates Datalog queries over a symbolic database of facts, instead of a concrete set of facts. The result of symbolic query evaluation allows us to infer what changes to a given set of Datalog facts repair the program so that it meets the desired analysis goals. We developed a symbolic executor for Datalog called Symlog, on top of which we built a repair tool SymlogRepair. We show the versatility of our  
approach on several analysis problems --- repairing null pointer exceptions in Java programs, repairing data leaks in Python notebooks, and repairing four types of security vulnerabilities in Solidity smart contracts.",Security,Sense of Belonging,The proposal of a method to repair null pointer exceptions in Java programs; purely assists the software user's interacting with the software by fostering a sense of belonging and improving community relationships as the abstract specifically mentioned about repairing bugs which ultimately leads to more secure; reliable and bug-free software usage.,"To further clarify my justification, the alignment of 'Paper X' with the value item Sense of Belonging and its corresponding value Security is based on the fact that the proposed method of repairing null pointer exceptions in Java programs directly contributes to improving the overall user experience by enhancing the software's reliability and reducing potential bugs. By addressing these issues, the software user will have a greater sense of belonging and trust in the software, leading to increased satisfaction and a stronger sense of security in using the application.",Agreed-Clarified,Agree,,Agree,,
Exp F,Exp J,1311,ESEC/FSE,Accessibility & User Experience,Automated and Context-Aware Repair of Color-Related Accessibility Issues for Android Apps,"Approximately 15\% of the world's population is suffering from various disabilities or impairments. However, many mobile UX designers and developers disregard the significance of accessibility for those with disabilities when developing apps. It is unbelievable that one in seven people might not have the same level of access that other users have, which actually violates many legal and regulatory standards. On the contrary, if the apps are developed with accessibility in mind, it will drastically improve the user experience for all users as well as maximize revenue. Thus, a large number of studies and some effective tools for detecting accessibility issues have been conducted and proposed to mitigate such a severe problem.  
However, compared with detection, the repair work is obviously falling behind. Especially for the color-related accessibility issues, which is one of the top issues in apps with a greatly negative impact on vision and user experience. Apps with such issues are difficult to use for people with low vision and the elderly. Unfortunately, such an issue type cannot be directly fixed by existing repair techniques. To this end, we propose Iris, an automated and context-aware repair method to fix the color-related accessibility issues (i.e., the text contrast issues and the image contrast issues) for apps. By leveraging a novel context-aware technique that resolves the optimal colors and a vital phase of attribute-to-repair localization, Iris not only repairs the color contrast issues but also guarantees the consistency of the design style between the original UI page and repaired UI page. Our experiments unveiled that Iris can achieve a 91.38\% repair success rate with high effectiveness and efficiency. The usefulness of Iris has also been evaluated by a user study with a high satisfaction rate as well as developers' positive feedback. 9 of 40 submitted pull requests on GitHub repositories have been accepted and merged into the projects by app developers, and another 4 developers are actively discussing with us for further repair. Iris is publicly available to facilitate this new research direction.",Security,Healthy,The paper proposes a repair method to fix the color-related accessibility issues for apps; directly aim to improve the user experience for all; especially the people with low vision and the elderly. This aligns with the value item Healthy and its corresponding value Security.,"The justification for aligning 'Paper X' with the value item Healthy and its corresponding value Security is that the proposed repair method directly aims to improve the user experience for all users, including those with low vision and the elderly. By fixing color-related accessibility issues, the app becomes easier to use for people with low vision and promotes a healthier and more secure user experience overall.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,1311,ESEC/FSE,Accessibility & User Experience,Automated and Context-Aware Repair of Color-Related Accessibility Issues for Android Apps,"Approximately 15\% of the world's population is suffering from various disabilities or impairments. However, many mobile UX designers and developers disregard the significance of accessibility for those with disabilities when developing apps. It is unbelievable that one in seven people might not have the same level of access that other users have, which actually violates many legal and regulatory standards. On the contrary, if the apps are developed with accessibility in mind, it will drastically improve the user experience for all users as well as maximize revenue. Thus, a large number of studies and some effective tools for detecting accessibility issues have been conducted and proposed to mitigate such a severe problem.  
However, compared with detection, the repair work is obviously falling behind. Especially for the color-related accessibility issues, which is one of the top issues in apps with a greatly negative impact on vision and user experience. Apps with such issues are difficult to use for people with low vision and the elderly. Unfortunately, such an issue type cannot be directly fixed by existing repair techniques. To this end, we propose Iris, an automated and context-aware repair method to fix the color-related accessibility issues (i.e., the text contrast issues and the image contrast issues) for apps. By leveraging a novel context-aware technique that resolves the optimal colors and a vital phase of attribute-to-repair localization, Iris not only repairs the color contrast issues but also guarantees the consistency of the design style between the original UI page and repaired UI page. Our experiments unveiled that Iris can achieve a 91.38\% repair success rate with high effectiveness and efficiency. The usefulness of Iris has also been evaluated by a user study with a high satisfaction rate as well as developers' positive feedback. 9 of 40 submitted pull requests on GitHub repositories have been accepted and merged into the projects by app developers, and another 4 developers are actively discussing with us for further repair. Iris is publicly available to facilitate this new research direction.",Stimulation,Variation in Life,The introduction of Iris increases the Variation in Life for people with vision impairments by improving their access and usability of mobile apps. This aligns with the value Stimulation.,"The introduction of Iris, as stated in the abstract, aims to repair color-related accessibility issues in mobile apps, specifically those impacting vision and user experience. By enhancing access and usability for people with vision impairments, Iris contributes to increasing the variation in life for these individuals within the software context. This aligns with the value item Variation in Life and its corresponding value Stimulation, as it provides an opportunity for a more diverse and enriched user experience, stimulating engagement and interaction with mobile apps for individuals with disabilities.",Agreed-Clarified,Agree,,Agree,,
Exp B,Exp H,1313,ESEC/FSE,Software Engineering Practices,aEURoeWe Feel Like WeaEUR(tm)re Winging It:aEUR A Study on Navigating Open-Source Dependency Abandonment,"While lots of research has explored how to prevent maintainers from abandoning the open-source projects that serve as our digital infras- tructure, there are very few insights on addressing abandonment when it occurs. We argue open-source sustainability research must expand its focus beyond trying to keep particular projects alive, to also cover the sustainable use of open source by supporting users when they face potential or actual abandonment. We interviewed 33 developers who have experienced open-source dependency aban- donment. Often, they used multiple strategies to cope with aban- donment, for example, first reaching out to the community to find potential alternatives, then switching to a community-accepted alternative if one exists. We found many developers felt they had little to no support or guidance when facing abandonment, leaving them to figure out what to do through a trial-and-error process on their own. Abandonment introduces cost for otherwise seem- ingly free dependencies, but users can decide whether and how to prepare for abandonment through a number of different strategies, such as dependency monitoring, building abstraction layers, and community involvement. In many cases, community members can invest in resources that help others facing the same abandoned dependency, but often do not because of the many other competing demands on their time aEUR"" a form of the volunteeraEUR(tm)s dilemma. We dis- cuss cost reduction strategies and ideas to overcome this volunteeraEUR(tm)s dilemma. Our findings can be used directly by open-source users seeking resources on dealing with dependency abandonment, or by researchers to motivate future work supporting the sustainable use of open source.",Security,Healthy,The paper contributes to strategies for coping with abandonment in open-source projects; which promotes the users' security and healthy maintenance of their software environment.,"In the context of a software user, the alignment of 'Paper X' with the value item Healthy and its corresponding value Security is justified by the fact that the paper addresses strategies for coping with abandonment in open-source projects. Abandonment of software dependencies can introduce risks and vulnerabilities to a software system, potentially compromising the security and stability of the user's software environment. By providing insights on how to effectively cope with abandonment, the paper contributes to the user's ability to maintain a healthy software environment by minimizing the potential negative impacts and ensuring the security of their system.",Agreed-Clarified,Disagree,reconcile with coder_2,Disagree,"this justification is not supported by the evidence in the abstract, which focuses on 'While lots of research has explored how to prevent maintainers from abandoning the open-source project.",
Exp C,Exp K,1313,ESEC/FSE,Software Engineering Practices,aEURoeWe Feel Like WeaEUR(tm)re Winging It:aEUR A Study on Navigating Open-Source Dependency Abandonment,"While lots of research has explored how to prevent maintainers from abandoning the open-source projects that serve as our digital infras- tructure, there are very few insights on addressing abandonment when it occurs. We argue open-source sustainability research must expand its focus beyond trying to keep particular projects alive, to also cover the sustainable use of open source by supporting users when they face potential or actual abandonment. We interviewed 33 developers who have experienced open-source dependency aban- donment. Often, they used multiple strategies to cope with aban- donment, for example, first reaching out to the community to find potential alternatives, then switching to a community-accepted alternative if one exists. We found many developers felt they had little to no support or guidance when facing abandonment, leaving them to figure out what to do through a trial-and-error process on their own. Abandonment introduces cost for otherwise seem- ingly free dependencies, but users can decide whether and how to prepare for abandonment through a number of different strategies, such as dependency monitoring, building abstraction layers, and community involvement. In many cases, community members can invest in resources that help others facing the same abandoned dependency, but often do not because of the many other competing demands on their time aEUR"" a form of the volunteeraEUR(tm)s dilemma. We dis- cuss cost reduction strategies and ideas to overcome this volunteeraEUR(tm)s dilemma. Our findings can be used directly by open-source users seeking resources on dealing with dependency abandonment, or by researchers to motivate future work supporting the sustainable use of open source.",Security,Family Security,The paper discusses measures for dealing with project abandonment; thereby contributing to the safety and security of the software users and their projects.,"In the abstract of 'Paper X', it is mentioned that users of open-source projects often face potential or actual abandonment, and they have little to no support or guidance in such situations. This lack of support can lead to uncertainty and insecurity for software users who rely on these projects. By addressing abandonment and providing strategies for coping with it, the paper contributes to the security and safety of software users' projects.",Agreed-Clarified,Agree,,Agree,,
Exp E,Exp J,1317,ESEC/FSE,Emerging Technologies,EtherDiffer: Differential Testing on RPC Services of Ethereum Nodes,"Blockchain is a distributed ledger that records transactions among users  
on top of a peer-to-peer network. Among all, Ethereum is the most popular  
general-purpose platform and its support of smart contracts led to a new  
form of applications called decentralized applications (DApps). A typical  
DApp has an off-chain frontend and on-chain backend architecture, and the  
frontend often needs interactions with the backend network, e.g., to  
acquire chain data or make transactions. Therefore, Ethereum nodes implement  
the official RPC specification and expose a uniform set of RPC methods to  
the frontend. However, the specification is not sufficient in two points:  
(1) lack of clarification for non-deterministic event handling, and  
(2) lack of specification for invalid arguments. To effectively disclose  
any deviations caused by the insufficiency, this paper introduces  
EtherDiffer that automatically performs differential testing on four major  
node implementations in terms of their RPC services. EtherDiffer first  
generates a non-deterministic chain by multi-concurrent transactions and  
propagation delay. Then, it applies our key techniques called  
property-based generation and type-preserving mutation to generate  
both semantically-valid and semantically-invalid-yet-executable test  
cases. EtherDiffer executes the test cases on target nodes and reports  
any deviations in error handling or return values. The evaluation showed  
the effectiveness of our test case generation techniques with the success  
ratios of 98.8\% and 95.4\%, respectively. Also, EtherDiffer detected  
48 different classes of deviations including 11 implementation bugs  
such as crash and denial-of-service bugs. We reported 44 of the detected  
classes to the specification and node developers and received  
acknowledgements as well as bug patches. Lastly, it significantly  
outperformed the official node testing tool in every technical aspect.  
We believe that our research findings can contribute to more stable  
DApp ecosystem by reducing the inconsistencies among nodes.",Achievement,Intelligent,The introduction of the EtherDiffer tool in the paper contributes to a potential 'Software User' becoming more intelligent in handling Ethereum transactions and managing decentralized applications (DApp) due to automated differential testing and detection of deviations on the four major node implementations of their RPC services.,"The alignment of 'Paper X' with the value item Intelligent and its corresponding value Achievement from a ""Software User"" perspective is justified by the fact that the introduction of EtherDiffer tool enhances the user's intelligence in handling Ethereum transactions and managing DApps. This is achieved through automated differential testing that detects deviations in the major node implementations of their RPC services. By identifying and addressing potential bugs or inconsistencies, the tool contributes to a more reliable and stable DApp ecosystem, allowing users to make informed decisions and achieve successful outcomes when interacting with the blockchain.",Agreed-Clarified,Agree,,Agree,,
Exp F,Exp J,1317,ESEC/FSE,Emerging Technologies,EtherDiffer: Differential Testing on RPC Services of Ethereum Nodes,"Blockchain is a distributed ledger that records transactions among users  
on top of a peer-to-peer network. Among all, Ethereum is the most popular  
general-purpose platform and its support of smart contracts led to a new  
form of applications called decentralized applications (DApps). A typical  
DApp has an off-chain frontend and on-chain backend architecture, and the  
frontend often needs interactions with the backend network, e.g., to  
acquire chain data or make transactions. Therefore, Ethereum nodes implement  
the official RPC specification and expose a uniform set of RPC methods to  
the frontend. However, the specification is not sufficient in two points:  
(1) lack of clarification for non-deterministic event handling, and  
(2) lack of specification for invalid arguments. To effectively disclose  
any deviations caused by the insufficiency, this paper introduces  
EtherDiffer that automatically performs differential testing on four major  
node implementations in terms of their RPC services. EtherDiffer first  
generates a non-deterministic chain by multi-concurrent transactions and  
propagation delay. Then, it applies our key techniques called  
property-based generation and type-preserving mutation to generate  
both semantically-valid and semantically-invalid-yet-executable test  
cases. EtherDiffer executes the test cases on target nodes and reports  
any deviations in error handling or return values. The evaluation showed  
the effectiveness of our test case generation techniques with the success  
ratios of 98.8\% and 95.4\%, respectively. Also, EtherDiffer detected  
48 different classes of deviations including 11 implementation bugs  
such as crash and denial-of-service bugs. We reported 44 of the detected  
classes to the specification and node developers and received  
acknowledgements as well as bug patches. Lastly, it significantly  
outperformed the official node testing tool in every technical aspect.  
We believe that our research findings can contribute to more stable  
DApp ecosystem by reducing the inconsistencies among nodes.",Achievement,Capable,The EtherDiffer tool enhances the capability of 'Software User' as it can automatically detect any deviations and implementation bugs like crash and denial-of-service bugs. This tool is effectively designed to increase the capability of the user to handle transactions and manage decentralized applications (DApps).,"The EtherDiffer tool aligns with the value item Capable and its corresponding value Achievement from a ""Software User"" perspective as it enhances the user's capability to handle transactions and manage DApps by automatically detecting deviations and implementation bugs. This capability empowers the user to effectively navigate and interact with the blockchain network, ensuring the stability and smooth functioning of their applications.",Agreed-Clarified,Agree,,Agree,,
Exp A,Exp H,1318,ESEC/FSE,AI & Machine Learning,Dynamic Data Fault Localization for Deep Neural Networks,"Rich datasets have empowered various deep learning (DL) applications, leading to remarkable success in many fields.  
However, data faults hidden in the datasets could result in DL applications behaving unpredictably and even cause massive monetary and life losses.  
To alleviate this problem, in this paper, we propose a dynamic data fault localization approach, namely DFauLo, to locate the mislabeled and noisy data in the deep learning datasets.  
DFauLo is inspired by the conventional mutation-based code fault localization, but utilizes the differences between DNN mutants to amplify and identify the potential data faults.  
Specifically, it first generates multiple DNN model mutants of the original trained model. Then it extracts features from these mutants and maps them into a suspiciousness score indicating the probability of the given data being a data fault.  
Moreover, DFauLo is the first dynamic data fault localization technique, prioritizing the suspected data based on user feedback, and providing the generalizability to unseen data faults during training.  
To validate DFauLo, we extensively evaluate it on 26 cases with various fault types, data types, and model structures.  
We also evaluate DFauLo on three widely-used benchmark datasets.  
The results show that DFauLo outperforms the state-of-the-art techniques in almost all cases and locates hundreds of different types of real data faults in benchmark datasets.",Stimulation,Excitement in Life,The paper proposes a dynamic data fault localization approach; namely DFauLo; to locate the mislabeled and noisy data in the deep learning datasets; leading to an excitement in life for software users due to its potential to create more accurate and reliable applications.,"In the context of a software user, the alignment between the main contributions of 'Paper X' and the value item Excitement in Life and its corresponding value Stimulation is justified by the fact that the proposed dynamic data fault localization approach, DFauLo, has the potential to enhance the accuracy and reliability of deep learning applications. As a result, software users can experience excitement in their interactions with these applications, knowing that they will perform predictably and effectively. This aligns with the value of Stimulation, as it brings novelty, interest, and excitement to the software user's life.",Agreed-Justified,Agree,,Agree,,
Exp B,Exp H,1318,ESEC/FSE,AI & Machine Learning,Dynamic Data Fault Localization for Deep Neural Networks,"Rich datasets have empowered various deep learning (DL) applications, leading to remarkable success in many fields.  
However, data faults hidden in the datasets could result in DL applications behaving unpredictably and even cause massive monetary and life losses.  
To alleviate this problem, in this paper, we propose a dynamic data fault localization approach, namely DFauLo, to locate the mislabeled and noisy data in the deep learning datasets.  
DFauLo is inspired by the conventional mutation-based code fault localization, but utilizes the differences between DNN mutants to amplify and identify the potential data faults.  
Specifically, it first generates multiple DNN model mutants of the original trained model. Then it extracts features from these mutants and maps them into a suspiciousness score indicating the probability of the given data being a data fault.  
Moreover, DFauLo is the first dynamic data fault localization technique, prioritizing the suspected data based on user feedback, and providing the generalizability to unseen data faults during training.  
To validate DFauLo, we extensively evaluate it on 26 cases with various fault types, data types, and model structures.  
We also evaluate DFauLo on three widely-used benchmark datasets.  
The results show that DFauLo outperforms the state-of-the-art techniques in almost all cases and locates hundreds of different types of real data faults in benchmark datasets.",Security,Healthy,By addressing the issue of faults in deep learning datasets; the paper aligns with the value item of Healthiness as it enhances the reliability of applications developed using these data sets; thereby reducing risks to software users.,"The justification for aligning 'Paper X' with the value item Healthy and its corresponding value Security is that by locating and addressing faults in deep learning datasets, the paper enhances the reliability and trustworthiness of the applications developed using these datasets. This directly contributes to the security and well-being of software users by reducing the potential risks and negative impacts that could arise from unpredictable and faulty behavior of deep learning applications.",Agreed-Justified,Disagree,reconcile with coder_2,Disagree,"The justification claiming that the paper aligns with the value 'Security' because of 'By addressing the issue of faults in deep learning datasets; the paper aligns with the value item of Healthiness as it enhances the reliability of applications developed using these data sets; thereby reducing risks to software users. ' is not supported by the evidence in the abstract, which focuses on 'Rich datasets have empowered various deep learning (DL) applications, leading to remarkable success.",
Exp E,Exp J,1319,ESEC/FSE,AI & Machine Learning,Understanding the Bug Characteristics and Fix Strategies of Federated Learning Systems,"Federated learning (FL) is an emerging machine learning paradigm that aims to address the problem of isolated data islands. To preserve privacy, FL allows machine learning models and deep neural networks to be trained from decentralized data kept privately at individual devices. FL has been increasingly adopted in missioncritical fields such as finance and healthcare. However, bugs in FL systems are inevitable and may result in catastrophic consequences such as financial loss, inappropriate medical decision, and violation of data privacy ordinance. While many recent studies were conducted to understand the bugs in machine learning systems, there is no existing study to characterize the bugs arising from the unique nature of FL systems. To fill the gap, we collected 395 real bugs from six popular FL frameworks (Tensorflow Federated, PySyft, FATE, Flower, PaddleFL, and Fedlearner) in GitHub and StackOverflow, and then manually analyzed their symptoms and impacts, prone stages, root causes, and fix strategies. Furthermore, we report a series of findings and actionable implications that can potentially facilitate the detection of FL bugs.",Self Direction,Privacy,The focus on analyzing bugs in machine learning models trained from decentralized data which are kept privately at individual devices directly aligns with the value item Privacy; which is part of Self Direction value; the user data remain private while improving the performance and functionality of the software.,"The main contribution of 'Paper X' is aligned with the value item of Privacy and its corresponding value of Self Direction because the paper focuses on bugs in machine learning models trained from decentralized data kept privately at individual devices. This directly addresses the importance of privacy as it ensures that user data remains private while still improving the performance and functionality of the software. By highlighting and analyzing the bugs and their implications, the paper acknowledges the need to safeguard user data and aligns with the value of Self Direction by allowing individuals to maintain control over their private information.",Agreed-Justified,Agree,,Agree,,
Exp F,Exp J,1319,ESEC/FSE,AI & Machine Learning,Understanding the Bug Characteristics and Fix Strategies of Federated Learning Systems,"Federated learning (FL) is an emerging machine learning paradigm that aims to address the problem of isolated data islands. To preserve privacy, FL allows machine learning models and deep neural networks to be trained from decentralized data kept privately at individual devices. FL has been increasingly adopted in missioncritical fields such as finance and healthcare. However, bugs in FL systems are inevitable and may result in catastrophic consequences such as financial loss, inappropriate medical decision, and violation of data privacy ordinance. While many recent studies were conducted to understand the bugs in machine learning systems, there is no existing study to characterize the bugs arising from the unique nature of FL systems. To fill the gap, we collected 395 real bugs from six popular FL frameworks (Tensorflow Federated, PySyft, FATE, Flower, PaddleFL, and Fedlearner) in GitHub and StackOverflow, and then manually analyzed their symptoms and impacts, prone stages, root causes, and fix strategies. Furthermore, we report a series of findings and actionable implications that can potentially facilitate the detection of FL bugs.",Power,Preserving My Public Image,The whole process of debugging in federated learning systems contributes to Preserving My Public Image by ensuring the systems do not contain bugs that would cause financial loss; inappropriate medical decisions; or violate data privacy ordinances; there by ensuring the user's public image isn‚Äö√Ñ√∂‚àö√ë‚àö¬•t damaged due to these errors.,"In the context of the value item ""Preserving My Public Image"" and its corresponding value Power from a ""Software User"" perspective, the main contribution of 'Paper X' is aligned with this value by addressing the issue of bugs in federated learning systems. By conducting a thorough analysis of real bugs in popular federated learning frameworks, the paper aims to improve the detection of these bugs and ultimately prevent catastrophic consequences such as financial loss and violation of data privacy. This directly aligns with the value of Power because it empowers the software user to maintain a positive public image by avoiding the negative impacts associated with buggy federated learning systems.",Agreed-Justified,Agree,,Agree,,
Exp G,Exp K,1334,ESEC/FSE,AI & Machine Learning,Evaluating Transfer Learning for Simplifying GitHub READMEs,"Software documentation captures detailed knowledge about a software product, e.g., code, technologies, and design. It plays an important role in the coordination of development teams and in conveying ideas to various stakeholders. However, software documentation can be hard to comprehend if it is written with jargon and complicated sentence structure. In this study, we explored the potential of text simplification techniques in the domain of software engineering to automatically simplify GitHub README files. We collected software-related pairs of GitHub README files consisting of 14,588 entries, aligned difficult sentences with their simplified counterparts, and trained a Transformer-based model to automatically simplify difficult versions. To mitigate the sparse and noisy nature of the software-related simplification dataset, we applied general text simplification knowledge to this field. Since many general-domain difficult-to-simple Wikipedia document pairs are already publicly available, we explored the potential of transfer learning by first training the model on the Wikipedia data and then fine-tuning it on the README data. Using automated BLEU scores and human evaluation, we compared the performance of different transfer learning schemes and the baseline models without transfer learning. The transfer learning model using the best checkpoint trained on a general topic corpus achieved the best performance of 34.68 BLEU score and statistically significantly higher human annotation scores compared to the rest of the schemes and baselines. We conclude that using transfer learning is a promising direction to circumvent the lack of data and drift style problem in software README files simplification and achieved a better trade-off between simplification and preservation of meaning.",Self Direction,Creativity,The paper explores new techniques for simplifying GitHub README files; which ultimately supports the value of Creativity under the value category Self-direction. The development of an automated simplification model is an innovative approach in itself.,"The justification for aligning 'Paper X' with the value item Creativity and its corresponding value Self Direction is based on the paper's exploration of new techniques for simplifying GitHub README files. By developing an automated simplification model, the paper demonstrates innovation and the ability to think independently, which aligns with the value of Creativity under the value category Self-direction. This approach allows software users to have more freedom and independence in comprehending software documentation, thereby promoting their own self-direction.",Agreed-Clarified,Agree,,Agree,,
Exp A,Exp H,1334,ESEC/FSE,AI & Machine Learning,Evaluating Transfer Learning for Simplifying GitHub READMEs,"Software documentation captures detailed knowledge about a software product, e.g., code, technologies, and design. It plays an important role in the coordination of development teams and in conveying ideas to various stakeholders. However, software documentation can be hard to comprehend if it is written with jargon and complicated sentence structure. In this study, we explored the potential of text simplification techniques in the domain of software engineering to automatically simplify GitHub README files. We collected software-related pairs of GitHub README files consisting of 14,588 entries, aligned difficult sentences with their simplified counterparts, and trained a Transformer-based model to automatically simplify difficult versions. To mitigate the sparse and noisy nature of the software-related simplification dataset, we applied general text simplification knowledge to this field. Since many general-domain difficult-to-simple Wikipedia document pairs are already publicly available, we explored the potential of transfer learning by first training the model on the Wikipedia data and then fine-tuning it on the README data. Using automated BLEU scores and human evaluation, we compared the performance of different transfer learning schemes and the baseline models without transfer learning. The transfer learning model using the best checkpoint trained on a general topic corpus achieved the best performance of 34.68 BLEU score and statistically significantly higher human annotation scores compared to the rest of the schemes and baselines. We conclude that using transfer learning is a promising direction to circumvent the lack of data and drift style problem in software README files simplification and achieved a better trade-off between simplification and preservation of meaning.",Hedonism,Enjoying Life,The paper commits to improving the software users' ability to comprehend the complex documentation by simplifying the text; promoting the Importance of Enjoying Life under the Hedonism value category. Easier and simplified language can make interaction with the software documentation more enjoyable.,"In 'Paper X', the main contribution is focused on simplifying software documentation to improve comprehension for software users. This aligns with the value item Enjoying Life and its corresponding value Hedonism from Schwartz's Taxonomy. By simplifying the text, the paper aims to make the interaction with software documentation more enjoyable and less burdensome for users, thus promoting the idea of enjoying life while using software.",Agreed-Justified,Agree,,Agree,,
Exp B,Exp H,1334,ESEC/FSE,AI & Machine Learning,Evaluating Transfer Learning for Simplifying GitHub READMEs,"Software documentation captures detailed knowledge about a software product, e.g., code, technologies, and design. It plays an important role in the coordination of development teams and in conveying ideas to various stakeholders. However, software documentation can be hard to comprehend if it is written with jargon and complicated sentence structure. In this study, we explored the potential of text simplification techniques in the domain of software engineering to automatically simplify GitHub README files. We collected software-related pairs of GitHub README files consisting of 14,588 entries, aligned difficult sentences with their simplified counterparts, and trained a Transformer-based model to automatically simplify difficult versions. To mitigate the sparse and noisy nature of the software-related simplification dataset, we applied general text simplification knowledge to this field. Since many general-domain difficult-to-simple Wikipedia document pairs are already publicly available, we explored the potential of transfer learning by first training the model on the Wikipedia data and then fine-tuning it on the README data. Using automated BLEU scores and human evaluation, we compared the performance of different transfer learning schemes and the baseline models without transfer learning. The transfer learning model using the best checkpoint trained on a general topic corpus achieved the best performance of 34.68 BLEU score and statistically significantly higher human annotation scores compared to the rest of the schemes and baselines. We conclude that using transfer learning is a promising direction to circumvent the lack of data and drift style problem in software README files simplification and achieved a better trade-off between simplification and preservation of meaning.",Security,Healthy,The exploration of text simplification techniques can potentially improve the user's understanding of the software; aligning with the value of Health contained in the Security value category. By reducing the cognitive burden related to understanding complex software documentation; users will maintain better mental health.,"The justification for aligning the main contribution of 'Paper X' with the value item Healthy and its corresponding value Security from a ""Software User"" perspective is that by simplifying complex software documentation, users can have a better understanding of the software, leading to reduced cognitive burden and ultimately better mental health. This alignment is based on the assumption that improved understanding of the software through simplified documentation can contribute to a sense of security and well-being for the users.",Agreed-Justified,Agree,,Agree,,
Exp C,Exp K,1335,ESEC/FSE,Security & Privacy,CodeMark: Imperceptible Watermarking for Code Datasets against Neural Code Completion Models,"Code datasets are of immense value for training neural-network-based code completion models, where companies or organizations have made substantial investments to establish and process these datasets.  
Unluckily, these datasets, either built for proprietary or public usage, face the high risk of unauthorized exploits, resulting from data leakages, license violations, etc.  
Even worse, the ""black-box"" nature of neural models sets a high barrier for externals to audit their training datasets, which further connives these unauthorized usages.  
Currently, watermarking methods have been proposed to prohibit inappropriate usage of image and natural language datasets.  
However, due to domain specificity, they are not directly applicable to code datasets, leaving the copyright protection of this emerging and important field of code data still exposed to threats.  
To fill this gap, we propose a method, named CodeMark, to embed user-defined imperceptible watermarks into code datasets to trace their usage in training neural code completion models.  
CodeMark is based on adaptive semantic-preserving transformations, which preserve the exact functionality of the code data and keep the changes covert against rule-breakers.  
We implement CodeMark in a toolkit and conduct an extensive evaluation of code completion models.  
CodeMark is validated to fulfill all desired properties of practical watermarks, including  
harmlessness to model accuracy, verifiability, robustness, and imperceptibility.",Security,Social Order,The paper contributes a method; CodeMark; to prevent unauthorized usage of code datasets; which aligns with the value item Social Order and its corresponding value Security.,"The main contribution of 'Paper X', CodeMark, aligns with the value item Social Order and its corresponding value Security from a ""Software User"" perspective because it introduces a method to embed imperceptible watermarks into code datasets, aiming to trace their usage in training neural code completion models. This method helps to maintain order and security by preventing unauthorized usage of code datasets, which ultimately promotes a sense of security and stability within the software context.",Agreed-Justified,Agree,,Agree,,
Exp D,Exp K,1335,ESEC/FSE,Security & Privacy,CodeMark: Imperceptible Watermarking for Code Datasets against Neural Code Completion Models,"Code datasets are of immense value for training neural-network-based code completion models, where companies or organizations have made substantial investments to establish and process these datasets.  
Unluckily, these datasets, either built for proprietary or public usage, face the high risk of unauthorized exploits, resulting from data leakages, license violations, etc.  
Even worse, the ""black-box"" nature of neural models sets a high barrier for externals to audit their training datasets, which further connives these unauthorized usages.  
Currently, watermarking methods have been proposed to prohibit inappropriate usage of image and natural language datasets.  
However, due to domain specificity, they are not directly applicable to code datasets, leaving the copyright protection of this emerging and important field of code data still exposed to threats.  
To fill this gap, we propose a method, named CodeMark, to embed user-defined imperceptible watermarks into code datasets to trace their usage in training neural code completion models.  
CodeMark is based on adaptive semantic-preserving transformations, which preserve the exact functionality of the code data and keep the changes covert against rule-breakers.  
We implement CodeMark in a toolkit and conduct an extensive evaluation of code completion models.  
CodeMark is validated to fulfill all desired properties of practical watermarks, including  
harmlessness to model accuracy, verifiability, robustness, and imperceptibility.",Benevolence,Responsibility,The method proposed by the paper enables responsibility in the usage of code datasets; preventing unauthorized exploitation. This aligns with the value item Responsibility and its corresponding value Benevolence.,"In the paper abstract, 'Paper X' introduces a method called CodeMark that embeds imperceptible watermarks into code datasets to trace their usage and prevent unauthorized exploitation. This demonstrates a sense of responsibility in ensuring the proper and ethical use of code datasets, aligning with the value item Responsibility and its corresponding value Benevolence from a ""Software User"" perspective. By implementing CodeMark, software users have the ability to uphold their responsibility in protecting code data and promoting benevolent actions within their software usage.",Agreed-Clarified,Agree,,Agree,,
Exp E,Exp J,1335,ESEC/FSE,Security & Privacy,CodeMark: Imperceptible Watermarking for Code Datasets against Neural Code Completion Models,"Code datasets are of immense value for training neural-network-based code completion models, where companies or organizations have made substantial investments to establish and process these datasets.  
Unluckily, these datasets, either built for proprietary or public usage, face the high risk of unauthorized exploits, resulting from data leakages, license violations, etc.  
Even worse, the ""black-box"" nature of neural models sets a high barrier for externals to audit their training datasets, which further connives these unauthorized usages.  
Currently, watermarking methods have been proposed to prohibit inappropriate usage of image and natural language datasets.  
However, due to domain specificity, they are not directly applicable to code datasets, leaving the copyright protection of this emerging and important field of code data still exposed to threats.  
To fill this gap, we propose a method, named CodeMark, to embed user-defined imperceptible watermarks into code datasets to trace their usage in training neural code completion models.  
CodeMark is based on adaptive semantic-preserving transformations, which preserve the exact functionality of the code data and keep the changes covert against rule-breakers.  
We implement CodeMark in a toolkit and conduct an extensive evaluation of code completion models.  
CodeMark is validated to fulfill all desired properties of practical watermarks, including  
harmlessness to model accuracy, verifiability, robustness, and imperceptibility.",Stimulation,Excitement in Life,The implementation of CodeMark and the extensive evaluation of code completion models provide an element of Excitement in Life; one of the value items under the value Stimulation.,"The implementation of CodeMark in 'Paper X' introduces a new and innovative method for embedding imperceptible watermarks into code datasets. This is aligned with the value item Excitement in Life and its corresponding value Stimulation, as it brings excitement and novelty to the field of code data protection. The extensive evaluation of code completion models also adds to the stimulation by showcasing the practical application and effectiveness of CodeMark. Overall, the contributions of 'Paper X' directly align with the value item Excitement in Life and the value Stimulation, bringing a sense of excitement and innovation to software users in the context of code data protection.",Agreed-Clarified,Agree,,Agree,,
Exp F,Exp J,1336,ESEC/FSE,Security & Privacy,Mate! Are You Really Aware? An Explainability-Guided Testing Framework for Robustness of Malware Detectors,"Numerous open-source and commercial malware detectors are available. However, their efficacy is threatened by new adversarial attacks, whereby malware attempts to evade detection, e.g., by performing feature-space manipulation. In this work, we propose an explainability-guided and model-agnostic testing framework for robustness of malware detectors when confronted with adversarial attacks. The framework introduces the concept of Accrued Malicious Magnitude (AMM) to identify which malware features could be manipulated to maximize the likelihood of evading detection. We then use this framework to test several state-of-the-art malware detectors' ability to detect manipulated malware. We find that (i) commercial antivirus engines are vulnerable to AMM-guided test cases; (ii) the ability of a manipulated malware generated using one detector to evade detection by another detector (i.e., transferability) depends on the overlap of features with large AMM values between the different detectors; and (iii) AMM values effectively measure the fragility of features (i.e., capability of feature-space manipulation to flip the prediction results) and explain the robustness of malware detectors facing evasion attacks. Our findings shed light on the limitations of current malware detectors, as well as how they can be improved.",Security,Social Order,The study identifies vulnerabilities of malware detectors; which would help to increase the social order by helping protect against cyber crimes; enhancing the value of Security.,"The main contribution of 'Paper X' in aligning with the value item Social Order and its corresponding value Security is that it identifies vulnerabilities in malware detectors. By addressing these vulnerabilities, the paper aims to enhance the security of software systems, which in turn contributes to maintaining social order by protecting against cyber crimes. This alignment is important for software users as it highlights the significance of ensuring a secure and stable software environment, ultimately contributing to a sense of order and safety in the digital realm.",Agreed-Justified,Agree,,Agree,,
Exp G,Exp K,1336,ESEC/FSE,Security & Privacy,Mate! Are You Really Aware? An Explainability-Guided Testing Framework for Robustness of Malware Detectors,"Numerous open-source and commercial malware detectors are available. However, their efficacy is threatened by new adversarial attacks, whereby malware attempts to evade detection, e.g., by performing feature-space manipulation. In this work, we propose an explainability-guided and model-agnostic testing framework for robustness of malware detectors when confronted with adversarial attacks. The framework introduces the concept of Accrued Malicious Magnitude (AMM) to identify which malware features could be manipulated to maximize the likelihood of evading detection. We then use this framework to test several state-of-the-art malware detectors' ability to detect manipulated malware. We find that (i) commercial antivirus engines are vulnerable to AMM-guided test cases; (ii) the ability of a manipulated malware generated using one detector to evade detection by another detector (i.e., transferability) depends on the overlap of features with large AMM values between the different detectors; and (iii) AMM values effectively measure the fragility of features (i.e., capability of feature-space manipulation to flip the prediction results) and explain the robustness of malware detectors facing evasion attacks. Our findings shed light on the limitations of current malware detectors, as well as how they can be improved.",Security,National Security,The study could potentially improve the security of national infrastructures and networks that depend on computer systems; which aligns with the value item of National Security under the value of Security.,"In 'Paper X', the proposed framework for testing the robustness of malware detectors addresses the threat of adversarial attacks and the potential evasion of malware detection. By identifying the Accrued Malicious Magnitude (AMM) and analyzing the fragility of features, the study aims to shed light on the limitations of current detectors and improve their effectiveness. From a ""Software User"" perspective, this directly aligns with the value item National Security as it focuses on enhancing the security of computer systems and potentially contributes to the overall security of national infrastructures and networks.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,1336,ESEC/FSE,Security & Privacy,Mate! Are You Really Aware? An Explainability-Guided Testing Framework for Robustness of Malware Detectors,"Numerous open-source and commercial malware detectors are available. However, their efficacy is threatened by new adversarial attacks, whereby malware attempts to evade detection, e.g., by performing feature-space manipulation. In this work, we propose an explainability-guided and model-agnostic testing framework for robustness of malware detectors when confronted with adversarial attacks. The framework introduces the concept of Accrued Malicious Magnitude (AMM) to identify which malware features could be manipulated to maximize the likelihood of evading detection. We then use this framework to test several state-of-the-art malware detectors' ability to detect manipulated malware. We find that (i) commercial antivirus engines are vulnerable to AMM-guided test cases; (ii) the ability of a manipulated malware generated using one detector to evade detection by another detector (i.e., transferability) depends on the overlap of features with large AMM values between the different detectors; and (iii) AMM values effectively measure the fragility of features (i.e., capability of feature-space manipulation to flip the prediction results) and explain the robustness of malware detectors facing evasion attacks. Our findings shed light on the limitations of current malware detectors, as well as how they can be improved.",Universalism,Protecting the Environment,The paper discusses vulnerabilities and improvements of malware detectors; which indirectly contribute to protecting the environment around computer systems by making it safer against cyber threats; aligning the contribution with the value item Protecting the Environment under the value Universalism.,"In the context of malware detection and software usage, protecting the environment can be understood as safeguarding the software ecosystem and the computer systems from potential harm caused by cyber threats. The main contributions of the paper aim to enhance the robustness of malware detectors against adversarial attacks, which ultimately contribute to making the software environment safer. By identifying vulnerabilities and proposing a testing framework for improved detection, the paper aligns with the value item Protecting the Environment under the broader value of Universalism. This alignment reflects the intention to promote safety, security, and the well-being of the software users within the software context.",Agreed-Justified,Disagree,More incline to Achievement: Capable as it capable of identifying malwares,Disagree,agree with ccoder_1,
Exp B,Exp H,1338,ESEC/FSE,Mobile & IoT,ViaLin: Path-Aware Dynamic Taint Analysis for Android,"Dynamic taint analysis - a program analysis technique that checks whether information flows between particular source and sink locations in the program, has numerous applications in security, program comprehension, and software testing. Specifically, in mobile software, taint analysis is often used to determine whether mobile apps contain stealthy behaviors that leak user-sensitive information to unauthorized third-party servers. While a number of dynamic taint analysis techniques for Android software have been recently proposed, none of them are able to report the complete information propagation path, only reporting flow endpoints, i.e., sources and sinks of the detected information flows. This design optimizes for runtime performance and allows the techniques to run efficiently on a mobile device. Yet, it impedes the applicability and usefulness of the techniques: an analyst using the tool would need to manually identify information propagation paths, e.g., to determine whether information was properly handled before being released, which is a challenging task in large real-world applications.  

In this paper, we address this problem by proposing a dynamic taint analysis technique that reports accurate taint propagation paths. We implement it in a tool, ViaLin, and evaluate it on a set of existing benchmark applications and on 16 large Android applications from the Google Play store. Our evaluation shows that ViaLin accurately detects taint flow paths while running on a mobile device with a reasonable time and memory overhead.",Security,Healthy,The paper proposes a dynamic taint analysis technique that can improve the security of mobile software; which directly aligns with the value item Healthy in Schwartz's Taxonomy; reflecting a user's need for a safe and secure software environment.,"In Schwartz's Taxonomy, the value item Healthy is associated with the value of Security. In the context of 'Paper X', the proposed dynamic taint analysis technique aims to improve the security of mobile software by accurately detecting taint propagation paths. This directly aligns with the value item Healthy and the corresponding value of Security, as it reflects a user's need for a safe and secure software environment. By identifying potential information leaks and stealthy behaviors, the technique contributes to enhancing the security of mobile apps, thereby promoting a sense of security for software users.",Agreed-Justified,Agree,,Agree,,
Exp C,Exp K,1338,ESEC/FSE,Mobile & IoT,ViaLin: Path-Aware Dynamic Taint Analysis for Android,"Dynamic taint analysis - a program analysis technique that checks whether information flows between particular source and sink locations in the program, has numerous applications in security, program comprehension, and software testing. Specifically, in mobile software, taint analysis is often used to determine whether mobile apps contain stealthy behaviors that leak user-sensitive information to unauthorized third-party servers. While a number of dynamic taint analysis techniques for Android software have been recently proposed, none of them are able to report the complete information propagation path, only reporting flow endpoints, i.e., sources and sinks of the detected information flows. This design optimizes for runtime performance and allows the techniques to run efficiently on a mobile device. Yet, it impedes the applicability and usefulness of the techniques: an analyst using the tool would need to manually identify information propagation paths, e.g., to determine whether information was properly handled before being released, which is a challenging task in large real-world applications.  

In this paper, we address this problem by proposing a dynamic taint analysis technique that reports accurate taint propagation paths. We implement it in a tool, ViaLin, and evaluate it on a set of existing benchmark applications and on 16 large Android applications from the Google Play store. Our evaluation shows that ViaLin accurately detects taint flow paths while running on a mobile device with a reasonable time and memory overhead.",Self Direction,Independent,The proposed technique enables a more independent analysis of information propagation paths; as it provides accurate paths without requiring analysts to manually identify them. This enhances user's independence; aligning with the value item Independent under the value Self Direction.,"The proposed technique of dynamic taint analysis in 'Paper X' aligns with the value item Independent and its corresponding value Self Direction from a ""Software User"" perspective because it enables users to have more independence in the analysis process. By accurately detecting taint flow paths without requiring manual identification from analysts, the technique empowers users to navigate and understand the information propagation paths on their own, which promotes self-directed decision-making and control over their own sensitive information within a software context.",Agreed-Justified,Agree,,Agree,,
Exp D,Exp K,1338,ESEC/FSE,Mobile & IoT,ViaLin: Path-Aware Dynamic Taint Analysis for Android,"Dynamic taint analysis - a program analysis technique that checks whether information flows between particular source and sink locations in the program, has numerous applications in security, program comprehension, and software testing. Specifically, in mobile software, taint analysis is often used to determine whether mobile apps contain stealthy behaviors that leak user-sensitive information to unauthorized third-party servers. While a number of dynamic taint analysis techniques for Android software have been recently proposed, none of them are able to report the complete information propagation path, only reporting flow endpoints, i.e., sources and sinks of the detected information flows. This design optimizes for runtime performance and allows the techniques to run efficiently on a mobile device. Yet, it impedes the applicability and usefulness of the techniques: an analyst using the tool would need to manually identify information propagation paths, e.g., to determine whether information was properly handled before being released, which is a challenging task in large real-world applications.  

In this paper, we address this problem by proposing a dynamic taint analysis technique that reports accurate taint propagation paths. We implement it in a tool, ViaLin, and evaluate it on a set of existing benchmark applications and on 16 large Android applications from the Google Play store. Our evaluation shows that ViaLin accurately detects taint flow paths while running on a mobile device with a reasonable time and memory overhead.",Stimulation,Variation in Life,By reporting accurate taint propagation paths; the proposed technique introduces a variation in the way user-sensitive information flow is analyzed in mobile software. This aligns with the value item Variation in Life under the value Stimulation as users are provided with a different way of understanding their software's behavior.,"The proposed technique in 'Paper X' introduces a variation in the analysis of user-sensitive information flow in mobile software by reporting accurate taint propagation paths. This aligns with the value item Variation in Life and the corresponding value of Stimulation because it provides software users with a different and potentially more comprehensive understanding of their software's behavior, stimulating their curiosity and offering a new perspective on its functioning.",Agreed-Clarified,Agree,,Agree,,
Exp E,Exp J,1343,ESEC/FSE,Software Deployment & Operations,Assess and Summarize: Improve Outage Understanding with Large Language Models,"Cloud systems have become increasingly popular in recent years due to their flexibility and scalability. Each time cloud computing applications and services hosted on the cloud are affected by a cloud outage, users can experience slow response times, connection issues or total service disruption, resulting in a significant negative business impact. Outages are usually comprised of several concurring events/source causes, and therefore understanding the context of outages is a very challenging yet crucial first step toward mitigating and resolving outages. In current practice, on-call engineers with in-depth domain knowledge, have to manually assess and summarize outages when they happen, which is time-consuming and labor-intensive. In this paper, we first present a large-scale empirical study investigating the way on-call engineers currently deal with cloud outages at Microsoft, and then present and empirically validate a novel approach (dubbed Oasis) to help the engineers in this task. Oasis is able to automatically assess the impact scope of outages as well as to produce human-readable summarization. Specifically, Oasis first assesses the impact scope of an outage by aggregating relevant incidents via multiple techniques. Then, it generates a human-readable summary by leveraging fine-tuned large language models like GPT-3.x. The impact assessment component of Oasis was introduced in Microsoft over three years ago, and it is now widely adopted, while the outage summarization component has been recently introduced, and in this article we present the results of an empirical evaluation we carried out on 18 real-world cloud systems as well as a human-based evaluation with outage owners. The results obtained show that Oasis can effectively and efficiently summarize outages, and lead Microsoft to deploy its first prototype which is currently under experimental adoption by some of the incident teams.",Security,Healthy,The paper presents Oasis; a tool aimed at preventing cloud outages and by extension; ensuring that software users can continue to use cloud services uninterruptedly; this resonates with the value item Healthy under Security value as it strives for consistently good performance of software.,"The justification for aligning 'Paper X' with the value item Healthy and its corresponding value Security from a ""Software User"" perspective is based on the fact that the paper introduces Oasis, a tool that aims to prevent cloud outages and ensure uninterrupted usage of cloud services. This aligns with the value item Healthy under Security, as it emphasizes consistently good performance of software, which is essential for software users to have a secure and reliable user experience.",Agreed-Clarified,Agree,,Agree,,
Exp F,Exp J,1343,ESEC/FSE,Software Deployment & Operations,Assess and Summarize: Improve Outage Understanding with Large Language Models,"Cloud systems have become increasingly popular in recent years due to their flexibility and scalability. Each time cloud computing applications and services hosted on the cloud are affected by a cloud outage, users can experience slow response times, connection issues or total service disruption, resulting in a significant negative business impact. Outages are usually comprised of several concurring events/source causes, and therefore understanding the context of outages is a very challenging yet crucial first step toward mitigating and resolving outages. In current practice, on-call engineers with in-depth domain knowledge, have to manually assess and summarize outages when they happen, which is time-consuming and labor-intensive. In this paper, we first present a large-scale empirical study investigating the way on-call engineers currently deal with cloud outages at Microsoft, and then present and empirically validate a novel approach (dubbed Oasis) to help the engineers in this task. Oasis is able to automatically assess the impact scope of outages as well as to produce human-readable summarization. Specifically, Oasis first assesses the impact scope of an outage by aggregating relevant incidents via multiple techniques. Then, it generates a human-readable summary by leveraging fine-tuned large language models like GPT-3.x. The impact assessment component of Oasis was introduced in Microsoft over three years ago, and it is now widely adopted, while the outage summarization component has been recently introduced, and in this article we present the results of an empirical evaluation we carried out on 18 real-world cloud systems as well as a human-based evaluation with outage owners. The results obtained show that Oasis can effectively and efficiently summarize outages, and lead Microsoft to deploy its first prototype which is currently under experimental adoption by some of the incident teams.",Security,Family Security,Oasis' aim to mitigate and resolve outages ultimately contributes to a continuous availability and reliability of the cloud services. This relates to the value item Family Security from the value Security given it contributes to the reliability and security of the software from the user perspective.,"The alignment with the value item Family Security and its corresponding value Security from a ""Software User"" perspective stems from the fact that the main contribution of 'Paper X', the Oasis approach, specifically aims to mitigate and resolve outages in cloud systems. By ensuring continuous availability and reliability of the cloud services, Oasis directly contributes to the security and reliability of the software, which in turn provides a sense of security to the families or users relying on these services for their personal or professional needs.",Agreed-Clarified,Agree,,Agree,,
Exp G,Exp K,1343,ESEC/FSE,Software Deployment & Operations,Assess and Summarize: Improve Outage Understanding with Large Language Models,"Cloud systems have become increasingly popular in recent years due to their flexibility and scalability. Each time cloud computing applications and services hosted on the cloud are affected by a cloud outage, users can experience slow response times, connection issues or total service disruption, resulting in a significant negative business impact. Outages are usually comprised of several concurring events/source causes, and therefore understanding the context of outages is a very challenging yet crucial first step toward mitigating and resolving outages. In current practice, on-call engineers with in-depth domain knowledge, have to manually assess and summarize outages when they happen, which is time-consuming and labor-intensive. In this paper, we first present a large-scale empirical study investigating the way on-call engineers currently deal with cloud outages at Microsoft, and then present and empirically validate a novel approach (dubbed Oasis) to help the engineers in this task. Oasis is able to automatically assess the impact scope of outages as well as to produce human-readable summarization. Specifically, Oasis first assesses the impact scope of an outage by aggregating relevant incidents via multiple techniques. Then, it generates a human-readable summary by leveraging fine-tuned large language models like GPT-3.x. The impact assessment component of Oasis was introduced in Microsoft over three years ago, and it is now widely adopted, while the outage summarization component has been recently introduced, and in this article we present the results of an empirical evaluation we carried out on 18 real-world cloud systems as well as a human-based evaluation with outage owners. The results obtained show that Oasis can effectively and efficiently summarize outages, and lead Microsoft to deploy its first prototype which is currently under experimental adoption by some of the incident teams.",Benevolence,Responsibility,The paper makes a significant contribution towards supporting the Responsibility of cloud engineers by developing a novel approach (Oasis) to help them in identifying; mitigating; and resolving outages effectively and efficiently. This is closely aligned with the value item Responsibility under Benevolence value as Oasis makes the job of on-call engineers easier and faster there by fulfilling their responsibilities more effectively.,"The paper aligns with the value item Responsibility and its corresponding value Benevolence from a ""Software User"" perspective because it presents a novel approach (Oasis) that assists on-call engineers in fulfilling their responsibilities of identifying, mitigating, and resolving outages more effectively and efficiently. By automating the assessment of outage impact scope and generating human-readable summaries, Oasis simplifies and expedites the process for engineers, allowing them to fulfill their responsibilities in a timely manner and minimize the negative business impact of outages. This directly aligns with the value item Responsibility under Benevolence as it supports the engineers in their duty to maintain and improve the quality and reliability of cloud systems for the benefit of the software users.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,1347,ESEC/FSE,Code Generation & Analysis,Dead Code Removal at Meta: Automatically Deleting Millions of Lines of Code and Petabytes of Deprecated Data,"Software constantly evolves in response to user needs: new features are built, deployed, mature and grow old, and eventually their usage drops enough to merit switching them off. In any large codebase, this feature lifecycle can naturally lead to retaining unnecessary code and data. Removing these respects usersaEUR(tm) privacy expectations, as well as helping engineers to work efficiently. In prior software engineering research, we have found little evidence of code deprecation or dead-code removal at industrial scale. We describe Systematic Code and Asset Removal Framework (SCARF), a product deprecation system to assist engineers working in large codebases. SCARF identifies unused code and data assets and safely removes them. It operates fully automatically, including committing code and dropping database tables. It also gathers developer input where it cannot take automated actions, leading to further removals. Dead code removal increases the quality and consistency of large codebases, aids with knowledge management and improves reliability. SCARF has had an important impact at Meta. In the last year alone, it has removed petabytes of data across 12.8 million distinct assets, and deleted over 104 million lines of code.",Self Direction,Privacy,Paper X's Systematic Code and Asset Removal Framework (SCARF) respects users‚Äö√Ñ√∂‚àö√ë‚àö‚àÇ‚Äö√†√∂‚àö√´‚Äö√†√∂‚Äö√†√á‚Äö√Ñ√∂‚àö‚Ä†‚àö‚àÇ‚Äö√†√∂‚àö¬¥‚Äö√Ñ√∂‚àö‚Ä†‚àö‚àÇ¬¨¬®‚Äö√Ñ¬¢ privacy expectations by removing unnecessary code and data. This directly aligns with the value item Privacy and its corresponding value Self Direction.,"In 'Paper X', the Systematic Code and Asset Removal Framework (SCARF) is described as a product deprecation system that identifies and removes unused code and data assets. This process of removing unnecessary code and data aligns with the value item Privacy, as it respects users' privacy expectations by eliminating any potential risks associated with retaining such unused assets. This directly corresponds to the value of Self-Direction, as users have the freedom to ensure their privacy and control over their software experience.",Agreed-Justified,Agree,,Agree,,
Exp B,Exp H,1347,ESEC/FSE,Code Generation & Analysis,Dead Code Removal at Meta: Automatically Deleting Millions of Lines of Code and Petabytes of Deprecated Data,"Software constantly evolves in response to user needs: new features are built, deployed, mature and grow old, and eventually their usage drops enough to merit switching them off. In any large codebase, this feature lifecycle can naturally lead to retaining unnecessary code and data. Removing these respects usersaEUR(tm) privacy expectations, as well as helping engineers to work efficiently. In prior software engineering research, we have found little evidence of code deprecation or dead-code removal at industrial scale. We describe Systematic Code and Asset Removal Framework (SCARF), a product deprecation system to assist engineers working in large codebases. SCARF identifies unused code and data assets and safely removes them. It operates fully automatically, including committing code and dropping database tables. It also gathers developer input where it cannot take automated actions, leading to further removals. Dead code removal increases the quality and consistency of large codebases, aids with knowledge management and improves reliability. SCARF has had an important impact at Meta. In the last year alone, it has removed petabytes of data across 12.8 million distinct assets, and deleted over 104 million lines of code.",Security,Healthy,SCARF enables software that respects users' privacy by removing unnecessary data; which could be seen as contributing to a 'healthy' digital environment by reducing the risk of data breaches. Hence it aligns with the value item Healthy under the value Security.,"In the context of a ""Software User,"" the alignment between 'Paper X' and the value item Healthy is based on the contribution of the SCARF system in removing unnecessary data, which ultimately reduces the risk of data breaches. By eliminating unused code and data assets, SCARF helps to create a more secure software environment, promoting the well-being and security of the users' digital experience. This aligns with the value item Healthy, as it focuses on ensuring the protection and integrity of user data and maintaining a safe software environment.",Agreed-Justified,Disagree,reconcile with coder_2,Disagree,"The abstract does not explicitly mention improvements to 'health' or direct implications for 'security', making the alignment to 'Healthy' under 'Security' questionable.",
Exp D,Exp K,1352,ESEC/FSE,AI & Machine Learning,TraceDiag: Adaptive Interpretable and Efficient Root Cause Analysis on Large-Scale Microservice Systems,"Root Cause Analysis (RCA) is becoming increasingly crucial for ensuring the reliability of microservice systems. However, performing RCA on modern microservice systems can be challenging due to their large scale, as they usually comprise hundreds of components, leading significant human effort. This paper proposes TraceDiag, an end-to-end RCA framework that addresses the challenges for large-scale microservice systems. It leverages reinforcement learning to learn a pruning policy for the service dependency graph to automatically eliminates redundant components, thereby significantly improving the RCA efficiency. The learned pruning policy is interpretable and fully adaptive to new RCA instances. With the pruned graph, a causal-based method can be executed with high accuracy and efficiency. The proposed TraceDiag framework is evaluated on real data traces collected from the Microsoft Exchange system, and demonstrates superior performance compared to state-of-the-art RCA approaches. Notably, TraceDiag has been integrated as a critical component in the Microsoft M365 Exchange, resulting in a significant improvement in the system's reliability and a considerable reduction in the human effort required for RCA.",Achievement,Successful,The paper presents a framework that results in significant improvement in the system's reliability; which aligns with the value item Successful and its corresponding value Achievement.,"Based on the abstract, the paper proposes a framework called TraceDiag that improves the reliability of microservice systems. By utilizing reinforcement learning to prune redundant components in the system, the paper demonstrates a significant improvement in the system's reliability. This aligns with the value item Successful and its corresponding value Achievement because the paper's main contribution directly leads to a successful outcome in terms of increasing the reliability of the system.",Agreed-Justified,Agree,,Agree,,
Exp E,Exp J,1352,ESEC/FSE,AI & Machine Learning,TraceDiag: Adaptive Interpretable and Efficient Root Cause Analysis on Large-Scale Microservice Systems,"Root Cause Analysis (RCA) is becoming increasingly crucial for ensuring the reliability of microservice systems. However, performing RCA on modern microservice systems can be challenging due to their large scale, as they usually comprise hundreds of components, leading significant human effort. This paper proposes TraceDiag, an end-to-end RCA framework that addresses the challenges for large-scale microservice systems. It leverages reinforcement learning to learn a pruning policy for the service dependency graph to automatically eliminates redundant components, thereby significantly improving the RCA efficiency. The learned pruning policy is interpretable and fully adaptive to new RCA instances. With the pruned graph, a causal-based method can be executed with high accuracy and efficiency. The proposed TraceDiag framework is evaluated on real data traces collected from the Microsoft Exchange system, and demonstrates superior performance compared to state-of-the-art RCA approaches. Notably, TraceDiag has been integrated as a critical component in the Microsoft M365 Exchange, resulting in a significant improvement in the system's reliability and a considerable reduction in the human effort required for RCA.",Achievement,Intelligent,The use of reinforcement learning in the proposed framework indicates that the system is intelligent enough to adapt to new RCA instances; this aligns with the value item Intelligent and its corresponding value Achievement.,"In the context of a ""Software User,"" the alignment between the proposed framework in 'Paper X' and the value item Intelligent from Schwartz's Taxonomy can be justified by the use of reinforcement learning. By leveraging reinforcement learning, the framework is able to adapt to new instances of Root Cause Analysis (RCA), indicating a level of intelligence in its decision-making process. This aligns with the value of Achievement, as the framework's ability to automatically eliminate redundant components contributes to the efficiency and reliability of microservice systems, ultimately achieving a significant improvement in the system's performance and reducing the human effort required for RCA.",Agreed-Clarified,Agree,,Agree,,
Exp F,Exp J,1352,ESEC/FSE,AI & Machine Learning,TraceDiag: Adaptive Interpretable and Efficient Root Cause Analysis on Large-Scale Microservice Systems,"Root Cause Analysis (RCA) is becoming increasingly crucial for ensuring the reliability of microservice systems. However, performing RCA on modern microservice systems can be challenging due to their large scale, as they usually comprise hundreds of components, leading significant human effort. This paper proposes TraceDiag, an end-to-end RCA framework that addresses the challenges for large-scale microservice systems. It leverages reinforcement learning to learn a pruning policy for the service dependency graph to automatically eliminates redundant components, thereby significantly improving the RCA efficiency. The learned pruning policy is interpretable and fully adaptive to new RCA instances. With the pruned graph, a causal-based method can be executed with high accuracy and efficiency. The proposed TraceDiag framework is evaluated on real data traces collected from the Microsoft Exchange system, and demonstrates superior performance compared to state-of-the-art RCA approaches. Notably, TraceDiag has been integrated as a critical component in the Microsoft M365 Exchange, resulting in a significant improvement in the system's reliability and a considerable reduction in the human effort required for RCA.",Security,Healthy,By improving the reliability of microservice systems; the paper contributes to maintaining the system's health; which aligns with the value item Healthy and its corresponding value Security.,"The justification for aligning 'Paper X' with the value item Healthy and its corresponding value Security is based on the fact that by improving the reliability of microservice systems, the paper contributes to maintaining the overall health and stability of the system. A healthy system implies a secure system, as it ensures that the software functions as intended without risks of failures or vulnerabilities. Therefore, the main contribution of 'Paper X' directly aligns with the value item Healthy and its corresponding value Security from the perspective of a software user who values the stability and safety of the system.",Agreed-Justified,Agree,,Agree,,
Exp G,Exp K,1354,ESEC/FSE,Software Testing & QA,Appaction: Automatic GUI Interaction for Mobile Apps via Holistic Widget Perception,"In industrial practice, GUI (Graphic User Interface) testing of mobile apps still inevitably relies on huge manual efforts. The major efforts are those on understanding the GUIs, so that testing scripts can be written accordingly. Quality assurance could therefore be very labor-intensive, especially for modern commercial mobile apps, where one may include tremendous, diverse, and complex GUIs, e.g., those for placing orders of different commercial items. To reduce such human efforts, we propose Appaction, a learning-based automatic GUI interaction approach we developed for Meituan, one of the largest E-commerce providers with over 600 million users. Appaction can automatically analyze the target GUI and understand what each input of the GUI is about, so that corresponding valid inputs can be entered accordingly. To this end, Appaction adopts a multi-modal model to learn from human experiences in perceiving a GUI. This allows it to infer corresponding valid input events that can properly interact with the GUI. In this way, the target app can be effectively exercised. We present our experiences in Meituan on applying Appaction to popular commercial apps. We demonstrate the effectiveness of Appaction in GUI analysis, and it can perform correct interactions for numerous form pages.",Stimulation,Variation in Life,The paper proposes an automatic GUI interaction method which will bring more variety to the use of various apps; aligning with Variation in Life.,"The justification for aligning 'Paper X' with the value item Variation in Life and its corresponding value Stimulation is based on the fact that the paper proposes an automatic GUI interaction approach that allows for diverse and complex interactions with mobile apps. This variety and stimulation in the user experience align with Variation in Life, as it introduces new and different ways of interacting with different commercial apps.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,1354,ESEC/FSE,Software Testing & QA,Appaction: Automatic GUI Interaction for Mobile Apps via Holistic Widget Perception,"In industrial practice, GUI (Graphic User Interface) testing of mobile apps still inevitably relies on huge manual efforts. The major efforts are those on understanding the GUIs, so that testing scripts can be written accordingly. Quality assurance could therefore be very labor-intensive, especially for modern commercial mobile apps, where one may include tremendous, diverse, and complex GUIs, e.g., those for placing orders of different commercial items. To reduce such human efforts, we propose Appaction, a learning-based automatic GUI interaction approach we developed for Meituan, one of the largest E-commerce providers with over 600 million users. Appaction can automatically analyze the target GUI and understand what each input of the GUI is about, so that corresponding valid inputs can be entered accordingly. To this end, Appaction adopts a multi-modal model to learn from human experiences in perceiving a GUI. This allows it to infer corresponding valid input events that can properly interact with the GUI. In this way, the target app can be effectively exercised. We present our experiences in Meituan on applying Appaction to popular commercial apps. We demonstrate the effectiveness of Appaction in GUI analysis, and it can perform correct interactions for numerous form pages.",Achievement,Intelligent,Through the use of Appaction; users can effectively exercise different apps which can improve their intelligence in operating various apps; which aligns with Intelligent in Achievement.,"The use of Appaction in 'Paper X' aligns with the value item Intelligent and its corresponding value Achievement from a ""Software User"" perspective. By automatically analyzing and understanding the GUI of different apps, users can effectively interact with various interfaces and improve their intelligence in operating different apps. This aligns with the value of Achievement, as users are able to successfully navigate and engage with complex GUIs, demonstrating their capability, intelligence, and success in using a wide range of software applications.",Agreed-Justified,Agree,,Agree,,
Exp B,Exp H,1354,ESEC/FSE,Software Testing & QA,Appaction: Automatic GUI Interaction for Mobile Apps via Holistic Widget Perception,"In industrial practice, GUI (Graphic User Interface) testing of mobile apps still inevitably relies on huge manual efforts. The major efforts are those on understanding the GUIs, so that testing scripts can be written accordingly. Quality assurance could therefore be very labor-intensive, especially for modern commercial mobile apps, where one may include tremendous, diverse, and complex GUIs, e.g., those for placing orders of different commercial items. To reduce such human efforts, we propose Appaction, a learning-based automatic GUI interaction approach we developed for Meituan, one of the largest E-commerce providers with over 600 million users. Appaction can automatically analyze the target GUI and understand what each input of the GUI is about, so that corresponding valid inputs can be entered accordingly. To this end, Appaction adopts a multi-modal model to learn from human experiences in perceiving a GUI. This allows it to infer corresponding valid input events that can properly interact with the GUI. In this way, the target app can be effectively exercised. We present our experiences in Meituan on applying Appaction to popular commercial apps. We demonstrate the effectiveness of Appaction in GUI analysis, and it can perform correct interactions for numerous form pages.",Security,Sense of Belonging,Using the Appaction will drastically reduce the effort required in using modern commercial mobile apps; giving users a sense of belonging to the digital commerce society; aligning with Sense of Belonging in Security.,"The main contributions of 'Paper X' in developing Appaction align with the value item Sense of Belonging and its corresponding value Security from a ""Software User"" perspective because by reducing manual efforts in GUI testing and providing an automated and efficient way to interact with complex GUIs, Appaction ensures a seamless and convenient user experience. This sense of belonging stems from the fact that users can confidently and easily navigate through modern commercial mobile apps, feeling secure in their ability to engage with the digital commerce society.",Agreed-Justified,Agree,,Agree,,
Exp C,Exp K,1357,ESEC/FSE,AI & Machine Learning,On-Premise AIOps Infrastructure for a Software Editor SME: An Experience Report,"Information Technology has become a critical component in various industries, leading to an increased focus on software maintenance and monitoring. With the complexities of modern software systems, traditional maintenance approaches have become insufficient. The concept of AIOps has emerged to enhance predictive maintenance using Big Data and Machine Learning capabilities. However, exploiting AIOps requires addressing several challenges related to the complexity of data and incident management. Commercial solutions exist, but they may not be suitable for certain companies due to high costs, data governance issues, and limitations in covering private software. This paper investigates the feasibility of implementing on-premise AIOps solutions by leveraging open-source tools. We introduce a comprehensive AIOps infrastructure that we have successfully deployed in our company, and we provide the rationale behind different choices that we made to build its various components. Particularly, we provide insights into our approach and criteria for selecting a data management system and we explain its integration. Our experience can be beneficial for companies seeking to internally manage their software maintenance processes with a modern AIOps approach.",Achievement,Successful,The paper introduces a comprehensive AIOps infrastructure successfully deployed in a company that could make software users more successful in managing their software maintenance processes.,"In the abstract of 'Paper X', it is explicitly stated that the paper introduces a comprehensive AIOps infrastructure that has been successfully deployed in a company. This infrastructure is aimed at enhancing predictive maintenance and addressing the complexities of data and incident management in software systems. As a software user, having access to such a system can greatly contribute to their success in effectively managing their software maintenance processes. Therefore, aligning with the value item Successful and its corresponding value Achievement is justified based on the paper's direct contribution towards enabling software users to achieve success in their software maintenance tasks.",Agreed-Justified,Agree,,Agree,,
Exp D,Exp K,1357,ESEC/FSE,AI & Machine Learning,On-Premise AIOps Infrastructure for a Software Editor SME: An Experience Report,"Information Technology has become a critical component in various industries, leading to an increased focus on software maintenance and monitoring. With the complexities of modern software systems, traditional maintenance approaches have become insufficient. The concept of AIOps has emerged to enhance predictive maintenance using Big Data and Machine Learning capabilities. However, exploiting AIOps requires addressing several challenges related to the complexity of data and incident management. Commercial solutions exist, but they may not be suitable for certain companies due to high costs, data governance issues, and limitations in covering private software. This paper investigates the feasibility of implementing on-premise AIOps solutions by leveraging open-source tools. We introduce a comprehensive AIOps infrastructure that we have successfully deployed in our company, and we provide the rationale behind different choices that we made to build its various components. Particularly, we provide insights into our approach and criteria for selecting a data management system and we explain its integration. Our experience can be beneficial for companies seeking to internally manage their software maintenance processes with a modern AIOps approach.",Security,Healthy,The AIOps approach presented in the paper could arguably contribute to the health of the software systems the users interact with which aligns with the value item Healthy.,"The justification for labeling 'Paper X' as aligning with the value item Healthy and its corresponding value Security from a ""Software User"" perspective is that the AIOps approach presented in the paper aims to enhance predictive maintenance and monitoring of software systems. By improving the maintenance and monitoring processes, it can contribute to the overall health and stability of the software systems. Since users rely on the software for their tasks, a healthy and secure software system is crucial for their experience and productivity. Therefore, the alignment with the value item Healthy and its corresponding value Security is evident in the paper's focus on addressing challenges related to software maintenance and providing a comprehensive AIOps infrastructure.",Agreed-Clarified,Agree,,Disagree,Value healthy concerns Human health not software health,
Exp E,Exp J,1357,ESEC/FSE,AI & Machine Learning,On-Premise AIOps Infrastructure for a Software Editor SME: An Experience Report,"Information Technology has become a critical component in various industries, leading to an increased focus on software maintenance and monitoring. With the complexities of modern software systems, traditional maintenance approaches have become insufficient. The concept of AIOps has emerged to enhance predictive maintenance using Big Data and Machine Learning capabilities. However, exploiting AIOps requires addressing several challenges related to the complexity of data and incident management. Commercial solutions exist, but they may not be suitable for certain companies due to high costs, data governance issues, and limitations in covering private software. This paper investigates the feasibility of implementing on-premise AIOps solutions by leveraging open-source tools. We introduce a comprehensive AIOps infrastructure that we have successfully deployed in our company, and we provide the rationale behind different choices that we made to build its various components. Particularly, we provide insights into our approach and criteria for selecting a data management system and we explain its integration. Our experience can be beneficial for companies seeking to internally manage their software maintenance processes with a modern AIOps approach.",Security,Family Security,With emphasis on self-sufficient; on-premise solutions; users can secure their software systems and processes from external reliance; which can be related to the value item Family Security.,"In the context of a ""Software User,"" the alignment with the value item Family Security and its corresponding value Security in 'Paper X' is evident by the emphasis on self-sufficient, on-premise AIOps solutions. By internally managing their software maintenance processes, software users can ensure the security and stability of their software systems, minimizing the reliance on external entities and potential vulnerabilities. This aligns directly with the value of Security, as software users prioritize the protection and safety of their software systems and the sensitive data they contain, ultimately contributing to a sense of Family Security.",Agreed-Clarified,Agree,,Agree,,
Exp F,Exp J,1361,ESEC/FSE,AI & Machine Learning,KDDT: Knowledge Distillation-Empowered Digital Twin for Anomaly Detection,"Cyber-physical systems (CPSs), like train control and management systems (TCMS), are becoming ubiquitous in critical infrastructures. As safety-critical systems, ensuring their dependability during operation is crucial. Digital twins (DTs) have been increasingly studied for this purpose owing to their capability of runtime monitoring and warning, prediction and detection of anomalies, etc. However, constructing a DT for anomaly detection in TCMS necessitates sufficient training data and extracting both chronological and context features with high quality. Hence, in this paper, we propose a novel method named KDDT for TCMS anomaly detection. KDDT harnesses a language model (LM) and a long short-term memory (LSTM) network to extract contexts and chronological features, respectively. To enrich data volume, KDDT benefits from out-of-domain data with knowledge distillation (KD). We evaluated KDDT with two datasets from our industry partner Alstom and obtained the F1 scores of 0.931 and 0.915, respectively, demonstrating the effectiveness of KDDT. We also explored individual contributions of the DT model, LM, and KD to the overall performance of KDDT, via a comprehensive empirical study, and observed average F1 score improvements of 12.4\%, 3\%, and 6.05\%, respectively.",Achievement,Capable,The paper contributes a method (KDDT) that significantly improves the capability of train control and management systems (TCMS); demonstrating the effectiveness of KDDT. This aligns with the value item Capable and its corresponding value Achievement.,"In the paper, the authors propose a novel method called KDDT for anomaly detection in train control and management systems (TCMS). They highlight the effectiveness of KDDT by evaluating it with two datasets from an industry partner and obtaining high F1 scores. This directly aligns with the value item Capable and its corresponding value Achievement because the paper's main contribution is the development of a method that significantly improves the capability of TCMS, ultimately achieving a high level of performance in detecting anomalies.",Agreed-Clarified,Agree,,Agree,,
Exp G,Exp K,1361,ESEC/FSE,AI & Machine Learning,KDDT: Knowledge Distillation-Empowered Digital Twin for Anomaly Detection,"Cyber-physical systems (CPSs), like train control and management systems (TCMS), are becoming ubiquitous in critical infrastructures. As safety-critical systems, ensuring their dependability during operation is crucial. Digital twins (DTs) have been increasingly studied for this purpose owing to their capability of runtime monitoring and warning, prediction and detection of anomalies, etc. However, constructing a DT for anomaly detection in TCMS necessitates sufficient training data and extracting both chronological and context features with high quality. Hence, in this paper, we propose a novel method named KDDT for TCMS anomaly detection. KDDT harnesses a language model (LM) and a long short-term memory (LSTM) network to extract contexts and chronological features, respectively. To enrich data volume, KDDT benefits from out-of-domain data with knowledge distillation (KD). We evaluated KDDT with two datasets from our industry partner Alstom and obtained the F1 scores of 0.931 and 0.915, respectively, demonstrating the effectiveness of KDDT. We also explored individual contributions of the DT model, LM, and KD to the overall performance of KDDT, via a comprehensive empirical study, and observed average F1 score improvements of 12.4\%, 3\%, and 6.05\%, respectively.",Security,Family Security,The paper contributes a method for ensuring the dependability of safety-critical systems; like train control and management systems; during operation. This aligns with the value item Family Security and its corresponding value Security.,"The main contribution of 'Paper X' in ensuring the dependability of safety-critical systems aligns with the value item Family Security and its corresponding value Security from a ""Software User"" perspective. This alignment is based on the fact that safety-critical systems, such as train control and management systems, are vital for the well-being and security of individuals and families. The paper aims to detect anomalies and provide warnings in these systems, which directly contributes to the overall security and stability of such systems. By ensuring the dependability of these systems, the paper indirectly promotes the safety and security of families who rely on them for their transportation needs.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,1361,ESEC/FSE,AI & Machine Learning,KDDT: Knowledge Distillation-Empowered Digital Twin for Anomaly Detection,"Cyber-physical systems (CPSs), like train control and management systems (TCMS), are becoming ubiquitous in critical infrastructures. As safety-critical systems, ensuring their dependability during operation is crucial. Digital twins (DTs) have been increasingly studied for this purpose owing to their capability of runtime monitoring and warning, prediction and detection of anomalies, etc. However, constructing a DT for anomaly detection in TCMS necessitates sufficient training data and extracting both chronological and context features with high quality. Hence, in this paper, we propose a novel method named KDDT for TCMS anomaly detection. KDDT harnesses a language model (LM) and a long short-term memory (LSTM) network to extract contexts and chronological features, respectively. To enrich data volume, KDDT benefits from out-of-domain data with knowledge distillation (KD). We evaluated KDDT with two datasets from our industry partner Alstom and obtained the F1 scores of 0.931 and 0.915, respectively, demonstrating the effectiveness of KDDT. We also explored individual contributions of the DT model, LM, and KD to the overall performance of KDDT, via a comprehensive empirical study, and observed average F1 score improvements of 12.4\%, 3\%, and 6.05\%, respectively.",Security,Healthy,The method proposed in the paper contributes to maintaining the health of cyber-physical systems; as it aids in runtime monitoring; warns; predicts and detects anomalies; etc. This aligns with the value item Healthy and its corresponding value Security.,"The justification for aligning 'Paper X' with the value item Healthy and its corresponding value Security is based on the fact that the proposed method in the paper focuses on ensuring the dependability and operational safety of cyber-physical systems, specifically train control and management systems. By utilizing a digital twin for anomaly detection and leveraging a language model and LSTM network, the method aims to extract chronological and contextual features to effectively monitor, warn, predict, and detect anomalies. These contributions directly align with the value item Healthy as they can help maintain the health and well-being of the cyber-physical systems, ensuring their proper functioning and security.",Agreed-Justified,Agree,,Disagree,The abstract does not explicitly mention improvements to 'health',
Exp B,Exp H,1362,ESEC/FSE,Software Testing & QA,AG3: Automated Game GUI Text Glitch Detection Based on Computer Vision,"With the advancement of device software and hardware performance, and the evolution of game engines, an increasing number of emerging high-quality games are captivating game players from all around the world who speak different languages. However, due to the vast fragmentation of the device and platform market, a well-tested game may still experience text glitches when installed on a new device with an unseen screen resolution and system version, which can significantly impact the user experience. In our testing pipeline, current testing techniques for identifying multilingual text glitches are laborious and inefficient. In this paper, we present AG3, which offers intelligent game traversal, precise visual text glitch detection, and integrated quality report generation capabilities. Our empirical evaluation and internal industrial deployment demonstrate that AG3 can detect various real-world multilingual text glitches with minimal human involvement.",Stimulation,Excitement in Life,Paper X introduces AG3; an efficient solution able to identify and rectify text glitches in games; ensuring an increase in excitement and overall enjoyment for the software user as their gaming experience is no longer hindered by disrupted system versions or screen resolutions.,"In 'Paper X', the main contribution of AG3 is the ability to detect and rectify text glitches in games caused by different system versions and screen resolutions. This directly aligns with the value item Excitement in Life and its corresponding value Stimulation from a software user perspective because with the implementation of AG3, software users can enjoy a seamless gaming experience without disruptions or inconsistencies in the text display, leading to increased excitement and overall enjoyment in their gaming experience.",Agreed-Justified,Agree,,Agree,,
Exp C,Exp K,1362,ESEC/FSE,Software Testing & QA,AG3: Automated Game GUI Text Glitch Detection Based on Computer Vision,"With the advancement of device software and hardware performance, and the evolution of game engines, an increasing number of emerging high-quality games are captivating game players from all around the world who speak different languages. However, due to the vast fragmentation of the device and platform market, a well-tested game may still experience text glitches when installed on a new device with an unseen screen resolution and system version, which can significantly impact the user experience. In our testing pipeline, current testing techniques for identifying multilingual text glitches are laborious and inefficient. In this paper, we present AG3, which offers intelligent game traversal, precise visual text glitch detection, and integrated quality report generation capabilities. Our empirical evaluation and internal industrial deployment demonstrate that AG3 can detect various real-world multilingual text glitches with minimal human involvement.",Achievement,Successful,With the introduction of AG3; user experience and performance is boosted by mitigating issues that could inhibit successful gaming; aligning with the value item of Success from Achievement.,"The justification is based on the fact that the main contribution of 'Paper X' is the development of AG3, which is specifically designed to detect and mitigate text glitches in multilingual games. By addressing these glitches and improving the user experience and performance, 'Paper X' aligns with the value item of Success from Achievement, as it aims to ensure the successful gaming experience for users.",Agreed-Justified,Agree,,Agree,,
Exp E,Exp J,1363,ESEC/FSE,Software Deployment & Operations,Detection Is Better Than Cure: A Cloud Incidents Perspective,"Cloud providers use automated watchdogs or monitors to continuously observe service availability and to proactively report incidents when system performance degrades. Improper monitoring can lead to delays in the detection and mitigation of production incidents, which can be extremely expensive in terms of customer impacts and manual toil from engineering resources. Therefore, a systematic understanding of the pitfalls in current monitoring practices and how they can lead to production incidents is crucial for ensuring continuous reliability of cloud services. In this work, we carefully study the production incidents from the past year at Microsoft to understand the monitoring gaps in a hyperscale cloud platform. We conduct an extensive empirical study to answer: (1) What are the major causes of failures in early detection of production incidents and what are the steps taken for mitigation, (2) What is the impact of failures in early detection, (3) How do we recommend best monitoring practices for different services, and (4) How can we leverage the insights from this study to enhance the reliability of the cloud services. This study provides a deeper understanding of existing monitoring gaps in cloud platforms, uncover interesting insights and provide guidance for best monitoring practices for ensuring continuous reliability.",Security,Healthy,As the paper discusses about ensuring continuous reliability of cloud services; from a software user perspective; it could be said that the paper is contributing to living a healthy digital life; which aligns with the 'Healthy' value item in the 'Security' value.,"Certainly! In the context of cloud services, ensuring continuous reliability is crucial for software users to have a seamless and secure experience. By studying the production incidents in a hyperscale cloud platform, 'Paper X' aims to identify monitoring gaps and recommend best practices for enhancing the reliability of cloud services. This directly aligns with the value item of 'Healthy' and its corresponding value of 'Security' in Schwartz's Taxonomy. When cloud services are reliable and secure, software users can trust that their data and activities are protected, leading to a healthier digital life.",Agreed-Justified,Agree,,Agree,,
Exp F,Exp J,1363,ESEC/FSE,Software Deployment & Operations,Detection Is Better Than Cure: A Cloud Incidents Perspective,"Cloud providers use automated watchdogs or monitors to continuously observe service availability and to proactively report incidents when system performance degrades. Improper monitoring can lead to delays in the detection and mitigation of production incidents, which can be extremely expensive in terms of customer impacts and manual toil from engineering resources. Therefore, a systematic understanding of the pitfalls in current monitoring practices and how they can lead to production incidents is crucial for ensuring continuous reliability of cloud services. In this work, we carefully study the production incidents from the past year at Microsoft to understand the monitoring gaps in a hyperscale cloud platform. We conduct an extensive empirical study to answer: (1) What are the major causes of failures in early detection of production incidents and what are the steps taken for mitigation, (2) What is the impact of failures in early detection, (3) How do we recommend best monitoring practices for different services, and (4) How can we leverage the insights from this study to enhance the reliability of the cloud services. This study provides a deeper understanding of existing monitoring gaps in cloud platforms, uncover interesting insights and provide guidance for best monitoring practices for ensuring continuous reliability.",Security,Family Security,The paper conducts an extensive empirical study to understand monitoring gaps in a hyperscale cloud platform and recommends best monitoring practices. Such contributions can enhance security of user data which aligns with the 'Family Security' value item within the 'Security' value.,"In 'Paper X', the authors highlight the importance of proper monitoring practices in cloud platforms to ensure continuous reliability. By conducting an empirical study and recommending best monitoring practices, the paper directly contributes to enhancing the security of user data. This aligns with the value item of Family Security within the broader value of Security, as the recommended practices can help protect sensitive user information and ensure the overall safety and privacy of families who utilize cloud services.",Agreed-Justified,Agree,,Agree,,
Exp G,Exp K,1363,ESEC/FSE,Software Deployment & Operations,Detection Is Better Than Cure: A Cloud Incidents Perspective,"Cloud providers use automated watchdogs or monitors to continuously observe service availability and to proactively report incidents when system performance degrades. Improper monitoring can lead to delays in the detection and mitigation of production incidents, which can be extremely expensive in terms of customer impacts and manual toil from engineering resources. Therefore, a systematic understanding of the pitfalls in current monitoring practices and how they can lead to production incidents is crucial for ensuring continuous reliability of cloud services. In this work, we carefully study the production incidents from the past year at Microsoft to understand the monitoring gaps in a hyperscale cloud platform. We conduct an extensive empirical study to answer: (1) What are the major causes of failures in early detection of production incidents and what are the steps taken for mitigation, (2) What is the impact of failures in early detection, (3) How do we recommend best monitoring practices for different services, and (4) How can we leverage the insights from this study to enhance the reliability of the cloud services. This study provides a deeper understanding of existing monitoring gaps in cloud platforms, uncover interesting insights and provide guidance for best monitoring practices for ensuring continuous reliability.",Security,Social Order,By aiming to detect and mitigate production incidents early to enhance the reliability of cloud services; the paper is contributing to maintaining 'Social Order' since the services monitored are likely to be used in a social context. This aligns with the 'Social Order' value item under the 'Security' value.,"The paper's focus on early detection and mitigation of production incidents in cloud services directly contributes to the value of 'Social Order' as it ensures the smooth operation and reliability of these services within a social context. By proactively addressing system performance issues, the paper helps maintain a sense of order and stability in the software ecosystem, which is crucial for users relying on these services for various social interactions and activities. Therefore, the alignment with the value item 'Social Order' under the value of 'Security' is evident and relevant in the context of a 'Software User.'",Agreed-Clarified,Agree,,Agree,,
Exp A,Exp H,1368,ESEC/FSE,Software Testing & QA,Test Case Generation for Drivability Requirements of an Automotive Cruise Controller: An Experience with an Industrial Simulator,"Automotive software development requires engineers to test their systems to detect violations of both functional and drivability requirements. Functional requirements define the functionality of the automotive software. Drivability requirements refer to the driver's perception of the interactions with the vehicle; for example, they typically require limiting the acceleration and jerk perceived by the driver within given thresholds. While functional requirements are extensively considered by the research literature, drivability requirements garner less attention.  
This industrial paper describes our experience assessing the usefulness of an automated search-based software testing (SBST) framework in generating failure-revealing test cases for functional and drivability requirements. We report on our experience with the VI-CarRealTime simulator, an industrial virtual modeling and simulation environment widely used in the automotive domain.  
We designed a Cruise Control system in Simulink for a four-wheel vehicle, in an iterative fashion, by producing 21 model versions. We used the SBST framework for each version of the model to search for failure-revealing test cases revealing requirement violations.  
Our results show that the SBST framework successfully identified a failure-revealing test case for 66.7\% of our model versions, requiring, on average, 245.9s and 3.8 iterations. We present lessons learned, reflect on the generality of our results, and discuss how our results improve the state of practice.",Achievement,Intelligent,The paper focuses on improving the effectiveness of the automotive software testing; which can be interpreted as an effort to increase the 'Intelligence' of the system. This aligns with the value item 'Intelligent'; corresponding to the value 'Achievement'.,"The paper's aim to improve the effectiveness of automotive software testing through the use of an automated search-based testing framework demonstrates a pursuit of higher intelligence in this domain. By successfully identifying failure-revealing test cases for a majority of the model versions, the paper contributes to achieving the value of intelligence in software systems, aligning with the value item 'Intelligent' and its corresponding value 'Achievement'.",Agreed-Justified,Agree,,Agree,,
Exp B,Exp H,1368,ESEC/FSE,Software Testing & QA,Test Case Generation for Drivability Requirements of an Automotive Cruise Controller: An Experience with an Industrial Simulator,"Automotive software development requires engineers to test their systems to detect violations of both functional and drivability requirements. Functional requirements define the functionality of the automotive software. Drivability requirements refer to the driver's perception of the interactions with the vehicle; for example, they typically require limiting the acceleration and jerk perceived by the driver within given thresholds. While functional requirements are extensively considered by the research literature, drivability requirements garner less attention.  
This industrial paper describes our experience assessing the usefulness of an automated search-based software testing (SBST) framework in generating failure-revealing test cases for functional and drivability requirements. We report on our experience with the VI-CarRealTime simulator, an industrial virtual modeling and simulation environment widely used in the automotive domain.  
We designed a Cruise Control system in Simulink for a four-wheel vehicle, in an iterative fashion, by producing 21 model versions. We used the SBST framework for each version of the model to search for failure-revealing test cases revealing requirement violations.  
Our results show that the SBST framework successfully identified a failure-revealing test case for 66.7\% of our model versions, requiring, on average, 245.9s and 3.8 iterations. We present lessons learned, reflect on the generality of our results, and discuss how our results improve the state of practice.",Security,Healthy,The paper describes the testing of both functional and drivability requirements; which directly helps in maintaining the healthiness of the system and indirectly; the well-being of the end user. This aligns with the value item 'Healthy'; corresponding to the value 'Security'.,"The justification for aligning 'Paper X' with the value item Healthy and its corresponding value Security is that by testing both functional and drivability requirements, the paper contributes to ensuring the healthiness and well-being of the system, ultimately impacting the end user. This aligns with the value item 'Healthy' in terms of providing security and stability to the software, thereby creating a sense of safety and peace of mind for the software user.",Agreed-Justified,Agree,,Agree,,
Exp C,Exp K,1368,ESEC/FSE,Software Testing & QA,Test Case Generation for Drivability Requirements of an Automotive Cruise Controller: An Experience with an Industrial Simulator,"Automotive software development requires engineers to test their systems to detect violations of both functional and drivability requirements. Functional requirements define the functionality of the automotive software. Drivability requirements refer to the driver's perception of the interactions with the vehicle; for example, they typically require limiting the acceleration and jerk perceived by the driver within given thresholds. While functional requirements are extensively considered by the research literature, drivability requirements garner less attention.  
This industrial paper describes our experience assessing the usefulness of an automated search-based software testing (SBST) framework in generating failure-revealing test cases for functional and drivability requirements. We report on our experience with the VI-CarRealTime simulator, an industrial virtual modeling and simulation environment widely used in the automotive domain.  
We designed a Cruise Control system in Simulink for a four-wheel vehicle, in an iterative fashion, by producing 21 model versions. We used the SBST framework for each version of the model to search for failure-revealing test cases revealing requirement violations.  
Our results show that the SBST framework successfully identified a failure-revealing test case for 66.7\% of our model versions, requiring, on average, 245.9s and 3.8 iterations. We present lessons learned, reflect on the generality of our results, and discuss how our results improve the state of practice.",Security,National Security,The better testing system ensures the security of the software system; which can be considered as a contribution to 'National Security'; in the context that the software is used nationwide in vehicles. This aligns with the value item 'National Security'; corresponding to the value 'Security'.,"Sure, in the context of 'Paper X', the focus of the research is on the development of a software testing framework for automotive systems. By ensuring that the software functions correctly and meets drivability requirements, it contributes to the overall security of the software system. The software being used in vehicles nationwide, its proper functioning and adherence to drivability requirements directly impact the security and safety of both the drivers and the general public. Therefore, aligning the main contributions of 'Paper X' with the value item National Security and its corresponding value Security is justified.",Agreed-Clarified,Agree,,Agree,,
Exp D,Exp K,1369,ESEC/FSE,Software Testing & QA,Prioritizing Natural Language Test Cases Based on Highly-Used Game Features,"Software testing is still a manual activity in many industries, such as the gaming industry. But manually executing tests becomes impractical as the system grows and resources are restricted, mainly in a scenario with short release cycles. Test case prioritization is a commonly used technique to optimize the test execution. However, most prioritization approaches do not work for manual test cases as they require source code information or test execution history, which is often not available in a manual testing scenario. In this paper, we propose a prioritization approach for manual test cases written in natural language based on the tested application features (in particular, highly-used application features). Our approach consists of (1) identifying the tested features from natural language test cases (with zero-shot classification techniques) and (2) prioritizing test cases based on the features that they test. We leveraged the NSGA-II genetic algorithm for the multi-objective optimization of the test case ordering to maximize the coverage of highly-used features while minimizing the cumulative execution time. Our findings show that we can successfully identify the application features covered by test cases using an ensemble of pre-trained models with strong zero-shot capabilities (an F-score of 76.1\%). Also, our prioritization approaches can find test case orderings that cover highly-used application features early in the test execution while keeping the time required to execute test cases short. QA engineers can use our approach to focus the test execution on test cases that cover features that are relevant to users.",Achievement,Intelligent,The paper contributes an approach; which uses pre-trained models for identifying application features in manual test cases; showing the user's intelligence in discerning relevant features for testing.,"In 'Paper X', the proposed approach for prioritizing manual test cases based on the tested application features aligns with the value item Intelligent and its corresponding value Achievement from a ""Software User"" perspective. This alignment is evident as the approach utilizes pre-trained models to intelligently identify the relevant features in test cases, showcasing the user's intelligence in discerning which features to focus on for testing. By prioritizing test cases based on these features, the approach aims to optimize the test execution process, demonstrating an achievement in effectively utilizing resources and maximizing the coverage of important application features.",Agreed-Justified,Agree,,Agree,,
Exp E,Exp J,1369,ESEC/FSE,Software Testing & QA,Prioritizing Natural Language Test Cases Based on Highly-Used Game Features,"Software testing is still a manual activity in many industries, such as the gaming industry. But manually executing tests becomes impractical as the system grows and resources are restricted, mainly in a scenario with short release cycles. Test case prioritization is a commonly used technique to optimize the test execution. However, most prioritization approaches do not work for manual test cases as they require source code information or test execution history, which is often not available in a manual testing scenario. In this paper, we propose a prioritization approach for manual test cases written in natural language based on the tested application features (in particular, highly-used application features). Our approach consists of (1) identifying the tested features from natural language test cases (with zero-shot classification techniques) and (2) prioritizing test cases based on the features that they test. We leveraged the NSGA-II genetic algorithm for the multi-objective optimization of the test case ordering to maximize the coverage of highly-used features while minimizing the cumulative execution time. Our findings show that we can successfully identify the application features covered by test cases using an ensemble of pre-trained models with strong zero-shot capabilities (an F-score of 76.1\%). Also, our prioritization approaches can find test case orderings that cover highly-used application features early in the test execution while keeping the time required to execute test cases short. QA engineers can use our approach to focus the test execution on test cases that cover features that are relevant to users.",Achievement,Capable,The prioritization approach for manual test cases; which is proposed in this paper; enables the users to determine the ordering of test cases effectively that reflects their capacity for executing tasks.,"The prioritization approach proposed in 'Paper X' aligns with the value item Capable and its corresponding value Achievement from a ""Software User"" perspective because it enables users to effectively determine the ordering of test cases based on their ability to execute tasks. By prioritizing test cases that cover highly-used features early in the test execution while keeping the execution time short, the approach empowers users to demonstrate their competence in efficiently testing software and achieving successful outcomes.",Agreed-Clarified,Agree,,Agree,,
Exp F,Exp J,1369,ESEC/FSE,Software Testing & QA,Prioritizing Natural Language Test Cases Based on Highly-Used Game Features,"Software testing is still a manual activity in many industries, such as the gaming industry. But manually executing tests becomes impractical as the system grows and resources are restricted, mainly in a scenario with short release cycles. Test case prioritization is a commonly used technique to optimize the test execution. However, most prioritization approaches do not work for manual test cases as they require source code information or test execution history, which is often not available in a manual testing scenario. In this paper, we propose a prioritization approach for manual test cases written in natural language based on the tested application features (in particular, highly-used application features). Our approach consists of (1) identifying the tested features from natural language test cases (with zero-shot classification techniques) and (2) prioritizing test cases based on the features that they test. We leveraged the NSGA-II genetic algorithm for the multi-objective optimization of the test case ordering to maximize the coverage of highly-used features while minimizing the cumulative execution time. Our findings show that we can successfully identify the application features covered by test cases using an ensemble of pre-trained models with strong zero-shot capabilities (an F-score of 76.1\%). Also, our prioritization approaches can find test case orderings that cover highly-used application features early in the test execution while keeping the time required to execute test cases short. QA engineers can use our approach to focus the test execution on test cases that cover features that are relevant to users.",Achievement,Successful,The test case prioritization approach assists in achieving successful test case orderings that emphasis on highly-used application features; reflecting user success in software testing.,"The test case prioritization approach proposed in 'Paper X' focuses on maximizing the coverage of highly-used application features while minimizing the execution time of test cases. This aligns with the value item Successful and its corresponding value Achievement from a ""Software User"" perspective because it helps users achieve successful test case orderings that prioritize the features that are relevant to them, ultimately leading to a more effective and efficient software testing process.",Agreed-Clarified,Agree,,Agree,,
Exp G,Exp K,1370,ESEC/FSE,AI & Machine Learning,EvoCLINICAL: Evolving Cyber-Cyber Digital Twin with Active Transfer Learning for Automated Cancer Registry System,"The Cancer Registry of Norway (CRN) collects information on cancer patients by receiving cancer messages from different medical entities (e.g., medical labs, hospitals) in Norway. Such messages are validated by an automated cancer registry system: GURI. Its correct operation is crucial since it lays the foundation for cancer research and provides critical cancer-related statistics to its stakeholders. Constructing a cyber-cyber digital twin (CCDT) for GURI can facilitate various experiments and advanced analyses of the operational state of GURI without requiring intensive interactions with the real system. However, GURI constantly evolves due to novel medical diagnostics and treatment, technological advances, etc. Accordingly, CCDT should evolve as well to synchronize with GURI. A key challenge of achieving such synchronization is that evolving CCDT needs abundant data labelled by the new GURI. To tackle this challenge, we propose EvoCLINICAL, which considers the CCDT developed for the previous version of GURI as the pretrained model and fine-tunes it with the dataset labelled by querying a new GURI version. EvoCLINICAL employs a genetic algorithm to select an optimal subset of cancer messages from a candidate dataset and query GURI with it. We evaluate EvoCLINICAL on three evolution processes. The precision, recall, and F1 score are all greater than 91\%, demonstrating the effectiveness of EvoCLINICAL. Furthermore, we replace the active learning part of EvoCLINICAL with random selection to study the contribution of transfer learning to the overall performance of EvoCLINICAL. Results show that employing active learning in EvoCLINICAL increases its performances consistently.",Achievement,Capable,EvoCLINICAL aims to optimize the operation of the cancer registry system for improved results. This can be seen as aiding the software user to be more Capable in their task; which aligns with the value of Achievement.,"In the context of a software user, the alignment between 'Paper X' and the value item Capable, along with its corresponding value Achievement, is justified because EvoCLINICAL, as stated in the abstract, aims to enhance the operation of the cancer registry system. By optimizing the system and improving its performance, software users can achieve more effective and efficient results in their tasks, thus increasing their capability to accomplish their goals. This aligns with the value of Achievement, as it enables users to successfully and competently carry out their responsibilities within the software context.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,1370,ESEC/FSE,AI & Machine Learning,EvoCLINICAL: Evolving Cyber-Cyber Digital Twin with Active Transfer Learning for Automated Cancer Registry System,"The Cancer Registry of Norway (CRN) collects information on cancer patients by receiving cancer messages from different medical entities (e.g., medical labs, hospitals) in Norway. Such messages are validated by an automated cancer registry system: GURI. Its correct operation is crucial since it lays the foundation for cancer research and provides critical cancer-related statistics to its stakeholders. Constructing a cyber-cyber digital twin (CCDT) for GURI can facilitate various experiments and advanced analyses of the operational state of GURI without requiring intensive interactions with the real system. However, GURI constantly evolves due to novel medical diagnostics and treatment, technological advances, etc. Accordingly, CCDT should evolve as well to synchronize with GURI. A key challenge of achieving such synchronization is that evolving CCDT needs abundant data labelled by the new GURI. To tackle this challenge, we propose EvoCLINICAL, which considers the CCDT developed for the previous version of GURI as the pretrained model and fine-tunes it with the dataset labelled by querying a new GURI version. EvoCLINICAL employs a genetic algorithm to select an optimal subset of cancer messages from a candidate dataset and query GURI with it. We evaluate EvoCLINICAL on three evolution processes. The precision, recall, and F1 score are all greater than 91\%, demonstrating the effectiveness of EvoCLINICAL. Furthermore, we replace the active learning part of EvoCLINICAL with random selection to study the contribution of transfer learning to the overall performance of EvoCLINICAL. Results show that employing active learning in EvoCLINICAL increases its performances consistently.",Security,Healthy,EvoCLINICAL contributes to ensuring the healthy functioning of the cancer registry system; which is critical for the making of health policy decisions and thereby contributes to the general Health of the society; aligning with the value of Security.,"EvoCLINICAL contributes to ensuring the healthy functioning of the cancer registry system, which is essential for providing accurate cancer-related statistics and facilitating cancer research. By continuously evolving to synchronize with the latest version of the system, EvoCLINICAL helps maintain the integrity and reliability of the registry, thereby promoting a sense of security for stakeholders. This alignment with the value of security demonstrates the importance of maintaining a secure and trustworthy software system for the well-being and trust of the software users.",Agreed-Justified,Agree,,Agree,,
Exp B,Exp H,1370,ESEC/FSE,AI & Machine Learning,EvoCLINICAL: Evolving Cyber-Cyber Digital Twin with Active Transfer Learning for Automated Cancer Registry System,"The Cancer Registry of Norway (CRN) collects information on cancer patients by receiving cancer messages from different medical entities (e.g., medical labs, hospitals) in Norway. Such messages are validated by an automated cancer registry system: GURI. Its correct operation is crucial since it lays the foundation for cancer research and provides critical cancer-related statistics to its stakeholders. Constructing a cyber-cyber digital twin (CCDT) for GURI can facilitate various experiments and advanced analyses of the operational state of GURI without requiring intensive interactions with the real system. However, GURI constantly evolves due to novel medical diagnostics and treatment, technological advances, etc. Accordingly, CCDT should evolve as well to synchronize with GURI. A key challenge of achieving such synchronization is that evolving CCDT needs abundant data labelled by the new GURI. To tackle this challenge, we propose EvoCLINICAL, which considers the CCDT developed for the previous version of GURI as the pretrained model and fine-tunes it with the dataset labelled by querying a new GURI version. EvoCLINICAL employs a genetic algorithm to select an optimal subset of cancer messages from a candidate dataset and query GURI with it. We evaluate EvoCLINICAL on three evolution processes. The precision, recall, and F1 score are all greater than 91\%, demonstrating the effectiveness of EvoCLINICAL. Furthermore, we replace the active learning part of EvoCLINICAL with random selection to study the contribution of transfer learning to the overall performance of EvoCLINICAL. Results show that employing active learning in EvoCLINICAL increases its performances consistently.",Benevolence,Meaning in Life,By ensuring the correct operation of GURI; EvoCLINICAL allows the software users to make decisions that are enriched with Meaning; thereby; contributing to the value item Meaning in Life under the value Benevolence.,"By constructing a cyber-cyber digital twin (CCDT) for GURI and fine-tuning it with the dataset labelled by querying a new GURI version, EvoCLINICAL ensures the correct operation of the system. This enables software users to have access to accurate and reliable cancer-related statistics, which in turn allows them to make informed decisions regarding cancer research and treatment. By providing this valuable information, EvoCLINICAL contributes to the value item Meaning in Life under the value Benevolence, as it helps users in their pursuit of improving the lives of individuals affected by cancer.",Agreed-Clarified,Agree,,Agree,,
Exp C,Exp K,1375,ESEC/FSE,AI & Machine Learning,Issue Report Validation in an Industrial Context,"Effective issue triaging is crucial for software development teams to improve software quality, and thus customer satisfaction. Validating issue reports manually can be time-consuming, hindering the overall efficiency of the triaging process. This paper presents an approach on automating the validation of issue reports to accelerate the issue triaging process in an industrial set-up. We work on 1,200 randomly selected issue reports in banking domain, written in Turkish, an agglutinative language, meaning that new words can be formed with linear concatenation of suffixes to express entire sentences. We manually label these reports for validity, and extract the relevant patterns indicating that they are invalid. Since the issue reports we work on are written in an agglutinative language, we use morphological analysis to extract the features. Using the proposed feature extractors, we utilize a machine learning based approach to predict the issue reportsaEUR(tm) validity, performing a 0.77 F1-score.",Achievement,Successful,The paper contributes to an approach that automates the validation of issue reports to accelerate the issue triaging process; improving the customer satisfaction by increasing their success in getting their issues resolved; hence aligning with Successful and its corresponding value Achievement.,"The justification for aligning 'Paper X' with the value item Successful and its corresponding value Achievement is based on the fact that the paper presents an approach that automates the validation of issue reports, which improves the efficiency of the issue triaging process. By accelerating this process, it increases the likelihood of resolving customer issues in a timely manner, leading to improved customer satisfaction. This aligns with the value of Achievement as customers can achieve successful issue resolution through the use of the proposed automated validation approach.",Agreed-Clarified,Agree,,Agree,,
Exp D,Exp K,1375,ESEC/FSE,AI & Machine Learning,Issue Report Validation in an Industrial Context,"Effective issue triaging is crucial for software development teams to improve software quality, and thus customer satisfaction. Validating issue reports manually can be time-consuming, hindering the overall efficiency of the triaging process. This paper presents an approach on automating the validation of issue reports to accelerate the issue triaging process in an industrial set-up. We work on 1,200 randomly selected issue reports in banking domain, written in Turkish, an agglutinative language, meaning that new words can be formed with linear concatenation of suffixes to express entire sentences. We manually label these reports for validity, and extract the relevant patterns indicating that they are invalid. Since the issue reports we work on are written in an agglutinative language, we use morphological analysis to extract the features. Using the proposed feature extractors, we utilize a machine learning based approach to predict the issue reportsaEUR(tm) validity, performing a 0.77 F1-score.",Security,Sense of Belonging,By automating the issue triaging process and improving software quality; the paper can contribute to creating a sense of belonging for software users as they feel their issues are heard and addressed. This aligns with the value item Sense of Belonging and its corresponding value Security.,"By automating the issue triaging process and improving software quality, 'Paper X' can contribute to a sense of belonging for software users. This sense of belonging arises from the software users feeling heard and reassured that their issues are being addressed promptly and effectively. This aligns with the value item Sense of Belonging and its corresponding value Security, as software users feel secure and valued within the software context.",Agreed-Justified,Agree,,Agree,,
Exp E,Exp J,1375,ESEC/FSE,AI & Machine Learning,Issue Report Validation in an Industrial Context,"Effective issue triaging is crucial for software development teams to improve software quality, and thus customer satisfaction. Validating issue reports manually can be time-consuming, hindering the overall efficiency of the triaging process. This paper presents an approach on automating the validation of issue reports to accelerate the issue triaging process in an industrial set-up. We work on 1,200 randomly selected issue reports in banking domain, written in Turkish, an agglutinative language, meaning that new words can be formed with linear concatenation of suffixes to express entire sentences. We manually label these reports for validity, and extract the relevant patterns indicating that they are invalid. Since the issue reports we work on are written in an agglutinative language, we use morphological analysis to extract the features. Using the proposed feature extractors, we utilize a machine learning based approach to predict the issue reportsaEUR(tm) validity, performing a 0.77 F1-score.",Universalism,Equality,The automated validation of issue reports ensures an equal treatment of all issues by treating them unbiasedly; providing the users with a sense of Equality; hence aligning with the value of Universalism.,"In 'Paper X', the automated validation of issue reports ensures equality in the treatment of all issues by eliminating bias. This aligns with the value of Universalism from a ""Software User"" perspective as it promotes equal consideration and fair treatment for all users' issues. By automating the validation process, the paper contributes to providing an equal platform for users, enhancing their sense of equality within the software context.",Agreed-Clarified,Agree,,Agree,,
Exp F,Exp J,1377,ESEC/FSE,Security & Privacy,Analyzing Microservice Connectivity with Kubesonde,"Modern cloud-based applications are composed of several microservices that interact over a network. They are complex distributed systems, to the point that developers may not even be aware of how microservices connect to each other and to the Internet. As a consequence, the security of these applications can be greatly compromised. This work explicitly targets this context by providing a methodology to assess microservice connectivity, a software tool that implements it, and findings from analyzing real cloud applications. Specifically, it introduces Kubesonde, a cloud-native software that instruments live applications running on a Kubernetes cluster to analyze microservice connectivity, with minimal impact on performance. An assessment of microservices in 200 popular cloud applications with Kubesonde revealed significant issues in terms of network isolation: more than 60\% of them had discrepancies between their declared and actual connectivity, and none restricted outbound connections towards the Internet. Our analysis shows that Kubesonde offers valuable insights on the connectivity between microservices, beyond what is possible with existing tools.",Power,Authority,The abstract mentions a software tool named Kubesonde that assists in assessing microservice connectivity to better protect cloud-based applications. This act of overseeing and maintaining connectivity aligns with the value item Authority and its corresponding value Power.,"The alignment of 'Paper X' with the value item Authority and its corresponding value Power from a ""Software User"" perspective is justified based on the abstract's mention of the software tool Kubesonde that assists in overseeing and maintaining microservice connectivity in cloud-based applications. This act of having the authority to assess connectivity and ensure its proper functioning aligns with the value of power as it gives the software user the ability to control and manage the connectivity aspect of their applications.",Agreed-Clarified,Agree,,Agree,,
Exp G,Exp K,1377,ESEC/FSE,Security & Privacy,Analyzing Microservice Connectivity with Kubesonde,"Modern cloud-based applications are composed of several microservices that interact over a network. They are complex distributed systems, to the point that developers may not even be aware of how microservices connect to each other and to the Internet. As a consequence, the security of these applications can be greatly compromised. This work explicitly targets this context by providing a methodology to assess microservice connectivity, a software tool that implements it, and findings from analyzing real cloud applications. Specifically, it introduces Kubesonde, a cloud-native software that instruments live applications running on a Kubernetes cluster to analyze microservice connectivity, with minimal impact on performance. An assessment of microservices in 200 popular cloud applications with Kubesonde revealed significant issues in terms of network isolation: more than 60\% of them had discrepancies between their declared and actual connectivity, and none restricted outbound connections towards the Internet. Our analysis shows that Kubesonde offers valuable insights on the connectivity between microservices, beyond what is possible with existing tools.",Security,Healthy,By examining network isolation and identifying potential security issues in cloud-based applications; Kubesonde promotes the health and safety of the software and the user's data therein. This aligns with the value item Healthy and its corresponding value Security.,"The analysis of network isolation and identification of potential security issues in cloud-based applications, as performed by Kubesonde, directly contributes to ensuring the health and safety of the software and the user's data within that software. By accurately assessing microservice connectivity and highlighting discrepancies between declared and actual connectivity, Kubesonde aids in addressing security vulnerabilities and promoting a secure environment for the user, aligning with the value item Healthy and its corresponding value Security.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,1377,ESEC/FSE,Security & Privacy,Analyzing Microservice Connectivity with Kubesonde,"Modern cloud-based applications are composed of several microservices that interact over a network. They are complex distributed systems, to the point that developers may not even be aware of how microservices connect to each other and to the Internet. As a consequence, the security of these applications can be greatly compromised. This work explicitly targets this context by providing a methodology to assess microservice connectivity, a software tool that implements it, and findings from analyzing real cloud applications. Specifically, it introduces Kubesonde, a cloud-native software that instruments live applications running on a Kubernetes cluster to analyze microservice connectivity, with minimal impact on performance. An assessment of microservices in 200 popular cloud applications with Kubesonde revealed significant issues in terms of network isolation: more than 60\% of them had discrepancies between their declared and actual connectivity, and none restricted outbound connections towards the Internet. Our analysis shows that Kubesonde offers valuable insights on the connectivity between microservices, beyond what is possible with existing tools.",Benevolence,Responsibility,The Kubesonde tool analyses the cloud applications and provides valuable insights which can be seen as taking responsibility for security and proper functioning of the software; aligning with the value item Responsibility and the corresponding value Benevolence.,"My justification for labeling 'Paper X' as aligning with the value item Responsibility and its corresponding value Benevolence is based on the fact that the Kubesonde tool, as described in the abstract, assesses the microservice connectivity in cloud applications and identifies discrepancies and issues. By doing so, it highlights the importance of taking responsibility for the security and proper functioning of the software. This aligns with the value item Responsibility, as it emphasizes the duty of the software user to be accountable for the well-being and integrity of the application, and the corresponding value Benevolence, which relates to the desire to act in a helpful and responsible manner.",Agreed-Clarified,Agree,,Agree,,
Exp B,Exp H,1378,ESEC/FSE,Mobile & IoT,Testing Real-World Healthcare IoT Application: Experiences and Lessons Learned,"Healthcare Internet of Things (IoT) applications require rigorous testing to ensure their dependability. Such applications are typically integrated with various third-party healthcare applications and medical devices through REST APIs. This integrated network of healthcare IoT applications leads to REST APIs with complicated and interdependent structures, thus creating a major challenge for automated system-level testing. We report an industrial evaluation of a state-of-the-art REST APIs testing approach (RESTest) on a real-world healthcare IoT application. We analyze the effectiveness of RESTestaEUR(tm)s testing strategies regarding REST APIs failures, faults in the application, and REST API coverage, by experimenting with six REST APIs of 41 API endpoints of the healthcare IoT application. Results show that several failures are discovered in different REST APIs with a%0^56\% coverage using RESTest. Moreover, nine potential faults are identified. Using the evidence collected from the experiments, we provide our experiences and lessons learned.",Security,Healthy,"The paper directly contributes to the development and testing of healthcare IoT applications; which directly aligns with the value item ""Healthy"" under the value group ""Security""; since ensuring the reliability and efficiency of healthcare applications contributes to the health security of the software users.","In 'Paper X,' the main contribution of developing and testing healthcare IoT applications aligns with the value item ""Healthy"" and its corresponding value ""Security"" from a ""Software User"" perspective. By ensuring the dependability and effectiveness of these applications, the paper directly contributes to the security and well-being of software users, as healthcare applications play a crucial role in maintaining the health and safety of individuals.",Agreed-Justified,Agree,,Agree,,
Exp D,Exp K,1378,ESEC/FSE,Mobile & IoT,Testing Real-World Healthcare IoT Application: Experiences and Lessons Learned,"Healthcare Internet of Things (IoT) applications require rigorous testing to ensure their dependability. Such applications are typically integrated with various third-party healthcare applications and medical devices through REST APIs. This integrated network of healthcare IoT applications leads to REST APIs with complicated and interdependent structures, thus creating a major challenge for automated system-level testing. We report an industrial evaluation of a state-of-the-art REST APIs testing approach (RESTest) on a real-world healthcare IoT application. We analyze the effectiveness of RESTestaEUR(tm)s testing strategies regarding REST APIs failures, faults in the application, and REST API coverage, by experimenting with six REST APIs of 41 API endpoints of the healthcare IoT application. Results show that several failures are discovered in different REST APIs with a%0^56\% coverage using RESTest. Moreover, nine potential faults are identified. Using the evidence collected from the experiments, we provide our experiences and lessons learned.",Benevolence,Responsibility,"Paper X has a significant contribution in identifying failures and faults in the healthcare IoT application; this aligns with the value item ""Responsibility"" under the value ""Benevolence""; as discovering and rectifying these faults can be seen as a responsible action towards the software users by ensuring the smooth functioning of the application and by preventing any potential harm caused by these faults.","Certainly! In the context of 'Paper X', aligning with the value item Responsibility and its corresponding value Benevolence from a ""Software User"" perspective, the identification of failures and faults in the healthcare IoT application demonstrates a responsible approach towards ensuring the well-being and safety of the software users. By actively recognizing and addressing these issues, the paper aids in preventing any potential harm or negative impact on the users' experience and overall satisfaction. Thus, the alignment with the value item Responsibility directly reflects the commitment of the authors towards the user's welfare and the responsible handling of software in the healthcare domain.",Agreed-Justified,Agree,,Agree,,
Exp E,Exp J,1379,ESEC/FSE,Data Management & Processing,Diffusion-Based Time Series Data Imputation for Cloud Failure Prediction at Microsoft 365,"Ensuring reliability in large-scale cloud systems like Microsoft 365 is crucial. Cloud failures, such as disk and node failure, threaten service reliability, causing service interruptions and financial loss. Existing works focus on failure prediction and proactively taking action before failures happen. However, they suffer from poor data quality, like data missing in model training and prediction, which limits performance. In this paper, we focus on enhancing data quality through data imputation by the proposed Diffusion+, a sample-efficient diffusion model, to impute the missing data efficiently conditioned on the observed data. Experiments with industrial datasets and application practice show that our model contributes to improving the performance of downstream failure prediction.",Security,Healthy,The paper contributes to developing methods to increase the reliability of cloud services; ensuring them to function correctly and consistently. This aligns with the value item Healthy from the value Security; as the functioning of these systems is critical to the smooth operations of organizations and can indirectly impact the well-being of software users.,"The justification for aligning the paper with the value item Healthy and its corresponding value Security is based on the fact that reliable cloud systems play a crucial role in ensuring the smooth operations of organizations. By developing methods to increase the reliability of these systems, the paper indirectly contributes to the well-being of software users who rely on these systems for their work. Inadequate reliability can lead to service interruptions and financial loss, which can have a negative impact on the users' productivity and overall satisfaction. Thus, by emphasizing the importance of ensuring the healthy functioning of cloud systems, the paper aligns with the value item Healthy and its corresponding value Security.",Agreed-Justified,Agree,,Agree,,
Exp F,Exp J,1379,ESEC/FSE,Data Management & Processing,Diffusion-Based Time Series Data Imputation for Cloud Failure Prediction at Microsoft 365,"Ensuring reliability in large-scale cloud systems like Microsoft 365 is crucial. Cloud failures, such as disk and node failure, threaten service reliability, causing service interruptions and financial loss. Existing works focus on failure prediction and proactively taking action before failures happen. However, they suffer from poor data quality, like data missing in model training and prediction, which limits performance. In this paper, we focus on enhancing data quality through data imputation by the proposed Diffusion+, a sample-efficient diffusion model, to impute the missing data efficiently conditioned on the observed data. Experiments with industrial datasets and application practice show that our model contributes to improving the performance of downstream failure prediction.",Security,Social Order,The paper focuses on improving a system's reliability to prevent interruptions; contributing to the stability of digital and social structures; which corresponds to the value item Social Order of the value Security.,"In ""Paper X,"" the main contribution is enhancing data quality through data imputation to improve the performance of downstream failure prediction in cloud systems. By ensuring reliability and preventing service interruptions, this directly aligns with the value item of Social Order in Schwartz's Taxonomy. Social Order, which is a part of the value of Security, pertains to contributing to the stability of digital and social structures. Therefore, the focus on improving the system's reliability in 'Paper X' directly aligns with the value item of Social Order and its corresponding value of Security from a ""Software User"" perspective.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,1380,ESEC/FSE,Software Development Methodologies,The Most Agile Teams Are the Most Disciplined: On Scaling out Agile Development,"As one of the next frontiers of software engineering, agile development at scale has attracted more and more research interests and efforts. When following the existing autonomy-focused and goal-driven lessons and guidelines to scale agile development for a large astronomy project, however, we encountered surprising tech stack sprawl and spreading team coordination issues. By revisiting the unique features of our project (e.g., the data processing-intensive nature and the frequent team member changes), and by identifying a fractal pattern from various data processing logic and processes, we defined disciplined agile teams to clone the best practices of pioneer agile teams, and to work on similar system modules with similar user stories. Such a targeted strategy effectively relieved the tech stack sprawl and facilitated teamwork handover, at least for refactoring and growing the data processing modules in our project. Based on this emerging result and our reflections, we distinguish this targeted strategy as scaling out agile development from the existing agile scaling approaches that are generally in a scaling-up fashion. Considering the popularity of data processing-intensive projects, and also considering the pervasive fractal patterns in modern businesses and organisations, we claim that this targeted strategy still has broad application opportunities. Therefore, developing a well-defined methodology for scaling out agility, and combining both scaling up and scaling out agility, will deserve attentions and new research efforts in the future.",Achievement,Successful,The paper introduces a new approach to agile development to help handle large-scale astronomy projects. The targeted strategy improved the management and productivity of the teams involved; reflecting Success (v4.3) in terms of project management for the user of the software.,"The justification for aligning 'Paper X' with the value item Successful (v4.3) and its corresponding value Achievement from a ""Software User"" perspective is based on the paper's contribution to improving project management in large-scale astronomy projects. The targeted strategy introduced in the paper specifically addresses the challenges of tech stack sprawl and team coordination issues, resulting in improved management and productivity for the teams involved. This directly aligns with the value of Achievement, as it demonstrates the successful implementation of strategies to achieve project goals and deliver high-quality software to the users.",Agreed-Justified,Agree,,Agree,,
Exp B,Exp H,1380,ESEC/FSE,Software Development Methodologies,The Most Agile Teams Are the Most Disciplined: On Scaling out Agile Development,"As one of the next frontiers of software engineering, agile development at scale has attracted more and more research interests and efforts. When following the existing autonomy-focused and goal-driven lessons and guidelines to scale agile development for a large astronomy project, however, we encountered surprising tech stack sprawl and spreading team coordination issues. By revisiting the unique features of our project (e.g., the data processing-intensive nature and the frequent team member changes), and by identifying a fractal pattern from various data processing logic and processes, we defined disciplined agile teams to clone the best practices of pioneer agile teams, and to work on similar system modules with similar user stories. Such a targeted strategy effectively relieved the tech stack sprawl and facilitated teamwork handover, at least for refactoring and growing the data processing modules in our project. Based on this emerging result and our reflections, we distinguish this targeted strategy as scaling out agile development from the existing agile scaling approaches that are generally in a scaling-up fashion. Considering the popularity of data processing-intensive projects, and also considering the pervasive fractal patterns in modern businesses and organisations, we claim that this targeted strategy still has broad application opportunities. Therefore, developing a well-defined methodology for scaling out agility, and combining both scaling up and scaling out agility, will deserve attentions and new research efforts in the future.",Security,Sense of Belonging,By enabling teams to work on similar modules and handover projects effectively; the paper can be considered to contribute to the Sense of Belonging (v6.6) of software users as it creates a cohesive and unified working environment.,"In the context of the paper, the targeted strategy of disciplined agile teams working on similar system modules with similar user stories creates a sense of belonging for software users by fostering a cohesive and unified working environment. This unified approach allows for effective project handovers and promotes a sense of ownership and shared responsibility among team members. By aligning with the value item Sense of Belonging (v6.6) of Schwartz's Taxonomy, 'Paper X' addresses the need for software users to feel connected to their work and team, enhancing their overall level of security and satisfaction within the software development process.",Agreed-Justified,Disagree,reconcile with coder_2,Disagree,The justification doesn't align with the value item of the Sense of Belonging,
Exp C,Exp K,1380,ESEC/FSE,Software Development Methodologies,The Most Agile Teams Are the Most Disciplined: On Scaling out Agile Development,"As one of the next frontiers of software engineering, agile development at scale has attracted more and more research interests and efforts. When following the existing autonomy-focused and goal-driven lessons and guidelines to scale agile development for a large astronomy project, however, we encountered surprising tech stack sprawl and spreading team coordination issues. By revisiting the unique features of our project (e.g., the data processing-intensive nature and the frequent team member changes), and by identifying a fractal pattern from various data processing logic and processes, we defined disciplined agile teams to clone the best practices of pioneer agile teams, and to work on similar system modules with similar user stories. Such a targeted strategy effectively relieved the tech stack sprawl and facilitated teamwork handover, at least for refactoring and growing the data processing modules in our project. Based on this emerging result and our reflections, we distinguish this targeted strategy as scaling out agile development from the existing agile scaling approaches that are generally in a scaling-up fashion. Considering the popularity of data processing-intensive projects, and also considering the pervasive fractal patterns in modern businesses and organisations, we claim that this targeted strategy still has broad application opportunities. Therefore, developing a well-defined methodology for scaling out agility, and combining both scaling up and scaling out agility, will deserve attentions and new research efforts in the future.",Benevolence,Responsibility,The paper's new method for scaling out agile development requires software user teams to take Responsibility (v9.8) in disciplines and best practices; reflecting the value of Benevolence (v9).,"In the paper, the authors propose a new strategy for scaling out agile development, specifically for data processing-intensive projects. This strategy involves forming disciplined agile teams that clone the best practices of pioneer teams and work on similar system modules with similar user stories. By implementing this strategy, software user teams are taking responsibility (v9.8) for following disciplines and best practices, which aligns with the value of benevolence (v9) as they are working towards the overall well-being and success of the project. This alignment is evident in the abstract as the authors emphasize the need for a well-defined methodology for scaling out agility, combining both scaling up and scaling out approaches, which indicates a focus on responsibility and benevolence in the context of software development.",Agreed-Clarified,Agree,,Agree,,
Exp D,Exp K,1384,ESEC/FSE,Security & Privacy,Lessons from the Long Tail: Analysing Unsafe Dependency Updates across Software Ecosystems,"A risk in adopting third-party dependencies into an application is their potential to serve as a doorway for malicious code to be injected (most often unknowingly). While many initiatives from both industry and research communities focus on the most critical dependencies (i.e., those most depended upon within the ecosystem), little is known about whether the rest of the ecosystem suffers the same fate. Our vision is to promote and establish safer practises throughout the ecosystem. To motivate our vision, in this paper, we present preliminary data based on three representative samples from a population of 88,416 pull requests (PRs) and identify unsafe dependency updates (i.e., any pull request that risks being unsafe during runtime), which clearly shows that unsafe dependency updates are not limited to highly impactful libraries. To draw attention to the long tail, we propose a research agenda comprising six key research questions that further explore how to safeguard against these unsafe activities. This includes developing best practises to address unsafe dependency updates not only in top-tier libraries but throughout the entire ecosystem.",Security,Healthy,"The paper promotes safer software usage practices; thereby enhancing the health and integrity of the software user's digital environment. This aligns with the value item ""Healthy"" and its corresponding value ""Security"".","In the paper, the authors explicitly state their vision to promote and establish safer practices throughout the software ecosystem. They analyze unsafe dependency updates and propose a research agenda to address these activities. By focusing on improving the security of software by addressing potential vulnerabilities and risks, the paper directly aligns with the value item ""Healthy"" and its corresponding value ""Security"" from the perspective of a software user.",Agreed-Clarified,Agree,,Agree,,
Exp E,Exp J,1384,ESEC/FSE,Security & Privacy,Lessons from the Long Tail: Analysing Unsafe Dependency Updates across Software Ecosystems,"A risk in adopting third-party dependencies into an application is their potential to serve as a doorway for malicious code to be injected (most often unknowingly). While many initiatives from both industry and research communities focus on the most critical dependencies (i.e., those most depended upon within the ecosystem), little is known about whether the rest of the ecosystem suffers the same fate. Our vision is to promote and establish safer practises throughout the ecosystem. To motivate our vision, in this paper, we present preliminary data based on three representative samples from a population of 88,416 pull requests (PRs) and identify unsafe dependency updates (i.e., any pull request that risks being unsafe during runtime), which clearly shows that unsafe dependency updates are not limited to highly impactful libraries. To draw attention to the long tail, we propose a research agenda comprising six key research questions that further explore how to safeguard against these unsafe activities. This includes developing best practises to address unsafe dependency updates not only in top-tier libraries but throughout the entire ecosystem.",Security,Social Order,"The paper seeks to curb unsafe dependency updates throughout the software ecosystem; thereby contributing to a sense of Social Order in the digital domain that aligns with the software user's need for ""Security"".","In 'Paper X,' the main contribution is focused on identifying and addressing unsafe dependency updates in the software ecosystem. By targeting these potential security risks, the paper aims to establish safer practices and promote a sense of Social Order in the digital domain. This aligns with the value item of Social Order and its corresponding value of Security from a ""Software User"" perspective. Software users prioritize the security and stability of the systems they interact with, and the paper directly addresses this concern by addressing unsafe dependencies. Therefore, the alignment with Social Order and Security is evident in the paper's explicit focus on curbing unsafe activities and promoting a safer ecosystem for software users.",Agreed-Justified,Agree,,Agree,,
Exp F,Exp J,1384,ESEC/FSE,Security & Privacy,Lessons from the Long Tail: Analysing Unsafe Dependency Updates across Software Ecosystems,"A risk in adopting third-party dependencies into an application is their potential to serve as a doorway for malicious code to be injected (most often unknowingly). While many initiatives from both industry and research communities focus on the most critical dependencies (i.e., those most depended upon within the ecosystem), little is known about whether the rest of the ecosystem suffers the same fate. Our vision is to promote and establish safer practises throughout the ecosystem. To motivate our vision, in this paper, we present preliminary data based on three representative samples from a population of 88,416 pull requests (PRs) and identify unsafe dependency updates (i.e., any pull request that risks being unsafe during runtime), which clearly shows that unsafe dependency updates are not limited to highly impactful libraries. To draw attention to the long tail, we propose a research agenda comprising six key research questions that further explore how to safeguard against these unsafe activities. This includes developing best practises to address unsafe dependency updates not only in top-tier libraries but throughout the entire ecosystem.",Benevolence,Responsibility,"By proposing a research agenda to develop safer practices throughout the ecosystem; the paper contributes a responsibility towards software users in providing a secure environment. This aligns with the value item ""Responsibility"" and its corresponding value ""Benevolence"".","The justification for aligning 'Paper X' with the value item Responsibility and its corresponding value Benevolence is that the paper's proposal to develop safer practices throughout the ecosystem demonstrates a sense of responsibility towards software users by prioritizing their security and well-being. This aligns with the value item Responsibility, which encompasses the values of caring for others and taking actions that promote their welfare.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,1385,ESEC/FSE,AI & Machine Learning,Getting pwnaEUR(tm)d by AI: Penetration Testing with Large Language Models,"The field of software security testing, more specifically penetration testing, requires high levels of expertise and involves many manual testing and analysis steps. This paper explores the potential use of large-language models, such as GPT3.5, to augment penetration testers with AI sparring partners. We explore two distinct use cases: high-level task planning for security testing assignments and low-level vulnerability hunting within a vulnerable virtual machine. For the latter, we implemented a closed-feedback loop between LLM-generated low-level actions with a vulnerable virtual machine (connected through SSH) and allowed the LLM to analyze the machine state for vulnerabilities and suggest concrete attack vectors which were automatically executed within the virtual machine. We discuss promising initial results, detail avenues for improvement, and close deliberating on the ethics of AI sparring partners.",Achievement,Successful,The AI sparring partner in security testing can potentially lead to successful detection of vulnerabilities in the system; aligning with the value item of successful under the value of achievement.,"Sure, let me clarify my justification. In the context of 'Paper X,' the use of AI sparring partners can enhance penetration testing and lead to the successful detection of vulnerabilities in software systems. This aligns with the value item of ""Successful"" under the value of ""Achievement"" because by employing AI sparring partners, software users can achieve the desired outcome of identifying vulnerabilities and ensuring the security of their systems. This aligns with the values and aspirations of software users who want to achieve successful outcomes in terms of the security and reliability of their software applications.",Agreed-Justified,Agree,,Agree,,
Exp B,Exp H,1385,ESEC/FSE,AI & Machine Learning,Getting pwnaEUR(tm)d by AI: Penetration Testing with Large Language Models,"The field of software security testing, more specifically penetration testing, requires high levels of expertise and involves many manual testing and analysis steps. This paper explores the potential use of large-language models, such as GPT3.5, to augment penetration testers with AI sparring partners. We explore two distinct use cases: high-level task planning for security testing assignments and low-level vulnerability hunting within a vulnerable virtual machine. For the latter, we implemented a closed-feedback loop between LLM-generated low-level actions with a vulnerable virtual machine (connected through SSH) and allowed the LLM to analyze the machine state for vulnerabilities and suggest concrete attack vectors which were automatically executed within the virtual machine. We discuss promising initial results, detail avenues for improvement, and close deliberating on the ethics of AI sparring partners.",Security,Healthy,Through the detection and possible prevention of security threats; the software user's data and system health can be maintained; hence aligning with the value item of Health under the value of Security.,"The justification for aligning 'Paper X' with the value item Healthy and its corresponding value Security from a ""Software User"" perspective is that the implementation of AI sparring partners in software security testing can help detect and prevent security threats. By doing so, it ensures the protection and well-being of the software user's data and system, thereby promoting their overall health and aligning with the value of Security.",Agreed-Justified,Agree,,Agree,,
Exp C,Exp K,1387,ESEC/FSE,AI & Machine Learning,Exploring Moral Principles Exhibited in OSS: A Case Study on GitHub Heated Issues,"To foster collaboration and inclusivity in Open Source Software (OSS) projects, it is crucial to understand and detect patterns of toxic language that may drive contributors away, especially those from underrepresented communities. Although machine learning-based toxicity detection tools trained on domain-specific data have shown promise, their design lacks an understanding of the unique nature and triggers of toxicity in OSS discussions, highlighting the need for further investigation. In this study, we employ Moral Foundations Theory to examine the relationship between moral principles and toxicity in OSS. Specifically, we analyze toxic communications in GitHub issue threads to identify and understand five types of moral principles exhibited in text, and explore their potential association with toxic behavior. Our preliminary findings suggest a possible link between moral principles and toxic comments in OSS communications, with each moral principle associated with at least one type of toxicity. The potential of MFT in toxicity detection warrants further investigation.",Benevolence,Forgiving,The paper's main aim of trying to eliminate toxic behaviors in online software discussions directly aligns with value item Forgiving from the Benevolence value as it promotes positive interactions among users.,"My justification for aligning 'Paper X' with the value item Forgiving and its corresponding value Benevolence is based on the fact that the paper focuses on understanding and detecting toxic language in online software discussions, with the aim of fostering collaboration and inclusivity. By addressing toxic behaviors, the paper promotes a forgiving and understanding attitude towards the mistakes or negative interactions that may occur in these discussions. This aligns with the value of Benevolence, as it emphasizes the importance of fostering positive interactions and creating a supportive environment for software users.",Agreed-Justified,Disagree,it is mostly about collaborating and should be in universalism category,Disagree,Agree with coder_1,
Exp D,Exp K,1387,ESEC/FSE,AI & Machine Learning,Exploring Moral Principles Exhibited in OSS: A Case Study on GitHub Heated Issues,"To foster collaboration and inclusivity in Open Source Software (OSS) projects, it is crucial to understand and detect patterns of toxic language that may drive contributors away, especially those from underrepresented communities. Although machine learning-based toxicity detection tools trained on domain-specific data have shown promise, their design lacks an understanding of the unique nature and triggers of toxicity in OSS discussions, highlighting the need for further investigation. In this study, we employ Moral Foundations Theory to examine the relationship between moral principles and toxicity in OSS. Specifically, we analyze toxic communications in GitHub issue threads to identify and understand five types of moral principles exhibited in text, and explore their potential association with toxic behavior. Our preliminary findings suggest a possible link between moral principles and toxic comments in OSS communications, with each moral principle associated with at least one type of toxicity. The potential of MFT in toxicity detection warrants further investigation.",Conformity,Self-Discipline,The paper‚Äö√Ñ√∂‚àö√ë‚àö¬•s main contribution in detecting and avoiding toxic behaviours in online discussions aligns with value item Self-Discipline within the Conformity value as the understanding and avoiding toxic language are indicative of self control.,"I apologize for any confusion. My justification for aligning 'Paper X' with the value item Self-Discipline and its corresponding value Conformity is based on the fact that the paper's main contribution involves detecting and avoiding toxic behaviors in online discussions. This aligns with Self-Discipline because understanding and avoiding toxic language require self-control and the adherence to social norms or rules. By recognizing and addressing toxic behaviors, individuals are exercising self-discipline to promote a more positive and inclusive environment in OSS projects. This alignment emphasizes the importance of self-control and conformity to create a respectful and collaborative software user community.",Agreed-Justified,Agree,,Agree,,
Exp E,Exp J,1387,ESEC/FSE,AI & Machine Learning,Exploring Moral Principles Exhibited in OSS: A Case Study on GitHub Heated Issues,"To foster collaboration and inclusivity in Open Source Software (OSS) projects, it is crucial to understand and detect patterns of toxic language that may drive contributors away, especially those from underrepresented communities. Although machine learning-based toxicity detection tools trained on domain-specific data have shown promise, their design lacks an understanding of the unique nature and triggers of toxicity in OSS discussions, highlighting the need for further investigation. In this study, we employ Moral Foundations Theory to examine the relationship between moral principles and toxicity in OSS. Specifically, we analyze toxic communications in GitHub issue threads to identify and understand five types of moral principles exhibited in text, and explore their potential association with toxic behavior. Our preliminary findings suggest a possible link between moral principles and toxic comments in OSS communications, with each moral principle associated with at least one type of toxicity. The potential of MFT in toxicity detection warrants further investigation.",Universalism,Social Justice,The paper‚Äö√Ñ√∂‚àö√ë‚àö¬•s overall objective is to promote a just and inclusive conversation environment; aligning it with the Social Justice value item within the Universalism value.,"The main contribution of 'Paper X' is aligned with the value item of Social Justice and its corresponding value of Universalism from a ""Software User"" perspective because the paper aims to foster collaboration and inclusivity in Open Source Software projects by detecting and understanding patterns of toxic language that may drive contributors, especially those from underrepresented communities, away. By examining the relationship between moral principles and toxicity in OSS, the paper seeks to promote a just and inclusive conversation environment, which aligns with the principles of Social Justice and Universalism that emphasize equality, unity, and fairness for all individuals.",Agreed-Clarified,Agree,,Agree,,
Exp F,Exp J,1388,ESEC/FSE,Software Engineering Practices,Towards Understanding Emotions in Informal Developer Interactions: A Gitter Chat Study,"Emotions play a significant role in teamwork and collaborative activities like software development. While researchers have analyzed developer emotions in various software artifacts (e.g., issues, pull requests), few studies have focused on understanding the broad spectrum of emotions expressed in chats. As one of the most widely used means of communication, chats contain valuable information in the form of informal conversations, such as negative perspectives about adopting a tool. In this paper, we present a dataset of developer chat messages manually annotated with a wide range of emotion labels (and sub-labels), and analyze the type of information present in those messages. We also investigate the unique signals of emotions specific to chats and distinguish them from other forms of software communication. Our findings suggest that chats have fewer expressions of Approval and Fear but more expressions of Curiosity compared to GitHub comments. We also notice that Confusion is frequently observed when discussing programming-related information such as unexpected software behavior. Overall, our study highlights the potential of mining emotions in developer chats for supporting software maintenance and evolution tools.",Self Direction,Curiosity,The paper investigates chat messages from developers and found a curiosity to understand the spectrum of emotions expressed in chats; which aligns with the value item Curiosity in the Schwartz's Taxonomy.,"The justification for aligning 'Paper X' with the value item Curiosity and its corresponding value Self Direction from a ""Software User"" perspective is based on the fact that the paper explicitly states that it aims to understand the broad spectrum of emotions expressed in chats. This investigation showcases the curiosity of users to explore and comprehend emotions in this form of communication. By analyzing developer chat messages and identifying unique signals of emotions specific to chats, the paper directly aligns with the value item Curiosity, which emphasizes the value of exploring new experiences and seeking knowledge, and the corresponding value Self Direction, which pertains to individuals choosing their own goals and expressing independent thinking. This alignment highlights the relevance of the paper for software users who are interested in understanding and leveraging emotions in chat conversations within the context of software maintenance and evolution.",Agreed-Clarified,Agree,,Agree,,
Exp G,Exp K,1388,ESEC/FSE,Software Engineering Practices,Towards Understanding Emotions in Informal Developer Interactions: A Gitter Chat Study,"Emotions play a significant role in teamwork and collaborative activities like software development. While researchers have analyzed developer emotions in various software artifacts (e.g., issues, pull requests), few studies have focused on understanding the broad spectrum of emotions expressed in chats. As one of the most widely used means of communication, chats contain valuable information in the form of informal conversations, such as negative perspectives about adopting a tool. In this paper, we present a dataset of developer chat messages manually annotated with a wide range of emotion labels (and sub-labels), and analyze the type of information present in those messages. We also investigate the unique signals of emotions specific to chats and distinguish them from other forms of software communication. Our findings suggest that chats have fewer expressions of Approval and Fear but more expressions of Curiosity compared to GitHub comments. We also notice that Confusion is frequently observed when discussing programming-related information such as unexpected software behavior. Overall, our study highlights the potential of mining emotions in developer chats for supporting software maintenance and evolution tools.",Stimulation,Variation in Life,This research brings a variation in the type of information used in software development; especially chat messages of developers. This aligns with the Schwartz's Taxonomy value item of Variation in Life.,"In the abstract of 'Paper X', it is explicitly mentioned that the research focuses on understanding emotions expressed in developer chats, which are a widely used means of communication in software development. By introducing the analysis of emotions specifically in chat messages, the paper brings a variation in the type of information utilized in software development processes. This aligns with the value item of Variation in Life from Schwartz's Taxonomy, as it introduces a new aspect of communication and information exchange in the software development context, providing users with a different experience and perspective in their software-related interactions. Hence, the alignment with Variation in Life and its corresponding value Stimulation is supported by the novelty and diversity introduced in the form of chat messages in software development.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,1388,ESEC/FSE,Software Engineering Practices,Towards Understanding Emotions in Informal Developer Interactions: A Gitter Chat Study,"Emotions play a significant role in teamwork and collaborative activities like software development. While researchers have analyzed developer emotions in various software artifacts (e.g., issues, pull requests), few studies have focused on understanding the broad spectrum of emotions expressed in chats. As one of the most widely used means of communication, chats contain valuable information in the form of informal conversations, such as negative perspectives about adopting a tool. In this paper, we present a dataset of developer chat messages manually annotated with a wide range of emotion labels (and sub-labels), and analyze the type of information present in those messages. We also investigate the unique signals of emotions specific to chats and distinguish them from other forms of software communication. Our findings suggest that chats have fewer expressions of Approval and Fear but more expressions of Curiosity compared to GitHub comments. We also notice that Confusion is frequently observed when discussing programming-related information such as unexpected software behavior. Overall, our study highlights the potential of mining emotions in developer chats for supporting software maintenance and evolution tools.",Security,Social Order,By analyzing developer chat messages that include negative perspectives about adopting a tool; the study supports maintaining social order within the developer community. Therefore; it aligns with the Schwartz's Taxonomy value item of Social Order.,"In 'Paper X', the analysis of developer chat messages provides insights into negative perspectives about adopting a tool, which can contribute to maintaining social order within the developer community. By understanding and addressing these negative perspectives, software users can ensure a harmonious and organized development environment, promoting cooperation and stability. This alignment with the value item of Social Order from Schwartz's Taxonomy highlights the relevance of 'Paper X' in addressing the software user's need for a secure and well-regulated software ecosystem.",Agreed-Clarified,Agree,,Disagree,"The justification doesn't align with the value item of the Social Order
",
Exp B,Exp H,1393,ESEC/FSE,AI & Machine Learning,Deeper Notions of Correctness in Image-Based DNNs: Lifting Properties from Pixel to Entities,"Deep Neural Networks (DNNs) that process images are being widely used for many safety-critical tasks, from autonomous vehicles to medical diagnosis. Currently, DNN correctness properties are defined at the pixel level over the entire input. Such properties are useful to expose system failures related to sensor noise or adversarial attacks, but they cannot capture features that are relevant to domain-specific entities and reflect richer types of behaviors. To overcome this limitation, we envision the specification of properties based on the entities that may be present in image input, capturing their semantics and how they change. Creating such properties today is difficult as it requires determining where the entities appear in images, defining how each entity can change, and writing a specification that is compatible with each particular V&amp;V client. We introduce an initial framework structured around those challenges to assist in the generation of Domain-specific Entity-based properties automatically by leveraging object detection models to identify entities in images and creating properties based on entity features. Our feasibility study provides initial evidence that the new properties can uncover interesting system failures, such as changes in skin color can modify the output of a gender classification network. We conclude by analyzing the framework potential to implement the vision and by outlining directions for future work.",Achievement,Intelligent,"The paper contributes a framework that identifies failures in DNNs; enabling improvements and learning for the software users; aligning with the value item ""Intelligent"" and its corresponding value ""Achievement"".","The justification for aligning 'Paper X' with the value item Intelligent and its corresponding value Achievement from a ""Software User"" perspective is that the framework proposed in the paper allows for the identification of system failures in DNNs, which leads to improvements and learning for the users of the software. By addressing the limitations of current pixel-level properties and capturing richer types of behaviors related to domain-specific entities, the paper enables software users to achieve intelligent outcomes by enhancing the correctness and reliability of DNNs in safety-critical tasks such as autonomous vehicles and medical diagnosis.",Agreed-Justified,Agree,,Agree,,
Exp C,Exp K,1393,ESEC/FSE,AI & Machine Learning,Deeper Notions of Correctness in Image-Based DNNs: Lifting Properties from Pixel to Entities,"Deep Neural Networks (DNNs) that process images are being widely used for many safety-critical tasks, from autonomous vehicles to medical diagnosis. Currently, DNN correctness properties are defined at the pixel level over the entire input. Such properties are useful to expose system failures related to sensor noise or adversarial attacks, but they cannot capture features that are relevant to domain-specific entities and reflect richer types of behaviors. To overcome this limitation, we envision the specification of properties based on the entities that may be present in image input, capturing their semantics and how they change. Creating such properties today is difficult as it requires determining where the entities appear in images, defining how each entity can change, and writing a specification that is compatible with each particular V&amp;V client. We introduce an initial framework structured around those challenges to assist in the generation of Domain-specific Entity-based properties automatically by leveraging object detection models to identify entities in images and creating properties based on entity features. Our feasibility study provides initial evidence that the new properties can uncover interesting system failures, such as changes in skin color can modify the output of a gender classification network. We conclude by analyzing the framework potential to implement the vision and by outlining directions for future work.",Security,Cleanliness,"The identification of system failures in DNNs in the paper helps maintain a clean software environment for users; aligning with the value item ""Cleanliness"" and its corresponding value ""Security"".","In the context of the ""Software User"" perspective, the alignment of 'Paper X' with the value item Cleanliness and its corresponding value Security can be justified by the fact that the paper's focus on detecting system failures in DNNs contributes to maintaining a clean software environment for users. By identifying potential issues related to sensor noise or adversarial attacks, the paper enables the development of more reliable and secure software systems, which aligns with the value of Security and indirectly promotes a clean and stable user experience.",Agreed-Clarified,Agree,,Agree,,
Exp E,Exp J,1394,ESEC/FSE,Mobile & IoT,LazyCow: A Lightweight Crowdsourced Testing Tool for Taming Android Fragmentation,"Android fragmentation refers to the increasing variety of Android devices and operating system versions. Their number make it impossible to test an app on every supported device, resulting in many device compatibility issues and leading to poor user experiences. To mitigate this, a number of works that automatically detect compatibility issues have been proposed. However, current state-of-the-art techniques can only be used to detect specific types of compatibility issues (i.e., compatibility issues caused by API signature evolution), i.e., many other essential categories of compatibility issues are still unknown. For instance, customised OS versions on real devices and semantic OS modifications could result in severe compatibility issues that are difficult to detect statically. In order to address this research gap and facilitate the prospect of taming Android frag- mentation through crowdsourced efforts, we propose LazyCow, a novel, lightweight, crowdsourced testing tool. Our experimental results involving thousands of test cases on real Android devices demonstrate that LazyCow is effective at autonomously identifying and validating API-induced compatibility issues. The source code of both client side and server side are all made publicly available in our artifact package. A demo video of our tool is available at https://www.youtube.com/watch?v=_xzWv_mo5xQ.",Achievement,Intelligent,Paper X contributes to the development of LazyCow; a software testing tool; which aids in resolving compatibility issues in Android applications. This improves the overall performance of these apps; helping software users feel more Intelligent and capable while using them. This aligns with the value item Intelligent and its corresponding value Achievement.,"Certainly! My justification aligns with the value item Intelligent and its corresponding value Achievement because the main contribution of 'Paper X', LazyCow, is a software testing tool that helps resolve compatibility issues in Android applications. By improving the performance and compatibility of these apps, software users are able to use them more effectively and feel a sense of accomplishment and success in their ability to navigate and utilize complex software systems. This directly aligns with the value item Intelligent, as users feel more knowledgeable and capable while using these applications.",Agreed-Justified,Agree,,Agree,,
Exp F,Exp J,1394,ESEC/FSE,Mobile & IoT,LazyCow: A Lightweight Crowdsourced Testing Tool for Taming Android Fragmentation,"Android fragmentation refers to the increasing variety of Android devices and operating system versions. Their number make it impossible to test an app on every supported device, resulting in many device compatibility issues and leading to poor user experiences. To mitigate this, a number of works that automatically detect compatibility issues have been proposed. However, current state-of-the-art techniques can only be used to detect specific types of compatibility issues (i.e., compatibility issues caused by API signature evolution), i.e., many other essential categories of compatibility issues are still unknown. For instance, customised OS versions on real devices and semantic OS modifications could result in severe compatibility issues that are difficult to detect statically. In order to address this research gap and facilitate the prospect of taming Android frag- mentation through crowdsourced efforts, we propose LazyCow, a novel, lightweight, crowdsourced testing tool. Our experimental results involving thousands of test cases on real Android devices demonstrate that LazyCow is effective at autonomously identifying and validating API-induced compatibility issues. The source code of both client side and server side are all made publicly available in our artifact package. A demo video of our tool is available at https://www.youtube.com/watch?v=_xzWv_mo5xQ.",Security,Healthy,By resolving compatibility issues in Android applications; Paper X indirectly contributes to the safety of software users. This aligns with the value item Healthy and its corresponding value Security.,"By resolving compatibility issues in Android applications, 'Paper X' ensures that the software functions properly and does not pose any threats or risks to the users' devices. This directly aligns with the value item Healthy and its corresponding value Security because it contributes to the overall safety and protection of the software users' personal information and device integrity, ensuring a secure and reliable software experience.",Agreed-Justified,Agree,,Agree,,
Exp G,Exp K,1394,ESEC/FSE,Mobile & IoT,LazyCow: A Lightweight Crowdsourced Testing Tool for Taming Android Fragmentation,"Android fragmentation refers to the increasing variety of Android devices and operating system versions. Their number make it impossible to test an app on every supported device, resulting in many device compatibility issues and leading to poor user experiences. To mitigate this, a number of works that automatically detect compatibility issues have been proposed. However, current state-of-the-art techniques can only be used to detect specific types of compatibility issues (i.e., compatibility issues caused by API signature evolution), i.e., many other essential categories of compatibility issues are still unknown. For instance, customised OS versions on real devices and semantic OS modifications could result in severe compatibility issues that are difficult to detect statically. In order to address this research gap and facilitate the prospect of taming Android frag- mentation through crowdsourced efforts, we propose LazyCow, a novel, lightweight, crowdsourced testing tool. Our experimental results involving thousands of test cases on real Android devices demonstrate that LazyCow is effective at autonomously identifying and validating API-induced compatibility issues. The source code of both client side and server side are all made publicly available in our artifact package. A demo video of our tool is available at https://www.youtube.com/watch?v=_xzWv_mo5xQ.",Universalism,Protecting the Environment,Through the development of LazyCow; Paper X indirectly promotes the protection of the digital environment by enhancing the compatibility of various Android applications and thus reducing the potentially negative impact of fragmented Android systems. This aligns with the value item Protecting the Environment and its corresponding value Universalism.,"LazyCow, as described in 'Paper X', addresses the challenge of Android fragmentation by automatically detecting compatibility issues and improving user experiences. By ensuring the compatibility and smooth functioning of Android apps across different devices and operating system versions, LazyCow indirectly contributes to the protection of the digital environment. Fragmentation can lead to poor user experiences, resulting in the abandonment of apps and increased resource consumption due to redundant development efforts. Through its focus on effective testing and identification of compatibility issues, LazyCow aligns with the value item Protecting the Environment and its corresponding value Universalism by promoting a more efficient and sustainable software ecosystem that reduces unnecessary waste and promotes broader access and usability.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,1397,ESEC/FSE,AI & Machine Learning,On Using Information Retrieval to Recommend Machine Learning Good Practices for Software Engineers,"Machine learning (ML) is nowadays widely used for different purposes and with several disciplines. From self-driving cars to automated medical diagnosis, machine learning models extensively support usersaEUR(tm) daily activities, and software engineering tasks are no exception. Not embracing good ML practices may lead to pitfalls that hinder the performance of an ML system and potentially lead to unexpected results. Despite the existence of documentation and literature about ML best practices, many non-ML experts turn towards gray literature like blogs and Q&amp;A systems when looking for help and guidance when implementing ML systems. To better aid users in distilling relevant knowledge from such sources, we propose a recommender system that recommends ML practices based on the useraEUR(tm)s context. As a first step in creating a recommender system for machine learning practices, we implemented Idaka. A tool that provides two different approaches for retrieving/generating ML best practices: i) an information retrieval (IR) engine and ii) a large language model. The IR-engine uses BM25 as the algorithm for retrieving the practices, and a large language model, in our case Alpaca. The platform has been designed to allow comparative studies of best practices retrieval tools. Idaka is publicly available at  GitHub: https://bit.ly/idaka. Video: https://youtu.be/cEb-AhIPxnM",Achievement,Intelligent,The recommender system proposed in 'Paper X' can provide intelligent recommendations to software users based on their specific context. This directly aligns with the value item 'Intelligent' and its corresponding value 'Achievement'.,"In 'Paper X', the recommender system aims to provide intelligent recommendations to software users based on their specific context. This aligns with the value item 'Intelligent' and its corresponding value 'Achievement' as it shows the capability of the system to analyze and understand the user's needs and provide recommendations that enable the user to achieve their goals effectively. This aligns with the perspective of a software user who seeks intelligent solutions that can enhance their software experience and lead to successful outcomes.",Agreed-Clarified,Agree,,Agree,,
Exp B,Exp H,1397,ESEC/FSE,AI & Machine Learning,On Using Information Retrieval to Recommend Machine Learning Good Practices for Software Engineers,"Machine learning (ML) is nowadays widely used for different purposes and with several disciplines. From self-driving cars to automated medical diagnosis, machine learning models extensively support usersaEUR(tm) daily activities, and software engineering tasks are no exception. Not embracing good ML practices may lead to pitfalls that hinder the performance of an ML system and potentially lead to unexpected results. Despite the existence of documentation and literature about ML best practices, many non-ML experts turn towards gray literature like blogs and Q&amp;A systems when looking for help and guidance when implementing ML systems. To better aid users in distilling relevant knowledge from such sources, we propose a recommender system that recommends ML practices based on the useraEUR(tm)s context. As a first step in creating a recommender system for machine learning practices, we implemented Idaka. A tool that provides two different approaches for retrieving/generating ML best practices: i) an information retrieval (IR) engine and ii) a large language model. The IR-engine uses BM25 as the algorithm for retrieving the practices, and a large language model, in our case Alpaca. The platform has been designed to allow comparative studies of best practices retrieval tools. Idaka is publicly available at  GitHub: https://bit.ly/idaka. Video: https://youtu.be/cEb-AhIPxnM",Stimulation,Excitement in Life,The Idaka tool; as a contribution of 'Paper X'; is designed to bring excitement to software users by recommending the best machine learning practices. This directly aligns with 'Excitement in Life' under the value 'Stimulation'.,"In the paper abstract, 'Paper X' introduces the Idaka tool, which is designed to recommend machine learning practices to software users. This recommendation system aims to aid users in distilling relevant knowledge from gray literature sources, providing guidance and help in implementing machine learning systems. By aligning with the value item 'Excitement in Life' and its corresponding value 'Stimulation', the Idaka tool brings excitement to software users by providing them with new and innovative approaches to enhance their machine learning systems. This aligns directly with the user's desire for stimulation and the excitement of exploring new practices and possibilities in software engineering.",Agreed-Clarified,Agree,,Agree,,
Exp D,Exp K,1398,ESEC/FSE,Software Testing & QA,Helion: Enabling Natural Testing of Smart Homes,"Prior work has developed numerous systems that test the security and safety of smart homes. For these systems to be applicable in practice, it is necessary to test them with realistic scenarios that represent the use of the smart home, i.e., home automation, in the wild. This demo paper presents the technical details and usage of Helion, a system that uses n-gram language modeling to learn the regularities in user-driven programs, i.e., routines developed for the smart home, and predicts natural scenarios of home automation, i.e., event sequences that reflect realistic home automation usage. We demonstrate the HelionHA platform, developed by integrating Helion with the popular Home Assistant smart home platform. HelionHA allows an end-to-end exploration of HelionaEUR(tm)s scenarios by executing them as test cases with real and virtual smart home devices.",Power,Authority,The main contribution of Paper X is Helion; a system that tests the security and safety of smart homes. This aligns with the 'Authority' value item; as it allows the software users to exert control over their smart homes and ensure their safety and security.,"In 'Paper X', the main contribution of the Helion system aligns with the value item Authority and its corresponding value Power from a ""Software User"" perspective because it empowers software users to have control and authority over their smart homes. With Helion, users can test the security and safety of their smart homes, ensuring that they have the authority to determine how their homes function and are protected. This aligns with the value of Power, which encompasses the desire for social recognition, authority, and control. By allowing users to exert authority over their smart homes, Helion enables them to exercise power and ensure the safety and security of their living environments.",Agreed-Justified,Agree,,Agree,,
Exp E,Exp J,1398,ESEC/FSE,Software Testing & QA,Helion: Enabling Natural Testing of Smart Homes,"Prior work has developed numerous systems that test the security and safety of smart homes. For these systems to be applicable in practice, it is necessary to test them with realistic scenarios that represent the use of the smart home, i.e., home automation, in the wild. This demo paper presents the technical details and usage of Helion, a system that uses n-gram language modeling to learn the regularities in user-driven programs, i.e., routines developed for the smart home, and predicts natural scenarios of home automation, i.e., event sequences that reflect realistic home automation usage. We demonstrate the HelionHA platform, developed by integrating Helion with the popular Home Assistant smart home platform. HelionHA allows an end-to-end exploration of HelionaEUR(tm)s scenarios by executing them as test cases with real and virtual smart home devices.",Security,Healthy,Helion aims to ensure the safety of smart homes as demonstrated in the abstract. This aligns with 'Healthy' as the security and safety of one's home contributes to the overall well-being of the software user.,"In the abstract of 'Paper X', it explicitly states that the system, Helion, aims to test the security and safety of smart homes. As security and safety are crucial aspects for the well-being and peace of mind of software users, aligning this contribution with the value item 'Healthy' and its corresponding value 'Security' makes sense. By ensuring the security of one's home, users can feel safer and more in control, leading to a healthier and more secure living environment.",Agreed-Justified,Agree,,Agree,,
Exp F,Exp J,1398,ESEC/FSE,Software Testing & QA,Helion: Enabling Natural Testing of Smart Homes,"Prior work has developed numerous systems that test the security and safety of smart homes. For these systems to be applicable in practice, it is necessary to test them with realistic scenarios that represent the use of the smart home, i.e., home automation, in the wild. This demo paper presents the technical details and usage of Helion, a system that uses n-gram language modeling to learn the regularities in user-driven programs, i.e., routines developed for the smart home, and predicts natural scenarios of home automation, i.e., event sequences that reflect realistic home automation usage. We demonstrate the HelionHA platform, developed by integrating Helion with the popular Home Assistant smart home platform. HelionHA allows an end-to-end exploration of HelionaEUR(tm)s scenarios by executing them as test cases with real and virtual smart home devices.",Security,Family Security,By ensuring the security of smart homes; Helion contributes to the 'Family Security;' as smart homes commonly house families. This software user-specific alignment directly relates to the well-being of the family in the user's safe environment.,"By ensuring the security of smart homes, Helion contributes to the 'Family Security' value item and its corresponding value 'Security.' This alignment is significant for a software user because it directly relates to the well-being and safety of the family within their home environment. Smart homes often accommodate families, and by providing a system that tests the security and safety of these homes, Helion helps protect them from potential threats or risks, ensuring a secure and protected environment for the software user's family.",Agreed-Clarified,Agree,,Agree,,
Exp G,Exp K,1404,ESEC/FSE,Mobile & IoT,D2S2: Drag aEUR(tm)naEUR(tm) Drop Mobile App Screen Search,"The lack of diverse UI element representations in publicly available datasets hinders the scalability of sketch-based interactive mobile search. This paper introduces D2S2, a novel approach that addresses this limitation via drag-and-drop mobile screen search, accommodating visual and text-based queries. D2S2 searches 58k Rico screens for relevant UI examples based on UI element attributes, including type, position, shape, and text. In an evaluation with 10 novice software developers D2S2 successfully retrieves target screens within the top-20 search results in 15/19 attempts within a minute. The tool offers interactive and iterative search, updating its search results each time the user modifies the search query. Interested users can freely access D2S2 (http://pixeltoapp.com/D2S2), build on D2S2 or replicate results via D2S2aEUR(tm)s open-source implementation (https://github.com/toni-tang/D2S2), or watch D2S2aEUR(tm)s video demonstration (https://youtu.be/fdoYiw8lAn0).",Stimulation,Excitement in Life,This tool; D2S2; offers an exciting new approach to mobile screen search by accommodating both visual and text-based queries which could make the search experience more exciting for the software users.,"The justification for aligning 'Paper X' with the value item Excitement in Life and its corresponding value Stimulation is based on the fact that the paper introduces a novel approach, D2S2, which offers a more interactive and iterative search experience for software users. By accommodating both visual and text-based queries, D2S2 expands the possibilities of mobile screen search and potentially makes the process more engaging and enjoyable for users. This aligns with the value item Excitement in Life, as it introduces a new and exciting way of conducting mobile searches that can stimulate user curiosity and interest.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,1404,ESEC/FSE,Mobile & IoT,D2S2: Drag aEUR(tm)naEUR(tm) Drop Mobile App Screen Search,"The lack of diverse UI element representations in publicly available datasets hinders the scalability of sketch-based interactive mobile search. This paper introduces D2S2, a novel approach that addresses this limitation via drag-and-drop mobile screen search, accommodating visual and text-based queries. D2S2 searches 58k Rico screens for relevant UI examples based on UI element attributes, including type, position, shape, and text. In an evaluation with 10 novice software developers D2S2 successfully retrieves target screens within the top-20 search results in 15/19 attempts within a minute. The tool offers interactive and iterative search, updating its search results each time the user modifies the search query. Interested users can freely access D2S2 (http://pixeltoapp.com/D2S2), build on D2S2 or replicate results via D2S2aEUR(tm)s open-source implementation (https://github.com/toni-tang/D2S2), or watch D2S2aEUR(tm)s video demonstration (https://youtu.be/fdoYiw8lAn0).",Achievement,Intelligent,The successful retrieval of target screens within the top-20 search results in 15 out of 19 attempts highlights the intelligence of the D2S2 tool ‚Äö√Ñ√∂‚àö√ë‚àö‚àÇ‚Äö√†√∂‚àö√´‚Äö√†√∂‚Äö√†√á‚Äö√Ñ√∂‚àö‚Ä†‚àö‚àÇ‚Äö√†√∂‚àö¬¥‚Äö√Ñ√∂‚àö‚Ä†‚àö‚àÇ¬¨¬®¬¨√Ü it has a smart design that matches user queries effectively and accurately; potentially making users feel more intelligent for using such an advanced tool.,"The justification for aligning 'Paper X' with the value item Intelligent and its corresponding value Achievement is based on the successful retrieval of target screens within the top-20 search results in 15 out of 19 attempts. This indicates that the D2S2 tool, with its smart design and effective matching of user queries, demonstrates a level of intelligence and sophistication. This can potentially enhance user's sense of achievement as they are able to quickly and accurately find the desired UI examples, making them feel more capable and successful in their software interaction.",Agreed-Justified,Agree,,Agree,,
Exp B,Exp H,1404,ESEC/FSE,Mobile & IoT,D2S2: Drag aEUR(tm)naEUR(tm) Drop Mobile App Screen Search,"The lack of diverse UI element representations in publicly available datasets hinders the scalability of sketch-based interactive mobile search. This paper introduces D2S2, a novel approach that addresses this limitation via drag-and-drop mobile screen search, accommodating visual and text-based queries. D2S2 searches 58k Rico screens for relevant UI examples based on UI element attributes, including type, position, shape, and text. In an evaluation with 10 novice software developers D2S2 successfully retrieves target screens within the top-20 search results in 15/19 attempts within a minute. The tool offers interactive and iterative search, updating its search results each time the user modifies the search query. Interested users can freely access D2S2 (http://pixeltoapp.com/D2S2), build on D2S2 or replicate results via D2S2aEUR(tm)s open-source implementation (https://github.com/toni-tang/D2S2), or watch D2S2aEUR(tm)s video demonstration (https://youtu.be/fdoYiw8lAn0).",Conformity,Self-Discipline,D2S2 enhances user's self-discipline to achieve better results by offering an interactive and iterative search; which can be updated each time the user modifies the search query.,"D2S2, as mentioned in the abstract, offers an interactive and iterative search which allows the user to update their search query and obtain better results. By providing this functionality, D2S2 encourages users to exercise self-discipline in refining their searches and investing time in finding the most relevant UI examples. This aligns with the value item of Self-Discipline and its corresponding value of Conformity, as users are encouraged to comply with the search process, follow guidelines, and adhere to the iterative nature of the search to achieve more satisfactory outcomes.",Agreed-Clarified,Agree,,Agree,,
Exp C,Exp K,1405,ESEC/FSE,Mobile & IoT,CONAN: Statically Detecting Connectivity Issues in Android Applications,"Mobile apps are increasingly used in daily activities. Most apps require Internet connectivity to be fully exploited. Despite the fact that global access to the Internet has improved over the years, there are still complex connectivity scenarios, including situations with zero/unreliable connectivity. In such scenarios, improper handling of Eventual Connectivity Issues may cause bugs and crashes that worsen the user experience. Even though these issues have been studied in the literature, no automatic detection techniques are available. To address the mentioned gap, we have created the open source CONAN tool. CONAN can statically detect 16 types of Eventual Connectivity Issues within Android apps; it works at the source code level and alerts developers of any connectivity issue, highlighting them directly in the IDE or generating a report explaining the detected errors. In this paper, we present the technical aspects and a video of our tool, which are publicly available at https://tinyurl.com/CONAN-lint.",Achievement,Capable,The paper contributes a tool (CONAN) that can detect 16 types of Eventual Connectivity Issues within Android apps; highlighting and explaining issues in a report. This aligns with the value item 'Capable' and its corresponding value 'Achievement'; as it empowers the software user to successfully handle and navigate issues related to connectivity.,"The alignment of 'Paper X' with the value item Capable and its corresponding value Achievement is justified because the paper's contribution of the CONAN tool directly enables software users to successfully handle and navigate issues related to connectivity in Android apps. By statically detecting 16 types of Eventual Connectivity Issues and providing alerts or generating reports, the tool empowers users to take control of their app experience, enhancing their ability to achieve their goals and be successful in using the software.",Agreed-Justified,Agree,,Agree,,
Exp E,Exp J,1405,ESEC/FSE,Mobile & IoT,CONAN: Statically Detecting Connectivity Issues in Android Applications,"Mobile apps are increasingly used in daily activities. Most apps require Internet connectivity to be fully exploited. Despite the fact that global access to the Internet has improved over the years, there are still complex connectivity scenarios, including situations with zero/unreliable connectivity. In such scenarios, improper handling of Eventual Connectivity Issues may cause bugs and crashes that worsen the user experience. Even though these issues have been studied in the literature, no automatic detection techniques are available. To address the mentioned gap, we have created the open source CONAN tool. CONAN can statically detect 16 types of Eventual Connectivity Issues within Android apps; it works at the source code level and alerts developers of any connectivity issue, highlighting them directly in the IDE or generating a report explaining the detected errors. In this paper, we present the technical aspects and a video of our tool, which are publicly available at https://tinyurl.com/CONAN-lint.",Benevolence,Responsibility,The paper contributes a tool that alerts developers of any connectivity issue; improving the quality; reliability and therefore responsibility of the software. This aligns with the value item 'Responsibility' and its corresponding value 'Benevolence'; as a part of users' well-being and positive user experience.,"The contribution of 'Paper X' in providing a tool that alerts developers of any connectivity issue aligns with the value item of Responsibility and its corresponding value of Benevolence from a ""Software User"" perspective. This alignment is evident as it improves the quality, reliability, and overall responsibility of the software, ultimately contributing to the users' well-being and positive user experience. By addressing connectivity issues, the paper shows a sense of responsibility towards the users' needs and ensures that the software operates in a way that is considerate and beneficial to them.",Agreed-Justified,Agree,,Agree,,
Exp F,Exp J,1412,ESEC/FSE,Security & Privacy,Privacy-Centric Log Parsing for Timely Proactive Personal Data Protection,"This paper presents a privacy-centric approach to log parsing, addressing the growing need for privacy compliance in log management. We propose a novel log parser that focuses on data minimization, a key principle in privacy protection. By integrating privacy considerations into the log parsing process, our approach enables proactive and timely privacy compliance and mitigation of privacy breaches.",Self Direction,Privacy,"The paper's main contribution is a privacy-centric approach to log parsing that addresses the need for privacy compliance in log management. This directly aligns with the value item ""Privacy"" (v1.6) and its corresponding value ""Self Direction"" (v1) in Schwartz's Taxonomy; as the software provides the users with control over their personal data thus protecting their privacy.","In the paper abstract, it is explicitly stated that the proposed log parser focuses on data minimization, which is a key principle in privacy protection. By integrating privacy considerations into the log parsing process, the paper enables proactive and timely privacy compliance and mitigation of privacy breaches. This aligns with the value item ""Privacy"" (v1.6) and its corresponding value ""Self Direction"" (v1) in Schwartz's Taxonomy because it provides software users with control over their personal data, allowing them to make independent choices and protect their privacy.",Agreed-Justified,Agree,,Agree,,
Exp G,Exp K,1412,ESEC/FSE,Security & Privacy,Privacy-Centric Log Parsing for Timely Proactive Personal Data Protection,"This paper presents a privacy-centric approach to log parsing, addressing the growing need for privacy compliance in log management. We propose a novel log parser that focuses on data minimization, a key principle in privacy protection. By integrating privacy considerations into the log parsing process, our approach enables proactive and timely privacy compliance and mitigation of privacy breaches.",Security,Family Security,"The paper introduces a proactive and timely privacy compliance in the software log management which eventually enhances the security of users' private information. As such; the contribution aligns with the value item ""Family Security"" (v6.2) and the corresponding value ""Security"" (v6); as it increases the users' sense of security by safeguarding their personal information.","The main contribution of 'Paper X' is the development of a privacy-centric log parsing approach that focuses on data minimization and enables proactive privacy compliance. This directly aligns with the value item of Family Security (v6.2) and its corresponding value of Security (v6) because it ensures that users' personal information is safeguarded and increases their sense of security. By integrating privacy considerations into log parsing, the paper addresses the need for privacy compliance in software log management, ultimately enhancing the security of users' private information.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,1412,ESEC/FSE,Security & Privacy,Privacy-Centric Log Parsing for Timely Proactive Personal Data Protection,"This paper presents a privacy-centric approach to log parsing, addressing the growing need for privacy compliance in log management. We propose a novel log parser that focuses on data minimization, a key principle in privacy protection. By integrating privacy considerations into the log parsing process, our approach enables proactive and timely privacy compliance and mitigation of privacy breaches.",Achievement,Successful,"Although the paper's main focus isn't directly on users' achievement; it enables them to adhere to privacy rules and regulations with its privacy-focused log parsing approach. This can make them successful in maintaining compliance and preventing privacy breaches. This aligns with the value item ""Successful"" (v4.3) and its corresponding value ""Achievement"" (v4) that emphasizes users' aspiration to succeed and accomplish their goals.","In the context of a software user, the alignment of 'Paper X' with the value item Successful and its corresponding value Achievement is justified by the fact that the paper's privacy-centric approach to log parsing enables users to achieve success in maintaining privacy compliance and preventing privacy breaches. By implementing proactive and timely privacy measures through data minimization, users can adhere to privacy rules and regulations, ensuring the achievement of their goals in terms of privacy protection and regulatory compliance.",Agreed-Clarified,Agree,,Agree,,
Exp B,Exp H,1513,TOSEM,AI & Machine Learning,Automating App Review Response Generation Based on Contextual Knowledge,"User experience of mobile apps is an essential ingredient that can influence the user base and app revenue. To ensure good user experience and assist app development, several prior studies resort to analysis of app reviews, a type of repository that directly reflects user opinions about the apps. Accurately responding to the app reviews is one of the ways to relieve user concerns and thus improve user experience. However, the response quality of the existing method relies on the pre-extracted features from other tools, including manually labelled keywords and predicted review sentiment, which may hinder the generalizability and flexibility of the method. In this article, we propose a novel neural network approach, named CoRe, with the contextual knowledge naturally incorporated and without involving external tools. Specifically, CoRe integrates two types of contextual knowledge in the training corpus, including official app descriptions from app store and responses of the retrieved semantically similar reviews, for enhancing the relevance and accuracy of the generated review responses. Experiments on practical review data show that CoRe can outperform the state-of-the-art method by 12.36\% in terms of BLEU-4, an accuracy metric that is widely used to evaluate text generation systems.",Stimulation,Excitement in Life,The paper proposes a novel neural network approach named CoRe which aims to enhance the relevance and accuracy of the generated review responses for mobile apps; improving user experience and adding a level of excitement or stimulation for users.,"In 'Paper X', the proposed neural network approach CoRe seeks to enhance the relevance and accuracy of generated review responses for mobile apps. By improving user experience through more accurate and contextually relevant responses, users are likely to experience a sense of excitement and stimulation when interacting with the app. The timely and appropriate responses generated by CoRe contribute to a dynamic and engaging user experience, aligning with the value item of Excitement in Life and its corresponding value of Stimulation from a ""Software User"" perspective.",Agreed-Justified,Agree,,Agree,,
Exp E,Exp J,1515,TOSEM,Security & Privacy,SPI: Automated Identification of Security Patches via Commits,"Security patches in open source software, providing security fixes to identified vulnerabilities, are crucial in protecting against cyber attacks. Security advisories and announcements are often publicly released to inform the users about potential security vulnerability. Despite the National Vulnerability Database (NVD) publishes identified vulnerabilities, a vast majority of vulnerabilities and their corresponding security patches remain beyond public exposure, e.g., in the open source libraries that are heavily relied on by developers. As many of these patches exist in open sourced projects, the problem of curating and gathering security patches can be difficult due to their hidden nature. An extensive and complete security patches dataset could help end-users such as security companies, e.g., building a security knowledge base, or researcher, e.g., aiding in vulnerability research.To efficiently curate security patches including undisclosed patches at large scale and low cost, we propose a deep neural-network-based approach built upon commits of open source repositories. First, we design and build security patch datasets that include 38,291 security-related commits and 1,045 Common Vulnerabilities and Exposures (CVE) patches from four large-scale C programming language libraries. We manually verify each commit, among the 38,291 security-related commits, to determine if they are security related.We devise and implement a deep learning-based security patch identification system that consists of two composite neural networks: one commit-message neural network that utilizes pretrained word representations learned from our commits dataset and one code-revision neural network that takes code before revision and after revision and learns the distinction on the statement level. Our system leverages the power of the two networks for Security Patch Identification. Evaluation results show that our system significantly outperforms SVM and K-fold stacking algorithms. The result on the combined dataset achieves as high as 87.93\% F1-score and precision of 86.24\%.We deployed our pipeline and learned model in an industrial production environment to evaluate the generalization ability of our approach. The industrial dataset consists of 298,917 commits from 410 new libraries that range from a wide functionalities. Our experiment results and observation on the industrial dataset proved that our approach can identify security patches effectively among open sourced projects.",Achievement,Intelligent,The security patch identification system in the paper demonstrates a significant degree of intelligence by leveraging deep learning networks for effectively picking out potential security risks. This aligns with the value item Intelligent and its corresponding value Achievement.,"The justification for aligning 'Paper X' with the value item Intelligent and its corresponding value Achievement from a ""Software User"" perspective is based on the fact that the paper's security patch identification system utilizes deep learning networks, which can be seen as an intelligent approach to effectively identify and address security risks. By leveraging advanced technology and achieving superior performance compared to other algorithms, 'Paper X' demonstrates a level of intelligence and accomplishment in the field of software security, thereby aligning with the value item Intelligent and its corresponding value Achievement.",Agreed-Clarified,Agree,,Agree,,
Exp F,Exp J,1515,TOSEM,Security & Privacy,SPI: Automated Identification of Security Patches via Commits,"Security patches in open source software, providing security fixes to identified vulnerabilities, are crucial in protecting against cyber attacks. Security advisories and announcements are often publicly released to inform the users about potential security vulnerability. Despite the National Vulnerability Database (NVD) publishes identified vulnerabilities, a vast majority of vulnerabilities and their corresponding security patches remain beyond public exposure, e.g., in the open source libraries that are heavily relied on by developers. As many of these patches exist in open sourced projects, the problem of curating and gathering security patches can be difficult due to their hidden nature. An extensive and complete security patches dataset could help end-users such as security companies, e.g., building a security knowledge base, or researcher, e.g., aiding in vulnerability research.To efficiently curate security patches including undisclosed patches at large scale and low cost, we propose a deep neural-network-based approach built upon commits of open source repositories. First, we design and build security patch datasets that include 38,291 security-related commits and 1,045 Common Vulnerabilities and Exposures (CVE) patches from four large-scale C programming language libraries. We manually verify each commit, among the 38,291 security-related commits, to determine if they are security related.We devise and implement a deep learning-based security patch identification system that consists of two composite neural networks: one commit-message neural network that utilizes pretrained word representations learned from our commits dataset and one code-revision neural network that takes code before revision and after revision and learns the distinction on the statement level. Our system leverages the power of the two networks for Security Patch Identification. Evaluation results show that our system significantly outperforms SVM and K-fold stacking algorithms. The result on the combined dataset achieves as high as 87.93\% F1-score and precision of 86.24\%.We deployed our pipeline and learned model in an industrial production environment to evaluate the generalization ability of our approach. The industrial dataset consists of 298,917 commits from 410 new libraries that range from a wide functionalities. Our experiment results and observation on the industrial dataset proved that our approach can identify security patches effectively among open sourced projects.",Security,Healthy,The paper is focused on developing a system that identifies and mitigates potential security vulnerabilities in open source software; thus enhancing the health of user software environments. This directly aligns with the value item Healthy and its corresponding value Security.,"The justification provided is based on the fact that the main focus of 'Paper X' is to develop a system that identifies and mitigates potential security vulnerabilities in open source software. By doing so, the paper directly contributes to enhancing the security of software environments for users. Security is an important aspect of maintaining a healthy software environment, as it protects users from cyber attacks and potential harm to their systems. Therefore, aligning 'Paper X' with the value item Healthy and its corresponding value Security is justified based on its contribution to ensuring the well-being and safety of software users.",Agreed-Justified,Agree,,Agree,,
Exp G,Exp K,1515,TOSEM,Security & Privacy,SPI: Automated Identification of Security Patches via Commits,"Security patches in open source software, providing security fixes to identified vulnerabilities, are crucial in protecting against cyber attacks. Security advisories and announcements are often publicly released to inform the users about potential security vulnerability. Despite the National Vulnerability Database (NVD) publishes identified vulnerabilities, a vast majority of vulnerabilities and their corresponding security patches remain beyond public exposure, e.g., in the open source libraries that are heavily relied on by developers. As many of these patches exist in open sourced projects, the problem of curating and gathering security patches can be difficult due to their hidden nature. An extensive and complete security patches dataset could help end-users such as security companies, e.g., building a security knowledge base, or researcher, e.g., aiding in vulnerability research.To efficiently curate security patches including undisclosed patches at large scale and low cost, we propose a deep neural-network-based approach built upon commits of open source repositories. First, we design and build security patch datasets that include 38,291 security-related commits and 1,045 Common Vulnerabilities and Exposures (CVE) patches from four large-scale C programming language libraries. We manually verify each commit, among the 38,291 security-related commits, to determine if they are security related.We devise and implement a deep learning-based security patch identification system that consists of two composite neural networks: one commit-message neural network that utilizes pretrained word representations learned from our commits dataset and one code-revision neural network that takes code before revision and after revision and learns the distinction on the statement level. Our system leverages the power of the two networks for Security Patch Identification. Evaluation results show that our system significantly outperforms SVM and K-fold stacking algorithms. The result on the combined dataset achieves as high as 87.93\% F1-score and precision of 86.24\%.We deployed our pipeline and learned model in an industrial production environment to evaluate the generalization ability of our approach. The industrial dataset consists of 298,917 commits from 410 new libraries that range from a wide functionalities. Our experiment results and observation on the industrial dataset proved that our approach can identify security patches effectively among open sourced projects.",Security,Social Order,The system proposed in the paper contributes to establishing order in the possibly chaotic realm of open source software by efficiently curating both disclosed and undisclosed security patches. Therefore; it aligns with the value item Social Order and its corresponding value Security.,"The paper's proposed system addresses the challenge of curating security patches in open source software, aiming to establish order by efficiently gathering both disclosed and undisclosed patches. This contributes to enhancing security and protecting against cyber attacks, which is a key aspect of maintaining social order within the software context. By providing a solution that helps ensure the integrity and stability of software systems, the paper aligns with the value item Social Order and its corresponding value Security from the perspective of a software user.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,1523,TOSEM,Mobile & IoT,Classifying Mobile Applications Using Word Embeddings,"Modern application stores enable developers to classify their apps by choosing from a set of generic categories, or genres, such as health, games, and music. These categories are typically staticaEUR""new categories do not necessarily emerge over time to reflect innovations in the mobile software landscape. With thousands of apps classified under each category, locating apps that match a specific consumer interest can be a challenging task. To overcome this challenge, in this article, we propose an automated approach for classifying mobile apps into more focused categories of functionally related application domains. Our aim is to enhance apps visibility and discoverability. Specifically, we employ word embeddings to generate numeric semantic representations of app descriptions. These representations are then classified to generate more cohesive categories of apps. Our empirical investigation is conducted using a dataset of 600 apps, sampled from the Education, Health&amp;Fitness, and Medical categories of the Apple App Store. The results show that our classification algorithms achieve their best performance when app descriptions are vectorized using GloVe, a count-based model of word embeddings. Our findings are further validated using a dataset of Sharing Economy apps and the results are evaluated by 12 human subjects. The results show that GloVe combined with Support Vector Machines can produce app classifications that are aligned to a large extent with human-generated classifications.",Self Direction,Freedom,Paper X contributes an automated approach for classifying mobile apps into more focused categories; thus enhancing users' freedom to choose applications that match their specific interests.,"In 'Paper X', the proposed automated approach for classifying mobile apps into more focused categories aligns with the value item ""Freedom"" and its corresponding value ""Self Direction"" from a ""Software User"" perspective. By providing users with more cohesive categories of apps, the paper enhances users' freedom to choose applications that align with their specific interests, allowing them to exercise their autonomy and independence in selecting the apps that best suit their needs and preferences. This aligns with the value item of ""Freedom"" as it empowers users to make informed choices and have control over their app selection process, promoting a sense of self-direction in their software usage experience.",Agreed-Justified,Disagree,More incline to Achievement: Capable as it capable of classifying different categories sucessfully,Agree,,
Exp B,Exp H,1523,TOSEM,Mobile & IoT,Classifying Mobile Applications Using Word Embeddings,"Modern application stores enable developers to classify their apps by choosing from a set of generic categories, or genres, such as health, games, and music. These categories are typically staticaEUR""new categories do not necessarily emerge over time to reflect innovations in the mobile software landscape. With thousands of apps classified under each category, locating apps that match a specific consumer interest can be a challenging task. To overcome this challenge, in this article, we propose an automated approach for classifying mobile apps into more focused categories of functionally related application domains. Our aim is to enhance apps visibility and discoverability. Specifically, we employ word embeddings to generate numeric semantic representations of app descriptions. These representations are then classified to generate more cohesive categories of apps. Our empirical investigation is conducted using a dataset of 600 apps, sampled from the Education, Health&amp;Fitness, and Medical categories of the Apple App Store. The results show that our classification algorithms achieve their best performance when app descriptions are vectorized using GloVe, a count-based model of word embeddings. Our findings are further validated using a dataset of Sharing Economy apps and the results are evaluated by 12 human subjects. The results show that GloVe combined with Support Vector Machines can produce app classifications that are aligned to a large extent with human-generated classifications.",Achievement,Capable,The paper's contribution allows users to discover and install applications that  match their particular needs; thus making them capable and more effective in using their mobile devices.,"In the context of ""Paper X,"" the alignment with the value item Capable and its corresponding value Achievement is evident in the paper's proposal to automate the classification of mobile apps into more focused categories. By enhancing app visibility and discoverability, users are empowered to find applications that specifically match their interests and needs. This aligns with the value of Achievement as users are able to achieve their goals and effectively utilize their mobile devices. Therefore, the paper's contribution directly enables users to become more capable and efficient in utilizing mobile apps, thereby aligning with the value item Capable and the corresponding value Achievement from a ""Software User"" perspective.",Agreed-Justified,Agree,,Agree,,
Exp D,Exp K,1537,TOSEM,AI & Machine Learning,SemMT: A Semantic-Based Testing Approach for Machine Translation Systems,"Machine translation has wide applications in daily life. In mission-critical applications such as translating official documents, incorrect translation can have unpleasant or sometimes catastrophic consequences. This motivates recent research on the testing methodologies for machine translation systems. Existing methodologies mostly rely on metamorphic relations designed at the textual level (e.g., Levenshtein distance) or syntactic level (e.g., distance between grammar structures) to determine the correctness of translation results. However, these metamorphic relations do not consider whether the original and the translated sentences have the same meaning (i.e., semantic similarity). To address this problem, in this article we propose SemMT, an automatic testing approach for machine translation systems based on semantic similarity checking. SemMT applies round-trip translation and measures the semantic similarity between the original and the translated sentences. Our insight is that the semantics concerning logical relations and quantifiers in sentences can be captured by regular expressions (or deterministic finite automata) where efficient semantic equivalence/similarity checking algorithms can be applied. Leveraging the insight, we propose three semantic similarity metrics and implement them in SemMT. We compared SemMT with related state-of-the-art testing techniques, demonstrating the effectiveness of mistranslation detection. The experiment results show that SemMT outperforms existing metrics, achieving an increase of 34.2\% and 15.4\% on accuracy and F-score, respectively. We also study the possibility of further enhancing the performance by combining various metrics. Finally, we discuss a solution to locate the suspicious trip in round-trip translation, which provides hints for bug diagnosis.",Achievement,Intelligent,The paper proposes SemMT; an automatic testing approach for machine translation systems based on semantic similarity checking. This system can enhance the intellectual capacities of the software user by ensuring translated sentences maintain their intended meaning; aligning with the value item Intelligent and its corresponding value Achievement.,"The paper's proposed SemMT system directly aligns with the value item Intelligent and its corresponding value Achievement because it enhances the intellectual capacities of the software user by ensuring accurate translation and semantic similarity. By utilizing regular expressions and semantic equivalence checking algorithms, SemMT detects mistranslations and improves the accuracy and effectiveness of machine translation systems, thereby achieving the goal of intelligent and successful translation, which aligns with the value item Intelligent and its corresponding value Achievement.",Agreed-Justified,Agree,,Agree,,
Exp E,Exp J,1537,TOSEM,AI & Machine Learning,SemMT: A Semantic-Based Testing Approach for Machine Translation Systems,"Machine translation has wide applications in daily life. In mission-critical applications such as translating official documents, incorrect translation can have unpleasant or sometimes catastrophic consequences. This motivates recent research on the testing methodologies for machine translation systems. Existing methodologies mostly rely on metamorphic relations designed at the textual level (e.g., Levenshtein distance) or syntactic level (e.g., distance between grammar structures) to determine the correctness of translation results. However, these metamorphic relations do not consider whether the original and the translated sentences have the same meaning (i.e., semantic similarity). To address this problem, in this article we propose SemMT, an automatic testing approach for machine translation systems based on semantic similarity checking. SemMT applies round-trip translation and measures the semantic similarity between the original and the translated sentences. Our insight is that the semantics concerning logical relations and quantifiers in sentences can be captured by regular expressions (or deterministic finite automata) where efficient semantic equivalence/similarity checking algorithms can be applied. Leveraging the insight, we propose three semantic similarity metrics and implement them in SemMT. We compared SemMT with related state-of-the-art testing techniques, demonstrating the effectiveness of mistranslation detection. The experiment results show that SemMT outperforms existing metrics, achieving an increase of 34.2\% and 15.4\% on accuracy and F-score, respectively. We also study the possibility of further enhancing the performance by combining various metrics. Finally, we discuss a solution to locate the suspicious trip in round-trip translation, which provides hints for bug diagnosis.",Security,Healthy,The paper's system; SemMT; works to ensure the accuracy of translated sentences. By avoiding mistranslations; it can contribute to the user's overall well-being in environments where translation errors could lead to unpleasant or catastrophic consequences; thus aligning with the value item Healthy and its corresponding value Security.,"In the context of a software user, the alignment between 'Paper X' and the value item Healthy, along with its corresponding value Security, can be seen in the paper's focus on ensuring the accuracy of machine translation systems. By mitigating mistranslations, the system can contribute to the user's overall well-being in critical scenarios, where incorrect translations could have unpleasant or even catastrophic consequences. This alignment reflects the value of Security in safeguarding the user against potential harm, ultimately promoting a sense of well-being and contributing to their overall health and safety.",Agreed-Justified,Agree,,Agree,,
Exp F,Exp J,1537,TOSEM,AI & Machine Learning,SemMT: A Semantic-Based Testing Approach for Machine Translation Systems,"Machine translation has wide applications in daily life. In mission-critical applications such as translating official documents, incorrect translation can have unpleasant or sometimes catastrophic consequences. This motivates recent research on the testing methodologies for machine translation systems. Existing methodologies mostly rely on metamorphic relations designed at the textual level (e.g., Levenshtein distance) or syntactic level (e.g., distance between grammar structures) to determine the correctness of translation results. However, these metamorphic relations do not consider whether the original and the translated sentences have the same meaning (i.e., semantic similarity). To address this problem, in this article we propose SemMT, an automatic testing approach for machine translation systems based on semantic similarity checking. SemMT applies round-trip translation and measures the semantic similarity between the original and the translated sentences. Our insight is that the semantics concerning logical relations and quantifiers in sentences can be captured by regular expressions (or deterministic finite automata) where efficient semantic equivalence/similarity checking algorithms can be applied. Leveraging the insight, we propose three semantic similarity metrics and implement them in SemMT. We compared SemMT with related state-of-the-art testing techniques, demonstrating the effectiveness of mistranslation detection. The experiment results show that SemMT outperforms existing metrics, achieving an increase of 34.2\% and 15.4\% on accuracy and F-score, respectively. We also study the possibility of further enhancing the performance by combining various metrics. Finally, we discuss a solution to locate the suspicious trip in round-trip translation, which provides hints for bug diagnosis.",Benevolence,Responsibility,By developing a testing methodology that significantly enhances detection of mistranslations; the paper is responsible for making machine translated communication more reliable and accurate for users. This aligns with the value item Responsibility and its corresponding value Benevolence.,"By improving the testing methodology for machine translation systems and enhancing the detection of mistranslations, 'Paper X' takes responsibility for ensuring the reliability and accuracy of machine-translated communication. This aligns with the value item Responsibility from Schwartz's Taxonomy, as it demonstrates a commitment to the welfare and well-being of software users who rely on accurate translations for their daily communication needs. This commitment to responsibility reflects the value of Benevolence, where the focus is on helping and providing benefit to others, in this case, software users who depend on translations.",Agreed-Clarified,Agree,,Agree,,
Exp G,Exp K,1538,TOSEM,Security & Privacy,Buddy Stacks: Protecting Return Addresses with Efficient Thread-Local Storage and Runtime Re-Randomization,"Shadow stacks play an important role in protecting return addresses to mitigate ROP attacks. Parallel shadow stacks, which shadow the call stack of each thread at the same constant offset for all threads, are known not to support multi-threading well. On the other hand, compact shadow stacks must maintain a separate shadow stack pointer in thread-local storage (TLS), which can be implemented in terms of a register or the per-thread Thread-Control-Block (TCB), suffering from poor compatibility in the former or high performance overhead in the latter. In addition, shadow stacks are vulnerable to information disclosure attacks.In this paper, we propose to mitigate ROP attacks for single- and multi-threaded server programs running on general-purpose computing systems by using a novel stack layout, called a buddy stack (referred to as Bustk), that is highly performant, compatible with existing code, and provides meaningful security. These goals are met due to three novel design aspects in Bustk. First, Bustk places a parallel shadow stack just below a threadaEUR(tm)s call stack (as each otheraEUR(tm)s buddies allocated together), avoiding the need to maintain a separate shadow stack pointer and making it now well-suited for multi-threading. Second, Bustk uses an efficient stack-based thread-local storage mechanism, denoted STK-TLS, to store thread-specific metadata in two TLS sections just below the shadow stack in dual redundancy (as each otheraEUR(tm)s buddies), so that both can be accessed and updated in a lightweight manner from the call stack pointer rsp alone. Finally, Bustk re-randomizes continuously (on the order of milliseconds) the return addresses on the shadow stack by using a new microsecond-level runtime re-randomization technique, denoted STK-MSR. This mechanism aims to obsolete leaked information, making it extremely unlikely for the attacker to hijack return addresses, particularly against a server program that sits often tens of milliseconds away from the attacker.Our evaluation using web servers, Nginx and Apache Httpd, shows that Bustk works well in terms of performance, compatibility, and security provided, with its parallel shadow stacks incurring acceptable memory overhead for real-world applications and its STK-TLS mechanism costing only two pages per thread. In particular, Bustk can protect the Nginx and Apache servers with an adaptive 1-ms re-randomization policy (without observable overheads when IO is intensive, with about 17,000 requests per second). In addition, we have also evaluated Bustk using other non-server applications, Firefox, Python, LLVM, JDK and SPEC CPU2006, to demonstrate further the same degree of performance and compatibility provided, but the protection provided for, say, browsers, is weaker (since network-access delays can no longer be assumed).",Security,Healthy,The main contribution of 'Paper X' is proposing a novel stack layout; referred to as Bustk; designed to provide meaningful security against ROP attacks. This aligns with the value item Healthy (v6.1) under the value Security (v6) because by protecting software users from ROP attacks; the software ensures the 'health' and safety of their digital environment.,"The justification for aligning 'Paper X' with the value item Healthy and its corresponding value Security is that the proposed stack layout, Bustk, aims to protect software users from ROP attacks. ROP attacks can manipulate a program's execution and potentially compromise the security of a user's digital environment. By mitigating these attacks, the software ensures the 'health' and safety of the user's digital environment, aligning with the value of Security and its value item Healthy.",Agreed-Justified,Agree,,Agree,,
Exp B,Exp H,1538,TOSEM,Security & Privacy,Buddy Stacks: Protecting Return Addresses with Efficient Thread-Local Storage and Runtime Re-Randomization,"Shadow stacks play an important role in protecting return addresses to mitigate ROP attacks. Parallel shadow stacks, which shadow the call stack of each thread at the same constant offset for all threads, are known not to support multi-threading well. On the other hand, compact shadow stacks must maintain a separate shadow stack pointer in thread-local storage (TLS), which can be implemented in terms of a register or the per-thread Thread-Control-Block (TCB), suffering from poor compatibility in the former or high performance overhead in the latter. In addition, shadow stacks are vulnerable to information disclosure attacks.In this paper, we propose to mitigate ROP attacks for single- and multi-threaded server programs running on general-purpose computing systems by using a novel stack layout, called a buddy stack (referred to as Bustk), that is highly performant, compatible with existing code, and provides meaningful security. These goals are met due to three novel design aspects in Bustk. First, Bustk places a parallel shadow stack just below a threadaEUR(tm)s call stack (as each otheraEUR(tm)s buddies allocated together), avoiding the need to maintain a separate shadow stack pointer and making it now well-suited for multi-threading. Second, Bustk uses an efficient stack-based thread-local storage mechanism, denoted STK-TLS, to store thread-specific metadata in two TLS sections just below the shadow stack in dual redundancy (as each otheraEUR(tm)s buddies), so that both can be accessed and updated in a lightweight manner from the call stack pointer rsp alone. Finally, Bustk re-randomizes continuously (on the order of milliseconds) the return addresses on the shadow stack by using a new microsecond-level runtime re-randomization technique, denoted STK-MSR. This mechanism aims to obsolete leaked information, making it extremely unlikely for the attacker to hijack return addresses, particularly against a server program that sits often tens of milliseconds away from the attacker.Our evaluation using web servers, Nginx and Apache Httpd, shows that Bustk works well in terms of performance, compatibility, and security provided, with its parallel shadow stacks incurring acceptable memory overhead for real-world applications and its STK-TLS mechanism costing only two pages per thread. In particular, Bustk can protect the Nginx and Apache servers with an adaptive 1-ms re-randomization policy (without observable overheads when IO is intensive, with about 17,000 requests per second). In addition, we have also evaluated Bustk using other non-server applications, Firefox, Python, LLVM, JDK and SPEC CPU2006, to demonstrate further the same degree of performance and compatibility provided, but the protection provided for, say, browsers, is weaker (since network-access delays can no longer be assumed).",Security,National Security,The Bustk method proposed by 'Paper X' provides secure and compatible solutions for server programs; web servers; non-server applications; etc. This wide-ranging and impactful contribution aligns directly with the value item of National Security (v6.7) under the value Security (v6) as it focuses on providing a secure and stabilized digital environment for software users on a larger scale; contributing to the overall 'security' of a system; a fundamental necessity in any digital nation.,"The Bustk method proposed by 'Paper X' directly aligns with the value item of National Security (v6.7) under the value Security (v6) as it addresses the critical need for secure and stable software systems, particularly in the context of server programs and web servers. By providing efficient and effective measures to protect against ROP attacks, Bustk contributes to the overall security of digital environments, ensuring the integrity and reliability of software systems. This directly aligns with the value of Security, which is essential for software users in their interactions with digital systems.",Agreed-Justified,Agree,,Agree,,
Exp C,Exp K,1539,TOSEM,Emerging Technologies,A Study on Blockchain Architecture Design Decisions and Their Security Attacks and Threats,"Blockchain is a disruptive technology intended to implement secure decentralised distributed systems, in which transactional data can be shared, stored, and verified by participants of the system without needing a central authentication/verification authority. Blockchain-based systems have several architectural components and variants, which architects can leverage to build secure software systems. However, there is a lack of studies to assist architects in making architecture design and configuration decisions for blockchain-based systems. This knowledge gap may increase the chance of making unsuitable design decisions and producing configurations prone to potential security risks. To address this limitation, we report our comprehensive systematic literature review to derive a taxonomy of commonly used architecture design decisions in blockchain-based systems. We map each of these decisions to potential security attacks and their posed threats. MITREaEUR(tm)s attack tactic categories and Microsoft STRIDE threat modeling are used to systematically classify threats and their associated attacks to identify potential attacks and threats in blockchain-based systems. Our mapping approach aims to guide architects to make justifiable design decisions that will result in more secure implementations.",Achievement,Intelligent,The paper provides a comprehensive systematic literature review to help guide architects make more informed and intelligent design decisions for blockchain-based systems; aligning with the value item 'Intelligent' under the 'Achievement' value.,"The justification for aligning 'Paper X' with the value item Intelligent and its corresponding value Achievement from a ""Software User"" perspective is that the paper contributes to the development of intelligent design decisions by providing a comprehensive systematic literature review for architects. By analyzing various architectural components and their potential security risks in blockchain-based systems, the paper aims to guide architects in making more informed and intelligent design decisions. This aligns with the value of Achievement as it emphasizes the desire for individuals to demonstrate their intelligence and capability in making successful and influential decisions in the software context as a software user.",Agreed-Justified,Agree,,Agree,,
Exp D,Exp K,1539,TOSEM,Emerging Technologies,A Study on Blockchain Architecture Design Decisions and Their Security Attacks and Threats,"Blockchain is a disruptive technology intended to implement secure decentralised distributed systems, in which transactional data can be shared, stored, and verified by participants of the system without needing a central authentication/verification authority. Blockchain-based systems have several architectural components and variants, which architects can leverage to build secure software systems. However, there is a lack of studies to assist architects in making architecture design and configuration decisions for blockchain-based systems. This knowledge gap may increase the chance of making unsuitable design decisions and producing configurations prone to potential security risks. To address this limitation, we report our comprehensive systematic literature review to derive a taxonomy of commonly used architecture design decisions in blockchain-based systems. We map each of these decisions to potential security attacks and their posed threats. MITREaEUR(tm)s attack tactic categories and Microsoft STRIDE threat modeling are used to systematically classify threats and their associated attacks to identify potential attacks and threats in blockchain-based systems. Our mapping approach aims to guide architects to make justifiable design decisions that will result in more secure implementations.",Security,Healthy,The paper provides guidance for selecting architecture design decisions which would result in secure blockchain-based system implementations; aligning with the value item 'Healthy' under the 'Security' value assuming healthy in this context refers to a well-functioning system.,"The alignment of 'Paper X' with the value item Healthy and its corresponding value Security from a ""Software User"" perspective can be justified by the fact that the paper aims to provide guidance for selecting architecture design decisions that result in more secure implementations of blockchain-based systems. By ensuring the security of the system, the paper indirectly contributes to the user's sense of safety and well-being, aligning with the value item Healthy under the value of Security.",Agreed-Clarified,Agree,,Disagree,Value healthy concerns Human health not software health,
Exp E,Exp J,1539,TOSEM,Emerging Technologies,A Study on Blockchain Architecture Design Decisions and Their Security Attacks and Threats,"Blockchain is a disruptive technology intended to implement secure decentralised distributed systems, in which transactional data can be shared, stored, and verified by participants of the system without needing a central authentication/verification authority. Blockchain-based systems have several architectural components and variants, which architects can leverage to build secure software systems. However, there is a lack of studies to assist architects in making architecture design and configuration decisions for blockchain-based systems. This knowledge gap may increase the chance of making unsuitable design decisions and producing configurations prone to potential security risks. To address this limitation, we report our comprehensive systematic literature review to derive a taxonomy of commonly used architecture design decisions in blockchain-based systems. We map each of these decisions to potential security attacks and their posed threats. MITREaEUR(tm)s attack tactic categories and Microsoft STRIDE threat modeling are used to systematically classify threats and their associated attacks to identify potential attacks and threats in blockchain-based systems. Our mapping approach aims to guide architects to make justifiable design decisions that will result in more secure implementations.",Universalism,A World of Beauty,The taxonomy's derived architecture design decisions and their mapping to potential security threats contribute to a more secure software environment perceived by users; which aligns with the 'A World of Beauty' under the 'Universalism' value assuming a 'World of Beauty' includes orderly and secure environments in this context.,"The alignment of 'Paper X' with the value item A World of Beauty and its corresponding value Universalism from a software user perspective can be justified by the fact that the derived architecture design decisions and their mapping to potential security threats contribute to creating a secure and orderly software environment. This secure environment is essential for users as it provides a sense of trust, beauty, and harmony, reflecting the value of Universalism in ensuring fairness, justice, and the protection of individual rights in the software context.",Agreed-Justified,Disagree,abstract is related with blockchain and sceurity this value doesnot match,Disagree,it is about a architecture design decisions in blockchain-based systems.,
Exp F,Exp J,1542,TOSEM,Software Testing & QA,Context- and Fairness-Aware In-Process Crowdworker Recommendation,"Identifying and optimizing open participation is essential to the success of open software development. Existing studies highlighted the importance of worker recommendation for crowdtesting tasks in order to improve bug detection efficiency, i.e., detect more bugs with fewer workers. However, there are a couple of limitations in existing work. First, these studies mainly focus on one-time recommendations based on expertise matching at the beginning of a new task. Second, the recommendation results suffer from severe popularity bias, i.e., highly experienced workers are recommended in almost all the tasks, while less experienced workers rarely get recommended. This article argues the need for context- and fairness-aware in-process crowdworker recommendation in order to address these limitations. We motivate this study through a pilot study, revealing the prevalence of long-sized non-yielding windows, i.e., no new bugs are revealed in consecutive test reports during the process of a crowdtesting task. This indicates the potential opportunity for accelerating crowdtesting by recommending appropriate workers in a dynamic manner, so that the non-yielding windows could be shortened. Besides, motivated by the popularity bias in existing crowdworker recommendation approach, this study also aims at alleviating the unfairness in recommendations.Driven by these observations, this article proposes a context- and fairness-aware in-process crowdworker recommendation approach, iRec2.0, to detect more bugs earlier, shorten the non-yielding windows, and alleviate the unfairness in recommendations. It consists of three main components: (1) the modeling of dynamic testing context, (2) the learning-based ranking component, and (3) the multi-objective optimization-based re-ranking component. The evaluation is conducted on 636 crowdtesting tasks from one of the largest crowdtesting platforms, and results show the potential of iRec2.0 in improving the cost-effectiveness of crowdtesting by saving the cost, shortening the testing process, and alleviating the unfairness among workers. In detail, iRec2.0 could shorten the non-yielding window by a median of 50\%aEUR""66\% in different application scenarios, and consequently have potential of saving testing cost by a median of 8\%aEUR""12\%. Meanwhile, the recommendation frequency of the crowdworker drop from 34\%aEUR""60\% to 5\%aEUR""26\% under different scenarios, indicating its potential in alleviating the unfairness among crowdworkers.",Stimulation,Variation in Life,The paper presents a crowd-worker recommendation approach which improves bug detection efficiency and dynamic worker participation; introducing variation in crowd-testing tasks and enhancing the stimulation for the software user.,"The paper introduces a crowd-worker recommendation approach that aims to improve bug detection efficiency and worker participation in crowd-testing tasks. By recommending appropriate workers in a dynamic manner, the paper introduces variation in the crowd-testing process, which can enhance the stimulation for the software user. This aligns with the value item Variation in Life and its corresponding value Stimulation from a ""Software User"" perspective.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,1542,TOSEM,Software Testing & QA,Context- and Fairness-Aware In-Process Crowdworker Recommendation,"Identifying and optimizing open participation is essential to the success of open software development. Existing studies highlighted the importance of worker recommendation for crowdtesting tasks in order to improve bug detection efficiency, i.e., detect more bugs with fewer workers. However, there are a couple of limitations in existing work. First, these studies mainly focus on one-time recommendations based on expertise matching at the beginning of a new task. Second, the recommendation results suffer from severe popularity bias, i.e., highly experienced workers are recommended in almost all the tasks, while less experienced workers rarely get recommended. This article argues the need for context- and fairness-aware in-process crowdworker recommendation in order to address these limitations. We motivate this study through a pilot study, revealing the prevalence of long-sized non-yielding windows, i.e., no new bugs are revealed in consecutive test reports during the process of a crowdtesting task. This indicates the potential opportunity for accelerating crowdtesting by recommending appropriate workers in a dynamic manner, so that the non-yielding windows could be shortened. Besides, motivated by the popularity bias in existing crowdworker recommendation approach, this study also aims at alleviating the unfairness in recommendations.Driven by these observations, this article proposes a context- and fairness-aware in-process crowdworker recommendation approach, iRec2.0, to detect more bugs earlier, shorten the non-yielding windows, and alleviate the unfairness in recommendations. It consists of three main components: (1) the modeling of dynamic testing context, (2) the learning-based ranking component, and (3) the multi-objective optimization-based re-ranking component. The evaluation is conducted on 636 crowdtesting tasks from one of the largest crowdtesting platforms, and results show the potential of iRec2.0 in improving the cost-effectiveness of crowdtesting by saving the cost, shortening the testing process, and alleviating the unfairness among workers. In detail, iRec2.0 could shorten the non-yielding window by a median of 50\%aEUR""66\% in different application scenarios, and consequently have potential of saving testing cost by a median of 8\%aEUR""12\%. Meanwhile, the recommendation frequency of the crowdworker drop from 34\%aEUR""60\% to 5\%aEUR""26\% under different scenarios, indicating its potential in alleviating the unfairness among crowdworkers.",Achievement,Capable,The crowdtesting tasks are made more capable through the dynamic and fair worker recommendations; shortened non-yielding windows; and improved cost-effectiveness; aligning with the value item Capable under the value Achievement for the software user.,"The main contributions of 'Paper X' align with the value item Capable and its corresponding value Achievement from a ""Software User"" perspective because the proposed iRec2.0 approach enhances the capability of crowdtesting tasks through the dynamic and fair worker recommendations. This leads to shortened non-yielding windows and improved cost-effectiveness, enabling users to achieve their testing goals more efficiently and effectively. The alignment with Capable and Achievement values is evident as it empowers software users to accomplish their desired outcomes through the enhanced capability provided by iRec2.0.",Agreed-Clarified,Agree,,Agree,,
Exp B,Exp H,1554,TOSEM,AI & Machine Learning,NPC: Neuron Path Coverage via Characterizing Decision Logic of Deep Neural Networks,"Deep learning has recently been widely applied to many applications across different domains, e.g., image classification and audio recognition. However, the quality of Deep Neural Networks (DNNs) still raises concerns in the practical operational environment, which calls for systematic testing, especially in safety-critical scenarios. Inspired by software testing, a number of structural coverage criteria are designed and proposed to measure the test adequacy of DNNs. However, due to the blackbox nature of DNN, the existing structural coverage criteria are difficult to interpret, making it hard to understand the underlying principles of these criteria. The relationship between the structural coverage and the decision logic of DNNs is unknown. Moreover, recent studies have further revealed the non-existence of correlation between the structural coverage and DNN defect detection, which further posts concerns on what a suitable DNN testing criterion should be.In this article, we propose the interpretable coverage criteria through constructing the decision structure of a DNN. Mirroring the control flow graph of the traditional program, we first extract a decision graph from a DNN based on its interpretation, where a path of the decision graph represents a decision logic of the DNN. Based on the control flow and data flow of the decision graph, we propose two variants of path coverage to measure the adequacy of the test cases in exercising the decision logic. The higher the path coverage, the more diverse decision logic the DNN is expected to be explored. Our large-scale evaluation results demonstrate that: The path in the decision graph is effective in characterizing the decision of the DNN, and the proposed coverage criteria are also sensitive with errors, including natural errors and adversarial examples, and strongly correlate with the output impartiality.",Security,Healthy,The paper proposes an interpretable coverage criteria for testing DNN's; potentially allowing for better quality and reliable DNN's; which align with the value item Healthy in the value Security; as the software users are being provided more secure and bug-free applications.,"In the context of a ""Software User,"" the alignment of the paper with the value item Healthy and its corresponding value Security can be justified by the fact that the proposed interpretable coverage criteria for testing DNN's aim to improve the quality and reliability of these networks. By ensuring that DNN's are thoroughly tested, the paper contributes to providing software users with more secure and bug-free applications, ultimately promoting a healthier and safer software environment.",Agreed-Clarified,Disagree,reconcile with coder_2,Disagree,The justification doesn't align with the value item of the Healthy,
Exp C,Exp K,1554,TOSEM,AI & Machine Learning,NPC: Neuron Path Coverage via Characterizing Decision Logic of Deep Neural Networks,"Deep learning has recently been widely applied to many applications across different domains, e.g., image classification and audio recognition. However, the quality of Deep Neural Networks (DNNs) still raises concerns in the practical operational environment, which calls for systematic testing, especially in safety-critical scenarios. Inspired by software testing, a number of structural coverage criteria are designed and proposed to measure the test adequacy of DNNs. However, due to the blackbox nature of DNN, the existing structural coverage criteria are difficult to interpret, making it hard to understand the underlying principles of these criteria. The relationship between the structural coverage and the decision logic of DNNs is unknown. Moreover, recent studies have further revealed the non-existence of correlation between the structural coverage and DNN defect detection, which further posts concerns on what a suitable DNN testing criterion should be.In this article, we propose the interpretable coverage criteria through constructing the decision structure of a DNN. Mirroring the control flow graph of the traditional program, we first extract a decision graph from a DNN based on its interpretation, where a path of the decision graph represents a decision logic of the DNN. Based on the control flow and data flow of the decision graph, we propose two variants of path coverage to measure the adequacy of the test cases in exercising the decision logic. The higher the path coverage, the more diverse decision logic the DNN is expected to be explored. Our large-scale evaluation results demonstrate that: The path in the decision graph is effective in characterizing the decision of the DNN, and the proposed coverage criteria are also sensitive with errors, including natural errors and adversarial examples, and strongly correlate with the output impartiality.",Stimulation,Excitement in Life,The application of the proposed method would lead to diverse decision logic of the DNN being explored; making the software more interesting and exciting for the user. This links to the value item Excitement in Life in the value Stimulation.,"By employing the proposed method in 'Paper X' to explore diverse decision logic of the DNN, the software becomes more interesting and exciting for the user. This aligns with the value item Excitement in Life from the value Stimulation in Schwartz's Taxonomy. The user will be stimulated and engaged by the dynamic and varied behavior of the software, enhancing their overall experience and satisfaction.",Agreed-Clarified,Disagree,The text discusses the challenges and concerns surrounding the quality and testing of Deep Neural Networks. it should be under capability value item,Disagree,Does not seem to make anything inetersting to the users,
Exp D,Exp K,1554,TOSEM,AI & Machine Learning,NPC: Neuron Path Coverage via Characterizing Decision Logic of Deep Neural Networks,"Deep learning has recently been widely applied to many applications across different domains, e.g., image classification and audio recognition. However, the quality of Deep Neural Networks (DNNs) still raises concerns in the practical operational environment, which calls for systematic testing, especially in safety-critical scenarios. Inspired by software testing, a number of structural coverage criteria are designed and proposed to measure the test adequacy of DNNs. However, due to the blackbox nature of DNN, the existing structural coverage criteria are difficult to interpret, making it hard to understand the underlying principles of these criteria. The relationship between the structural coverage and the decision logic of DNNs is unknown. Moreover, recent studies have further revealed the non-existence of correlation between the structural coverage and DNN defect detection, which further posts concerns on what a suitable DNN testing criterion should be.In this article, we propose the interpretable coverage criteria through constructing the decision structure of a DNN. Mirroring the control flow graph of the traditional program, we first extract a decision graph from a DNN based on its interpretation, where a path of the decision graph represents a decision logic of the DNN. Based on the control flow and data flow of the decision graph, we propose two variants of path coverage to measure the adequacy of the test cases in exercising the decision logic. The higher the path coverage, the more diverse decision logic the DNN is expected to be explored. Our large-scale evaluation results demonstrate that: The path in the decision graph is effective in characterizing the decision of the DNN, and the proposed coverage criteria are also sensitive with errors, including natural errors and adversarial examples, and strongly correlate with the output impartiality.",Self Direction,Freedom,The interpretable coverage criteria construction of the decision structure of a DNN allows software users to have more freedom in interacting with DNN's. This is directly aligned with the value item Freedom from the value Self Direction.,"The justification for aligning 'Paper X' with the value item Freedom and its corresponding value Self Direction is based on the fact that the proposed interpretable coverage criteria allow software users to have more freedom in interacting with DNNs. By providing a clear decision structure and path coverage metrics, users can understand the underlying principles of the DNN's decision-making process and explore its decision logic more effectively. This aligns with the value of Freedom, as it empowers software users to exercise independent choices and make informed decisions while utilizing DNNs within a software context.",Agreed-Justified,Agree,,Agree,,
Exp E,Exp J,1556,TOSEM,Security & Privacy,Detecting and Augmenting Missing Key Aspects in Vulnerability Descriptions,"Security vulnerabilities have been continually disclosed and documented. For the effective understanding, management, and mitigation of the fast-growing number of vulnerabilities, an important practice in documenting vulnerabilities is to describe the key vulnerability aspects, such as vulnerability type, root cause, affected product, impact, attacker type, and attack vector. In this article, we first investigate 133,639 vulnerability reports in the Common Vulnerabilities and Exposures (CVE) database over the past 20 years. We find that 56\%, 85\%, 38\%, and 28\% of CVEs miss vulnerability type, root cause, attack vector, and attacker type, respectively. By comparing the differences of the latest updated CVE reports across different databases, we observe that 1,476 missing key aspects in 1,320 CVE descriptions were augmented manually in the National Vulnerability Database (NVD), which indicates that the vulnerability database maintainers try to complete the vulnerability descriptions in practice to mitigate such a problem.To help complete the missing information of key vulnerability aspects and reduce human efforts, we propose a neural-network-based approach called PMA to predict the missing key aspects of a vulnerability based on its known aspects. We systematically explore the design space of the neural network models and empirically identify the most effective model design in the scenario. Our ablation study reveals the prominent correlations among vulnerability aspects when predicting. Trained with historical CVEs, our model achieves 88\%, 71\%, 61\%, and 81\% in F1 for predicting the missing vulnerability type, root cause, attacker type, and attack vector of 8,623 aEURoefutureaEUR CVEs across 3 years, respectively. Furthermore, we validate the predicting performance of key aspect augmentation of CVEs based on the manually augmented CVE data collected from NVD, which confirms the practicality of our approach. We finally highlight that PMA has the ability to reduce human efforts by recommending and augmenting missing key aspects for vulnerability databases, and to facilitate other research works such as severity level prediction of CVEs based on the vulnerability descriptions.",Security,Healthy,The paper contributes a method for predicting the missing key aspects (vulnerability type; root cause; attacker type; and attack vector) of a vulnerability data which aids in enhancing the security aspects of an application; improving overall health and safety for users.,"My justification is based on the fact that the paper presents a neural-network-based approach called PMA, which predicts the missing key aspects of a vulnerability based on its known aspects. By completing the missing information in vulnerability descriptions, the paper aims to enhance the security aspects of applications and ultimately contribute to the overall health and safety of software users. Security is a fundamental aspect of maintaining a healthy software environment, and by addressing vulnerabilities, the paper aligns with the value item Healthy and its corresponding value Security.",Agreed-Justified,Agree,,Agree,,
Exp F,Exp J,1556,TOSEM,Security & Privacy,Detecting and Augmenting Missing Key Aspects in Vulnerability Descriptions,"Security vulnerabilities have been continually disclosed and documented. For the effective understanding, management, and mitigation of the fast-growing number of vulnerabilities, an important practice in documenting vulnerabilities is to describe the key vulnerability aspects, such as vulnerability type, root cause, affected product, impact, attacker type, and attack vector. In this article, we first investigate 133,639 vulnerability reports in the Common Vulnerabilities and Exposures (CVE) database over the past 20 years. We find that 56\%, 85\%, 38\%, and 28\% of CVEs miss vulnerability type, root cause, attack vector, and attacker type, respectively. By comparing the differences of the latest updated CVE reports across different databases, we observe that 1,476 missing key aspects in 1,320 CVE descriptions were augmented manually in the National Vulnerability Database (NVD), which indicates that the vulnerability database maintainers try to complete the vulnerability descriptions in practice to mitigate such a problem.To help complete the missing information of key vulnerability aspects and reduce human efforts, we propose a neural-network-based approach called PMA to predict the missing key aspects of a vulnerability based on its known aspects. We systematically explore the design space of the neural network models and empirically identify the most effective model design in the scenario. Our ablation study reveals the prominent correlations among vulnerability aspects when predicting. Trained with historical CVEs, our model achieves 88\%, 71\%, 61\%, and 81\% in F1 for predicting the missing vulnerability type, root cause, attacker type, and attack vector of 8,623 aEURoefutureaEUR CVEs across 3 years, respectively. Furthermore, we validate the predicting performance of key aspect augmentation of CVEs based on the manually augmented CVE data collected from NVD, which confirms the practicality of our approach. We finally highlight that PMA has the ability to reduce human efforts by recommending and augmenting missing key aspects for vulnerability databases, and to facilitate other research works such as severity level prediction of CVEs based on the vulnerability descriptions.",Security,Social Order,This method of predicting and managing vulnerabilities aligns with the value item of ''social order'' because it can aid in creating a safer software environment by identifying potential areas of threat and mitigating them; which ultimately contributes to the organization or structure of the social software environment.,"The alignment of 'Paper X' with the value item of Social Order and its corresponding value Security is justified because the proposed method of predicting and managing vulnerabilities contributes to the overall security and order within the software environment. By identifying and mitigating vulnerabilities, the method helps establish a more secure and stable software system, reducing the potential for disruptions and maintaining a sense of order in the social software environment.",Agreed-Justified,Agree,,Agree,,
Exp G,Exp K,1556,TOSEM,Security & Privacy,Detecting and Augmenting Missing Key Aspects in Vulnerability Descriptions,"Security vulnerabilities have been continually disclosed and documented. For the effective understanding, management, and mitigation of the fast-growing number of vulnerabilities, an important practice in documenting vulnerabilities is to describe the key vulnerability aspects, such as vulnerability type, root cause, affected product, impact, attacker type, and attack vector. In this article, we first investigate 133,639 vulnerability reports in the Common Vulnerabilities and Exposures (CVE) database over the past 20 years. We find that 56\%, 85\%, 38\%, and 28\% of CVEs miss vulnerability type, root cause, attack vector, and attacker type, respectively. By comparing the differences of the latest updated CVE reports across different databases, we observe that 1,476 missing key aspects in 1,320 CVE descriptions were augmented manually in the National Vulnerability Database (NVD), which indicates that the vulnerability database maintainers try to complete the vulnerability descriptions in practice to mitigate such a problem.To help complete the missing information of key vulnerability aspects and reduce human efforts, we propose a neural-network-based approach called PMA to predict the missing key aspects of a vulnerability based on its known aspects. We systematically explore the design space of the neural network models and empirically identify the most effective model design in the scenario. Our ablation study reveals the prominent correlations among vulnerability aspects when predicting. Trained with historical CVEs, our model achieves 88\%, 71\%, 61\%, and 81\% in F1 for predicting the missing vulnerability type, root cause, attacker type, and attack vector of 8,623 aEURoefutureaEUR CVEs across 3 years, respectively. Furthermore, we validate the predicting performance of key aspect augmentation of CVEs based on the manually augmented CVE data collected from NVD, which confirms the practicality of our approach. We finally highlight that PMA has the ability to reduce human efforts by recommending and augmenting missing key aspects for vulnerability databases, and to facilitate other research works such as severity level prediction of CVEs based on the vulnerability descriptions.",Security,National Security,Further; the proposed method improves the national security as it helps in better understanding and mitigation of vulnerabilities preventing major security breaches.,"The alignment of 'Paper X' with the value item National Security and its corresponding value Security is justified based on the fact that the proposed method aims to enhance the understanding, management, and mitigation of vulnerabilities. By effectively documenting and predicting key aspects of vulnerabilities, the method can assist in preventing major security breaches, thereby contributing to the overall goal of ensuring national security within a software context.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,1561,TOSEM,Security & Privacy,Time-travel Investigation: Toward Building a Scalable Attack Detection Framework on Ethereum,"Ethereum has been attracting lots of attacks, hence there is a pressing need to perform timely investigation and detect more attack instances. However, existing systems suffer from the scalability issue due to the following reasons. First, the tight coupling between malicious contract detection and blockchain data importing makes them infeasible to repeatedly detect different attacks. Second, the coarse-grained archive data makes them inefficient to replay transactions. Third, the separation between malicious contract detection and runtime state recovery consumes lots of storage.In this article, we propose a scalable attack detection framework named EthScope, which overcomes the scalability issue by neatly re-organizing the Ethereum state and efficiently locating suspicious transactions. It leverages the fine-grained state to support the replay of arbitrary transactions and proposes a well-designed schema to optimize the storage consumption. The performance evaluation shows that EthScope can solve the scalability issue, i.e., efficiently performing a large-scale analysis on billions of transactions, and a speedup of around  ( text{2,300}times )  when replaying transactions. It also has lower storage consumption compared with existing systems. Further analysis shows that EthScope can help analysts understand attack behaviors and detect more attack instances.",Security,Healthy,The paper proposes EthScope; a new framework for detecting attacks on the Ethereum network; thus contributing to the software users' health by providing a safer and more secure environment.,"The justification for aligning 'Paper X' with the value item Healthy and its corresponding value Security from a ""Software User"" perspective is that the EthScope framework proposed in the paper aims to detect attacks on the Ethereum network. By efficiently locating suspicious transactions and optimizing storage consumption, EthScope provides a safer environment for software users, ensuring the security of their transactions and protecting their assets. This directly contributes to a sense of security and well-being, which is aligned with the value item Healthy.",Agreed-Justified,Disagree,More incline to Achievement: Capable as it capable of detecting threats succesfully,Disagree,The justification doesn't align with the value item of the Healthy,
Exp B,Exp H,1561,TOSEM,Security & Privacy,Time-travel Investigation: Toward Building a Scalable Attack Detection Framework on Ethereum,"Ethereum has been attracting lots of attacks, hence there is a pressing need to perform timely investigation and detect more attack instances. However, existing systems suffer from the scalability issue due to the following reasons. First, the tight coupling between malicious contract detection and blockchain data importing makes them infeasible to repeatedly detect different attacks. Second, the coarse-grained archive data makes them inefficient to replay transactions. Third, the separation between malicious contract detection and runtime state recovery consumes lots of storage.In this article, we propose a scalable attack detection framework named EthScope, which overcomes the scalability issue by neatly re-organizing the Ethereum state and efficiently locating suspicious transactions. It leverages the fine-grained state to support the replay of arbitrary transactions and proposes a well-designed schema to optimize the storage consumption. The performance evaluation shows that EthScope can solve the scalability issue, i.e., efficiently performing a large-scale analysis on billions of transactions, and a speedup of around  ( text{2,300}times )  when replaying transactions. It also has lower storage consumption compared with existing systems. Further analysis shows that EthScope can help analysts understand attack behaviors and detect more attack instances.",Security,Social Order,Through the introduction of EthScope; the paper aligns with the value item of Social Order under the value of Security; as it aims to maintain order within the software environment by detecting and preventing potential threats and attacks.,"In 'Paper X', the proposed framework EthScope directly aligns with the value item of Social Order from the perspective of a Software User. By detecting and preventing potential threats and attacks, EthScope aims to maintain a sense of order and stability within the software environment. Through its scalable attack detection capabilities and efficient replay of transactions, EthScope contributes to the overall security and integrity of the system, ensuring that users can confidently and securely interact with the Ethereum platform.",Agreed-Clarified,Agree,,Agree,,
Exp C,Exp K,1561,TOSEM,Security & Privacy,Time-travel Investigation: Toward Building a Scalable Attack Detection Framework on Ethereum,"Ethereum has been attracting lots of attacks, hence there is a pressing need to perform timely investigation and detect more attack instances. However, existing systems suffer from the scalability issue due to the following reasons. First, the tight coupling between malicious contract detection and blockchain data importing makes them infeasible to repeatedly detect different attacks. Second, the coarse-grained archive data makes them inefficient to replay transactions. Third, the separation between malicious contract detection and runtime state recovery consumes lots of storage.In this article, we propose a scalable attack detection framework named EthScope, which overcomes the scalability issue by neatly re-organizing the Ethereum state and efficiently locating suspicious transactions. It leverages the fine-grained state to support the replay of arbitrary transactions and proposes a well-designed schema to optimize the storage consumption. The performance evaluation shows that EthScope can solve the scalability issue, i.e., efficiently performing a large-scale analysis on billions of transactions, and a speedup of around  ( text{2,300}times )  when replaying transactions. It also has lower storage consumption compared with existing systems. Further analysis shows that EthScope can help analysts understand attack behaviors and detect more attack instances.",Achievement,Successful,By efficiently and effectively detecting more attack instances; EthScope can indirectly contribute to a sense of achievement for the software user as they can successfully maintain a safer and more secure software environment.,"I apologize for any confusion. The justification for aligning 'Paper X' with the value item Successful and its corresponding value Achievement is based on the fact that EthScope, as mentioned in the abstract, can efficiently and effectively detect more attack instances. By doing so, it enables the software user to successfully maintain a safer and more secure software environment, which can be seen as an achievement in ensuring the protection and integrity of their software system.",Agreed-Justified,Agree,,Agree,,
Exp D,Exp K,1564,TOSEM,Software Testing & QA,Verification Witnesses,"Over the last years, witness-based validation of verification results has become an established practice in software verification: An independent validator re-establishes verification results of a software verifier using verification witnesses, which are stored in a standardized exchange format. In addition to validation, such exchangable information about proofs and alarms found by a verifier can be shared across verification tools, and users can apply independent third-party tools to visualize and explore witnesses to help them comprehend the causes of bugs or the reasons why a given program is correct. To achieve the goal of making verification results more accessible to engineers, it is necessary to consider witnesses as first-class exchangeable objects, stored independently from the source code and checked independently from the verifier that produced them, respecting the important principle of separation of concerns. We present the conceptual principles of verification witnesses, give a description of how to use them, provide a technical specification of the exchange format for witnesses, and perform an extensive experimental study on the application of witness-based result validation, using the validators CPAchecker, UAutomizer, CPA-witness2test, and FShell-witness2test.",Achievement,Intelligent,The paper contributes to the user's independence in validation of software verification results; aligning with the value item 'Intelligent' and its corresponding value 'Achievement'.,"In the abstract of 'Paper X', it is stated that witness-based validation allows independent third-party tools to visualize and explore witnesses, aiding users in comprehending the causes of bugs and the reasons for program correctness. This means that users can independently validate the verification results and understand the underlying reasons, demonstrating their intelligence and achievement in the software verification process. Therefore, the alignment with the value item 'Intelligent' and its corresponding value 'Achievement' is evident in the paper's contributions from a software user perspective.",Agreed-Justified,Agree,,Agree,,
Exp E,Exp J,1564,TOSEM,Software Testing & QA,Verification Witnesses,"Over the last years, witness-based validation of verification results has become an established practice in software verification: An independent validator re-establishes verification results of a software verifier using verification witnesses, which are stored in a standardized exchange format. In addition to validation, such exchangable information about proofs and alarms found by a verifier can be shared across verification tools, and users can apply independent third-party tools to visualize and explore witnesses to help them comprehend the causes of bugs or the reasons why a given program is correct. To achieve the goal of making verification results more accessible to engineers, it is necessary to consider witnesses as first-class exchangeable objects, stored independently from the source code and checked independently from the verifier that produced them, respecting the important principle of separation of concerns. We present the conceptual principles of verification witnesses, give a description of how to use them, provide a technical specification of the exchange format for witnesses, and perform an extensive experimental study on the application of witness-based result validation, using the validators CPAchecker, UAutomizer, CPA-witness2test, and FShell-witness2test.",Security,Reciprocation of Favors,The paper promotes reciprocation of favors between software tools through sharing of information; meeting the value item 'Reciprocation of Favors' from the 'Security' value.,"In the abstract, the paper highlights the sharing of information about proofs and alarms found by a verifier across verification tools while also promoting the use of independent third-party tools to visualize and explore witnesses. This emphasis on sharing and collaboration aligns with the value item of Reciprocation of Favors from the Security value in Schwartz's Taxonomy. By facilitating the exchange of information and promoting reciprocity among software tools, the paper contributes to the sense of security for software users, as they can benefit from the collective knowledge and resources within the software community.",Agreed-Clarified,Agree,,Agree,,
Exp F,Exp J,1564,TOSEM,Software Testing & QA,Verification Witnesses,"Over the last years, witness-based validation of verification results has become an established practice in software verification: An independent validator re-establishes verification results of a software verifier using verification witnesses, which are stored in a standardized exchange format. In addition to validation, such exchangable information about proofs and alarms found by a verifier can be shared across verification tools, and users can apply independent third-party tools to visualize and explore witnesses to help them comprehend the causes of bugs or the reasons why a given program is correct. To achieve the goal of making verification results more accessible to engineers, it is necessary to consider witnesses as first-class exchangeable objects, stored independently from the source code and checked independently from the verifier that produced them, respecting the important principle of separation of concerns. We present the conceptual principles of verification witnesses, give a description of how to use them, provide a technical specification of the exchange format for witnesses, and perform an extensive experimental study on the application of witness-based result validation, using the validators CPAchecker, UAutomizer, CPA-witness2test, and FShell-witness2test.",Self Direction,Independent,The paper contributes to providing users with platform independent tools for visualization and exploration; satisfying the value item 'Independent'; under 'Self Direction'.,"In 'Paper X', the author discusses the importance of making verification results more accessible to engineers by considering witnesses as independent and exchangeable objects. This means that users can apply third-party tools to visualize and explore the witnesses, helping them comprehend the causes of bugs or the reasons why a given program is correct. By providing platform-independent tools for visualization and exploration, the paper aligns with the value item 'Independent' and the corresponding value of 'Self Direction' because it empowers software users to independently analyze and understand verification results without relying solely on the original verifiers or developers.",Agreed-Clarified,Disagree,I couldn't find a connection to independency ,Agree,,
Exp G,Exp K,1566,TOSEM,Software Project Management,A Common Terminology for Software Risk Management,"In order to improve and sustain their competitiveness over time, organisations nowadays need to undertake different initiatives to adopt frameworks, models and standards that will allow them to align and improve their business processes. In spite of these efforts, organisations may still encounter governance and management problems. This is where Risk Management (RM) can play a major role, since its purpose is to contribute to the creation and preservation of value in the context of the organisation's processes. RM is a complex and subjective activity that requires experience and a high level of knowledge about risks, and it is for this reason that standardisation institutions and researchers have made great efforts to define initiatives to overcome these challenges. However, the RM field nevertheless presents a lack of uniformity in its terms and concepts, due to the different contexts and scopes of application, a situation that can generate ambiguities and misunderstandings. To address these issues, this paper aims to present an ontology called SRMO (Software Risk Management Ontology), which seeks to unify the terms and concepts associated with RM and provide an integrated and holistic view of risk. In doing so, the Pipeline framework has been applied in order to assure and verify the quality of the proposed ontology, and it has been implemented in Prot\'{e}g\'{e} and validated by means of competency questions. Three application scenarios of this ontology demonstrating their usefulness in the software engineering field are presented in this paper. We believe that this ontology can be useful for organisations that are interested in: (i) establishing an RM strategy from an integrated approach, (ii) defining the elements that help to identify risks and the criteria that support decision-making in risk assessment, and (iii) helping the involved stakeholders during the process of risk management.",Power,Social Recognition,There is evidence that the paper is concerned about social recognition by aligning organizations' processes with risk management frameworks which may improve and maintain trust in such organizations. This contributes to social recognition in society; which users may appreciate as a value.,"In the paper abstract, it is mentioned that organisations need to adopt frameworks, models, and standards to improve their business processes and sustain competitiveness. By implementing Risk Management (RM) initiatives, organisations are able to contribute to the creation and preservation of value within their processes. This aligns with the value item of Social Recognition, as it implies that organisations that effectively manage risks are more likely to gain recognition and trust from society. From a software user's perspective, being associated with organizations that are socially recognized for their risk management practices can be seen as a valuable attribute. Therefore, the alignment of 'Paper X' with the value item of Social Recognition and its corresponding value of Power is evident from the focus on improving and aligning organizations' processes through RM.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,1566,TOSEM,Software Project Management,A Common Terminology for Software Risk Management,"In order to improve and sustain their competitiveness over time, organisations nowadays need to undertake different initiatives to adopt frameworks, models and standards that will allow them to align and improve their business processes. In spite of these efforts, organisations may still encounter governance and management problems. This is where Risk Management (RM) can play a major role, since its purpose is to contribute to the creation and preservation of value in the context of the organisation's processes. RM is a complex and subjective activity that requires experience and a high level of knowledge about risks, and it is for this reason that standardisation institutions and researchers have made great efforts to define initiatives to overcome these challenges. However, the RM field nevertheless presents a lack of uniformity in its terms and concepts, due to the different contexts and scopes of application, a situation that can generate ambiguities and misunderstandings. To address these issues, this paper aims to present an ontology called SRMO (Software Risk Management Ontology), which seeks to unify the terms and concepts associated with RM and provide an integrated and holistic view of risk. In doing so, the Pipeline framework has been applied in order to assure and verify the quality of the proposed ontology, and it has been implemented in Prot\'{e}g\'{e} and validated by means of competency questions. Three application scenarios of this ontology demonstrating their usefulness in the software engineering field are presented in this paper. We believe that this ontology can be useful for organisations that are interested in: (i) establishing an RM strategy from an integrated approach, (ii) defining the elements that help to identify risks and the criteria that support decision-making in risk assessment, and (iii) helping the involved stakeholders during the process of risk management.",Power,Authority,The paper also highlights the role of authority by emphasizing the need for organizations to take control of their processes by instituting risk management strategies. Users may appreciate such authority as it contributes to more secure and stable environments.,"In the paper, it is stated that organizations need to adopt frameworks and models to align and improve their business processes to sustain their competitiveness. Risk management is highlighted as a means to contribute to the creation and preservation of value in this context. By implementing risk management strategies, organizations exercise authority and take control over their processes, promoting security and stability. From a software user's perspective, having an organization with authority over their processes can instill confidence and trust, as it demonstrates a commitment to maintaining a secure and stable environment. This aligns with the value item Authority and its corresponding value Power from Schwartz's Taxonomy.",Agreed-Clarified,Agree,,Agree,,
Exp B,Exp H,1566,TOSEM,Software Project Management,A Common Terminology for Software Risk Management,"In order to improve and sustain their competitiveness over time, organisations nowadays need to undertake different initiatives to adopt frameworks, models and standards that will allow them to align and improve their business processes. In spite of these efforts, organisations may still encounter governance and management problems. This is where Risk Management (RM) can play a major role, since its purpose is to contribute to the creation and preservation of value in the context of the organisation's processes. RM is a complex and subjective activity that requires experience and a high level of knowledge about risks, and it is for this reason that standardisation institutions and researchers have made great efforts to define initiatives to overcome these challenges. However, the RM field nevertheless presents a lack of uniformity in its terms and concepts, due to the different contexts and scopes of application, a situation that can generate ambiguities and misunderstandings. To address these issues, this paper aims to present an ontology called SRMO (Software Risk Management Ontology), which seeks to unify the terms and concepts associated with RM and provide an integrated and holistic view of risk. In doing so, the Pipeline framework has been applied in order to assure and verify the quality of the proposed ontology, and it has been implemented in Prot\'{e}g\'{e} and validated by means of competency questions. Three application scenarios of this ontology demonstrating their usefulness in the software engineering field are presented in this paper. We believe that this ontology can be useful for organisations that are interested in: (i) establishing an RM strategy from an integrated approach, (ii) defining the elements that help to identify risks and the criteria that support decision-making in risk assessment, and (iii) helping the involved stakeholders during the process of risk management.",Security,Healthy,The paper mentions the creation and preservation of value in organizations' processes; which may indirectly contribute to users' perception of safety and reliability in using the software; thus aligning with the value item Health and its associated value Security.,"In the abstract, the paper mentions that Risk Management (RM) contributes to the creation and preservation of value in organizations' processes. While the paper does not explicitly mention the concept of ""security"" or ""safety,"" it can be inferred that by effectively managing risks and ensuring the alignment and improvement of business processes, the software user can have a perception of safety and reliability in using the software. This aligns with the value item Healthy and its corresponding value Security as it indirectly addresses the user's need for security in the software context.",Agreed-Justified,Disagree,reconcile with coder_2,Disagree,The justification doesn't align with the value item of the Healthy,
Exp C,Exp K,1573,TOSEM,Accessibility & User Experience,Accessibility in Software Practice: A PractitioneraEUR(tm)s Perspective,"Being able to access software in daily life is vital for everyone, and thus accessibility is a fundamental challenge for software development. However, given the number of accessibility issues reported by many users, e.g., in app reviews, it is not clear if accessibility is widely integrated into current software projects and how software projects address accessibility issues. In this article, we report a study of the critical challenges and benefits of incorporating accessibility into software development and design. We applied a mixed qualitative and quantitative approach for gathering data from 15 interviews and 365 survey respondents from 26 countries across five continents to understand how practitioners perceive accessibility development and design in practice. We got 44 statements grouped into eight topics on accessibility from practitionersaEUR(tm) viewpoints and different software development stages. Our statistical analysis reveals substantial gaps between groups, e.g., practitioners have Direct vs. Indirect accessibility relevant work experience when they reviewed the summarized statements. These gaps might hinder the quality of accessibility development and design, and we use our findings to establish a set of guidelines to help practitioners be aware of accessibility challenges and benefit factors. We suggest development teams put accessibility as a first-class consideration throughout the software development process, and we also propose some remedies to resolve the gaps between groups and to highlight key future research directions to incorporate accessibility into software design and development.",Power,Social Recognition,The paper proposes guidelines to help practitioners be aware of accessibility challenges which can lead to better accessibility in software. This can enhance the social recognition of users with disabilities; aligning with the value item Social Recognition under the value Power.,"In the context of 'Paper X', the proposed guidelines for incorporating accessibility into software development can enhance the social recognition of users with disabilities. By addressing accessibility challenges, software projects become more inclusive, empowering individuals with disabilities to access and use software effectively. This aligns with the value item Social Recognition under the value Power as it contributes to the recognition and visibility of individuals with disabilities, elevating their status and empowering them within the software context.",Agreed-Justified,Agree,,Agree,,
Exp D,Exp K,1573,TOSEM,Accessibility & User Experience,Accessibility in Software Practice: A PractitioneraEUR(tm)s Perspective,"Being able to access software in daily life is vital for everyone, and thus accessibility is a fundamental challenge for software development. However, given the number of accessibility issues reported by many users, e.g., in app reviews, it is not clear if accessibility is widely integrated into current software projects and how software projects address accessibility issues. In this article, we report a study of the critical challenges and benefits of incorporating accessibility into software development and design. We applied a mixed qualitative and quantitative approach for gathering data from 15 interviews and 365 survey respondents from 26 countries across five continents to understand how practitioners perceive accessibility development and design in practice. We got 44 statements grouped into eight topics on accessibility from practitionersaEUR(tm) viewpoints and different software development stages. Our statistical analysis reveals substantial gaps between groups, e.g., practitioners have Direct vs. Indirect accessibility relevant work experience when they reviewed the summarized statements. These gaps might hinder the quality of accessibility development and design, and we use our findings to establish a set of guidelines to help practitioners be aware of accessibility challenges and benefit factors. We suggest development teams put accessibility as a first-class consideration throughout the software development process, and we also propose some remedies to resolve the gaps between groups and to highlight key future research directions to incorporate accessibility into software design and development.",Security,Family Security,By incorporating accessibility into software design; there is an increase in the security and safety of the families of software users; especially for those with family members who have disabilities. This aligns with the value item Family Security under the value Security.,"By incorporating accessibility into software design, 'Paper X' aims to address the challenges and benefits of ensuring software accessibility for all users. One of the main contributions of the paper is highlighting the gaps between practitioners' experiences in addressing accessibility issues and the potential impact of these gaps on the quality of accessibility development and design. By considering the value item Family Security, the paper emphasizes the importance of making software accessible for individuals with disabilities, thereby providing a sense of security and safety for their families. This aligns with the broader value of Security, as it promotes a secure and inclusive environment for software users and their loved ones.",Agreed-Clarified,Agree,,Agree,,
Exp E,Exp J,1573,TOSEM,Accessibility & User Experience,Accessibility in Software Practice: A PractitioneraEUR(tm)s Perspective,"Being able to access software in daily life is vital for everyone, and thus accessibility is a fundamental challenge for software development. However, given the number of accessibility issues reported by many users, e.g., in app reviews, it is not clear if accessibility is widely integrated into current software projects and how software projects address accessibility issues. In this article, we report a study of the critical challenges and benefits of incorporating accessibility into software development and design. We applied a mixed qualitative and quantitative approach for gathering data from 15 interviews and 365 survey respondents from 26 countries across five continents to understand how practitioners perceive accessibility development and design in practice. We got 44 statements grouped into eight topics on accessibility from practitionersaEUR(tm) viewpoints and different software development stages. Our statistical analysis reveals substantial gaps between groups, e.g., practitioners have Direct vs. Indirect accessibility relevant work experience when they reviewed the summarized statements. These gaps might hinder the quality of accessibility development and design, and we use our findings to establish a set of guidelines to help practitioners be aware of accessibility challenges and benefit factors. We suggest development teams put accessibility as a first-class consideration throughout the software development process, and we also propose some remedies to resolve the gaps between groups and to highlight key future research directions to incorporate accessibility into software design and development.",Universalism,Equality,The study promotes equality by addressing accessibility issues and advocating for its integration in software design and development. This aligns with the Equality value item under the Universalism value.,"The main contribution of 'Paper X' is to address accessibility issues in software design and development. By advocating for the integration of accessibility and highlighting the gaps between groups in addressing accessibility, the paper promotes equality by ensuring that software is accessible to all users, regardless of their abilities. This directly aligns with the value item Equality under the Universalism value, which emphasizes fairness, justice, and equal opportunities for all individuals.",Agreed-Clarified,Agree,,Agree,,
Exp F,Exp J,1581,TOSEM,Code Generation & Analysis,Verifix: Verified Repair of Programming&nbsp;Assignments,"Automated feedback generation for introductory programming assignments is useful for programming education. Most works try to generate feedback to correct a student program by comparing its behavior with an instructoraEUR(tm)s reference program on selected tests. In this work, our aim is to generate verifiably correct program repairs as student feedback. A student-submitted program is aligned and composed with a reference solution in terms of control flow, and the variables of the two programs are automatically aligned via predicates describing the relationship between the variables. When verification attempt for the obtained aligned program fails, we turn a verification problem into a MaxSMT problem whose solution leads to a minimal repair. We have conducted experiments on student assignments curated from a widely deployed intelligent tutoring system. Our results show that generating verified repair without sacrificing the overall repair rate is possible. In fact, our implementation, Verifix, is shown to outperform Clara, a state-of-the-art tool, in terms of repair rate. This shows the promise of using verified repair to generate high confidence feedback in programming pedagogy settings.",Stimulation,Excitement in Life,The automated feedback generation discussed in the paper provides excitement in learning programming for the students due to high confidence feedback; aligning with the value item 'Excitement in Life' and the corresponding value 'Stimulation'.,"In the context of the paper, the automated feedback generation approach discussed provides excitement and stimulation in the learning process of programming for students. By generating high-confidence feedback and verified program repairs, students can receive immediate and accurate guidance, which enhances their learning experience and potentially increases their excitement and engagement with the subject matter. This alignment with the value item 'Excitement in Life' and its corresponding value 'Stimulation' highlights the positive impact of the paper's contributions on the software user's perspective.",Agreed-Clarified,Agree,,Agree,,
Exp G,Exp K,1581,TOSEM,Code Generation & Analysis,Verifix: Verified Repair of Programming&nbsp;Assignments,"Automated feedback generation for introductory programming assignments is useful for programming education. Most works try to generate feedback to correct a student program by comparing its behavior with an instructoraEUR(tm)s reference program on selected tests. In this work, our aim is to generate verifiably correct program repairs as student feedback. A student-submitted program is aligned and composed with a reference solution in terms of control flow, and the variables of the two programs are automatically aligned via predicates describing the relationship between the variables. When verification attempt for the obtained aligned program fails, we turn a verification problem into a MaxSMT problem whose solution leads to a minimal repair. We have conducted experiments on student assignments curated from a widely deployed intelligent tutoring system. Our results show that generating verified repair without sacrificing the overall repair rate is possible. In fact, our implementation, Verifix, is shown to outperform Clara, a state-of-the-art tool, in terms of repair rate. This shows the promise of using verified repair to generate high confidence feedback in programming pedagogy settings.",Achievement,Successful,The tool 'Verifix' in the paper helps students in becoming successful in their programming assignments; aligning with the value item 'Successful' and the corresponding value 'Achievement'.,"The justification for aligning 'Paper X' with the value item Successful and its corresponding value Achievement is based on the fact that the tool 'Verifix' described in the paper aims to generate verified repairs for student programs in introductory programming assignments. By providing accurate and correct feedback to students, the tool helps them succeed in their programming tasks, thereby contributing to their achievement of successfully completing the assignments. This aligns with the value of Achievement and the desire for individuals to be successful in their endeavors.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,1581,TOSEM,Code Generation & Analysis,Verifix: Verified Repair of Programming&nbsp;Assignments,"Automated feedback generation for introductory programming assignments is useful for programming education. Most works try to generate feedback to correct a student program by comparing its behavior with an instructoraEUR(tm)s reference program on selected tests. In this work, our aim is to generate verifiably correct program repairs as student feedback. A student-submitted program is aligned and composed with a reference solution in terms of control flow, and the variables of the two programs are automatically aligned via predicates describing the relationship between the variables. When verification attempt for the obtained aligned program fails, we turn a verification problem into a MaxSMT problem whose solution leads to a minimal repair. We have conducted experiments on student assignments curated from a widely deployed intelligent tutoring system. Our results show that generating verified repair without sacrificing the overall repair rate is possible. In fact, our implementation, Verifix, is shown to outperform Clara, a state-of-the-art tool, in terms of repair rate. This shows the promise of using verified repair to generate high confidence feedback in programming pedagogy settings.",Security,Healthy,The automated program repairs as student feedback discussed in the paper help students maintain a healthy learning curve with programming assignments. This contributes to the value item 'Healthy' and its corresponding value 'Security'.,"In the context of a ""Software User,"" the alignment of 'Paper X' with the value item Healthy and its corresponding value Security can be justified by the fact that the automated program repairs as student feedback ensure that the learning process is secure and reliable. By generating verifiably correct program repairs, students can feel confident in their progress and have a sense of security in their learning journey. This contributes to their overall mental and emotional well-being, which is aligned with the value of Security.",Agreed-Clarified,Agree,,Disagree,The justification doesn't align with the value item of the Healthy,
Exp B,Exp H,1586,TOSEM,AI & Machine Learning,Uncertainty-aware Prediction Validator in Deep Learning Models for Cyber-physical System Data,"The use of Deep learning in Cyber-Physical Systems (CPSs) is gaining popularity due to its ability to bring intelligence to CPS behaviors. However, both CPSs and deep learning have inherent uncertainty. Such uncertainty, if not handled adequately, can lead to unsafe CPS behavior. The first step toward addressing such uncertainty in deep learning is to quantify uncertainty. Hence, we propose a novel method called NIRVANA (uNcertaInty pRediction ValidAtor iN Ai) for prediction validation based on uncertainty metrics. To this end, we first employ prediction-time Dropout-based Neural Networks to quantify uncertainty in deep learning models applied to CPS data. Second, such quantified uncertainty is taken as the input to predict wrong labels using a support vector machine, with the aim of building a highly discriminating prediction validator model with uncertainty values. In addition, we investigated the relationship between uncertainty quantification and prediction performance and conducted experiments to obtain optimal dropout ratios. We conducted all the experiments with four real-world CPS datasets. Results show that uncertainty quantification is negatively correlated to prediction performance of a deep learning model of CPS data. Also, our dropout ratio adjustment approach is effective in reducing uncertainty of correct predictions while increasing uncertainty of wrong predictions.",Self Direction,Independent,NIRVANA gives users independent control over the uncertainty in their deep learning models through its dropout ratio adjustment approach.,"In the context of 'Paper X', the alignment of the value item Independent and its corresponding value Self Direction from a ""Software User"" perspective is justified by the fact that NIRVANA provides users with the ability to independently control the uncertainty in their deep learning models through the proposed dropout ratio adjustment approach. This allows users to make informed decisions about the predictions and behavior of their CPSs, giving them the freedom to customize and tailor the models according to their specific needs and preferences. By having independent control over uncertainty, users are empowered to exercise their self-direction and take ownership of the performance and safety of their CPSs.",Agreed-Clarified,Agree,,Agree,,
Exp D,Exp K,1586,TOSEM,AI & Machine Learning,Uncertainty-aware Prediction Validator in Deep Learning Models for Cyber-physical System Data,"The use of Deep learning in Cyber-Physical Systems (CPSs) is gaining popularity due to its ability to bring intelligence to CPS behaviors. However, both CPSs and deep learning have inherent uncertainty. Such uncertainty, if not handled adequately, can lead to unsafe CPS behavior. The first step toward addressing such uncertainty in deep learning is to quantify uncertainty. Hence, we propose a novel method called NIRVANA (uNcertaInty pRediction ValidAtor iN Ai) for prediction validation based on uncertainty metrics. To this end, we first employ prediction-time Dropout-based Neural Networks to quantify uncertainty in deep learning models applied to CPS data. Second, such quantified uncertainty is taken as the input to predict wrong labels using a support vector machine, with the aim of building a highly discriminating prediction validator model with uncertainty values. In addition, we investigated the relationship between uncertainty quantification and prediction performance and conducted experiments to obtain optimal dropout ratios. We conducted all the experiments with four real-world CPS datasets. Results show that uncertainty quantification is negatively correlated to prediction performance of a deep learning model of CPS data. Also, our dropout ratio adjustment approach is effective in reducing uncertainty of correct predictions while increasing uncertainty of wrong predictions.",Security,Social Order,NIRVANA is instrumental in managing uncertainties and thereby maintaining social order by ensuring improved and more reliable predictions in CPS behaviors.,"The justification for aligning 'Paper X' with the value item Social Order and its corresponding value Security from a ""Software User"" perspective is based on the understanding that the NIRVANA method proposed in the paper addresses the uncertainty in deep learning applied to Cyber-Physical Systems (CPSs). By quantifying and validating uncertainty in CPS behaviors, the method aims to improve prediction performance and reduce the potential for unsafe behavior. This can contribute to maintaining social order within the software context by ensuring more reliable and accurate predictions, thus enhancing the overall security and stability of CPSs.",Agreed-Clarified,Agree,,Agree,,
Exp E,Exp J,1588,TOSEM,Software Engineering Practices,WomenaEUR(tm)s Participation in Open Source Software: A Survey of the Literature,"Women are underrepresented in Open Source Software (OSS) projects, as a result of which, not only do women lose career and skill development opportunities, but the projects themselves suffer from a lack of diversity of perspectives. Practitioners and researchers need to understand more about the phenomenon; however, studies about women in open source are spread across multiple fields, including information systems, software engineering, and social science. This article systematically maps, aggregates, and synthesizes the state-of-the-art on womenaEUR(tm)s participation in OSS. It focuses on women contributorsaEUR(tm) representation and demographics, how they contribute, their motivations and challenges, and strategies employed by communities to attract and retain women. We identified 51 articles (published between 2000 and 2021) that investigated womenaEUR(tm)s participation in OSS. We found evidence in these papers about who are the women who contribute, what motivates them to contribute, what types of contributions they make, challenges they face, and strategies proposed to support their participation. According to these studies, only about 5\% of projects were reported to have women as core developers, and women authored less than 5\% of pull-requests, but had similar or even higher rates of pull-request acceptances than men. Women make both code and non-code contributions, and their motivations to contribute include learning new skills, altruism, reciprocity, and kinship. Challenges that women face in OSS are mainly social, including lack of peer parity and non-inclusive communication from a toxic culture. We found 10 strategies reported in the literature, which we mapped to the reported challenges. Based on these results, we provide guidelines for future research and practice.",Benevolence,Helpful,The paper emphasizes the need for encouraging more women into OSS projects and explores strategies to attract and retain them. This aligns with the value item 'Helpful' and its corresponding value 'Benevolence' since the entire purpose is to assist and support women to participate in open source projects.,"The justification for aligning 'Paper X' with the value item Helpful and its corresponding value Benevolence from a ""Software User"" perspective is based on the explicit emphasis of the paper on encouraging women to participate in OSS projects and the exploration of strategies to attract and retain them. This aligns with the value of Benevolence as it involves assisting and supporting others, in this case, women, to actively participate in open source projects. The goal of the paper is to promote inclusivity and diversity within the software user community by addressing the underrepresentation of women and providing guidance for future research and practice.",Agreed-Justified,Agree,,Agree,,
Exp F,Exp J,1588,TOSEM,Software Engineering Practices,WomenaEUR(tm)s Participation in Open Source Software: A Survey of the Literature,"Women are underrepresented in Open Source Software (OSS) projects, as a result of which, not only do women lose career and skill development opportunities, but the projects themselves suffer from a lack of diversity of perspectives. Practitioners and researchers need to understand more about the phenomenon; however, studies about women in open source are spread across multiple fields, including information systems, software engineering, and social science. This article systematically maps, aggregates, and synthesizes the state-of-the-art on womenaEUR(tm)s participation in OSS. It focuses on women contributorsaEUR(tm) representation and demographics, how they contribute, their motivations and challenges, and strategies employed by communities to attract and retain women. We identified 51 articles (published between 2000 and 2021) that investigated womenaEUR(tm)s participation in OSS. We found evidence in these papers about who are the women who contribute, what motivates them to contribute, what types of contributions they make, challenges they face, and strategies proposed to support their participation. According to these studies, only about 5\% of projects were reported to have women as core developers, and women authored less than 5\% of pull-requests, but had similar or even higher rates of pull-request acceptances than men. Women make both code and non-code contributions, and their motivations to contribute include learning new skills, altruism, reciprocity, and kinship. Challenges that women face in OSS are mainly social, including lack of peer parity and non-inclusive communication from a toxic culture. We found 10 strategies reported in the literature, which we mapped to the reported challenges. Based on these results, we provide guidelines for future research and practice.",Universalism,Equality,The paper addresses the underrepresentation of women in OSS projects and by doing this; it‚Äö√Ñ√∂‚àö√ë‚àö¬•s promoting 'Equality'; which aligns with the value item 'Equality' in value 'Universalism'.,"The justification is that the paper's focus on addressing the underrepresentation of women in OSS projects aligns with the value item of 'Equality' in the value 'Universalism.' By examining the demographics, motivations, and challenges faced by women in OSS, the paper seeks to understand and promote equal opportunities for women in the software development community. This aligns with the broader value of Universalism, which emphasizes broadmindedness, equality, and social justice.",Agreed-Justified,Agree,,Agree,,
Exp G,Exp K,1588,TOSEM,Software Engineering Practices,WomenaEUR(tm)s Participation in Open Source Software: A Survey of the Literature,"Women are underrepresented in Open Source Software (OSS) projects, as a result of which, not only do women lose career and skill development opportunities, but the projects themselves suffer from a lack of diversity of perspectives. Practitioners and researchers need to understand more about the phenomenon; however, studies about women in open source are spread across multiple fields, including information systems, software engineering, and social science. This article systematically maps, aggregates, and synthesizes the state-of-the-art on womenaEUR(tm)s participation in OSS. It focuses on women contributorsaEUR(tm) representation and demographics, how they contribute, their motivations and challenges, and strategies employed by communities to attract and retain women. We identified 51 articles (published between 2000 and 2021) that investigated womenaEUR(tm)s participation in OSS. We found evidence in these papers about who are the women who contribute, what motivates them to contribute, what types of contributions they make, challenges they face, and strategies proposed to support their participation. According to these studies, only about 5\% of projects were reported to have women as core developers, and women authored less than 5\% of pull-requests, but had similar or even higher rates of pull-request acceptances than men. Women make both code and non-code contributions, and their motivations to contribute include learning new skills, altruism, reciprocity, and kinship. Challenges that women face in OSS are mainly social, including lack of peer parity and non-inclusive communication from a toxic culture. We found 10 strategies reported in the literature, which we mapped to the reported challenges. Based on these results, we provide guidelines for future research and practice.",Self Direction,Independent,The study offers insights into motivations and contributions of women in OSS; promoting independent thinking and action; which aligns with the value item 'Independent' in the value 'Self Direction'.,"The study on women's participation in OSS aligns with the value item 'Independent' and its corresponding value of 'Self Direction' because it focuses on understanding the motivations and contributions of women in open-source projects. By examining the factors that influence women's participation, the study promotes independent thinking and action by empowering women to make their own choices and actively contribute to OSS projects, thus aligning with the value of self-direction.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,1590,TOSEM,Security & Privacy,Consent Verification Monitoring,"Advances in personalization of digital services are driven by low-cost data collection and processing, in addition to the wide variety of third-party frameworks for authentication, storage, and marketing. New privacy regulations, such as the General Data Protection Regulation (GDPR) and the California Consumer Privacy Act, increasingly require organizations to explicitly state their data practices in privacy policies. When data practices change, a new version of the policy is released. This can occur a few times a year, when data collection or processing requirements are rapidly changing. Consent evolution raises specific challenges to ensuring GDPR compliance. We propose a formal consent framework to support organizations, data users, and data subjects in their understanding of policy evolution under a consent regime that supports both the retroactive and non-retroactive granting and withdrawal of consent. The contributions include (i) a formal framework to reason about data collection and access under multiple consent granting and revocation scenarios, (ii) a scripting language that implements the consent framework for encoding and executing different scenarios, (iii) five consent evolution use cases that illustrate how organizations would evolve their policies using this framework, and (iv) a scalability evaluation of the reasoning framework. The framework models are used to verify when user consent prevents or detects unauthorized data collection and access. The framework can be integrated into a runtime architecture to monitor policy violations as data practices evolve in real time. The framework was evaluated using the five use cases and a simulation to measure the framework scalability. The simulation results show that the approach is computationally scalable for use in runtime consent monitoring under a standard model of data collection and access and practice and policy evolution.",Security,Healthy,The paper contributes a formal consent framework that prevents or detects unauthorized data collection and access; which can contribute to the software user's health in terms of data security and privacy protection.,"In 'Paper X', the formal consent framework proposed aims to prevent or detect unauthorized data collection and access. By ensuring that users have control over their personal data and the ability to grant or revoke consent, the framework directly aligns with the value item of Security from Schwartz's Taxonomy. The protection of user privacy and data security contributes to the overall well-being and peace of mind of software users, promoting a sense of security within the digital environment.",Agreed-Justified,Agree,,Disagree,"The justification doesn't align with the value item of the Healthy
",
Exp B,Exp H,1590,TOSEM,Security & Privacy,Consent Verification Monitoring,"Advances in personalization of digital services are driven by low-cost data collection and processing, in addition to the wide variety of third-party frameworks for authentication, storage, and marketing. New privacy regulations, such as the General Data Protection Regulation (GDPR) and the California Consumer Privacy Act, increasingly require organizations to explicitly state their data practices in privacy policies. When data practices change, a new version of the policy is released. This can occur a few times a year, when data collection or processing requirements are rapidly changing. Consent evolution raises specific challenges to ensuring GDPR compliance. We propose a formal consent framework to support organizations, data users, and data subjects in their understanding of policy evolution under a consent regime that supports both the retroactive and non-retroactive granting and withdrawal of consent. The contributions include (i) a formal framework to reason about data collection and access under multiple consent granting and revocation scenarios, (ii) a scripting language that implements the consent framework for encoding and executing different scenarios, (iii) five consent evolution use cases that illustrate how organizations would evolve their policies using this framework, and (iv) a scalability evaluation of the reasoning framework. The framework models are used to verify when user consent prevents or detects unauthorized data collection and access. The framework can be integrated into a runtime architecture to monitor policy violations as data practices evolve in real time. The framework was evaluated using the five use cases and a simulation to measure the framework scalability. The simulation results show that the approach is computationally scalable for use in runtime consent monitoring under a standard model of data collection and access and practice and policy evolution.",Security,National Security,The paper proposes a framework for understanding policy evolution under a consent regime in compliance with privacy regulations; contributing to the software user's sense of national security by enforcing rules that protect individual data rights.,"In ""Paper X,"" the proposed framework for understanding policy evolution under a consent regime helps to establish and enforce rules that protect individual data rights, which ultimately contributes to the software user's sense of national security. By ensuring compliance with privacy regulations and providing a mechanism for granting and revoking consent, the framework helps to safeguard personal information and prevent unauthorized data collection and access. This aligns with the value item National Security, as it pertains to protecting the security and confidentiality of individuals' data within a software context.",Agreed-Justified,Agree,,Agree,,
Exp C,Exp K,1590,TOSEM,Security & Privacy,Consent Verification Monitoring,"Advances in personalization of digital services are driven by low-cost data collection and processing, in addition to the wide variety of third-party frameworks for authentication, storage, and marketing. New privacy regulations, such as the General Data Protection Regulation (GDPR) and the California Consumer Privacy Act, increasingly require organizations to explicitly state their data practices in privacy policies. When data practices change, a new version of the policy is released. This can occur a few times a year, when data collection or processing requirements are rapidly changing. Consent evolution raises specific challenges to ensuring GDPR compliance. We propose a formal consent framework to support organizations, data users, and data subjects in their understanding of policy evolution under a consent regime that supports both the retroactive and non-retroactive granting and withdrawal of consent. The contributions include (i) a formal framework to reason about data collection and access under multiple consent granting and revocation scenarios, (ii) a scripting language that implements the consent framework for encoding and executing different scenarios, (iii) five consent evolution use cases that illustrate how organizations would evolve their policies using this framework, and (iv) a scalability evaluation of the reasoning framework. The framework models are used to verify when user consent prevents or detects unauthorized data collection and access. The framework can be integrated into a runtime architecture to monitor policy violations as data practices evolve in real time. The framework was evaluated using the five use cases and a simulation to measure the framework scalability. The simulation results show that the approach is computationally scalable for use in runtime consent monitoring under a standard model of data collection and access and practice and policy evolution.",Self Direction,Privacy,By constructing a consent framework to support organizations; data users; and data subjects in their understanding of policy evolution; the paper can indirectly contribute to the software user's sense of privacy as the implementation of the consent framework may lead to better privacy policies and practices.,"The justification for aligning 'Paper X' with the value item Privacy and its corresponding value Self Direction from a ""Software User"" perspective is based on the premise that by providing a formal consent framework for understanding policy evolution, the paper indirectly contributes to the software user's sense of privacy. This is because the implementation of such a framework can potentially lead to better privacy policies and practices, giving users more control over their personal data and allowing them to make informed decisions about granting or revoking consent.",Agreed-Justified,Agree,,Agree,,
Exp D,Exp K,1591,TOSEM,Security & Privacy,Coverage-directed Differential Testing of X.509 Certificate Validation in SSL/TLS Implementations,"Secure Sockets Layer (SSL) and Transport Security (TLS) are two secure protocols for creating secure connections over the Internet. X.509 certificate validation is important for security and needs to be performed before an SSL/TLS connection is established. Some advanced testing techniques, such as frankencert, have revealed, through randomly mutating Internet accessible certificates, that there exist unexpected, sometimes critical, validation differences among different SSL/TLS implementations. Despite these efforts, X.509 certificate validation still needs to be thoroughly tested as this work shows. This article tackles this challenge by proposing transcert, a coverage-directed technique to much more effectively test real-world certificate validation code. Our core insight is to (1) leverage easily accessible Internet certificates as seed certificates and (2) use code coverage to direct certificate mutation toward generating a set of diverse certificates. The generated certificates are then used to reveal discrepancies, thus potential flaws, among different certificate validation implementations. We implement transcert and evaluate it against frankencert, NEZHA, and RFCcert (three advanced fuzzing techniques) on five widely used SSL/TLS implementations. The evaluation results clearly show the strengths of transcert: During 10,000 iterations, transcert reveals 71 unique validation differences, 12\texttimes{}, 1.4\texttimes{}, and 7\texttimes{} as many as those revealed by frankencert, NEZHA, and RFCcert, respectively; it also supplements RFCcert in conformance testing of the SSL/TLS implementations against 120 validation rules, 85 of which are exclusively covered by transcert-generated certificates. We identify 17 root causes of validation differences, all of which have been confirmed and 11 have never been reported previously. The transcert-generated X.509 certificates also reveal that the primary goal of certificate chain validation is stated ambiguously in the widely adopted public key infrastructure standard RFC 5280.",Achievement,Intelligent,The paper contributes to intelligent user decision making by providing a more effective testing method for verifying certificate validation; which is vital for secure Internet connections. This aligns with the value item Intelligent and its corresponding value Achievement.,"The main contribution of 'Paper X', which is the proposal of the transcert technique, directly aligns with the value item Intelligent and its corresponding value Achievement from a ""Software User"" perspective. This alignment is evident because the transcert technique aims to improve the testing of real-world certificate validation code, leading to more effective and accurate verification of certificate validation. By providing a more intelligent and thorough testing method, 'Paper X' contributes to the achievement of secure Internet connections, which is a crucial goal for software users.",Agreed-Justified,Agree,,Agree,,
Exp E,Exp J,1591,TOSEM,Security & Privacy,Coverage-directed Differential Testing of X.509 Certificate Validation in SSL/TLS Implementations,"Secure Sockets Layer (SSL) and Transport Security (TLS) are two secure protocols for creating secure connections over the Internet. X.509 certificate validation is important for security and needs to be performed before an SSL/TLS connection is established. Some advanced testing techniques, such as frankencert, have revealed, through randomly mutating Internet accessible certificates, that there exist unexpected, sometimes critical, validation differences among different SSL/TLS implementations. Despite these efforts, X.509 certificate validation still needs to be thoroughly tested as this work shows. This article tackles this challenge by proposing transcert, a coverage-directed technique to much more effectively test real-world certificate validation code. Our core insight is to (1) leverage easily accessible Internet certificates as seed certificates and (2) use code coverage to direct certificate mutation toward generating a set of diverse certificates. The generated certificates are then used to reveal discrepancies, thus potential flaws, among different certificate validation implementations. We implement transcert and evaluate it against frankencert, NEZHA, and RFCcert (three advanced fuzzing techniques) on five widely used SSL/TLS implementations. The evaluation results clearly show the strengths of transcert: During 10,000 iterations, transcert reveals 71 unique validation differences, 12\texttimes{}, 1.4\texttimes{}, and 7\texttimes{} as many as those revealed by frankencert, NEZHA, and RFCcert, respectively; it also supplements RFCcert in conformance testing of the SSL/TLS implementations against 120 validation rules, 85 of which are exclusively covered by transcert-generated certificates. We identify 17 root causes of validation differences, all of which have been confirmed and 11 have never been reported previously. The transcert-generated X.509 certificates also reveal that the primary goal of certificate chain validation is stated ambiguously in the widely adopted public key infrastructure standard RFC 5280.",Security,Healthy,The paper contributes to security by improving the testing and confirming process of secure connections with the propose method. This improves overall system health; aligning with the value item Healthy and its corresponding value Security.,"In the paper abstract, it is explicitly stated that the proposed technique, transcert, is aimed at effectively testing certificate validation code in SSL/TLS implementations. By using code coverage and generating diverse certificates, transcert reveals discrepancies and potential flaws in the validation process. This improvement in the validation process directly contributes to enhancing the security of secure connections. As security is a fundamental aspect of software systems, the alignment with the value item Healthy (which encompasses security) is evident from the paper's contribution in strengthening the overall system health.",Agreed-Justified,Agree,,Agree,,
Exp F,Exp J,1591,TOSEM,Security & Privacy,Coverage-directed Differential Testing of X.509 Certificate Validation in SSL/TLS Implementations,"Secure Sockets Layer (SSL) and Transport Security (TLS) are two secure protocols for creating secure connections over the Internet. X.509 certificate validation is important for security and needs to be performed before an SSL/TLS connection is established. Some advanced testing techniques, such as frankencert, have revealed, through randomly mutating Internet accessible certificates, that there exist unexpected, sometimes critical, validation differences among different SSL/TLS implementations. Despite these efforts, X.509 certificate validation still needs to be thoroughly tested as this work shows. This article tackles this challenge by proposing transcert, a coverage-directed technique to much more effectively test real-world certificate validation code. Our core insight is to (1) leverage easily accessible Internet certificates as seed certificates and (2) use code coverage to direct certificate mutation toward generating a set of diverse certificates. The generated certificates are then used to reveal discrepancies, thus potential flaws, among different certificate validation implementations. We implement transcert and evaluate it against frankencert, NEZHA, and RFCcert (three advanced fuzzing techniques) on five widely used SSL/TLS implementations. The evaluation results clearly show the strengths of transcert: During 10,000 iterations, transcert reveals 71 unique validation differences, 12\texttimes{}, 1.4\texttimes{}, and 7\texttimes{} as many as those revealed by frankencert, NEZHA, and RFCcert, respectively; it also supplements RFCcert in conformance testing of the SSL/TLS implementations against 120 validation rules, 85 of which are exclusively covered by transcert-generated certificates. We identify 17 root causes of validation differences, all of which have been confirmed and 11 have never been reported previously. The transcert-generated X.509 certificates also reveal that the primary goal of certificate chain validation is stated ambiguously in the widely adopted public key infrastructure standard RFC 5280.",Security,National Security,The paper‚Äö√Ñ√∂‚àö√ë‚àö¬•s primary focus is to improve secure connections over the Internet by testing certificate validations. This is aligned with National Security because secure Internet connections are essential to protecting national security infrastructure. This directly aligns with the value item National Security and its corresponding value Security.,"In the context of a software user, ensuring the security of internet connections is crucial for protecting sensitive information and preventing unauthorized access. By focusing on testing certificate validations, 'Paper X' directly aligns with the value item National Security and its corresponding value Security. Improving the robustness of SSL/TLS implementations and identifying potential flaws in certificate validation contributes to the overall security of internet communication, which is essential for safeguarding national security infrastructure and the valuable data shared through software applications.",Agreed-Justified,Agree,,Agree,,
Exp G,Exp K,1595,TOSEM,Security & Privacy,Scanner++: Enhanced Vulnerability Detection of Web Applications with Attack Intent Synchronization,"Scanners are commonly applied for detecting vulnerabilities in web applications. Various scanners with different strategies are widely in use, but their performance is challenged by the increasing diversity of target applications that have more complex attack surfaces (i.e., website paths) and covert vulnerabilities that can only be exploited by more sophisticated attack vectors (i.e., payloads). In this paper, we propose Scanner++, a framework that improves web vulnerability detection of existing scanners through combining their capabilities with attack intent synchronization. We design Scanner++ as a proxy-based architecture while using a package-based intent synchronization approach. Scanner++ first uses a purification mechanism to aggregate and refine attack intents, consisting of attack surfaces and attack vectors extracted from the base scannersaEUR(tm) request packets. Then, Scanner++ uses a runtime intent synchronization mechanism to select relevant attack intents according to the scannersaEUR(tm) detection spots to guide their scanning process. Consequently, base scanners can expand their attack surfaces, generate more diverse attack vectors and achieve better vulnerability detection performance.For evaluation, we implemented and integrated Scanner++ together with four widely used scanners, BurpSuite, AWVS, Arachni, and ZAP, testing it on ten benchmark web applications and three well-tested real-world web applications of a critical financial platform from our industry partner. Working under the Scanner++ framework helps BurpSuite, AWVS, Arachni, and ZAP cover 15.26\%, 37.14\%, 59.21\%, 68.54\% more pages, construct 12.95\texttimes{}, 1.13\texttimes{}, 15.03\texttimes{}, 52.66\texttimes{} more attack packets, and discover 77, 55, 77, 176 more bugs, respectively. Furthermore, Scanner++ detected eight serious previously unknown vulnerabilities on real-world applications, while the base scanners only found three of them.",Security,Healthy,Paper X proposes a web vulnerability detection framework; Scanner++; that can detect and discover more bugs and serious unknown vulnerabilities in web applications. This aligns with the value item 'Healthy' and its corresponding value 'Security' as it aims to robustly protect the end-users' data and prevent potential security breaches.,"The main contribution of 'Paper X' aligns with the value item 'Healthy' and its corresponding value 'Security' because its proposed framework, Scanner++, aims to improve web vulnerability detection and prevent potential security breaches. By detecting and discovering more bugs and serious unknown vulnerabilities in web applications, Scanner++ helps ensure the robust protection of end-users' data, thereby promoting a secure and healthy software environment.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,1595,TOSEM,Security & Privacy,Scanner++: Enhanced Vulnerability Detection of Web Applications with Attack Intent Synchronization,"Scanners are commonly applied for detecting vulnerabilities in web applications. Various scanners with different strategies are widely in use, but their performance is challenged by the increasing diversity of target applications that have more complex attack surfaces (i.e., website paths) and covert vulnerabilities that can only be exploited by more sophisticated attack vectors (i.e., payloads). In this paper, we propose Scanner++, a framework that improves web vulnerability detection of existing scanners through combining their capabilities with attack intent synchronization. We design Scanner++ as a proxy-based architecture while using a package-based intent synchronization approach. Scanner++ first uses a purification mechanism to aggregate and refine attack intents, consisting of attack surfaces and attack vectors extracted from the base scannersaEUR(tm) request packets. Then, Scanner++ uses a runtime intent synchronization mechanism to select relevant attack intents according to the scannersaEUR(tm) detection spots to guide their scanning process. Consequently, base scanners can expand their attack surfaces, generate more diverse attack vectors and achieve better vulnerability detection performance.For evaluation, we implemented and integrated Scanner++ together with four widely used scanners, BurpSuite, AWVS, Arachni, and ZAP, testing it on ten benchmark web applications and three well-tested real-world web applications of a critical financial platform from our industry partner. Working under the Scanner++ framework helps BurpSuite, AWVS, Arachni, and ZAP cover 15.26\%, 37.14\%, 59.21\%, 68.54\% more pages, construct 12.95\texttimes{}, 1.13\texttimes{}, 15.03\texttimes{}, 52.66\texttimes{} more attack packets, and discover 77, 55, 77, 176 more bugs, respectively. Furthermore, Scanner++ detected eight serious previously unknown vulnerabilities on real-world applications, while the base scanners only found three of them.",Security,National Security,Following above; Scanner++ is also able to cover and scan more pages of the web applications for vulnerabilities. This implies it could be used to ensure the National Security of digital infrastructures and systems; aligning with the value item 'National Security' and its corresponding value 'Security'.,"I apologize for any confusion caused. The justification for aligning 'Paper X' with the value item National Security and its corresponding value Security from a ""Software User"" perspective is based on the fact that Scanner++ increases the coverage and scanning capabilities of web applications for vulnerabilities. This expanded coverage implies that Scanner++ can help safeguard the security of digital infrastructures and systems, which directly aligns with the value item National Security and its corresponding value Security.",Agreed-Justified,Agree,,Disagree,"The justification claims that the paper aligns with the value 'National Security', but the text neither explains how nor confirms this.
",
Exp C,Exp K,1600,TOSEM,Software Development Methodologies,ActivFORMS: A Formally Founded Model-based Approach to Engineer Self-adaptive Systems,"Self-adaptation equips a computing system with a feedback loop that enables it to deal with change caused by uncertainties during operation, such as changing availability of resources and fluctuating workloads. To ensure that the system complies with the adaptation goals, recent research suggests the use of formal techniques at runtime. Yet, existing approaches have three limitations that affect their practical applicability: (i) they ignore correctness of the behavior of the feedback loop, (ii) they rely on exhaustive verification at runtime to select adaptation options to realize the adaptation goals, which is time- and resource-demanding, and (iii) they provide limited or no support for changing adaptation goals at runtime. To tackle these shortcomings, we present ActivFORMS (Active FORmal Models for Self-adaptation). ActivFORMS contributes an end-to-end approach for engineering self-adaptive systems, spanning four main stages of the life cycle of a feedback loop: design, deployment, runtime adaptation, and evolution. We also present ActivFORMS-ta, a tool-supported instance of ActivFORMS that leverages timed automata models and statistical model checking at runtime. We validate the research results using an IoT application for building security monitoring that is deployed in Leuven. The experimental results demonstrate that ActivFORMS supports correctness of the behavior of the feedback loop, achieves the adaptation goals in an efficient way, and supports changing adaptation goals at runtime.",Stimulation,Variation in Life,The paper discusses an innovative approach (ActivFORMS) for software self-adaptation; which contributes to Variation in Life as it enables a software user's computing system to adapt to changing conditions. This aligns with the value Stimulation.,"In the context of software, the value item ""Variation in Life"" aligns with the main contribution of 'Paper X' because it introduces the concept of self-adaptation in computing systems, enabling them to adapt to changing conditions. This introduces variation and novelty in the software user's life as their computing system is able to dynamically respond to uncertainties and provide a stimulating and dynamic user experience. Therefore, by aligning with Variation in Life, 'Paper X' also aligns with the corresponding value Stimulation from a software user perspective.",Agreed-Clarified,Agree,,Agree,,
Exp D,Exp K,1600,TOSEM,Software Development Methodologies,ActivFORMS: A Formally Founded Model-based Approach to Engineer Self-adaptive Systems,"Self-adaptation equips a computing system with a feedback loop that enables it to deal with change caused by uncertainties during operation, such as changing availability of resources and fluctuating workloads. To ensure that the system complies with the adaptation goals, recent research suggests the use of formal techniques at runtime. Yet, existing approaches have three limitations that affect their practical applicability: (i) they ignore correctness of the behavior of the feedback loop, (ii) they rely on exhaustive verification at runtime to select adaptation options to realize the adaptation goals, which is time- and resource-demanding, and (iii) they provide limited or no support for changing adaptation goals at runtime. To tackle these shortcomings, we present ActivFORMS (Active FORmal Models for Self-adaptation). ActivFORMS contributes an end-to-end approach for engineering self-adaptive systems, spanning four main stages of the life cycle of a feedback loop: design, deployment, runtime adaptation, and evolution. We also present ActivFORMS-ta, a tool-supported instance of ActivFORMS that leverages timed automata models and statistical model checking at runtime. We validate the research results using an IoT application for building security monitoring that is deployed in Leuven. The experimental results demonstrate that ActivFORMS supports correctness of the behavior of the feedback loop, achieves the adaptation goals in an efficient way, and supports changing adaptation goals at runtime.",Achievement,Successful,The aim of the paper is to ensure the computing system achieves those adaptation goals efficiently; which could indicate Successful in terms of system performance and user satisfaction. This aligns with the value Achievement from the user's perspective.,"The justification for aligning 'Paper X' with the value item Successful and the corresponding value Achievement from a ""Software User"" perspective is based on the goal of the paper to ensure that the system achieves its adaptation goals efficiently. This can be interpreted as a measure of success in terms of system performance and user satisfaction. By using formal techniques and providing support for changing adaptation goals at runtime, the paper contributes to the achievement of these goals, which aligns with the value of Achievement from the perspective of a software user.",Agreed-Justified,Agree,,Agree,,
Exp E,Exp J,1600,TOSEM,Software Development Methodologies,ActivFORMS: A Formally Founded Model-based Approach to Engineer Self-adaptive Systems,"Self-adaptation equips a computing system with a feedback loop that enables it to deal with change caused by uncertainties during operation, such as changing availability of resources and fluctuating workloads. To ensure that the system complies with the adaptation goals, recent research suggests the use of formal techniques at runtime. Yet, existing approaches have three limitations that affect their practical applicability: (i) they ignore correctness of the behavior of the feedback loop, (ii) they rely on exhaustive verification at runtime to select adaptation options to realize the adaptation goals, which is time- and resource-demanding, and (iii) they provide limited or no support for changing adaptation goals at runtime. To tackle these shortcomings, we present ActivFORMS (Active FORmal Models for Self-adaptation). ActivFORMS contributes an end-to-end approach for engineering self-adaptive systems, spanning four main stages of the life cycle of a feedback loop: design, deployment, runtime adaptation, and evolution. We also present ActivFORMS-ta, a tool-supported instance of ActivFORMS that leverages timed automata models and statistical model checking at runtime. We validate the research results using an IoT application for building security monitoring that is deployed in Leuven. The experimental results demonstrate that ActivFORMS supports correctness of the behavior of the feedback loop, achieves the adaptation goals in an efficient way, and supports changing adaptation goals at runtime.",Security,Healthy,The paper also mentions a validation exercise employing an IoT application for building security monitoring; which potentially contributes to the user's Health (security ensuring physical well-being) in the context of a secured building environment. This aligns with the value Security.,"In alignment with the value item of Security, the paper's validation exercise using an IoT application for building security monitoring directly contributes to the user's well-being and physical safety in a secured building environment. This aligns with the user's need for a sense of security and protection, making it a relevant value item for a software user.",Agreed-Justified,Agree,,Agree,,
Exp F,Exp J,1601,TOSEM,Mobile & IoT,Combatting Energy Issues for Mobile Applications,"Energy efficiency is an important criterion to judge the quality of mobile apps, but one third of our arbitrarily sampled apps suffer from energy issues that can quickly drain battery power. To understand these issues, we conduct an empirical study on 36 well-maintained apps such as Chrome and Firefox, whose issue tracking systems are publicly accessible. Our study involves issue causes, manifestation, fixing efforts, detection techniques, reasons of no-fixes, and debugging techniques. Inspired by the empirical study, we propose a novel testing framework for detecting energy issues in real-world mobile apps. Our framework examines apps with well-designed input sequences and runtime context. We develop leading edge technologies, e.g., pre-designing input sequences with potential energy overuse and tuning tests on-the-fly, to achieve high efficacy in detecting energy issues. A large-scale evaluation shows that 90.4\% of the detected issues in our experiments were previously unknown to developers. On average, these issues can double the energy consumption of the test cases where the issues were detected. And our test achieves a low number of false positives. Finally, we show how our test reports can help developers fix the issues.",Security,Healthy,The main objective of 'Paper X' is to improve the energy efficiency of mobile apps. The successful implementation of their proposed testing framework would result in apps that consume less battery power; leading to an improvement in the overall health and longevity of the user's mobile device. Thus; it aligns with the value item 'Healthy' and its corresponding value 'Security'.,"In 'Paper X', the authors emphasize the importance of energy efficiency in mobile apps and the impact it has on the user's device, specifically the battery power. By proposing a testing framework that detects energy issues in real-world mobile apps, the authors aim to help developers fix these issues and ultimately reduce the energy consumption of the apps. This directly aligns with the value item 'Healthy' because a more energy-efficient app leads to a healthier mobile device by preserving battery life and ensuring smooth and uninterrupted usage for the software user. This, in turn, contributes to the value of 'Security' as the user can rely on their device to function properly and not be constantly concerned about battery drain.",Agreed-Justified,Agree,,Agree,,
Exp G,Exp K,1601,TOSEM,Mobile & IoT,Combatting Energy Issues for Mobile Applications,"Energy efficiency is an important criterion to judge the quality of mobile apps, but one third of our arbitrarily sampled apps suffer from energy issues that can quickly drain battery power. To understand these issues, we conduct an empirical study on 36 well-maintained apps such as Chrome and Firefox, whose issue tracking systems are publicly accessible. Our study involves issue causes, manifestation, fixing efforts, detection techniques, reasons of no-fixes, and debugging techniques. Inspired by the empirical study, we propose a novel testing framework for detecting energy issues in real-world mobile apps. Our framework examines apps with well-designed input sequences and runtime context. We develop leading edge technologies, e.g., pre-designing input sequences with potential energy overuse and tuning tests on-the-fly, to achieve high efficacy in detecting energy issues. A large-scale evaluation shows that 90.4\% of the detected issues in our experiments were previously unknown to developers. On average, these issues can double the energy consumption of the test cases where the issues were detected. And our test achieves a low number of false positives. Finally, we show how our test reports can help developers fix the issues.",Universalism,Protecting the Environment,By targeting energy issues in mobile apps; 'Paper X' aims to reduce energy consumption. Although the paper does not directly mention the environment; lower energy consumption implies less resource use; indirectly contributing to 'Protecting the Environment' and its corresponding value 'Universalism'.,"The justification for aligning 'Paper X' with the value item Protecting the Environment and its corresponding value Universalism is based on the understanding that reducing energy consumption directly results in less resource use and ultimately contributes to the broader goal of protecting the environment. By addressing energy issues in mobile apps, the paper indirectly promotes the value of Universalism by recognizing the interconnectedness of ecological systems and the importance of taking actions to ensure a sustainable and harmonious relationship with nature.",Agreed-Justified,Agree,,Agree,,
Exp B,Exp H,1612,TOSEM,Security & Privacy,APIRO: A Framework for Automated Security Tools API Recommendation,"Security Orchestration, Automation, and Response (SOAR) platforms integrate and orchestrate a wide variety of security tools to accelerate the operational activities of Security Operation Center (SOC). Integration of security tools in a SOAR platform is mostly done manually using APIs, plugins, and scripts. SOC teams need to navigate through API calls of different security tools to find a suitable API to define or update an incident response action. Analyzing various types of API documentation with diverse API format and presentation structure involves significant challenges such as data availability, data heterogeneity, and semantic variation for automatic identification of security tool APIs specific to a particular task. Given these challenges can have negative impact on SOC teamaEUR(tm)s ability to handle security incident effectively and efficiently, we consider it important to devise suitable automated support solutions to address these challenges. We propose a novel learning-based framework for automated security tool API Recommendation for security Orchestration, automation, and response, APIRO. To mitigate data availability constraint, APIRO enriches security tool API description by applying a wide variety of data augmentation techniques. To learn data heterogeneity of the security tools and semantic variation in API descriptions, APIRO consists of an API-specific word embedding model and a Convolutional Neural Network (CNN) model that are used for prediction of top three relevant APIs for a task. We experimentally demonstrate the effectiveness of APIRO in recommending APIs for different tasks using three security tools and 36 augmentation techniques. Our experimental results demonstrate the feasibility of APIRO for achieving 91.9\% Top-1 Accuracy. Compared to the state-of-the-art baseline, APIRO is 26.93\%, 23.03\%, and 20.87\% improved in terms of Top-1, Top-2, and Top-3 Accuracy and outperforms the baseline by 23.7\% in terms of Mean Reciprocal Rank (MRR).",Achievement,Successful,The main contribution of 'Paper X' enables the software users to effectively and efficiently handle security incidents within a Security Operation Center (SOC). This would make the software users more successful in their roles; therefore aligning with the value of 'Achievement' and the value item 'Successful'.,"The justification for aligning 'Paper X' with the value item Successful and its corresponding value Achievement from a ""Software User"" perspective is based on the fact that the main contribution of the paper is to propose a learning-based framework for automated security tool API recommendation in a Security Operation Center. By enabling software users to effectively and efficiently handle security incidents, the paper directly addresses the operational activities of a SOC, which can lead to successful outcomes in terms of incident response. Therefore, the alignment with the value of 'Achievement' and the value item 'Successful' is evident as the paper aims to enhance the performance and effectiveness of software users in their security-related roles.",Agreed-Clarified,Agree,,Agree,,
Exp C,Exp K,1612,TOSEM,Security & Privacy,APIRO: A Framework for Automated Security Tools API Recommendation,"Security Orchestration, Automation, and Response (SOAR) platforms integrate and orchestrate a wide variety of security tools to accelerate the operational activities of Security Operation Center (SOC). Integration of security tools in a SOAR platform is mostly done manually using APIs, plugins, and scripts. SOC teams need to navigate through API calls of different security tools to find a suitable API to define or update an incident response action. Analyzing various types of API documentation with diverse API format and presentation structure involves significant challenges such as data availability, data heterogeneity, and semantic variation for automatic identification of security tool APIs specific to a particular task. Given these challenges can have negative impact on SOC teamaEUR(tm)s ability to handle security incident effectively and efficiently, we consider it important to devise suitable automated support solutions to address these challenges. We propose a novel learning-based framework for automated security tool API Recommendation for security Orchestration, automation, and response, APIRO. To mitigate data availability constraint, APIRO enriches security tool API description by applying a wide variety of data augmentation techniques. To learn data heterogeneity of the security tools and semantic variation in API descriptions, APIRO consists of an API-specific word embedding model and a Convolutional Neural Network (CNN) model that are used for prediction of top three relevant APIs for a task. We experimentally demonstrate the effectiveness of APIRO in recommending APIs for different tasks using three security tools and 36 augmentation techniques. Our experimental results demonstrate the feasibility of APIRO for achieving 91.9\% Top-1 Accuracy. Compared to the state-of-the-art baseline, APIRO is 26.93\%, 23.03\%, and 20.87\% improved in terms of Top-1, Top-2, and Top-3 Accuracy and outperforms the baseline by 23.7\% in terms of Mean Reciprocal Rank (MRR).",Security,Healthy,The proposed system 'APIRO' in the abstract helps Security Operation Center (SOC) teams to improve the response actions towards security incidents. This directly aligns with 'Security' and contributes towards the software user's 'Health'; as security breaches could result in a threat to user's digital well-being.,"The alignment of 'Paper X' with the value item Healthy and its corresponding value Security is justified based on the fact that the proposed system 'APIRO' aims to improve the response actions towards security incidents. This contributes to the software user's 'Health' because effective and efficient security incident handling helps ensure the user's digital well-being, preventing potential threats and breaches that could compromise their security and data. By providing automated support solutions for security orchestration and automation, 'APIRO' directly aligns with the value of Security and indirectly contributes to the user's overall well-being and sense of safety in a software context.",Agreed-Clarified,Agree,,Agree,,
Exp E,Exp J,1613,TOSEM,Security & Privacy,An In-depth Study of Java Deserialization Remote-Code Execution Exploits and Vulnerabilities,"Nowadays, an increasing number of applications use deserialization. This technique, based on rebuilding the instance of objects from serialized byte streams, can be dangerous since it can open the application to attacks such as remote code execution (RCE) if the data to deserialize is originating from an untrusted source. Deserialization vulnerabilities are so critical that they are in OWASPaEUR(tm)s list of top 10 security risks for web applications. This is mainly caused by faults in the development process of applications and by flaws in their dependencies, i.e., flaws in the libraries used by these applications. No previous work has studied deserialization attacks in-depth: How are they performed? How are weaknesses introduced and patched? And for how long are vulnerabilities present in the codebase? To yield a deeper understanding of this important kind of vulnerability, we perform two main analyses: one on attack gadgets, i.e., exploitable pieces of code, present in Java libraries, and one on vulnerabilities present in Java applications. For the first analysis, we conduct an exploratory large-scale study by running 256515 &nbsp;experiments in which we vary the versions of libraries for each of the 19 publicly available exploits. Such attacks rely on a combination of gadgets present in one or multiple Java libraries. A gadget is a method which is using objects or fields that can be attacker-controlled. Our goal is to precisely identify library versions containing gadgets and to understand how gadgets have been introduced and how they have been patched. We observe that the modification of one innocent-looking detail in a class aEUR"" such as making it public aEUR"" can already introduce a gadget. Furthermore, we noticed that among the studied libraries, 37.5\% are not patched, leaving gadgets available for future attacks.For the second analysis, we manually analyze 104 deserialization vulnerabilities CVEs to understand how vulnerabilities are introduced and patched in real-life Java applications. Results indicate that the vulnerabilities are not always completely patched or that a workaround solution is proposed. With a workaround solution, applications are still vulnerable since the code itself is unchanged.",Security,Social Order,The paper analyses and highlights deserialization vulnerabilities in Java applications; which can improve Social Order (in terms of stable and secure software environments).,"The paper's analysis and identification of deserialization vulnerabilities in Java applications directly contribute to the value item of Social Order by addressing potential security risks and helping create a more stable and secure software environment. This aligns with the importance of maintaining a secure and stable social order in the realm of software usage, where users can have confidence in the reliability and safety of the applications they rely on.",Agreed-Justified,Agree,,Agree,,
Exp F,Exp J,1613,TOSEM,Security & Privacy,An In-depth Study of Java Deserialization Remote-Code Execution Exploits and Vulnerabilities,"Nowadays, an increasing number of applications use deserialization. This technique, based on rebuilding the instance of objects from serialized byte streams, can be dangerous since it can open the application to attacks such as remote code execution (RCE) if the data to deserialize is originating from an untrusted source. Deserialization vulnerabilities are so critical that they are in OWASPaEUR(tm)s list of top 10 security risks for web applications. This is mainly caused by faults in the development process of applications and by flaws in their dependencies, i.e., flaws in the libraries used by these applications. No previous work has studied deserialization attacks in-depth: How are they performed? How are weaknesses introduced and patched? And for how long are vulnerabilities present in the codebase? To yield a deeper understanding of this important kind of vulnerability, we perform two main analyses: one on attack gadgets, i.e., exploitable pieces of code, present in Java libraries, and one on vulnerabilities present in Java applications. For the first analysis, we conduct an exploratory large-scale study by running 256515 &nbsp;experiments in which we vary the versions of libraries for each of the 19 publicly available exploits. Such attacks rely on a combination of gadgets present in one or multiple Java libraries. A gadget is a method which is using objects or fields that can be attacker-controlled. Our goal is to precisely identify library versions containing gadgets and to understand how gadgets have been introduced and how they have been patched. We observe that the modification of one innocent-looking detail in a class aEUR"" such as making it public aEUR"" can already introduce a gadget. Furthermore, we noticed that among the studied libraries, 37.5\% are not patched, leaving gadgets available for future attacks.For the second analysis, we manually analyze 104 deserialization vulnerabilities CVEs to understand how vulnerabilities are introduced and patched in real-life Java applications. Results indicate that the vulnerabilities are not always completely patched or that a workaround solution is proposed. With a workaround solution, applications are still vulnerable since the code itself is unchanged.",Security,Healthy,By studying vulnerabilities and suggesting improvements; this paper contributes to software Health in terms of fixing potential problems; ensuring smoother operation.,"In the paper abstract, 'Paper X' specifically mentions that it analyzes vulnerabilities in Java applications and libraries, and aims to understand how weaknesses are introduced and patched. By addressing these vulnerabilities and suggesting improvements, the paper contributes to the security and overall health of software systems. This alignment with the value item Healthy and its corresponding value Security from a ""Software User"" perspective can be understood as the paper's efforts in fixing potential problems and ensuring the smoother and safer operation of software applications, which directly aligns with the value of security and well-being for software users.",Agreed-Justified,Agree,,Agree,,
Exp G,Exp K,1613,TOSEM,Security & Privacy,An In-depth Study of Java Deserialization Remote-Code Execution Exploits and Vulnerabilities,"Nowadays, an increasing number of applications use deserialization. This technique, based on rebuilding the instance of objects from serialized byte streams, can be dangerous since it can open the application to attacks such as remote code execution (RCE) if the data to deserialize is originating from an untrusted source. Deserialization vulnerabilities are so critical that they are in OWASPaEUR(tm)s list of top 10 security risks for web applications. This is mainly caused by faults in the development process of applications and by flaws in their dependencies, i.e., flaws in the libraries used by these applications. No previous work has studied deserialization attacks in-depth: How are they performed? How are weaknesses introduced and patched? And for how long are vulnerabilities present in the codebase? To yield a deeper understanding of this important kind of vulnerability, we perform two main analyses: one on attack gadgets, i.e., exploitable pieces of code, present in Java libraries, and one on vulnerabilities present in Java applications. For the first analysis, we conduct an exploratory large-scale study by running 256515 &nbsp;experiments in which we vary the versions of libraries for each of the 19 publicly available exploits. Such attacks rely on a combination of gadgets present in one or multiple Java libraries. A gadget is a method which is using objects or fields that can be attacker-controlled. Our goal is to precisely identify library versions containing gadgets and to understand how gadgets have been introduced and how they have been patched. We observe that the modification of one innocent-looking detail in a class aEUR"" such as making it public aEUR"" can already introduce a gadget. Furthermore, we noticed that among the studied libraries, 37.5\% are not patched, leaving gadgets available for future attacks.For the second analysis, we manually analyze 104 deserialization vulnerabilities CVEs to understand how vulnerabilities are introduced and patched in real-life Java applications. Results indicate that the vulnerabilities are not always completely patched or that a workaround solution is proposed. With a workaround solution, applications are still vulnerable since the code itself is unchanged.",Security,National Security,This paper potentially improves National Security by identifying vulnerabilities in common applications and libraries; reducing the risk of cyber-attacks gaining access to secure networks and information.,"In the paper, the authors specifically mention that deserialization vulnerabilities can open the application to attacks such as remote code execution (RCE) and that these vulnerabilities are listed as one of the top 10 security risks for web applications. By performing analyses on attack gadgets present in Java libraries and vulnerabilities in Java applications, the paper aims to identify and understand how weaknesses are introduced and patched. The identification and patching of these vulnerabilities can significantly contribute to improving the security of software systems, including those related to national security. By reducing the risk of cyber-attacks gaining access to secure networks and information, the paper aligns with the value item National Security and its corresponding value Security from a ""Software User"" perspective.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,1614,TOSEM,Emerging Technologies,TokenAware: Accurate and Efficient Bookkeeping Recognition for Token Smart Contracts,"Tokens have become an essential part of blockchain ecosystem, so recognizing token transfer behaviors is crucial for applications depending on blockchain. Unfortunately, existing solutions cannot recognize token transfer behaviors accurately and efficiently because of their incomplete patterns and inefficient designs. This work proposes TokenAware, a novel online system for recognizing token transfer behaviors. To improve accuracy, TokenAware infers token transfer behaviors from modifications of internal bookkeeping of a token smart contract for recording the information of token holders (e.g., their addresses and shares). However, recognizing bookkeeping is challenging, because smart contract bytecode does not contain type information. TokenAware overcomes the challenge by first learning the instruction sequences for locating basic types and then deriving the instruction sequences for locating sophisticated types that are composed of basic types. To improve efficiency, TokenAware introduces four optimizations. We conduct extensive experiments to evaluate TokenAware with real blockchain data. Results show that TokenAware can automatically identify new types of bookkeeping and recognize 107,202 tokens with 98.7\% precision. TokenAware with optimizations merely incurs 4\% overhead, which is 1/345 of the overhead led by the counterpart with no optimization. Moreover, we develop an application based on TokenAware to demonstrate how it facilitates malicious behavior detection.",Power,Social Recognition,TokenAware allows for software users of a blockchain system to acquire social recognition in their respective communities for recognizing token transfer behaviors accurately. Therefore; this is in alignment with social recognition under the power value.,"In the context of the ""Software User"" perspective, TokenAware aligns with the value item of Social Recognition and the corresponding value of Power because it enables software users to gain social recognition within their blockchain communities by accurately recognizing token transfer behaviors. This recognition can be seen as a form of power, as users who are knowledgeable and proficient in identifying and understanding token transfers are likely to have influence and authority within the community. By providing a system that enhances users' ability to recognize and understand these behaviors, TokenAware empowers software users to gain social recognition and assert their power within the blockchain ecosystem.",Agreed-Clarified,Agree,,Agree,,
Exp B,Exp H,1614,TOSEM,Emerging Technologies,TokenAware: Accurate and Efficient Bookkeeping Recognition for Token Smart Contracts,"Tokens have become an essential part of blockchain ecosystem, so recognizing token transfer behaviors is crucial for applications depending on blockchain. Unfortunately, existing solutions cannot recognize token transfer behaviors accurately and efficiently because of their incomplete patterns and inefficient designs. This work proposes TokenAware, a novel online system for recognizing token transfer behaviors. To improve accuracy, TokenAware infers token transfer behaviors from modifications of internal bookkeeping of a token smart contract for recording the information of token holders (e.g., their addresses and shares). However, recognizing bookkeeping is challenging, because smart contract bytecode does not contain type information. TokenAware overcomes the challenge by first learning the instruction sequences for locating basic types and then deriving the instruction sequences for locating sophisticated types that are composed of basic types. To improve efficiency, TokenAware introduces four optimizations. We conduct extensive experiments to evaluate TokenAware with real blockchain data. Results show that TokenAware can automatically identify new types of bookkeeping and recognize 107,202 tokens with 98.7\% precision. TokenAware with optimizations merely incurs 4\% overhead, which is 1/345 of the overhead led by the counterpart with no optimization. Moreover, we develop an application based on TokenAware to demonstrate how it facilitates malicious behavior detection.",Security,Social Order,By recognizing token transfer behaviors accurately and efficiently; TokenAware supports the order and smooth running of the blockchain system that software users are part of; closely aligning with Social Order under Security.,"TokenAware aligns with the value item Social Order and its corresponding value Security because it accurately and efficiently recognizes token transfer behaviors in the blockchain system. This contributes to maintaining the order and smooth running of the system, ensuring that software users can trust the security of their transactions and interactions within the blockchain network. By identifying and recording the information of token holders, TokenAware enhances the overall security and stability of the system, directly aligning with the value of Security under the value item Social Order.",Agreed-Justified,Disagree,reconcile with coder_2,Disagree,"The justification doesn't align with the value item of the Social Order
",
Exp C,Exp K,1614,TOSEM,Emerging Technologies,TokenAware: Accurate and Efficient Bookkeeping Recognition for Token Smart Contracts,"Tokens have become an essential part of blockchain ecosystem, so recognizing token transfer behaviors is crucial for applications depending on blockchain. Unfortunately, existing solutions cannot recognize token transfer behaviors accurately and efficiently because of their incomplete patterns and inefficient designs. This work proposes TokenAware, a novel online system for recognizing token transfer behaviors. To improve accuracy, TokenAware infers token transfer behaviors from modifications of internal bookkeeping of a token smart contract for recording the information of token holders (e.g., their addresses and shares). However, recognizing bookkeeping is challenging, because smart contract bytecode does not contain type information. TokenAware overcomes the challenge by first learning the instruction sequences for locating basic types and then deriving the instruction sequences for locating sophisticated types that are composed of basic types. To improve efficiency, TokenAware introduces four optimizations. We conduct extensive experiments to evaluate TokenAware with real blockchain data. Results show that TokenAware can automatically identify new types of bookkeeping and recognize 107,202 tokens with 98.7\% precision. TokenAware with optimizations merely incurs 4\% overhead, which is 1/345 of the overhead led by the counterpart with no optimization. Moreover, we develop an application based on TokenAware to demonstrate how it facilitates malicious behavior detection.",Achievement,Capable,TokenAware are helping the software users to prove the capabilities in recognizing token transfer behaviours; that aligns with Capable under Achievement.,"In the abstract of 'Paper X', it is stated that TokenAware is a system for recognizing token transfer behaviors accurately and efficiently. By inferring token transfer behaviors from modifications of internal bookkeeping of a token smart contract, TokenAware helps software users prove their capabilities in recognizing these behaviors. This directly aligns with the value item Capable under the value Achievement from a software user perspective, as it enables users to demonstrate their competence and success in understanding and identifying token transfer behaviors.",Agreed-Justified,Agree,,Agree,,
Exp D,Exp K,1615,TOSEM,Data Management & Processing,Defining a Knowledge Graph Development Process Through a Systematic Review,"Knowledge graphs are widely used in industry and studied within the academic community. However, the models applied in the development of knowledge graphs vary. Analysing and providing a synthesis of the commonly used approaches to knowledge graph development would provide researchers and practitioners a better understanding of the overall process and methods involved. Hence, this article aims at defining the overall process of knowledge graph development and its key constituent steps. For this purpose, a systematic review and a conceptual analysis of the literature was conducted. The resulting process was compared to case studies to evaluate its applicability. The proposed process suggests a unified approach and provides guidance for both researchers and practitioners when constructing and managing knowledge graphs.",Achievement,Intelligent,The paper contributes to the understanding of the process of knowledge graph development and its key stages. This may enhance the intellectual understanding of software users; thus aligning with the value item Intelligent in the Achievement value.,"The alignment of 'Paper X' with the value item Intelligent and its corresponding value Achievement is justified because the paper's contribution in defining the overall process of knowledge graph development and its constituent steps enhances the intellectual understanding of software users. By providing a systematic review and conceptual analysis of the literature, the paper equips software users with knowledge and insights that enable them to make intelligent decisions and achieve their goals effectively in the context of knowledge graph development.",Agreed-Justified,Agree,,Agree,,
Exp E,Exp J,1615,TOSEM,Data Management & Processing,Defining a Knowledge Graph Development Process Through a Systematic Review,"Knowledge graphs are widely used in industry and studied within the academic community. However, the models applied in the development of knowledge graphs vary. Analysing and providing a synthesis of the commonly used approaches to knowledge graph development would provide researchers and practitioners a better understanding of the overall process and methods involved. Hence, this article aims at defining the overall process of knowledge graph development and its key constituent steps. For this purpose, a systematic review and a conceptual analysis of the literature was conducted. The resulting process was compared to case studies to evaluate its applicability. The proposed process suggests a unified approach and provides guidance for both researchers and practitioners when constructing and managing knowledge graphs.",Benevolence,Helpful,By defining the overall process of knowledge graph development; it helps the users of this information to be more helpful when working on such development tasks within a team; aligning with the value item Helpful in the Benevolence value.,"The justification for labeling 'Paper X' aligning with the value item Helpful and its corresponding value Benevolence from a ""Software User"" perspective is based on the fact that by defining the overall process of knowledge graph development, the paper aims to provide guidance to both researchers and practitioners. This guidance can be seen as helpful in the sense that it assists software users in effectively carrying out their tasks related to knowledge graph development, ultimately contributing to a more efficient and productive team environment. This aligns with the value item Helpful under the value of Benevolence, which emphasizes actions that promote the well-being and support of others.",Agreed-Justified,Agree,,Agree,,
Exp F,Exp J,1615,TOSEM,Data Management & Processing,Defining a Knowledge Graph Development Process Through a Systematic Review,"Knowledge graphs are widely used in industry and studied within the academic community. However, the models applied in the development of knowledge graphs vary. Analysing and providing a synthesis of the commonly used approaches to knowledge graph development would provide researchers and practitioners a better understanding of the overall process and methods involved. Hence, this article aims at defining the overall process of knowledge graph development and its key constituent steps. For this purpose, a systematic review and a conceptual analysis of the literature was conducted. The resulting process was compared to case studies to evaluate its applicability. The proposed process suggests a unified approach and provides guidance for both researchers and practitioners when constructing and managing knowledge graphs.",Universalism,Broadmindedness,The aim of the paper in providing synthesis and understanding of various approaches and methods for developing knowledge graphs might contribute to broadening the software users' perspective; aligning with the value item Broadmindedness within the Universalism value.,"The paper's focus on providing a synthesis and understanding of various approaches and methods for developing knowledge graphs aligns with the value item Broadmindedness within the Universalism value. By exploring different perspectives and approaches, software users are likely to gain a broader understanding of knowledge graph development, promoting an open-minded and inclusive mindset. This alignment supports the idea that ""Paper X"" contributes to broadening the software user's perspective, in line with the value item Broadmindedness within the Universalism value.",Agreed-Justified,Agree,,Agree,,
Exp G,Exp K,1616,TOSEM,Software Testing & QA,Single and Multi-objective Test Cases Prioritization for Self-driving Cars in Virtual Environments,"Testing with simulation environments helps to identify critical failing scenarios for self-driving cars (SDCs). Simulation-based tests are safer than in-field operational tests and allow detecting software defects before deployment. However, these tests are very expensive and are too many to be run frequently within limited time constraints.In this article, we investigate test case prioritization techniques to increase the ability to detect SDC regression faults with virtual tests earlier. Our approach, called SDC-Prioritizer, prioritizes virtual tests for SDCs according to static features of the roads we designed to be used within the driving scenarios. These features can be collected without running the tests, which means that they do not require past execution results. We introduce two evolutionary approaches to prioritize the test cases using diversity metrics (black-box heuristics) computed on these static features. These two approaches, called SO-SDC-Prioritizer and MO-SDC-Prioritizer, use single-objective and multi-objective genetic algorithms (GA), respectively, to find trade-offs between executing the less expensive tests and the most diverse test cases earlier.Our empirical study conducted in the SDC domain shows that MO-SDC-Prioritizer significantly (P- value &lt;=0.1e-10) improves the ability to detect safety-critical failures at the same level of execution time compared to baselines: random and greedy-based test case orderings. Besides, our study indicates that multi-objective meta-heuristics outperform single-objective approaches when prioritizing simulation-based tests for SDCs.MO-SDC-Prioritizer prioritizes test cases with a large improvement in fault detection while its overhead (up to 0.45\% of the test execution cost) is negligible.",Security,Healthy,The paper introduces test case prioritization techniques for self-driving cars; potentially improving the safety and overall health of users by reducing the chances of software defects before deployment.,"In the abstract of 'Paper X', the authors mention that their approach, called SDC-Prioritizer, prioritizes virtual tests for self-driving cars based on static features of the roads. By identifying critical failing scenarios and detecting software defects before deployment, the prioritization technique can potentially improve the safety of self-driving cars. This aligns with the value item ""Healthy"" from Schwartz's Taxonomy, as it focuses on ensuring the well-being and physical safety of users by reducing the chances of software defects that could lead to accidents or harm. Additionally, the paper's emphasis on test case prioritization and early detection of faults relates to the value of ""Security"" in providing users with the reassurance and confidence that the software being used is reliable and secure.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,1616,TOSEM,Software Testing & QA,Single and Multi-objective Test Cases Prioritization for Self-driving Cars in Virtual Environments,"Testing with simulation environments helps to identify critical failing scenarios for self-driving cars (SDCs). Simulation-based tests are safer than in-field operational tests and allow detecting software defects before deployment. However, these tests are very expensive and are too many to be run frequently within limited time constraints.In this article, we investigate test case prioritization techniques to increase the ability to detect SDC regression faults with virtual tests earlier. Our approach, called SDC-Prioritizer, prioritizes virtual tests for SDCs according to static features of the roads we designed to be used within the driving scenarios. These features can be collected without running the tests, which means that they do not require past execution results. We introduce two evolutionary approaches to prioritize the test cases using diversity metrics (black-box heuristics) computed on these static features. These two approaches, called SO-SDC-Prioritizer and MO-SDC-Prioritizer, use single-objective and multi-objective genetic algorithms (GA), respectively, to find trade-offs between executing the less expensive tests and the most diverse test cases earlier.Our empirical study conducted in the SDC domain shows that MO-SDC-Prioritizer significantly (P- value &lt;=0.1e-10) improves the ability to detect safety-critical failures at the same level of execution time compared to baselines: random and greedy-based test case orderings. Besides, our study indicates that multi-objective meta-heuristics outperform single-objective approaches when prioritizing simulation-based tests for SDCs.MO-SDC-Prioritizer prioritizes test cases with a large improvement in fault detection while its overhead (up to 0.45\% of the test execution cost) is negligible.",Security,Family Security,The paper's prioritizing of critical failing scenarios for SDCs aligns with the value item of 'family security' as it protects family members who may use SDCs.,"Sure, I can provide further clarification. In the context of the paper's focus on self-driving cars (SDCs), prioritizing critical failing scenarios aligns with the value item of 'family security' as it aims to protect the well-being and safety of individuals, including family members, who may use SDCs. By identifying potential software defects through simulation-based tests, the paper contributes to ensuring that SDCs operate reliably and securely, which directly relates to the value of security and, in turn, family security.",Agreed-Clarified,Agree,,Disagree,"The justification doesn't align with the value item of the Family security
",
Exp B,Exp H,1616,TOSEM,Software Testing & QA,Single and Multi-objective Test Cases Prioritization for Self-driving Cars in Virtual Environments,"Testing with simulation environments helps to identify critical failing scenarios for self-driving cars (SDCs). Simulation-based tests are safer than in-field operational tests and allow detecting software defects before deployment. However, these tests are very expensive and are too many to be run frequently within limited time constraints.In this article, we investigate test case prioritization techniques to increase the ability to detect SDC regression faults with virtual tests earlier. Our approach, called SDC-Prioritizer, prioritizes virtual tests for SDCs according to static features of the roads we designed to be used within the driving scenarios. These features can be collected without running the tests, which means that they do not require past execution results. We introduce two evolutionary approaches to prioritize the test cases using diversity metrics (black-box heuristics) computed on these static features. These two approaches, called SO-SDC-Prioritizer and MO-SDC-Prioritizer, use single-objective and multi-objective genetic algorithms (GA), respectively, to find trade-offs between executing the less expensive tests and the most diverse test cases earlier.Our empirical study conducted in the SDC domain shows that MO-SDC-Prioritizer significantly (P- value &lt;=0.1e-10) improves the ability to detect safety-critical failures at the same level of execution time compared to baselines: random and greedy-based test case orderings. Besides, our study indicates that multi-objective meta-heuristics outperform single-objective approaches when prioritizing simulation-based tests for SDCs.MO-SDC-Prioritizer prioritizes test cases with a large improvement in fault detection while its overhead (up to 0.45\% of the test execution cost) is negligible.",Security,National Security,By working towards eliminating software defects before the deployment of self-driving cars; the paper aligns with the value item 'National Security'; making the roads safer for everyone.,"In the context of a software user's perspective, the alignment between 'Paper X' and the value item of National Security can be justified by the fact that the paper focuses on prioritizing virtual tests for self-driving cars to detect software defects before deployment. By doing so, the paper indirectly contributes to improving the security and safety of the roads, which is an essential aspect of national security. Ensuring that self-driving cars are free from critical software failures increases the overall security and reduces potential risks and accidents, benefiting not only individual users but also society as a whole.",Agreed-Justified,Agree,,Agree,,
Exp C,Exp K,1619,TOSEM,AI & Machine Learning,Dealing with Belief Uncertainty in Domain Models,"There are numerous domains in which information systems need to deal with uncertain information. These uncertainties may originate from different reasons such as vagueness, imprecision, incompleteness, or inconsistencies, and in many cases, they cannot be neglected. In this article, we are interested in representing and processing uncertain information in domain models, considering the stakeholdersaEUR(tm) beliefs (opinions). We show how to associate beliefs to model elements and how to propagate and operate with their associated uncertainty so that domain experts can individually reason about their models enriched with their personal opinions. In addition, we address the challenge of combining the opinions of different domain experts on the same model elements, with the goal to come up with informed collective decisions. We provide different strategies and a methodology to optimally merge individual opinions.",Achievement,Intelligent,The paper discusses the usage of information systems to facilitate domain experts to individually reason about their models. This aligns with the value item Intelligent and its corresponding value Achievement.,"The alignment of 'Paper X' with the value item Intelligent and its corresponding value Achievement from a ""Software User"" perspective is justified by the fact that the paper focuses on enabling domain experts to reason about their models, indicating a level of intelligence and capability in utilizing information systems for achieving successful outcomes in their respective domains.",Agreed-Justified,Agree,,Agree,,
Exp D,Exp K,1619,TOSEM,AI & Machine Learning,Dealing with Belief Uncertainty in Domain Models,"There are numerous domains in which information systems need to deal with uncertain information. These uncertainties may originate from different reasons such as vagueness, imprecision, incompleteness, or inconsistencies, and in many cases, they cannot be neglected. In this article, we are interested in representing and processing uncertain information in domain models, considering the stakeholdersaEUR(tm) beliefs (opinions). We show how to associate beliefs to model elements and how to propagate and operate with their associated uncertainty so that domain experts can individually reason about their models enriched with their personal opinions. In addition, we address the challenge of combining the opinions of different domain experts on the same model elements, with the goal to come up with informed collective decisions. We provide different strategies and a methodology to optimally merge individual opinions.",Self Direction,Independent,The paper emphasizes on stakeholders to make individual reasoning using their model. This aligns with the value item Independent and its corresponding value Self Direction.,The paper explicitly states that it focuses on allowing domain experts to individually reason about their models enriched with their personal opinions. This aligns with the value item Independent and its corresponding value Self Direction because it empowers software users to have autonomy in their decision-making process and exercise their own judgment based on their beliefs and opinions.,Agreed-Clarified,Agree,,Agree,,
Exp E,Exp J,1619,TOSEM,AI & Machine Learning,Dealing with Belief Uncertainty in Domain Models,"There are numerous domains in which information systems need to deal with uncertain information. These uncertainties may originate from different reasons such as vagueness, imprecision, incompleteness, or inconsistencies, and in many cases, they cannot be neglected. In this article, we are interested in representing and processing uncertain information in domain models, considering the stakeholdersaEUR(tm) beliefs (opinions). We show how to associate beliefs to model elements and how to propagate and operate with their associated uncertainty so that domain experts can individually reason about their models enriched with their personal opinions. In addition, we address the challenge of combining the opinions of different domain experts on the same model elements, with the goal to come up with informed collective decisions. We provide different strategies and a methodology to optimally merge individual opinions.",Self Direction,Choosing Own Goals,The paper encourages domain experts to use their personal opinions for deciding the value of model elements. This aligns with the value item Choosing Own Goals and its corresponding value Self Direction.,"In Paper X, the authors emphasize that domain experts are able to individually reason about their models and make informed decisions based on their personal opinions. This aligns with the value item Choosing Own Goals and its corresponding value Self Direction from Schwartz's Taxonomy. The encouragement for domain experts to use their personal opinions shows a focus on allowing individuals to have autonomy and make choices based on their own values and goals within the software context.",Agreed-Clarified,Agree,,Agree,,
Exp F,Exp J,1623,TOSEM,Software Project Management,Nudge: Accelerating Overdue Pull Requests toward Completion,"Pull requests are a key part of the collaborative software development and code review process today. However, pull requests can also slow down the software development process when the reviewer(s) or the author do not actively engage with the pull request. In this work, we design an end-to-end service, Nudge, for accelerating overdue pull requests toward completion by reminding the author or the reviewer(s) to engage with their overdue pull requests. First, we use models based on effort estimation and machine learning to predict the completion time for a given pull request. Second, we use activity detection to filter out pull requests that may be overdue but for which sufficient action is taking place nonetheless. Last, we use actor identification to understand who the blocker of the pull request is and nudge the appropriate actor (author or reviewer(s)). The&nbsp;key novelty of Nudge is that it succeeds in reducing pull request resolution time, while ensuring that developers perceive the notifications sent as useful, at the scale of thousands of repositories. In a randomized trial on 147 repositories in use at Microsoft, Nudge was able to reduce pull request resolution time by 60\% for 8,500 pull requests, when compared to overdue pull requests for which Nudge did not send a notification. Furthermore, developers receiving Nudge notifications resolved 73\% of these notifications as positive. We&nbsp;observed similar results when scaling up the deployment of Nudge to 8,000 repositories at Microsoft, for which Nudge sent 210,000 notifications during a full year. This demonstrates NudgeaEUR(tm)s ability to scale to thousands of repositories. Last, our qualitative analysis of a selection of Nudge notifications indicates areas for future research, such as taking dependencies among pull requests and developer availability into account.",Achievement,Intelligent,The paper devises a machine learning model to predict 'the completion time for a given pull request' which demonstrates the intelligent functionality of the software for the user.,"In the abstract of 'Paper X', it is stated that the paper utilizes models based on effort estimation and machine learning to predict the completion time for pull requests. This intelligent functionality aligns with the value item Intelligent and its corresponding value Achievement from a ""Software User"" perspective because it demonstrates the ability of the software to provide intelligent predictions and facilitate efficient completion of tasks, enhancing the user's sense of achievement in their software development process.",Agreed-Clarified,Agree,,Agree,,
Exp G,Exp K,1623,TOSEM,Software Project Management,Nudge: Accelerating Overdue Pull Requests toward Completion,"Pull requests are a key part of the collaborative software development and code review process today. However, pull requests can also slow down the software development process when the reviewer(s) or the author do not actively engage with the pull request. In this work, we design an end-to-end service, Nudge, for accelerating overdue pull requests toward completion by reminding the author or the reviewer(s) to engage with their overdue pull requests. First, we use models based on effort estimation and machine learning to predict the completion time for a given pull request. Second, we use activity detection to filter out pull requests that may be overdue but for which sufficient action is taking place nonetheless. Last, we use actor identification to understand who the blocker of the pull request is and nudge the appropriate actor (author or reviewer(s)). The&nbsp;key novelty of Nudge is that it succeeds in reducing pull request resolution time, while ensuring that developers perceive the notifications sent as useful, at the scale of thousands of repositories. In a randomized trial on 147 repositories in use at Microsoft, Nudge was able to reduce pull request resolution time by 60\% for 8,500 pull requests, when compared to overdue pull requests for which Nudge did not send a notification. Furthermore, developers receiving Nudge notifications resolved 73\% of these notifications as positive. We&nbsp;observed similar results when scaling up the deployment of Nudge to 8,000 repositories at Microsoft, for which Nudge sent 210,000 notifications during a full year. This demonstrates NudgeaEUR(tm)s ability to scale to thousands of repositories. Last, our qualitative analysis of a selection of Nudge notifications indicates areas for future research, such as taking dependencies among pull requests and developer availability into account.",Achievement,Capable,The paper creates the software 'Nudge' which interacts with user's overdue pull requests and acts as a capable tool to reduce 'pull request resolution time'.,"The justification for aligning 'Paper X' with the value item Capable and its corresponding value Achievement from a ""Software User"" perspective is that the software 'Nudge' designed in the paper demonstrates its capability in reducing 'pull request resolution time'. This achievement directly aligns with the user's desire for efficient software development and collaboration, as it enables them to receive timely reminders and engage with overdue pull requests, ultimately improving their productivity and success in completing their tasks.",Agreed-Justified,Agree,,Agree,,
Exp B,Exp H,1635,TOSEM,AI & Machine Learning,A Machine Learning Approach for Automated Filling of Categorical Fields in Data Entry Forms,"Users frequently interact with software systems through data entry forms. However, form filling is time-consuming and error-prone. Although several techniques have been proposed to auto-complete or pre-fill fields in the forms, they provide limited support to help users fill categorical fields, i.e., fields that require users to choose the right value among a large set of options.In this article, we propose LAFF, a learning-based automated approach for filling categorical fields in data entry forms. LAFF first builds Bayesian Network models by learning field dependencies from a set of historical input instances, representing the values of the fields that have been filled in the past. To improve its learning ability, LAFF uses local modeling to effectively mine the local dependencies of fields in a cluster of input instances. During the form filling phase, LAFF uses such models to predict possible values of a target field, based on the values in the already-filled fields of the form and their dependencies; the predicted values (endorsed based on field dependencies and prediction confidence) are then provided to the end-user as a list of suggestions.We evaluated LAFF by assessing its effectiveness and efficiency in form filling on two datasets, one of them proprietary from the banking domain. Experimental results show that LAFF is able to provide accurate suggestions with a Mean Reciprocal Rank value above 0.73. Furthermore, LAFF is efficient, requiring at most 317 ms per suggestion.",Self Direction,Freedom,"In 'Paper X'; the software ""LAFF"" provides users the freedom to quickly and effectively fill out forms by providing relevant suggestions. This corresponds to the value item ""Freedom"" under the value ""Self Direction"".","In 'Paper X', the software ""LAFF"" aligns with the value item of Freedom and its corresponding value of Self Direction from a ""Software User"" perspective because it provides users with the freedom and autonomy to choose the right values among a large set of options when filling out categorical fields in data entry forms. By offering accurate suggestions based on field dependencies and prediction confidence, LAFF empowers users to make their own choices and enables them to complete the form-filling process quickly and effectively. This aligns with the value of Freedom, which is a key aspect of Self Direction as outlined in Schwartz's Taxonomy.",Agreed-Justified,Agree,,Agree,,
Exp C,Exp K,1635,TOSEM,AI & Machine Learning,A Machine Learning Approach for Automated Filling of Categorical Fields in Data Entry Forms,"Users frequently interact with software systems through data entry forms. However, form filling is time-consuming and error-prone. Although several techniques have been proposed to auto-complete or pre-fill fields in the forms, they provide limited support to help users fill categorical fields, i.e., fields that require users to choose the right value among a large set of options.In this article, we propose LAFF, a learning-based automated approach for filling categorical fields in data entry forms. LAFF first builds Bayesian Network models by learning field dependencies from a set of historical input instances, representing the values of the fields that have been filled in the past. To improve its learning ability, LAFF uses local modeling to effectively mine the local dependencies of fields in a cluster of input instances. During the form filling phase, LAFF uses such models to predict possible values of a target field, based on the values in the already-filled fields of the form and their dependencies; the predicted values (endorsed based on field dependencies and prediction confidence) are then provided to the end-user as a list of suggestions.We evaluated LAFF by assessing its effectiveness and efficiency in form filling on two datasets, one of them proprietary from the banking domain. Experimental results show that LAFF is able to provide accurate suggestions with a Mean Reciprocal Rank value above 0.73. Furthermore, LAFF is efficient, requiring at most 317 ms per suggestion.",Achievement,Successful,"The paper highlights that the technique implemented by ""LAFF"" is successful in accurately predicting the input values with a beneficial Mean Reciprocal Rank value above 0.73. This promotes the user's successful interaction with the software; aligning with the value item ""Successful"" from the value of ""Achievement"".","Based on the abstract, the main contribution of 'Paper X' is the development of the LAFF technique, which successfully predicts values for categorical fields in data entry forms. This aligns with the value item ""Successful"" from the value of ""Achievement"" because the paper explicitly states that LAFF is able to provide accurate suggestions with a Mean Reciprocal Rank value above 0.73. Thus, the user's interaction with the software becomes successful in terms of accurately filling categorical fields, which is a significant achievement in the context of data entry forms.",Agreed-Justified,Agree,,Agree,,
Exp D,Exp K,1635,TOSEM,AI & Machine Learning,A Machine Learning Approach for Automated Filling of Categorical Fields in Data Entry Forms,"Users frequently interact with software systems through data entry forms. However, form filling is time-consuming and error-prone. Although several techniques have been proposed to auto-complete or pre-fill fields in the forms, they provide limited support to help users fill categorical fields, i.e., fields that require users to choose the right value among a large set of options.In this article, we propose LAFF, a learning-based automated approach for filling categorical fields in data entry forms. LAFF first builds Bayesian Network models by learning field dependencies from a set of historical input instances, representing the values of the fields that have been filled in the past. To improve its learning ability, LAFF uses local modeling to effectively mine the local dependencies of fields in a cluster of input instances. During the form filling phase, LAFF uses such models to predict possible values of a target field, based on the values in the already-filled fields of the form and their dependencies; the predicted values (endorsed based on field dependencies and prediction confidence) are then provided to the end-user as a list of suggestions.We evaluated LAFF by assessing its effectiveness and efficiency in form filling on two datasets, one of them proprietary from the banking domain. Experimental results show that LAFF is able to provide accurate suggestions with a Mean Reciprocal Rank value above 0.73. Furthermore, LAFF is efficient, requiring at most 317 ms per suggestion.",Security,Healthy,"The paper abstract suggests that ""LAFF"" is efficient in providing useful predictions quickly for form filling; which can contribute to stress-free; healthy interaction with the software. This matches with the value item ""Healthy"" from the value ""Security"".","In the paper abstract, it is mentioned that LAFF is an efficient approach for form filling, providing accurate suggestions with a quick response time. This can contribute to a stress-free and smooth interaction with the software, which aligns with the value item ""Healthy"" from the value ""Security."" The ability to quickly and accurately fill in data entry forms can enhance the user's sense of security and well-being by reducing the potential frustration and errors associated with manual form filling.",Agreed-Clarified,Agree,,Agree,,
Exp E,Exp J,1637,TOSEM,AI & Machine Learning,Efficient and Effective Feature Space Exploration for Testing Deep Learning Systems,"Assessing the quality of Deep Learning (DL) systems is crucial, as they are increasingly adopted in safety-critical domains. Researchers have proposed several input generation techniques for DL systems. While such techniques can expose failures, they do not explain which features of the test inputs influenced the systemaEUR(tm)s (mis-) behaviour. DeepHyperion was the first test generator to overcome this limitation by exploring the DL systemsaEUR(tm) feature space at large. In this article, we propose DeepHyperion-CS, a test generator for DL systems that enhances DeepHyperion by promoting the inputs that contributed more to feature space exploration during the previous search iterations. We performed an empirical study involving two different test subjects (i.e., a digit classifier and a lane-keeping system for self-driving cars). Our results proved that the contribution-based guidance implemented within DeepHyperion-CS outperforms state-of-the-art tools and significantly improves the efficiency and the effectiveness of DeepHyperion. DeepHyperion-CS exposed significantly more misbehaviours for five out of six feature combinations and was up to 65\% more efficient than DeepHyperion in finding misbehaviour-inducing inputs and exploring the feature space. DeepHyperion-CS was useful for expanding the datasets used to train the DL systems, populating up to 200\% more feature map cells than the original training set.",Achievement,Capable,The paper introduces DeepHyperion-CS; a tool that is capable of exposing significantly more misbehaviours in Deep Learning systems than existing state-of-the-art tools. This enhancement aligns with the value item 'Capable' under the value 'Achievement'. It directly contributes to improving the software user's experience by making the underlying systems more reliable and thus competent.,"The justification provided aligns with the value item Capable and its corresponding value Achievement from a ""Software User"" perspective. The paper's contribution of introducing DeepHyperion-CS, a tool that can expose significantly more misbehaviours in Deep Learning systems, directly enhances the user's experience by improving the reliability and competence of the underlying software systems. This aligns with the value of Achievement as it enables the software user to have a more capable and effective tool for ensuring the quality and safety of Deep Learning systems.",Agreed-Justified,Agree,,Agree,,
Exp F,Exp J,1637,TOSEM,AI & Machine Learning,Efficient and Effective Feature Space Exploration for Testing Deep Learning Systems,"Assessing the quality of Deep Learning (DL) systems is crucial, as they are increasingly adopted in safety-critical domains. Researchers have proposed several input generation techniques for DL systems. While such techniques can expose failures, they do not explain which features of the test inputs influenced the systemaEUR(tm)s (mis-) behaviour. DeepHyperion was the first test generator to overcome this limitation by exploring the DL systemsaEUR(tm) feature space at large. In this article, we propose DeepHyperion-CS, a test generator for DL systems that enhances DeepHyperion by promoting the inputs that contributed more to feature space exploration during the previous search iterations. We performed an empirical study involving two different test subjects (i.e., a digit classifier and a lane-keeping system for self-driving cars). Our results proved that the contribution-based guidance implemented within DeepHyperion-CS outperforms state-of-the-art tools and significantly improves the efficiency and the effectiveness of DeepHyperion. DeepHyperion-CS exposed significantly more misbehaviours for five out of six feature combinations and was up to 65\% more efficient than DeepHyperion in finding misbehaviour-inducing inputs and exploring the feature space. DeepHyperion-CS was useful for expanding the datasets used to train the DL systems, populating up to 200\% more feature map cells than the original training set.",Security,Healthy,By testing Deep Learning systems more thoroughly and efficiently; DeepHyperion-CS contributes to the health of these systems. This aligns with the value item 'Healthy' under the value 'Security'; as it enables software users to use these systems with more confidence about their functionality and dependability.,"DeepHyperion-CS's contribution to the health of Deep Learning systems aligns with the value item 'Healthy' under the value 'Security' because it addresses the crucial need to assess the quality of these systems in safety-critical domains. By enhancing the efficiency and effectiveness of DeepHyperion in exposing misbehaviors and exploring the feature space, DeepHyperion-CS enables software users to have more confidence in the functionality and dependability of these systems. This directly aligns with the value of 'Security' as it promotes a secure and trustworthy software environment by mitigating potential failures and ensuring that the Deep Learning systems are reliable and safe to use.",Agreed-Justified,Agree,,Agree,,
Exp G,Exp K,1637,TOSEM,AI & Machine Learning,Efficient and Effective Feature Space Exploration for Testing Deep Learning Systems,"Assessing the quality of Deep Learning (DL) systems is crucial, as they are increasingly adopted in safety-critical domains. Researchers have proposed several input generation techniques for DL systems. While such techniques can expose failures, they do not explain which features of the test inputs influenced the systemaEUR(tm)s (mis-) behaviour. DeepHyperion was the first test generator to overcome this limitation by exploring the DL systemsaEUR(tm) feature space at large. In this article, we propose DeepHyperion-CS, a test generator for DL systems that enhances DeepHyperion by promoting the inputs that contributed more to feature space exploration during the previous search iterations. We performed an empirical study involving two different test subjects (i.e., a digit classifier and a lane-keeping system for self-driving cars). Our results proved that the contribution-based guidance implemented within DeepHyperion-CS outperforms state-of-the-art tools and significantly improves the efficiency and the effectiveness of DeepHyperion. DeepHyperion-CS exposed significantly more misbehaviours for five out of six feature combinations and was up to 65\% more efficient than DeepHyperion in finding misbehaviour-inducing inputs and exploring the feature space. DeepHyperion-CS was useful for expanding the datasets used to train the DL systems, populating up to 200\% more feature map cells than the original training set.",Universalism,Protecting the Environment,Although the paper does not discuss the specific application domains of the Deep Learning systems tested by DeepHyperion-CS; it mentions the use of these systems in safety-critical domains. Assuming such domains can also include ones that are environmentally critical (like climate modeling; weather predictions; etc.); the proposed tool could indirectly contribute towards 'Protecting the Environment'; a value item under the value 'Universalism'.,"I apologize for any confusion. In my previous justification, I made an assumption that safety-critical domains can include environmentally critical domains, such as climate modeling or weather predictions. By enhancing the efficiency of DeepHyperion in identifying misbehaviors and exploring the feature space of Deep Learning systems, it indirectly contributes to protecting the environment. This is aligned with the value item ""Protecting the Environment"" and its corresponding value ""Universalism"" from a software user perspective.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,1638,TOSEM,Security & Privacy,Demystifying Hidden Sensitive Operations in Android Apps,"Security of Android devices is now paramount, given their wide adoption among consumers. As researchers develop tools for statically or dynamically detecting suspicious apps, malware writers regularly update their attack mechanisms to hide malicious behavior implementation. This poses two problems to current research techniques: static analysis approaches, given their over-approximations, can report an overwhelming number of false alarms, while dynamic approaches will miss those behaviors that are hidden through evasion techniques. We propose in this work a static approach specifically targeted at highlighting hidden sensitive operations (HSOs), mainly sensitive data flows. The prototype version of HiSenDroid has been evaluated on a large-scale dataset of thousands of malware and goodware samples on which it successfully revealed anti-analysis code snippets aiming at evading detection by dynamic analysis. We further experimentally show that, with FlowDroid, some of the hidden sensitive behaviors would eventually lead to private data leaks. Those leaks would have been hard to spot either manually among the large number of false positives reported by the state-of-the-art static analyzers, or by dynamic tools. Overall, by putting the light on hidden sensitive operations, HiSenDroid helps security analysts in validating potentially sensitive data operations, which would be previously unnoticed.",Security,Healthy,The abstract focuses on ensuring the security of Android devices; which aligns with the value of being Healthy in a software context; as it aims to maintain a safe and secure environment for users to interact with.,"The justification for labeling 'Paper X' as aligning with the value item Healthy and its corresponding value Security from a ""Software User"" perspective is that the paper specifically mentions the importance of security in Android devices and how malware writers constantly update their attack mechanisms to hide malicious behavior. By highlighting hidden sensitive operations and revealing anti-analysis code snippets, the paper aims to ensure a safe and secure environment for users to interact with their devices, thus aligning with the value of being Healthy in a software context.",Agreed-Justified,Agree,,Disagree,"The justification doesn't align with the value item of the Healthy
",
Exp B,Exp H,1638,TOSEM,Security & Privacy,Demystifying Hidden Sensitive Operations in Android Apps,"Security of Android devices is now paramount, given their wide adoption among consumers. As researchers develop tools for statically or dynamically detecting suspicious apps, malware writers regularly update their attack mechanisms to hide malicious behavior implementation. This poses two problems to current research techniques: static analysis approaches, given their over-approximations, can report an overwhelming number of false alarms, while dynamic approaches will miss those behaviors that are hidden through evasion techniques. We propose in this work a static approach specifically targeted at highlighting hidden sensitive operations (HSOs), mainly sensitive data flows. The prototype version of HiSenDroid has been evaluated on a large-scale dataset of thousands of malware and goodware samples on which it successfully revealed anti-analysis code snippets aiming at evading detection by dynamic analysis. We further experimentally show that, with FlowDroid, some of the hidden sensitive behaviors would eventually lead to private data leaks. Those leaks would have been hard to spot either manually among the large number of false positives reported by the state-of-the-art static analyzers, or by dynamic tools. Overall, by putting the light on hidden sensitive operations, HiSenDroid helps security analysts in validating potentially sensitive data operations, which would be previously unnoticed.",Security,Social Order,The abstract proposes a mechanism to detect hidden malicious activity within applications; which is in alignment with the value of maintaining Social Order; by providing a mechanism to analyze code snippets and potentially dangerous operations; thus reducing disorder and instability caused by malicious threats.,"In the abstract of 'Paper X', the authors propose a static approach called HiSenDroid to detect hidden sensitive operations (HSOs), which mainly involve sensitive data flows. By highlighting these hidden behaviors and revealing anti-analysis code snippets, the paper contributes to the value of Social Order by providing a mechanism to identify and mitigate potential malicious threats. This helps to ensure the overall security and stability of Android devices, aligning with the goal of maintaining social order within the software user context.",Agreed-Justified,Agree,,Agree,,
Exp C,Exp K,1638,TOSEM,Security & Privacy,Demystifying Hidden Sensitive Operations in Android Apps,"Security of Android devices is now paramount, given their wide adoption among consumers. As researchers develop tools for statically or dynamically detecting suspicious apps, malware writers regularly update their attack mechanisms to hide malicious behavior implementation. This poses two problems to current research techniques: static analysis approaches, given their over-approximations, can report an overwhelming number of false alarms, while dynamic approaches will miss those behaviors that are hidden through evasion techniques. We propose in this work a static approach specifically targeted at highlighting hidden sensitive operations (HSOs), mainly sensitive data flows. The prototype version of HiSenDroid has been evaluated on a large-scale dataset of thousands of malware and goodware samples on which it successfully revealed anti-analysis code snippets aiming at evading detection by dynamic analysis. We further experimentally show that, with FlowDroid, some of the hidden sensitive behaviors would eventually lead to private data leaks. Those leaks would have been hard to spot either manually among the large number of false positives reported by the state-of-the-art static analyzers, or by dynamic tools. Overall, by putting the light on hidden sensitive operations, HiSenDroid helps security analysts in validating potentially sensitive data operations, which would be previously unnoticed.",Achievement,Successful,By making hidden operations more noticeable; HiSenDroid supports a user's sense of achievement by ensuring that they receive recognition for their effort in validating potentially sensitive operations; thereby aligning with the value item Successful.,"In the context of a software user, the alignment of 'Paper X' with the value item Successful and its corresponding value Achievement is justified because HiSenDroid, the proposed static approach, allows users to uncover hidden sensitive operations and validate potentially sensitive data operations. By doing so, users can ensure the security of their Android devices and receive recognition for their effort in actively protecting their sensitive data, which aligns with the sense of accomplishment and success in achieving a secure software environment.",Agreed-Clarified,Agree,,Agree,,
Exp D,Exp K,1646,TOSEM,Software Testing & QA,Parameter Coverage for Testing of Autonomous Driving Systems under Uncertainty,"Autonomous Driving Systems (ADSs) are promising, but must show they are secure and trustworthy before adoption. Simulation-based testing is a widely adopted approach, where the ADS is run in a simulated environment over specific scenarios. Coverage criteria specify what needs to be covered to consider the ADS sufficiently tested. However, existing criteria do not guarantee to exercise the different decisions that the ADS can make, which is essential to assess its correctness. ADSs usually compute their decisions using parameterised rule-based systems and cost functions, such as cost components or decision thresholds. In this article, we argue that the parameters characterise the decision process, as their values affect the ADSaEUR(tm)s final decisions. Therefore, we propose parameter coverage, a criterion requiring to cover the ADSaEUR(tm)s parameters. A scenario covers a parameter if changing its value leads to different simulation results, meaning it is relevant for the driving decisions made in the scenario. Since ADS simulators are slightly uncertain, we employ statistical methods to assess multiple simulation runs for execution difference and coverage. Experiments using the Autonomoose ADS show that the criterion discriminates between different scenarios and that the cost of computing coverage can be managed with suitable heuristics.",Security,Healthy,The paper proposes a criterion for testing Autonomous Driving Systems (ADSs) to ensure they are secure and trustworthy which aligns with the value item Healthy and its corresponding value Security.,"In the context of a software user, the alignment between ""Paper X"" and the value item Healthy can be justified by the fact that the paper focuses on ensuring the security and trustworthiness of Autonomous Driving Systems (ADSs). By proposing a criterion for testing these systems, the paper aims to address potential risks and vulnerabilities, which directly contributes to the overall well-being and safety of users. Therefore, the alignment with the value item Healthy is evident as it pertains to the physical and mental well-being of individuals relying on these software systems.",Agreed-Justified,Agree,,Agree,,
Exp E,Exp J,1646,TOSEM,Software Testing & QA,Parameter Coverage for Testing of Autonomous Driving Systems under Uncertainty,"Autonomous Driving Systems (ADSs) are promising, but must show they are secure and trustworthy before adoption. Simulation-based testing is a widely adopted approach, where the ADS is run in a simulated environment over specific scenarios. Coverage criteria specify what needs to be covered to consider the ADS sufficiently tested. However, existing criteria do not guarantee to exercise the different decisions that the ADS can make, which is essential to assess its correctness. ADSs usually compute their decisions using parameterised rule-based systems and cost functions, such as cost components or decision thresholds. In this article, we argue that the parameters characterise the decision process, as their values affect the ADSaEUR(tm)s final decisions. Therefore, we propose parameter coverage, a criterion requiring to cover the ADSaEUR(tm)s parameters. A scenario covers a parameter if changing its value leads to different simulation results, meaning it is relevant for the driving decisions made in the scenario. Since ADS simulators are slightly uncertain, we employ statistical methods to assess multiple simulation runs for execution difference and coverage. Experiments using the Autonomoose ADS show that the criterion discriminates between different scenarios and that the cost of computing coverage can be managed with suitable heuristics.",Security,Social Order,By advocating for a more thorough simulation-based testing that includes different decisions that ADS can make; the paper aligns with the value item Social Order and its corresponding value Security.,"The justification for aligning 'Paper X' with the value item Social Order and its corresponding value Security is that by advocating for a more thorough simulation-based testing that includes different decisions that ADS can make, the paper addresses the need to ensure that the autonomous driving systems operate within a framework of rules and regulations, thereby promoting social order. Additionally, by emphasizing the importance of assessing the correctness of the ADS's decisions through the coverage of its parameters, the paper contributes to enhancing the security of autonomous driving systems, which is essential for maintaining a sense of security and trust in the technology.",Agreed-Clarified,Agree,,Agree,,
Exp F,Exp J,1646,TOSEM,Software Testing & QA,Parameter Coverage for Testing of Autonomous Driving Systems under Uncertainty,"Autonomous Driving Systems (ADSs) are promising, but must show they are secure and trustworthy before adoption. Simulation-based testing is a widely adopted approach, where the ADS is run in a simulated environment over specific scenarios. Coverage criteria specify what needs to be covered to consider the ADS sufficiently tested. However, existing criteria do not guarantee to exercise the different decisions that the ADS can make, which is essential to assess its correctness. ADSs usually compute their decisions using parameterised rule-based systems and cost functions, such as cost components or decision thresholds. In this article, we argue that the parameters characterise the decision process, as their values affect the ADSaEUR(tm)s final decisions. Therefore, we propose parameter coverage, a criterion requiring to cover the ADSaEUR(tm)s parameters. A scenario covers a parameter if changing its value leads to different simulation results, meaning it is relevant for the driving decisions made in the scenario. Since ADS simulators are slightly uncertain, we employ statistical methods to assess multiple simulation runs for execution difference and coverage. Experiments using the Autonomoose ADS show that the criterion discriminates between different scenarios and that the cost of computing coverage can be managed with suitable heuristics.",Security,National Security,The complex simulation scenarios considered in the paper; contributing to a more securely tested ADS; aligns with the value item National Security and its corresponding value Security.,"The justification for aligning 'Paper X' with the value item National Security and its corresponding value Security is based on the fact that the paper focuses on the secure and trustworthy testing of Autonomous Driving Systems (ADSs). By proposing a new coverage criterion that specifically considers the parameters of the ADS, the paper aims to assess the correctness of the ADS's decisions, which is crucial for ensuring the safety and security of autonomous driving. Therefore, the paper's contribution aligns with the value of Security, particularly in the context of National Security where the focus is on safeguarding the well-being and protection of individuals and society.",Agreed-Clarified,Agree,,Agree,,
Exp G,Exp K,1647,TOSEM,Emerging Technologies,Is My Transaction Done Yet? An Empirical Study of Transaction Processing Times in the Ethereum Blockchain Platform,"Ethereum is one of the most popular platforms for the development of blockchain-powered applications. These applications are known as \DH{}Apps. When engineering \DH{}Apps, developers need to translate requests captured in the front-end of their application into one or more smart contract transactions. Developers need to pay for these transactions and, the more they pay (i.e., the higher the gas price), the faster the transaction is likely to be processed. Developing cost-effective \DH{}Apps is far from trivial, as developers need to optimize the balance between cost (transaction fees) and user experience (transaction processing times). Online services have been developed to provide transaction issuers (e.g., \DH{}App developers) with an estimate of how long transactions will take to be processed given a certain gas price. These estimation services are crucial in the Ethereum domain and several popular wallets such as Metamask rely on them. However, despite their key role, their accuracy has not been empirically investigated so far. In this article, we quantify the transaction processing times in Ethereum, investigate the relationship between processing times and gas prices, and determine the accuracy of state-of-the-practice estimation services. Our results indicate that transactions are processed in a median of 57 seconds and that 90\% of the transactions are processed within 8 minutes. We also show that higher gas prices result in faster transaction processing times with diminishing returns. In particular, we observe no practical difference in processing time between expensive and very expensive transactions. With regards to the accuracy of processing time estimation services, we observe that they are equivalent. However, when stratifying transactions by gas prices, we observe that EtherscanaEUR(tm)s Gas Tracker is the most accurate estimation service for the very cheap and cheap transactions. EthGasStationaEUR(tm)s Gas Price API, in turn, is the most accurate estimation service for regular, expensive, and very expensive transactions. In a post-hoc study, we design a simple linear regression model with only one feature that outperforms the Gas Tracker for very cheap and cheap transactions and that performs as accurately as the EthGasStation model for the remaining categories. Based on our findings, \DH{}App developers can make more informed decisions concerning the choice of the gas price of their application-issued transactions.",Conformity,Self-Discipline,The paper contributes methods for processing times in Ethereum and determining the accuracy of estimation services that are aligned with the value item of Self-Discipline considering users need patience to wait for the transaction to be completed which is clearly stated in the abstract.,"The alignment of ""Paper X"" with the value item Self-Discipline and its corresponding value Conformity is justified because the paper specifically addresses the need for users to have patience and wait for transactions to be completed. This aligns with the value of Conformity as users are expected to follow the established process of waiting for their transactions to be processed, demonstrating discipline and adherence to the prescribed norms and procedures of the Ethereum platform.",Agreed-Clarified,Agree,,Agree,,
Exp A,Exp H,1647,TOSEM,Emerging Technologies,Is My Transaction Done Yet? An Empirical Study of Transaction Processing Times in the Ethereum Blockchain Platform,"Ethereum is one of the most popular platforms for the development of blockchain-powered applications. These applications are known as \DH{}Apps. When engineering \DH{}Apps, developers need to translate requests captured in the front-end of their application into one or more smart contract transactions. Developers need to pay for these transactions and, the more they pay (i.e., the higher the gas price), the faster the transaction is likely to be processed. Developing cost-effective \DH{}Apps is far from trivial, as developers need to optimize the balance between cost (transaction fees) and user experience (transaction processing times). Online services have been developed to provide transaction issuers (e.g., \DH{}App developers) with an estimate of how long transactions will take to be processed given a certain gas price. These estimation services are crucial in the Ethereum domain and several popular wallets such as Metamask rely on them. However, despite their key role, their accuracy has not been empirically investigated so far. In this article, we quantify the transaction processing times in Ethereum, investigate the relationship between processing times and gas prices, and determine the accuracy of state-of-the-practice estimation services. Our results indicate that transactions are processed in a median of 57 seconds and that 90\% of the transactions are processed within 8 minutes. We also show that higher gas prices result in faster transaction processing times with diminishing returns. In particular, we observe no practical difference in processing time between expensive and very expensive transactions. With regards to the accuracy of processing time estimation services, we observe that they are equivalent. However, when stratifying transactions by gas prices, we observe that EtherscanaEUR(tm)s Gas Tracker is the most accurate estimation service for the very cheap and cheap transactions. EthGasStationaEUR(tm)s Gas Price API, in turn, is the most accurate estimation service for regular, expensive, and very expensive transactions. In a post-hoc study, we design a simple linear regression model with only one feature that outperforms the Gas Tracker for very cheap and cheap transactions and that performs as accurately as the EthGasStation model for the remaining categories. Based on our findings, \DH{}App developers can make more informed decisions concerning the choice of the gas price of their application-issued transactions.",Achievement,Intelligent,The abstract indicates that their research outcomes could make dApp users knowledgeable regarding the transaction process and the performance of gas estimation services; which aligns with the value item of Intelligent under the value of Achievement.,"The main contribution of 'Paper X' is providing empirical data on transaction processing times in Ethereum and evaluating the accuracy of estimation services. This allows dApp users to make more informed decisions about the gas price of their transactions, which aligns with the value item of Intelligent under the value of Achievement. By being knowledgeable about the transaction process and the performance of estimation services, software users can optimize the balance between cost and user experience, demonstrating their ability to make intelligent choices in achieving their desired outcomes.",Agreed-Justified,Agree,,Agree,,
Exp B,Exp H,1647,TOSEM,Emerging Technologies,Is My Transaction Done Yet? An Empirical Study of Transaction Processing Times in the Ethereum Blockchain Platform,"Ethereum is one of the most popular platforms for the development of blockchain-powered applications. These applications are known as \DH{}Apps. When engineering \DH{}Apps, developers need to translate requests captured in the front-end of their application into one or more smart contract transactions. Developers need to pay for these transactions and, the more they pay (i.e., the higher the gas price), the faster the transaction is likely to be processed. Developing cost-effective \DH{}Apps is far from trivial, as developers need to optimize the balance between cost (transaction fees) and user experience (transaction processing times). Online services have been developed to provide transaction issuers (e.g., \DH{}App developers) with an estimate of how long transactions will take to be processed given a certain gas price. These estimation services are crucial in the Ethereum domain and several popular wallets such as Metamask rely on them. However, despite their key role, their accuracy has not been empirically investigated so far. In this article, we quantify the transaction processing times in Ethereum, investigate the relationship between processing times and gas prices, and determine the accuracy of state-of-the-practice estimation services. Our results indicate that transactions are processed in a median of 57 seconds and that 90\% of the transactions are processed within 8 minutes. We also show that higher gas prices result in faster transaction processing times with diminishing returns. In particular, we observe no practical difference in processing time between expensive and very expensive transactions. With regards to the accuracy of processing time estimation services, we observe that they are equivalent. However, when stratifying transactions by gas prices, we observe that EtherscanaEUR(tm)s Gas Tracker is the most accurate estimation service for the very cheap and cheap transactions. EthGasStationaEUR(tm)s Gas Price API, in turn, is the most accurate estimation service for regular, expensive, and very expensive transactions. In a post-hoc study, we design a simple linear regression model with only one feature that outperforms the Gas Tracker for very cheap and cheap transactions and that performs as accurately as the EthGasStation model for the remaining categories. Based on our findings, \DH{}App developers can make more informed decisions concerning the choice of the gas price of their application-issued transactions.",Security,Family Security,The paper presents development of tools that ensure the safety of transactions in Ethereum which is aligned with the value item Family Security under value Security because users would feel secure when making transactions using Ethereum.,"The main contribution of ""Paper X"" is the development of tools to ensure the safety of transactions in Ethereum. This directly aligns with the value item Family Security and its corresponding value Security from a ""Software User"" perspective. Users will feel secure when making transactions using Ethereum because the paper addresses the issue of transaction processing times and provides insights into optimizing the balance between cost (transaction fees) and user experience. By improving transaction processing times and accuracy in estimation services, the paper enhances the sense of security for users in conducting their transactions on the platform.",Agreed-Justified,Disagree,reconcile with coder_2,Disagree,The justification doesn't align with the value item of the Family security,
Exp C,Exp K,1649,TOSEM,Security & Privacy,Pied-Piper: Revealing the Backdoor Threats in Ethereum ERC Token Contracts,"With the development of decentralized networks, smart contracts, especially those for ERC tokens, are attracting more and more Dapp users to implement their applications. There are some functions in ERC token contracts that only a specific group of accounts could invoke. Among those functions, some even can influence other accounts or the whole system without prior notice or permission. These functions are referred to as contract backdoors. Once exploited by an attacker, they can cause property losses and harm usersaEUR(tm) privacy.In this work, we propose Pied-Piper, a hybrid analysis method that integrates datalog analysis and directed fuzzing to detect backdoor threats in Ethereum ERC token contracts. First, datalog analysis is applied to abstract the data structures and identification rules related to the threats for preliminary static detection. Then, directed fuzzing is applied to eliminate false positives caused by the static analysis. We first evaluated Pied-Piper on 200 smart contracts, which are injected with different types of backdoors. It reported all problems without false positives, and none of the injected problems was missed. Then, we applied Pied-Piper on 13,484 real token contracts deployed on Ethereum. Pied-Piper reported 189 confirmed problems, four of which have been assigned unique CVE ids while others are still in the review process. Each contract takes 8.03 seconds for datalog analysis on average, and the fuzzing engine can eliminate the false positives within one minute.",Power,Social Recognition,The paper proposes a method for detecting backdoor threats in Ethereum ERC token contracts; eliminating false positives and thus ensuring the reliable functioning of the contracts. This directly aligns with the value item 'Social Recognition' under the value 'Power'; as the users would gain social recognition by successfully handling and implementing their decentralized applications without exploiting backdoors.,"In the context of 'Paper X', the alignment with the value item Social Recognition and the corresponding value Power is derived from the fact that the paper's proposed method for detecting and eliminating backdoor threats in Ethereum ERC token contracts ensures the reliable functioning of the contracts. By successfully handling and implementing their decentralized applications without exploiting backdoors, users would gain social recognition, as their applications would be considered trustworthy and secure. This aligns with the value of Power, as it implies influence, control, and the ability to achieve one's goals, which can be obtained through the recognition and reliability achieved by avoiding backdoor threats.",Agreed-Clarified,Agree,,Agree,,
Exp D,Exp K,1649,TOSEM,Security & Privacy,Pied-Piper: Revealing the Backdoor Threats in Ethereum ERC Token Contracts,"With the development of decentralized networks, smart contracts, especially those for ERC tokens, are attracting more and more Dapp users to implement their applications. There are some functions in ERC token contracts that only a specific group of accounts could invoke. Among those functions, some even can influence other accounts or the whole system without prior notice or permission. These functions are referred to as contract backdoors. Once exploited by an attacker, they can cause property losses and harm usersaEUR(tm) privacy.In this work, we propose Pied-Piper, a hybrid analysis method that integrates datalog analysis and directed fuzzing to detect backdoor threats in Ethereum ERC token contracts. First, datalog analysis is applied to abstract the data structures and identification rules related to the threats for preliminary static detection. Then, directed fuzzing is applied to eliminate false positives caused by the static analysis. We first evaluated Pied-Piper on 200 smart contracts, which are injected with different types of backdoors. It reported all problems without false positives, and none of the injected problems was missed. Then, we applied Pied-Piper on 13,484 real token contracts deployed on Ethereum. Pied-Piper reported 189 confirmed problems, four of which have been assigned unique CVE ids while others are still in the review process. Each contract takes 8.03 seconds for datalog analysis on average, and the fuzzing engine can eliminate the false positives within one minute.",Security,Healthy,The Pied-Piper method mentioned in the paper aims at protecting Ethereum ERC token contracts from backdoor threats. This approach to ensuring the safety of the software user indirectly aligns with 'Healthy'; a value item under the 'Security' value; as it maintains the well-being of the user in the digital environment.,"In the context of a ""Software User,"" the alignment between 'Paper X' and the value item Healthy from the Security value in Schwartz's Taxonomy becomes evident. The proposed method, Pied-Piper, focuses on detecting backdoor threats in Ethereum ERC token contracts, thereby safeguarding the user's digital environment. By preventing potential security breaches and ensuring the integrity of the software, this contribution directly promotes a sense of well-being and protection for the user, aligning with the value item Healthy under the Security value.",Agreed-Justified,Agree,,Agree,,
Exp E,Exp J,1649,TOSEM,Security & Privacy,Pied-Piper: Revealing the Backdoor Threats in Ethereum ERC Token Contracts,"With the development of decentralized networks, smart contracts, especially those for ERC tokens, are attracting more and more Dapp users to implement their applications. There are some functions in ERC token contracts that only a specific group of accounts could invoke. Among those functions, some even can influence other accounts or the whole system without prior notice or permission. These functions are referred to as contract backdoors. Once exploited by an attacker, they can cause property losses and harm usersaEUR(tm) privacy.In this work, we propose Pied-Piper, a hybrid analysis method that integrates datalog analysis and directed fuzzing to detect backdoor threats in Ethereum ERC token contracts. First, datalog analysis is applied to abstract the data structures and identification rules related to the threats for preliminary static detection. Then, directed fuzzing is applied to eliminate false positives caused by the static analysis. We first evaluated Pied-Piper on 200 smart contracts, which are injected with different types of backdoors. It reported all problems without false positives, and none of the injected problems was missed. Then, we applied Pied-Piper on 13,484 real token contracts deployed on Ethereum. Pied-Piper reported 189 confirmed problems, four of which have been assigned unique CVE ids while others are still in the review process. Each contract takes 8.03 seconds for datalog analysis on average, and the fuzzing engine can eliminate the false positives within one minute.",Security,National Security,The paper's focus on reducing potential vulnerabilities in ERC token contracts and thus securing user investments aligns with the 'National Security' value item under the 'Security' value. The user's financial stability and investment safety can be correlated to a sense of security at a larger scale.,"The focus of 'Paper X' on detecting backdoor threats in ERC token contracts aligns with the value item of National Security and its corresponding value of Security from a ""Software User"" perspective. By mitigating potential vulnerabilities in smart contracts, the paper contributes to securing users' investments and ensuring their financial stability, which can be seen as a form of security on a broader scale.",Agreed-Clarified,Agree,,Agree,,
Exp F,Exp J,1655,TOSEM,Software Testing & QA,Automated Identification and Qualitative Characterization of Safety Concerns Reported in UAV Software Platforms,"Unmanned Aerial Vehicles (UAVs) are nowadays used in a variety of applications. Given the cyber-physical nature of UAVs, software defects in these systems can cause issues with safety-critical implications. An important aspect of the lifecycle of UAV software is to minimize the possibility of harming humans or damaging properties through a continuous process of hazard identification and safety risk management. Specifically, safety-related concerns typically emerge during the operation of UAV systems, reported by end-users and developers in the form of issue reports and pull requests. However, popular UAV systems daily receive tens or hundreds of reports of varying types and quality. To help developers timely identify and triage safety-critical UAV issues, we (i) experiment with automated approaches (previously used for issue classification) for detecting the safety-related matters appearing in the titles and descriptions of issues and pull requests reported in UAV platforms and (ii) propose a categorization of the main hazards and accidents discussed in such issues. Our results (i) show that shallow machine learning (ML)-based approaches can identify safety-related sentences with precision, recall, and F-measure values of about 80\%; and (ii) provide a categorization and description of the relationships between safety issue hazards and accidents.",Security,Healthy,The paper aims to minimize software defects in UAV systems to prevent safety-critical issues; aligning with the value item 'Healthy' and its corresponding value 'Security'.,"The main contribution of 'Paper X' aligns with the value item 'Healthy' and its corresponding value 'Security' because the paper specifically addresses the importance of minimizing software defects in UAV systems to prevent safety-critical issues. By ensuring the reliability and safety of the software used in UAVs, the paper directly contributes to the overall security and well-being of individuals and properties, aligning with the value of 'Security.' This alignment is evident in the paper's focus on hazard identification, safety risk management, and the timely identification and triage of safety-related issues in UAV platforms. The direct impact on the safety and security of software users, who rely on properly functioning UAV systems, makes the alignment with the value item 'Healthy' and its corresponding value 'Security' clear and relevant.",Agreed-Justified,Agree,,Agree,,
Exp G,Exp K,1655,TOSEM,Software Testing & QA,Automated Identification and Qualitative Characterization of Safety Concerns Reported in UAV Software Platforms,"Unmanned Aerial Vehicles (UAVs) are nowadays used in a variety of applications. Given the cyber-physical nature of UAVs, software defects in these systems can cause issues with safety-critical implications. An important aspect of the lifecycle of UAV software is to minimize the possibility of harming humans or damaging properties through a continuous process of hazard identification and safety risk management. Specifically, safety-related concerns typically emerge during the operation of UAV systems, reported by end-users and developers in the form of issue reports and pull requests. However, popular UAV systems daily receive tens or hundreds of reports of varying types and quality. To help developers timely identify and triage safety-critical UAV issues, we (i) experiment with automated approaches (previously used for issue classification) for detecting the safety-related matters appearing in the titles and descriptions of issues and pull requests reported in UAV platforms and (ii) propose a categorization of the main hazards and accidents discussed in such issues. Our results (i) show that shallow machine learning (ML)-based approaches can identify safety-related sentences with precision, recall, and F-measure values of about 80\%; and (ii) provide a categorization and description of the relationships between safety issue hazards and accidents.",Security,Social Order,The paper proposes a continuous process of hazard identification and safety risk management; aligning with the value item 'Social Order' related to 'Security'.,"My justification for aligning 'Paper X' with the value item Social Order and its corresponding value Security is based on the fact that the paper specifically focuses on minimizing the possibility of harming humans or damaging properties through a continuous process of hazard identification and safety risk management. By addressing safety-related concerns and providing a categorization of hazards and accidents, the paper aims to contribute to maintaining order and ensuring the security of individuals and their surroundings within the context of UAV software use.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,1655,TOSEM,Software Testing & QA,Automated Identification and Qualitative Characterization of Safety Concerns Reported in UAV Software Platforms,"Unmanned Aerial Vehicles (UAVs) are nowadays used in a variety of applications. Given the cyber-physical nature of UAVs, software defects in these systems can cause issues with safety-critical implications. An important aspect of the lifecycle of UAV software is to minimize the possibility of harming humans or damaging properties through a continuous process of hazard identification and safety risk management. Specifically, safety-related concerns typically emerge during the operation of UAV systems, reported by end-users and developers in the form of issue reports and pull requests. However, popular UAV systems daily receive tens or hundreds of reports of varying types and quality. To help developers timely identify and triage safety-critical UAV issues, we (i) experiment with automated approaches (previously used for issue classification) for detecting the safety-related matters appearing in the titles and descriptions of issues and pull requests reported in UAV platforms and (ii) propose a categorization of the main hazards and accidents discussed in such issues. Our results (i) show that shallow machine learning (ML)-based approaches can identify safety-related sentences with precision, recall, and F-measure values of about 80\%; and (ii) provide a categorization and description of the relationships between safety issue hazards and accidents.",Security,National Security,The paper contributes to the identification and categorization of main hazards and accidents to improve safety in UAV systems; aligning with the value item 'National Security' under 'Security'.,"The main contributions of 'Paper X' directly align with the value item National Security and its corresponding value Security from a ""Software User"" perspective because the paper focuses on minimizing the possibility of harming humans or damaging properties through hazard identification and safety risk management in the context of UAV systems. By identifying and categorizing main hazards and accidents in UAV systems, the paper aims to improve safety, which is a vital component of national security.",Agreed-Justified,Agree,reconcile with coder_2,Agree,,
Exp B,Exp H,1663,TOSEM,Software Testing & QA,Similarity-based Web Element Localization for Robust Test Automation,"Non-robust (fragile) test execution is a commonly reported challenge in GUI-based test automation, despite much research and several proposed solutions. A test script needs to be resilient to (minor) changes in the tested application but, at the same time, fail when detecting potential issues that require investigation. Test script fragility is a multi-faceted problem. However, one crucial challenge is how to reliably identify and locate the correct target web elements when the website evolves between releases or otherwise fail and report an issue. This article proposes and evaluates a novel approach called similarity-based web element localization (Similo), which leverages information from multiple web element locator parameters to identify a target element using a weighted similarity score. This experimental study compares Similo to a baseline approach for web element localization. To get an extensive empirical basis, we target 48 of the most popular websites on the Internet in our evaluation. Robustness is considered by counting the number of web elements found in a recent website version compared to how many of these existed in an older version. Results of the experiment show that Similo outperforms the baseline; it failed to locate the correct target web element in 91 out of 801 considered cases (i.e., 11\%) compared to 214 failed cases (i.e., 27\%) for the baseline approach. The time efficiency of Similo was also considered, where the average time to locate a web element was determined to be 4 milliseconds. However, since the cost of web interactions (e.g., a click) is typically on the order of hundreds of milliseconds, the additional computational demands of Similo can be considered negligible. This study presents evidence that quantifying the similarity between multiple attributes of web elements when trying to locate them, as in our proposed Similo approach, is beneficial. With acceptable efficiency, Similo gives significantly higher effectiveness (i.e., robustness) than the baseline web element localization approach.",Achievement,Intelligent,The paper presents a novel approach for reliably identifying and locating the correct target web elements; which demonstrates the intelligence of the software user in handling automated tests; aligning with the value item Intelligent.,"The justification for aligning 'Paper X' with the value item Intelligent and its corresponding value Achievement from a ""Software User"" perspective is based on the fact that the paper proposes a novel approach (Similo) that demonstrates the intelligence of the software user in handling automated tests. By leveraging multiple web element locator parameters and using a weighted similarity score, the approach shows a higher effectiveness (robustness) in identifying and locating target web elements compared to the baseline approach. This implies that the software user is able to intelligently adapt the test scripts to changes in the tested applications, achieving successful and reliable execution.",Agreed-Justified,Agree,,Agree,,
Exp C,Exp K,1663,TOSEM,Software Testing & QA,Similarity-based Web Element Localization for Robust Test Automation,"Non-robust (fragile) test execution is a commonly reported challenge in GUI-based test automation, despite much research and several proposed solutions. A test script needs to be resilient to (minor) changes in the tested application but, at the same time, fail when detecting potential issues that require investigation. Test script fragility is a multi-faceted problem. However, one crucial challenge is how to reliably identify and locate the correct target web elements when the website evolves between releases or otherwise fail and report an issue. This article proposes and evaluates a novel approach called similarity-based web element localization (Similo), which leverages information from multiple web element locator parameters to identify a target element using a weighted similarity score. This experimental study compares Similo to a baseline approach for web element localization. To get an extensive empirical basis, we target 48 of the most popular websites on the Internet in our evaluation. Robustness is considered by counting the number of web elements found in a recent website version compared to how many of these existed in an older version. Results of the experiment show that Similo outperforms the baseline; it failed to locate the correct target web element in 91 out of 801 considered cases (i.e., 11\%) compared to 214 failed cases (i.e., 27\%) for the baseline approach. The time efficiency of Similo was also considered, where the average time to locate a web element was determined to be 4 milliseconds. However, since the cost of web interactions (e.g., a click) is typically on the order of hundreds of milliseconds, the additional computational demands of Similo can be considered negligible. This study presents evidence that quantifying the similarity between multiple attributes of web elements when trying to locate them, as in our proposed Similo approach, is beneficial. With acceptable efficiency, Similo gives significantly higher effectiveness (i.e., robustness) than the baseline web element localization approach.",Achievement,Successful,The results of the experiment show that the proposed approach outperforms the baseline approach; reflecting the value item Successful; as the user would be more successful in performing automated tests.,"The justification for aligning 'Paper X' with the value item Successful and its corresponding value Achievement from a ""Software User"" perspective is based on the results of the experiment mentioned in the abstract. The success of the proposed approach in outperforming the baseline approach indicates that users would be more successful in performing automated tests. By reliably identifying and locating the correct target web elements, the proposed Similo approach enhances the user's ability to accurately test GUI-based applications and detect potential issues. This aligns with the value item Successful, as users can achieve their testing goals effectively and efficiently.",Agreed-Justified,Agree,,Agree,,
Exp E,Exp J,1667,TOSEM,AI & Machine Learning,Black-box Safety Analysis and Retraining of DNNs based on Feature Extraction and Clustering,"Deep neural networks (DNNs) have demonstrated superior performance over classical machine learning to support many features in safety-critical systems. Although DNNs are now widely used in such systems (e.g., self driving cars), there is limited progress regarding automated support for functional safety analysis in DNN-based systems. For example, the identification of root causes of errors, to enable both risk analysis and DNN retraining, remains an open problem. In this article, we propose SAFE, a black-box approach to automatically characterize the root causes of DNN errors. SAFE relies on a transfer learning model pre-trained on ImageNet to extract the features from error-inducing images. It then applies a density-based clustering algorithm to detect arbitrary shaped clusters of images modeling plausible causes of error. Last, clusters are used to effectively retrain and improve the DNN. The black-box nature of SAFE is motivated by our objective not to require changes or even access to the DNN internals to facilitate adoption. Experimental results show the superior ability of SAFE in identifying different root causes of DNN errors based on case studies in the automotive domain. It also yields significant improvements in DNN accuracy after retraining, while saving significant execution time and memory when compared to alternatives.",Achievement,Intelligent,"The paper proposes a method SAFE for identifying root causes of DNN errors; which in results improves DNN-based systems' performance. This aligns with the value item ""Intelligent""; as it enables the software to perform tasks intelligently and accurately.","The justification for aligning 'Paper X' with the value item Intelligent and its corresponding value Achievement from a ""Software User"" perspective is based on the fact that the paper proposes a method, SAFE, which helps in identifying root causes of DNN errors. By being able to pinpoint the causes of errors, the DNN can be retrained and improved, resulting in higher performance and accuracy. This improvement in performance can be seen as the software being able to perform tasks intelligently and accurately, which aligns with the value of Achievement.",Agreed-Justified,Agree,,Agree,,
Exp F,Exp J,1667,TOSEM,AI & Machine Learning,Black-box Safety Analysis and Retraining of DNNs based on Feature Extraction and Clustering,"Deep neural networks (DNNs) have demonstrated superior performance over classical machine learning to support many features in safety-critical systems. Although DNNs are now widely used in such systems (e.g., self driving cars), there is limited progress regarding automated support for functional safety analysis in DNN-based systems. For example, the identification of root causes of errors, to enable both risk analysis and DNN retraining, remains an open problem. In this article, we propose SAFE, a black-box approach to automatically characterize the root causes of DNN errors. SAFE relies on a transfer learning model pre-trained on ImageNet to extract the features from error-inducing images. It then applies a density-based clustering algorithm to detect arbitrary shaped clusters of images modeling plausible causes of error. Last, clusters are used to effectively retrain and improve the DNN. The black-box nature of SAFE is motivated by our objective not to require changes or even access to the DNN internals to facilitate adoption. Experimental results show the superior ability of SAFE in identifying different root causes of DNN errors based on case studies in the automotive domain. It also yields significant improvements in DNN accuracy after retraining, while saving significant execution time and memory when compared to alternatives.",Security,National Security,"The paper focuses on safety-critical systems like self-driving cars which directly impacts ""National Security""; by contributing to the safe operation of vehicles and hence the safety of citizens.","In the paper, 'Paper X' specifically mentions the use of deep neural networks in safety-critical systems such as self-driving cars. These systems directly impact national security as they contribute to ensuring the safe operation of vehicles, which ultimately leads to the safety and security of citizens. This alignment is evident in the abstract where the paper states that the proposed approach aims to identify root causes of errors in DNN-based systems, enabling risk analysis and retraining to improve functional safety. Therefore, by addressing the challenges in functional safety analysis and contributing to the safe operation of vehicles, 'Paper X' aligns with the value item National Security and its corresponding value Security from the perspective of a software user.",Agreed-Clarified,Agree,,Agree,,
Exp A,Exp H,1668,TOSEM,AI & Machine Learning,I Know What You Are Searching for: Code Snippet Recommendation from Stack Overflow Posts,"Stack Overflow has been heavily used by software developers to seek programming-related information. More and more developers use Community Question and Answer forums, such as Stack Overflow, to search for code examples of how to accomplish a certain coding task. This is often considered to be more efficient than working from source documentation, tutorials, or full worked examples. However, due to the complexity of these online Question and Answer forums and the very large volume of information they contain, developers can be overwhelmed by the sheer volume of available information. This makes it hard to find and/or even be aware of the most relevant code examples to meet their needs. To alleviate this issue, in this work, we present a query-driven code recommendation tool, named Que2Code, that identifies the best code snippets for a user query from Stack Overflow posts. Our approach has two main stages: (i) semantically equivalent question retrieval and (ii) best code snippet recommendation. During the first stage, for a given query question formulated by a developer, we first generate paraphrase questions for the input query as a way of query boosting and then retrieve the relevant Stack Overflow posted questions based on these generated questions. In the second stage, we collect all of the code snippets within questions retrieved in the first stage and develop a novel scheme to rank code snippet candidates from Stack Overflow posts via pairwise comparisons. To evaluate the performance of our proposed model, we conduct a large-scale experiment to evaluate the effectiveness of the semantically equivalent question retrieval task and best code snippet recommendation task separately on Python and Java datasets in Stack Overflow. We also perform a human study to measure how real-world developers perceive the results generated by our model. Both the automatic and human evaluation results demonstrate the promising performance of our model, and we have released our code and data to assist other researchers.",Achievement,Intelligent,"The main contribution of Paper X is the development of ""a query-driven code recommendation tool; named Que2Code."" This tool helps users to find relevant code examples on Stack Overflow; which aligns with the value item Intelligent (v4.1) and its corresponding value Achievement (v4); as the tool assists users in making intelligent choices about code examples.","The justification for aligning 'Paper X' with the value item Intelligent and its corresponding value Achievement from a ""Software User"" perspective is based on the fact that the development of the query-driven code recommendation tool, Que2Code, assists users in finding relevant code examples on Stack Overflow. This tool enables users to make intelligent choices about code examples, as it helps them efficiently search for and identify the most suitable code snippets for their needs. By providing a tool that enhances the user's ability to accomplish coding tasks effectively, 'Paper X' aligns with the value item Intelligent, which emphasizes the importance of intelligence and competence in achieving goals, and the corresponding value Achievement, which highlights the desire for success and competence.",Agreed-Justified,Agree,,Agree,,
Exp B,Exp H,1668,TOSEM,AI & Machine Learning,I Know What You Are Searching for: Code Snippet Recommendation from Stack Overflow Posts,"Stack Overflow has been heavily used by software developers to seek programming-related information. More and more developers use Community Question and Answer forums, such as Stack Overflow, to search for code examples of how to accomplish a certain coding task. This is often considered to be more efficient than working from source documentation, tutorials, or full worked examples. However, due to the complexity of these online Question and Answer forums and the very large volume of information they contain, developers can be overwhelmed by the sheer volume of available information. This makes it hard to find and/or even be aware of the most relevant code examples to meet their needs. To alleviate this issue, in this work, we present a query-driven code recommendation tool, named Que2Code, that identifies the best code snippets for a user query from Stack Overflow posts. Our approach has two main stages: (i) semantically equivalent question retrieval and (ii) best code snippet recommendation. During the first stage, for a given query question formulated by a developer, we first generate paraphrase questions for the input query as a way of query boosting and then retrieve the relevant Stack Overflow posted questions based on these generated questions. In the second stage, we collect all of the code snippets within questions retrieved in the first stage and develop a novel scheme to rank code snippet candidates from Stack Overflow posts via pairwise comparisons. To evaluate the performance of our proposed model, we conduct a large-scale experiment to evaluate the effectiveness of the semantically equivalent question retrieval task and best code snippet recommendation task separately on Python and Java datasets in Stack Overflow. We also perform a human study to measure how real-world developers perceive the results generated by our model. Both the automatic and human evaluation results demonstrate the promising performance of our model, and we have released our code and data to assist other researchers.",Self Direction,Freedom,Que2Code offers Freedom (v1.2) to the users by making search and retrieval of code examples more efficient - an alignment with the value Self Direction (v1). The freedom provided to the user is because they are not restricted to source documentation; tutorials; or full worked examples due to the proposed solution.,"In the context of a ""Software User,"" the alignment of Que2Code with the value item Freedom and its corresponding value Self Direction is evident. The paper explicitly states that the tool provides users with a more efficient way of searching and retrieving code examples, offering them the freedom to no longer be restricted to source documentation, tutorials, or full worked examples. This aligns directly with the value of Self Direction, as it empowers users to independently explore and find the most relevant code snippets to meet their specific needs. By removing the limitations of traditional learning materials, Que2Code allows software users to take control of their own learning process and choose the most suitable resources for their programming tasks.",Agreed-Justified,Agree,,Agree,,
Exp D,Exp K,1671,TOSEM,Security & Privacy,Making Sense of the Unknown: How Managers Make Cyber Security Decisions,"Managers rarely have deep knowledge of cyber security and yet are expected to make decisions with cyber security implications for software-based systems. We investigate the decision-making conversations of seven teams of senior managers from the same organisation as they complete the Decisions \&amp; Disruptions cyber security exercise. We use grounded theory to situate our analysis of their decision-making and help us explore how these complex socio-cognitive interactions occur. We have developed a goal-model (using iStar 2.0) of the teamsaEUR(tm) dialogue that illustrates what cyber security goals teams identify and how they operationalise their decisions to reach these goals. We complement this with our model of cyber security reasoning that describes how these teams make their decisions, showing how each team membersaEUR(tm) experience, intuition, and understanding affects the teamaEUR(tm)s overall shared reasoning and decision-making. Our findings show how managers with little cyber security expertise are able to use logic and traditional risk management thinking to make cyber security decisions. Despite their lack of cyber securityaEUR""specific training, they demonstrate reasoning that closely resembles the decision-making approaches espoused in cyber securityaEUR""specific standards (e.g., NIST/ISO). Our work demonstrates how organisations and practitioners can enrich goal modelling to capture not only what security goals an organisation has (and how they can operationalise them) but also how and why these goals have been identified. Ultimately, nonaEUR""cyber security experts can develop their cyber security model based on their current context (and update it when new requirements appear or new incidents happen), whilst capturing their reasoning at every stage.",Security,Healthy,"Paper X' contributes to helping managers make better decisions regarding cyber security. This implies that the system's well-being or ""health"" is being considered; aligning with the value item Healthy.","In the context of 'Paper X', the alignment with the value item Healthy and its corresponding value Security is justified by the fact that the paper aims to help managers make decisions regarding cyber security. By effectively addressing cyber security concerns, managers contribute to the overall health and well-being of the software-based systems they are responsible for. Making informed decisions regarding cyber security ensures the security and stability of the system, minimizing the risks of potential threats and vulnerabilities. Therefore, this alignment directly relates to the value item Healthy and its associated value Security, as it emphasizes the importance of ensuring the system's well-being and protecting it from potential harm or disruptions.",Agreed-Clarified,Agree,,Agree,,
Exp E,Exp J,1671,TOSEM,Security & Privacy,Making Sense of the Unknown: How Managers Make Cyber Security Decisions,"Managers rarely have deep knowledge of cyber security and yet are expected to make decisions with cyber security implications for software-based systems. We investigate the decision-making conversations of seven teams of senior managers from the same organisation as they complete the Decisions \&amp; Disruptions cyber security exercise. We use grounded theory to situate our analysis of their decision-making and help us explore how these complex socio-cognitive interactions occur. We have developed a goal-model (using iStar 2.0) of the teamsaEUR(tm) dialogue that illustrates what cyber security goals teams identify and how they operationalise their decisions to reach these goals. We complement this with our model of cyber security reasoning that describes how these teams make their decisions, showing how each team membersaEUR(tm) experience, intuition, and understanding affects the teamaEUR(tm)s overall shared reasoning and decision-making. Our findings show how managers with little cyber security expertise are able to use logic and traditional risk management thinking to make cyber security decisions. Despite their lack of cyber securityaEUR""specific training, they demonstrate reasoning that closely resembles the decision-making approaches espoused in cyber securityaEUR""specific standards (e.g., NIST/ISO). Our work demonstrates how organisations and practitioners can enrich goal modelling to capture not only what security goals an organisation has (and how they can operationalise them) but also how and why these goals have been identified. Ultimately, nonaEUR""cyber security experts can develop their cyber security model based on their current context (and update it when new requirements appear or new incidents happen), whilst capturing their reasoning at every stage.",Security,Social Order,The paper focuses on developing a cyber security model that adheres to standards like NIST/ISO; which implies a desire for order; stability; and predictability in the cyber space; thereby aligning with the value item Social Order.,"In the context of the paper, the alignment with the value item Social Order and its corresponding value Security is justified because the development of a cyber security model that adheres to standards like NIST/ISO demonstrates a focus on maintaining order, stability, and predictability in the cyber space. By following established standards and guidelines, the paper aims to establish a sense of order and structure in the decision-making process regarding cyber security. This alignment is important for a software user perspective as it instills confidence and trust in the security measures implemented, ultimately contributing to a sense of security in the software-based systems being used.",Agreed-Justified,Agree,,Agree,,
Exp F,Exp J,1671,TOSEM,Security & Privacy,Making Sense of the Unknown: How Managers Make Cyber Security Decisions,"Managers rarely have deep knowledge of cyber security and yet are expected to make decisions with cyber security implications for software-based systems. We investigate the decision-making conversations of seven teams of senior managers from the same organisation as they complete the Decisions \&amp; Disruptions cyber security exercise. We use grounded theory to situate our analysis of their decision-making and help us explore how these complex socio-cognitive interactions occur. We have developed a goal-model (using iStar 2.0) of the teamsaEUR(tm) dialogue that illustrates what cyber security goals teams identify and how they operationalise their decisions to reach these goals. We complement this with our model of cyber security reasoning that describes how these teams make their decisions, showing how each team membersaEUR(tm) experience, intuition, and understanding affects the teamaEUR(tm)s overall shared reasoning and decision-making. Our findings show how managers with little cyber security expertise are able to use logic and traditional risk management thinking to make cyber security decisions. Despite their lack of cyber securityaEUR""specific training, they demonstrate reasoning that closely resembles the decision-making approaches espoused in cyber securityaEUR""specific standards (e.g., NIST/ISO). Our work demonstrates how organisations and practitioners can enrich goal modelling to capture not only what security goals an organisation has (and how they can operationalise them) but also how and why these goals have been identified. Ultimately, nonaEUR""cyber security experts can develop their cyber security model based on their current context (and update it when new requirements appear or new incidents happen), whilst capturing their reasoning at every stage.",Achievement,Intelligent,The paper contributes to helping non-expert managers make more intelligent security-related decisions using their intuition and traditional risk management thinking. This reflects a pursuit of intelligence; acknowledging their ability to make complex decisions; aligning with the value item Intelligent.,"In the paper abstract, it is mentioned that managers with little cyber security expertise are able to use logic and traditional risk management thinking to make cyber security decisions. This indicates that they are displaying intelligence in their decision-making process. By acknowledging their ability to make complex decisions and aligning with the value item Intelligent, 'Paper X' indirectly promotes the value of Achievement, as it contributes to the managers' pursuit of making intelligent security-related decisions despite their lack of expertise in the field.",Agreed-Justified,Agree,,Agree,,
Exp G,Exp K,1677,TOSEM,Security & Privacy,DAISY: Dynamic-Analysis-Induced Source Discovery for Sensitive Data,"Mobile apps are widely used and often process usersaEUR(tm) sensitive data. Many taint analysis tools have been applied to analyze sensitive information flows and report data leaks in apps. These tools require a list of sources (where sensitive data is accessed) as input, and researchers have constructed such lists within the Android platform by identifying Android API methods that allow access to sensitive data. However, app developers may also define methods or use third-party libraryaEUR(tm)s methods for accessing data. It is difficult to collect such source methods, because they are unique to the apps, and there are a large number of third-party libraries available on the market that evolve over time. To address this problem, we propose DAISY, a Dynamic-Analysis-Induced Source discoverY approach for identifying methods that return sensitive information from apps and third-party libraries. Trained on an automatically labeled dataset of methods and their calling context, DAISY identifies sensitive methods in unseen apps. We evaluated DAISY on real-world apps, and the results show that DAISY can achieve an overall precision of 77.9\% when reporting the most confident results. Most of the identified sources and leaks cannot be detected by existing technologies.",Power,Wealth,The paper presents DAISY for identifying sensitive data accessed by third-party apps. This aligns with the value item 'Wealth' as it aids in preserving user wealth by protecting sensitive financial information from unauthorized access.,"I apologize for any confusion. In my previous justification, I stated that the alignment between 'Paper X' and the value item Wealth is due to the paper presenting DAISY, which aids in preserving user wealth by protecting sensitive financial information from unauthorized access. By identifying methods that return sensitive information from apps and third-party libraries, DAISY helps ensure that users' financial data, which contributes to their wealth, is kept secure. This alignment directly addresses the value of Power, as users gain a sense of control and authority over their financial information, thus aligning with their desire for wealth and the power it provides.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,1677,TOSEM,Security & Privacy,DAISY: Dynamic-Analysis-Induced Source Discovery for Sensitive Data,"Mobile apps are widely used and often process usersaEUR(tm) sensitive data. Many taint analysis tools have been applied to analyze sensitive information flows and report data leaks in apps. These tools require a list of sources (where sensitive data is accessed) as input, and researchers have constructed such lists within the Android platform by identifying Android API methods that allow access to sensitive data. However, app developers may also define methods or use third-party libraryaEUR(tm)s methods for accessing data. It is difficult to collect such source methods, because they are unique to the apps, and there are a large number of third-party libraries available on the market that evolve over time. To address this problem, we propose DAISY, a Dynamic-Analysis-Induced Source discoverY approach for identifying methods that return sensitive information from apps and third-party libraries. Trained on an automatically labeled dataset of methods and their calling context, DAISY identifies sensitive methods in unseen apps. We evaluated DAISY on real-world apps, and the results show that DAISY can achieve an overall precision of 77.9\% when reporting the most confident results. Most of the identified sources and leaks cannot be detected by existing technologies.",Security,Healthy,DAISY's ability to identify methods that return sensitive information from apps could contribute to healthy software usage; aligning with the value item 'Healthy' by preventing harmful situations like data breaches affecting user's online safety.,"DAISY's ability to identify methods that return sensitive information from apps aligns with the value item 'Healthy' in Schwartz's Taxonomy. By preventing harmful situations like data breaches that can affect the user's online safety, DAISY contributes to creating a healthier software usage environment. This aligns with the value of 'Security,' as users can feel more secure in their interactions with mobile apps, knowing that sensitive information is being protected.",Agreed-Justified,Disagree,More incline to Selfdirection: Privacy as it takes care of privacy-related information,Disagree,"The justification doesn't align with the value item of the Healthy
",
Exp B,Exp H,1677,TOSEM,Security & Privacy,DAISY: Dynamic-Analysis-Induced Source Discovery for Sensitive Data,"Mobile apps are widely used and often process usersaEUR(tm) sensitive data. Many taint analysis tools have been applied to analyze sensitive information flows and report data leaks in apps. These tools require a list of sources (where sensitive data is accessed) as input, and researchers have constructed such lists within the Android platform by identifying Android API methods that allow access to sensitive data. However, app developers may also define methods or use third-party libraryaEUR(tm)s methods for accessing data. It is difficult to collect such source methods, because they are unique to the apps, and there are a large number of third-party libraries available on the market that evolve over time. To address this problem, we propose DAISY, a Dynamic-Analysis-Induced Source discoverY approach for identifying methods that return sensitive information from apps and third-party libraries. Trained on an automatically labeled dataset of methods and their calling context, DAISY identifies sensitive methods in unseen apps. We evaluated DAISY on real-world apps, and the results show that DAISY can achieve an overall precision of 77.9\% when reporting the most confident results. Most of the identified sources and leaks cannot be detected by existing technologies.",Security,Family Security,The paper's proposal assists family security by detecting methods within apps containing sensitive information. It aligns with the 'Family Security' value item in safeguarding user's family sensitive data from unlawful access.,"The paper's proposal of DAISY directly aligns with the value item Family Security and its corresponding value Security from a ""Software User"" perspective because it addresses the issue of sensitive data access in mobile apps, which has implications for the security and privacy of a user's family. By detecting methods within apps that contain sensitive information, DAISY helps safeguard the user's family sensitive data from unlawful access, thereby promoting family security as an important value in the context of software usage.",Agreed-Justified,Disagree,reconcile with coder_2,Disagree,"The justification doesn't align with the value item of the Family security

",
Exp D,Exp K,1679,TOSEM,Security & Privacy,On the Discoverability of npm Vulnerabilities in Node.js Projects,"The reliance on vulnerable dependencies is a major threat to software systems. Dependency vulnerabilities are common and remain undisclosed for years. However, once the vulnerability is discovered and publicly known to the community, the risk of exploitation reaches its peak, and developers have to work fast to remediate the problem. While there has been a lot of research to characterize vulnerabilities in software ecosystems, none have explored the problem taking the discoverability into account. Therefore, we perform a large-scale empirical study examining 6,546 Node.js applications. We define three discoverability levels based on vulnerabilities lifecycle (undisclosed, reported, and public). We find that although the majority of the affected applications (99.42\%) depend on undisclosed vulnerable packages, 206&nbsp;(4.63\%) applications were exposed to dependencies with public vulnerabilities. The major culprit for the applications being affected by public vulnerabilities is the lack of dependency updates; in 90.8\% of the cases, a fix is available but not patched by application maintainers. Moreover, we find that applications remain affected by public vulnerabilities for a long time (103 days). Finally, we devise DepReveal, a tool that supports our discoverability analysis approach, to help developers better understand vulnerabilities in their application dependencies and plan their project maintenance.",Security,Social Order,By helping developers understand vulnerabilities in their software application dependencies and plan their project maintenance; DepReveal supports adherence to the social order of software development. This falls under the value item Social Order and its corresponding value Security.,"In the context of software development, social order refers to the adherence to established norms, practices, and processes that contribute to the security and stability of software systems. By providing developers with a tool like DepReveal that helps them understand vulnerabilities in their software application dependencies and plan their project maintenance, 'Paper X' supports adherence to social order by enabling developers to identify and address potential security risks, ensuring the overall security and stability of the software ecosystem. This alignment with social order contributes to the value of security, as it helps to mitigate the risks associated with software vulnerabilities and maintain a secure software environment for software users.",Agreed-Clarified,Agree,,Agree,,
Exp E,Exp J,1679,TOSEM,Security & Privacy,On the Discoverability of npm Vulnerabilities in Node.js Projects,"The reliance on vulnerable dependencies is a major threat to software systems. Dependency vulnerabilities are common and remain undisclosed for years. However, once the vulnerability is discovered and publicly known to the community, the risk of exploitation reaches its peak, and developers have to work fast to remediate the problem. While there has been a lot of research to characterize vulnerabilities in software ecosystems, none have explored the problem taking the discoverability into account. Therefore, we perform a large-scale empirical study examining 6,546 Node.js applications. We define three discoverability levels based on vulnerabilities lifecycle (undisclosed, reported, and public). We find that although the majority of the affected applications (99.42\%) depend on undisclosed vulnerable packages, 206&nbsp;(4.63\%) applications were exposed to dependencies with public vulnerabilities. The major culprit for the applications being affected by public vulnerabilities is the lack of dependency updates; in 90.8\% of the cases, a fix is available but not patched by application maintainers. Moreover, we find that applications remain affected by public vulnerabilities for a long time (103 days). Finally, we devise DepReveal, a tool that supports our discoverability analysis approach, to help developers better understand vulnerabilities in their application dependencies and plan their project maintenance.",Security,Reciprocation of Favors,The tool created in 'Paper X' aids in the reciprocation of favors by helping other developers fix vulnerabilities in their software applications. This directly aligns with the value item Reciprocation of Favors; as it promotes cooperation and mutual aid within the developer community; thereby indirectly benefiting the software users.,"The tool created in 'Paper X' aligns with the value item Reciprocation of Favors and its corresponding value Security from a ""Software User"" perspective because it enables developers to better understand vulnerabilities in their application dependencies and plan their project maintenance. By providing this support, the tool indirectly benefits software users by ensuring the security and stability of the software applications they rely on, thus reciprocating the favor of trust and reliance placed on the developers and fostering a sense of security within the software ecosystem.",Agreed-Clarified,Agree,,Agree,,
Exp F,Exp J,1692,TOSEM,AI & Machine Learning,Simulator-based Explanation and Debugging of Hazard-triggering Events in DNN-based Safety-critical Systems,"When Deep Neural Networks (DNNs) are used in safety-critical systems, engineers should determine the safety risks associated with failures (i.e., erroneous outputs) observed during testing. For DNNs processing images, engineers visually inspect all failure-inducing images to determine common characteristics among them. Such characteristics correspond to hazard-triggering events (e.g., low illumination) that are essential inputs for safety analysis. Though informative, such activity is expensive and error prone.To support such safety analysis practices, we propose Simulator-based Explanations for DNN failurEs (SEDE), a technique that generates readable descriptions for commonalities in failure-inducing, real-world images and improves the DNN through effective retraining. SEDE leverages the availability of simulators, which are commonly used for cyber-physical systems. It relies on genetic algorithms to drive simulators toward the generation of images that are similar to failure-inducing, real-world images in the test set; it then employs rule learning algorithms to derive expressions that capture commonalities in terms of simulator parameter values. The derived expressions are then used to generate additional images to retrain and improve the DNN.With DNNs performing in-car sensing tasks, SEDE successfully characterized hazard-triggering events leading to a DNN accuracy drop. Also, SEDE enabled retraining leading to significant improvements in DNN accuracy, up to 18 percentage points.",Achievement,Successful,The SEDE technique proposed in the paper helps in improving the accuracy of DNNs performing in-car sensing tasks; thereby enabling the software user to be more successful in their use of the software. It directly aims at the achievement of success by the user in terms of better output accuracy.,"The alignment of 'Paper X' with the value item Successful and its corresponding value Achievement from a ""Software User"" perspective is justified by the fact that the SEDE technique proposed in the paper directly contributes to improving the accuracy of the software by retraining the DNNs. This improvement in accuracy translates to the software user achieving better results and being more successful in their use of the software. By addressing the safety risks associated with failures and providing readable descriptions for commonalities in failure-inducing images, 'Paper X' enables the user to achieve their desired level of accuracy, which is an important aspect of success in using software.",Agreed-Justified,Agree,,Agree,,
Exp G,Exp K,1692,TOSEM,AI & Machine Learning,Simulator-based Explanation and Debugging of Hazard-triggering Events in DNN-based Safety-critical Systems,"When Deep Neural Networks (DNNs) are used in safety-critical systems, engineers should determine the safety risks associated with failures (i.e., erroneous outputs) observed during testing. For DNNs processing images, engineers visually inspect all failure-inducing images to determine common characteristics among them. Such characteristics correspond to hazard-triggering events (e.g., low illumination) that are essential inputs for safety analysis. Though informative, such activity is expensive and error prone.To support such safety analysis practices, we propose Simulator-based Explanations for DNN failurEs (SEDE), a technique that generates readable descriptions for commonalities in failure-inducing, real-world images and improves the DNN through effective retraining. SEDE leverages the availability of simulators, which are commonly used for cyber-physical systems. It relies on genetic algorithms to drive simulators toward the generation of images that are similar to failure-inducing, real-world images in the test set; it then employs rule learning algorithms to derive expressions that capture commonalities in terms of simulator parameter values. The derived expressions are then used to generate additional images to retrain and improve the DNN.With DNNs performing in-car sensing tasks, SEDE successfully characterized hazard-triggering events leading to a DNN accuracy drop. Also, SEDE enabled retraining leading to significant improvements in DNN accuracy, up to 18 percentage points.",Security,Healthy,By identifying hazard-triggering events leading to DNN failures; the proposed technique improves the safety of the system. This would contribute towards the health and safety of the software user; providing an alignment with the value item Healthy of the value Security.,"The justification for aligning 'Paper X' with the value item Healthy and its corresponding value Security from a ""Software User"" perspective is that by identifying and addressing hazard-triggering events in DNN failures, the proposed technique improves the safety and reliability of the system. This directly impacts the user's well-being and contributes to a healthy and secure software experience.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,1692,TOSEM,AI & Machine Learning,Simulator-based Explanation and Debugging of Hazard-triggering Events in DNN-based Safety-critical Systems,"When Deep Neural Networks (DNNs) are used in safety-critical systems, engineers should determine the safety risks associated with failures (i.e., erroneous outputs) observed during testing. For DNNs processing images, engineers visually inspect all failure-inducing images to determine common characteristics among them. Such characteristics correspond to hazard-triggering events (e.g., low illumination) that are essential inputs for safety analysis. Though informative, such activity is expensive and error prone.To support such safety analysis practices, we propose Simulator-based Explanations for DNN failurEs (SEDE), a technique that generates readable descriptions for commonalities in failure-inducing, real-world images and improves the DNN through effective retraining. SEDE leverages the availability of simulators, which are commonly used for cyber-physical systems. It relies on genetic algorithms to drive simulators toward the generation of images that are similar to failure-inducing, real-world images in the test set; it then employs rule learning algorithms to derive expressions that capture commonalities in terms of simulator parameter values. The derived expressions are then used to generate additional images to retrain and improve the DNN.With DNNs performing in-car sensing tasks, SEDE successfully characterized hazard-triggering events leading to a DNN accuracy drop. Also, SEDE enabled retraining leading to significant improvements in DNN accuracy, up to 18 percentage points.",Security,National Security,The novelty of paper is enhancing the safety-critical systems utilizing DNNs. It helps in identification of issues in the system that if unattended; might have serious implications on National Security; aligning with the value item National Security of the value Security.,"In the paper abstract, it is mentioned that the technique proposed, SEDE, aims to identify common characteristics in failure-inducing images of DNNs. These characteristics correspond to hazard-triggering events that are essential inputs for safety analysis. By improving the DNN through effective retraining, SEDE enables the characterization of hazard-triggering events in in-car sensing tasks, which ultimately enhances the safety of these systems. As national security is closely tied to safety, the alignment between the main contribution of the paper and the value item of National Security from a ""Software User"" perspective is evident.",Agreed-Justified,Disagree,More incline to Achievement: Capable as it capable of identifying different issues sucessfully,Disagree,"The justification doesn't align with the value item of the National Security
",
Exp B,Exp H,1693,TOSEM,AI & Machine Learning,Testing Feedforward Neural Networks Training Programs,"At present, we are witnessing an increasing effort to improve the performance and trustworthiness of Deep Neural Networks (DNNs), with the aim to enable their adoption in safety critical systems such as self-driving cars or aircraft collision-avoidance systems. Multiple testing techniques are proposed to generate test cases that can expose inconsistencies in the behavior of DNN models. These techniques assume implicitly that the training program is bug-free and appropriately configured. However, satisfying this assumption for a novel problem requires significant engineering work to prepare the data, design the DNN, implement the training program, and tune the hyperparameters to produce the model for which current automated test data generators search for corner-case behaviors. All these model training steps can be error prone. Therefore, it is crucial to detect and correct errors throughout all the engineering steps of DNN-based software systems and not only on the resulting DNN model. In this article, we gather a catalog of training issues and based on their symptoms and their effects on the behavior of the training program, we propose practical verification routines to detect the aforementioned issues, automatically, by continuously validating that some important properties of the learning dynamics hold during the training. Then, we design TheDeepChecker, an end-to-end property-based debugging approach for DNN training programs and implement it as a TensorFlow-based library. As an empirical evaluation, we conduct a case study to assess the effectiveness of TheDeepChecker on synthetic and real-world buggy DL programs and compare its performance to that of the Amazon SageMaker Debugger (SMD). Results show that TheDeepCheckeraEUR(tm)s on-execution validation of DNN-based programaEUR(tm)s properties through three sequential phases (pre-, on-, and post-fitting) succeeds in revealing several coding bugs and system misconfigurations errors early on and at a low cost. Moreover, our property-based approach outperforms the SMDaEUR(tm)s offline rules verification on training logs in terms of detection accuracy for unstable learning issues and coverage of additional DL bugs.",Achievement,Intelligent,The paper's contribution in improving the performance and trustworthiness of DNNs; which are a part of safety critical systems like self-driving cars and aircraft collision-avoidance systems; enables the software user to better understand and trust these systems; aligning with the value item 'Intelligent' from the 'Achievement' value.,"The main contribution of 'Paper X' is focused on improving the performance and trustworthiness of Deep Neural Networks (DNNs) in safety critical systems. By addressing issues and proposing validation routines to detect errors throughout the engineering steps of DNN-based software systems, the paper enables software users to have a higher level of confidence and understanding in these systems. This aligns with the value item 'Intelligent' from the 'Achievement' value, as it emphasizes the attainment of successful and capable outcomes in the context of software usage.",Agreed-Clarified,Agree,,Agree,,
Exp C,Exp K,1693,TOSEM,AI & Machine Learning,Testing Feedforward Neural Networks Training Programs,"At present, we are witnessing an increasing effort to improve the performance and trustworthiness of Deep Neural Networks (DNNs), with the aim to enable their adoption in safety critical systems such as self-driving cars or aircraft collision-avoidance systems. Multiple testing techniques are proposed to generate test cases that can expose inconsistencies in the behavior of DNN models. These techniques assume implicitly that the training program is bug-free and appropriately configured. However, satisfying this assumption for a novel problem requires significant engineering work to prepare the data, design the DNN, implement the training program, and tune the hyperparameters to produce the model for which current automated test data generators search for corner-case behaviors. All these model training steps can be error prone. Therefore, it is crucial to detect and correct errors throughout all the engineering steps of DNN-based software systems and not only on the resulting DNN model. In this article, we gather a catalog of training issues and based on their symptoms and their effects on the behavior of the training program, we propose practical verification routines to detect the aforementioned issues, automatically, by continuously validating that some important properties of the learning dynamics hold during the training. Then, we design TheDeepChecker, an end-to-end property-based debugging approach for DNN training programs and implement it as a TensorFlow-based library. As an empirical evaluation, we conduct a case study to assess the effectiveness of TheDeepChecker on synthetic and real-world buggy DL programs and compare its performance to that of the Amazon SageMaker Debugger (SMD). Results show that TheDeepCheckeraEUR(tm)s on-execution validation of DNN-based programaEUR(tm)s properties through three sequential phases (pre-, on-, and post-fitting) succeeds in revealing several coding bugs and system misconfigurations errors early on and at a low cost. Moreover, our property-based approach outperforms the SMDaEUR(tm)s offline rules verification on training logs in terms of detection accuracy for unstable learning issues and coverage of additional DL bugs.",Security,Sense of Belonging,The paper proposes verification routines that can automatically detect issues in the performance of DNN-based systems. By ensuring the reliable performance of these systems; Paper X assists in fostering a sense of belonging and trust within a community that uses these systems; aligning with the value item 'Sense of Belonging' from the 'Security' value.,"In ""Paper X,"" the proposed verification routines for detecting issues in DNN-based systems directly contribute to the value item ""Sense of Belonging"" and its corresponding value ""Security"" from a ""Software User"" perspective. By ensuring the reliable performance of these systems, users can feel a sense of belonging and trust within the community that relies on these systems. This aligns with the value item as it addresses the need for security and stability in software applications, instilling a sense of belonging and confidence in users.",Agreed-Justified,Agree,,Agree,,
Exp E,Exp J,1698,TOSEM,Software Engineering Practices,Open Source License Inconsistencies on GitHub,"Almost all software, open or closed, builds on open source software and therefore needs to comply with the license obligations of the open source code. Not knowing which licenses to comply with poses a legal danger to anyone using open source software. This article investigates the extent of inconsistencies between licenses declared by an open source project at the top level of the repository and the licenses found in the code. We analyzed a sample of 1,000 open source GitHub repositories. We find that about half of the repositories did not fully declare all licenses found in the code. Of these, approximately 10\% represented a permissive vs. copyleft license mismatch. Furthermore, existing tools cannot fully identify licences. We conclude that users of open source code should not just look at the declared licenses of the open source code they intend to use, but rather examine the software to understand its actual licenses.",Power,Social Recognition,The paper assists in providing users with information that can enhance public recognition of their legal adherence; aligning with the value item Social Recognition under Power.,"In the abstract, the paper specifically mentions that not complying with open source licenses poses a legal danger to anyone using open source software. By addressing this issue and providing an analysis of license inconsistencies in open source GitHub repositories, the paper helps software users in ensuring their legal adherence. This aligns with the value item of Social Recognition under Power, as users can enhance their public recognition and reputation by complying with licenses and avoiding any legal risks associated with using open source software.",Agreed-Clarified,Agree,,Agree,,
Exp F,Exp J,1698,TOSEM,Software Engineering Practices,Open Source License Inconsistencies on GitHub,"Almost all software, open or closed, builds on open source software and therefore needs to comply with the license obligations of the open source code. Not knowing which licenses to comply with poses a legal danger to anyone using open source software. This article investigates the extent of inconsistencies between licenses declared by an open source project at the top level of the repository and the licenses found in the code. We analyzed a sample of 1,000 open source GitHub repositories. We find that about half of the repositories did not fully declare all licenses found in the code. Of these, approximately 10\% represented a permissive vs. copyleft license mismatch. Furthermore, existing tools cannot fully identify licences. We conclude that users of open source code should not just look at the declared licenses of the open source code they intend to use, but rather examine the software to understand its actual licenses.",Security,Social Order,The paper contributes in ensuring the adherence to legal requirements of using open source software; promoting a sense of order in society; aligning with the value item Social Order under Security.,"In 'Paper X', the main contribution is addressing the legal danger and inconsistencies in complying with open source licenses. By emphasizing the importance of understanding the actual licenses in open source code, the paper promotes adherence to legal requirements and therefore contributes to maintaining social order and a sense of security in the software community. This directly aligns with the value item Social Order under the broader value of Security from a ""Software User"" perspective.",Agreed-Clarified,Agree,,Agree,,
Exp G,Exp K,1698,TOSEM,Software Engineering Practices,Open Source License Inconsistencies on GitHub,"Almost all software, open or closed, builds on open source software and therefore needs to comply with the license obligations of the open source code. Not knowing which licenses to comply with poses a legal danger to anyone using open source software. This article investigates the extent of inconsistencies between licenses declared by an open source project at the top level of the repository and the licenses found in the code. We analyzed a sample of 1,000 open source GitHub repositories. We find that about half of the repositories did not fully declare all licenses found in the code. Of these, approximately 10\% represented a permissive vs. copyleft license mismatch. Furthermore, existing tools cannot fully identify licences. We conclude that users of open source code should not just look at the declared licenses of the open source code they intend to use, but rather examine the software to understand its actual licenses.",Benevolence,Honesty,The paper endorses honesty by advocating for users to understand the actual licenses of the software they intend to use. This highlights the importance of being truthful in software licensing; aligning with the value item Honesty under Benevolence.,"The justification for aligning 'Paper X' with the value item Honesty and its corresponding value Benevolence from a ""Software User"" perspective is based on the fact that the paper emphasizes the importance of users understanding the actual licenses of the software they intend to use. By advocating for this understanding, the paper promotes honesty in adhering to the legal obligations associated with the use of open source software. In doing so, it aligns with the value item Honesty under the broader value of Benevolence, which entails acting in a responsible and ethical manner.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,1701,TOSEM,AI & Machine Learning,Digital Twin-based Anomaly Detection with Curriculum Learning in Cyber-physical Systems,"Anomaly detection is critical to ensure the security of cyber-physical systems (CPS). However, due to the increasing complexity of attacks and CPS themselves, anomaly detection in CPS is becoming more and more challenging. In our previous work, we proposed a digital twin-based anomaly detection method, called ATTAIN, which takes advantage of both historical and real-time data of CPS. However, such data vary significantly in terms of difficulty. Therefore, similar to human learning processes, deep learning models (e.g., ATTAIN) can benefit from an easy-to-difficult curriculum. To this end, in this paper, we present a novel approach, named digitaL twin-based Anomaly deTecTion wIth Curriculum lEarning (LATTICE), which extends ATTAIN by introducing curriculum learning to optimize its learning paradigm. LATTICE attributes each sample with a difficulty score, before being fed into a training scheduler. The training scheduler samples batches of training data based on these difficulty scores such that learning from easy to difficult data can be performed. To evaluate LATTICE, we use five publicly available datasets collected from five real-world CPS testbeds. We compare LATTICE with ATTAIN and two other state-of-the-art anomaly detectors. Evaluation results show that LATTICE outperforms the three baselines and ATTAIN by 0.906\%-2.367\% in terms of the F1 score. LATTICE also, on average, reduces the training time of ATTAIN by 4.2\% on the five datasets and is on par with the baselines in terms of detection delay time.",Security,Healthy,The paper contributes to the value of 'Security' by improving the anomaly detection in CPS; which indirectly contributes to the software user's health by potentially preventing adverse events caused by anomalies.,"In the context of a ""Software User,"" the paper's contribution aligns with the value item of ""Healthy"" and its corresponding value of ""Security"" because by improving anomaly detection in cyber-physical systems, the paper aims to prevent potential security breaches and adverse events caused by anomalies. This indirectly contributes to the software user's health by ensuring the reliability and safety of the system they are interacting with, reducing the risk of any harm or negative impact on their well-being.",Agreed-Justified,Disagree,reconcile with coder_2,Disagree,"The justification doesn't align with the value item of the Healthy
",
Exp B,Exp H,1701,TOSEM,AI & Machine Learning,Digital Twin-based Anomaly Detection with Curriculum Learning in Cyber-physical Systems,"Anomaly detection is critical to ensure the security of cyber-physical systems (CPS). However, due to the increasing complexity of attacks and CPS themselves, anomaly detection in CPS is becoming more and more challenging. In our previous work, we proposed a digital twin-based anomaly detection method, called ATTAIN, which takes advantage of both historical and real-time data of CPS. However, such data vary significantly in terms of difficulty. Therefore, similar to human learning processes, deep learning models (e.g., ATTAIN) can benefit from an easy-to-difficult curriculum. To this end, in this paper, we present a novel approach, named digitaL twin-based Anomaly deTecTion wIth Curriculum lEarning (LATTICE), which extends ATTAIN by introducing curriculum learning to optimize its learning paradigm. LATTICE attributes each sample with a difficulty score, before being fed into a training scheduler. The training scheduler samples batches of training data based on these difficulty scores such that learning from easy to difficult data can be performed. To evaluate LATTICE, we use five publicly available datasets collected from five real-world CPS testbeds. We compare LATTICE with ATTAIN and two other state-of-the-art anomaly detectors. Evaluation results show that LATTICE outperforms the three baselines and ATTAIN by 0.906\%-2.367\% in terms of the F1 score. LATTICE also, on average, reduces the training time of ATTAIN by 4.2\% on the five datasets and is on par with the baselines in terms of detection delay time.",Power,Social Power,By providing a more effective method for anomaly detection; the paper contributes to the social power of users by giving them more control and certainty over the systems they use. This aligns with the value item 'Social Power' in the value of 'Power'.,"The main contribution of 'Paper X' is the introduction of a novel approach, LATTICE, which improves anomaly detection in cyber-physical systems. By optimizing the learning paradigm through curriculum learning, LATTICE enhances the effectiveness of anomaly detection methods like ATTAIN. This directly aligns with the value item Social Power and its corresponding value Power because it empowers software users with more control and certainty over the systems they use, thereby increasing their social power in the context of the software.",Agreed-Justified,Agree,,Agree,,
Exp C,Exp K,1701,TOSEM,AI & Machine Learning,Digital Twin-based Anomaly Detection with Curriculum Learning in Cyber-physical Systems,"Anomaly detection is critical to ensure the security of cyber-physical systems (CPS). However, due to the increasing complexity of attacks and CPS themselves, anomaly detection in CPS is becoming more and more challenging. In our previous work, we proposed a digital twin-based anomaly detection method, called ATTAIN, which takes advantage of both historical and real-time data of CPS. However, such data vary significantly in terms of difficulty. Therefore, similar to human learning processes, deep learning models (e.g., ATTAIN) can benefit from an easy-to-difficult curriculum. To this end, in this paper, we present a novel approach, named digitaL twin-based Anomaly deTecTion wIth Curriculum lEarning (LATTICE), which extends ATTAIN by introducing curriculum learning to optimize its learning paradigm. LATTICE attributes each sample with a difficulty score, before being fed into a training scheduler. The training scheduler samples batches of training data based on these difficulty scores such that learning from easy to difficult data can be performed. To evaluate LATTICE, we use five publicly available datasets collected from five real-world CPS testbeds. We compare LATTICE with ATTAIN and two other state-of-the-art anomaly detectors. Evaluation results show that LATTICE outperforms the three baselines and ATTAIN by 0.906\%-2.367\% in terms of the F1 score. LATTICE also, on average, reduces the training time of ATTAIN by 4.2\% on the five datasets and is on par with the baselines in terms of detection delay time.",Achievement,Intelligent,The improved accuracy of anomaly detection facilitates software users to be more intelligent in dealing with potential threats or issues. The alignment lies with 'Intelligent' under the value 'Achievement'.,"The alignment of 'Paper X' with the value item Intelligent and its corresponding value Achievement from a ""Software User"" perspective is justified because the enhanced accuracy of anomaly detection offered by the proposed approach allows software users to demonstrate their intelligence by effectively identifying and addressing potential threats or issues within the cyber-physical systems. This achievement is directly linked to the value of being intelligent and showcases the capability of software users to successfully navigate and protect the system.",Agreed-Justified,Agree,,Agree,,
Exp D,Exp K,1706,TOSEM,AI & Machine Learning,Automated Identification of Toxic Code Reviews Using ToxiCR,"Toxic conversations during software development interactions may have serious repercussions on a Free and Open Source Software (FOSS) development project. For example, victims of toxic conversations may become afraid to express themselves, therefore get demotivated, and may eventually leave the project. Automated filtering of toxic conversations may help a FOSS community maintain healthy interactions among its members. However, off-the-shelf toxicity detectors perform poorly on a software engineering dataset, such as one curated from code review comments. To counter this challenge, we present ToxiCR, a supervised learning based toxicity identification tool for code review interactions. ToxiCR includes a choice to select one of the 10 supervised learning algorithms, an option to select text vectorization techniques, eight preprocessing steps, and a large-scale labeled dataset of 19,651 code review comments. Two out of those eight preprocessing steps are software engineering domain specific. With our rigorous evaluation of the models with various combinations of preprocessing steps and vectorization techniques, we have identified the best combination for our dataset that boosts 95.8\% accuracy and an 88.9\% F1-score in identifying toxic texts. ToxiCR significantly outperforms existing toxicity detectors on our dataset. We have released our dataset, pre-trained models, evaluation results, and source code publicly, which is available at .",Security,Family Security,The paper contributes to a tool that maintains healthy interactions among the members of the FOSS community; supporting a secure; respectful environment which could directly contribute to the 'Family Security' from the perspective of a software user within a software context.,"I apologize for any confusion caused by my previous justification. To clarify, the main contribution of 'Paper X' is the development of a toxicity identification tool for code review interactions in the context of Free and Open Source Software (FOSS) development projects. By detecting and filtering toxic conversations, this tool helps maintain healthy interactions within the community. Specifically, from a ""Software User"" perspective, the tool contributes to a secure and respectful environment within the FOSS community. This alignment with the value item of Family Security can be seen as ensuring the well-being and safety of the community members and their families, fostering a user-friendly and positive software development ecosystem.",Agreed-Clarified,Agree,,Agree,,
Exp E,Exp J,1706,TOSEM,AI & Machine Learning,Automated Identification of Toxic Code Reviews Using ToxiCR,"Toxic conversations during software development interactions may have serious repercussions on a Free and Open Source Software (FOSS) development project. For example, victims of toxic conversations may become afraid to express themselves, therefore get demotivated, and may eventually leave the project. Automated filtering of toxic conversations may help a FOSS community maintain healthy interactions among its members. However, off-the-shelf toxicity detectors perform poorly on a software engineering dataset, such as one curated from code review comments. To counter this challenge, we present ToxiCR, a supervised learning based toxicity identification tool for code review interactions. ToxiCR includes a choice to select one of the 10 supervised learning algorithms, an option to select text vectorization techniques, eight preprocessing steps, and a large-scale labeled dataset of 19,651 code review comments. Two out of those eight preprocessing steps are software engineering domain specific. With our rigorous evaluation of the models with various combinations of preprocessing steps and vectorization techniques, we have identified the best combination for our dataset that boosts 95.8\% accuracy and an 88.9\% F1-score in identifying toxic texts. ToxiCR significantly outperforms existing toxicity detectors on our dataset. We have released our dataset, pre-trained models, evaluation results, and source code publicly, which is available at .",Benevolence,Forgiving,The tool introduced; ToxiCR; aims to curtail toxic conversations that may make the software user a victim; thereby fostering 'Forgiveness'- a value under 'Benevolence'. This is inferred from the motivation that reducing toxic conversations can indirectly contribute towards an environment where users can absolve one another.,"In 'Paper X,' the main contribution of ToxiCR is to identify and filter toxic conversations in code review comments. By reducing toxic interactions, the tool promotes a healthier environment for software users, allowing for forgiveness and a sense of benevolence to be fostered within the community. This aligns with the value item Forgiving and its corresponding value Benevolence from Schwartz's Taxonomy.",Agreed-Justified,Agree,,Agree,,
Exp F,Exp J,1706,TOSEM,AI & Machine Learning,Automated Identification of Toxic Code Reviews Using ToxiCR,"Toxic conversations during software development interactions may have serious repercussions on a Free and Open Source Software (FOSS) development project. For example, victims of toxic conversations may become afraid to express themselves, therefore get demotivated, and may eventually leave the project. Automated filtering of toxic conversations may help a FOSS community maintain healthy interactions among its members. However, off-the-shelf toxicity detectors perform poorly on a software engineering dataset, such as one curated from code review comments. To counter this challenge, we present ToxiCR, a supervised learning based toxicity identification tool for code review interactions. ToxiCR includes a choice to select one of the 10 supervised learning algorithms, an option to select text vectorization techniques, eight preprocessing steps, and a large-scale labeled dataset of 19,651 code review comments. Two out of those eight preprocessing steps are software engineering domain specific. With our rigorous evaluation of the models with various combinations of preprocessing steps and vectorization techniques, we have identified the best combination for our dataset that boosts 95.8\% accuracy and an 88.9\% F1-score in identifying toxic texts. ToxiCR significantly outperforms existing toxicity detectors on our dataset. We have released our dataset, pre-trained models, evaluation results, and source code publicly, which is available at .",Universalism,A World at Peace,The creation and deployment of ToxiCR reinforces unity and mutual respect among software users; contributing to 'A World at Peace' under the value 'Universalism'; as it aims to suppress toxic interactions; thus indirectly encouraging peaceful co-existence.,"In the context of a software user, the main contribution of 'Paper X' is the development of ToxiCR, a toxicity identification tool for code review interactions. By providing a means to filter out toxic conversations and promote healthy interactions among software users, ToxiCR indirectly contributes to the value item 'A World at Peace' and its corresponding value 'Universalism' from Schwartz's Taxonomy. This is because the tool reinforces unity and mutual respect within the software community, creating a more harmonious and peaceful environment for software users to collaborate and engage in discussions.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,1707,TOSEM,Software Deployment & Operations,FaaSLight: General Application-level Cold-start Latency Optimization for Function-as-a-Service in Serverless Computing,"Serverless computing is a popular cloud computing paradigm that frees developers from server management. Function-as-a-Service (FaaS) is the most popular implementation of serverless computing, representing applications as event-driven and stateless functions. However, existing studies report that functions of FaaS applications severely suffer from cold-start latency.In this article, we propose an approach, namely, FaaSLight, to accelerating the cold start for FaaS applications through application-level optimization. We first conduct a measurement study to investigate the possible root cause of the cold-start problem of FaaS. The result shows that application code loading latency is a significant overhead. Therefore, loading only indispensable code from FaaS applications can be an adequate solution. Based on this insight, we identify code related to application functionalities by constructing the function-level call graph and separate other code (i.e., optional code) from FaaS applications. The separated optional code can be loaded on demand to avoid the inaccurate identification of indispensable code causing application failure. In particular, a key principle guiding the design of FaaSLight is inherently general, i.e., platform- and language-agnostic. In practice, FaaSLight can be effectively applied to FaaS applications developed in different programming languages (Python and JavaScript), and can be seamlessly deployed on popular serverless platforms such as AWS Lambda and Google Cloud Functions, without having to modify the underlying OSes or hypervisors, nor introducing any additional manual engineering efforts to developers. The evaluation results on real-world FaaS applications show that FaaSLight can significantly reduce the code loading latency (up to 78.95\%, 28.78\% on average), thereby reducing the cold-start latency. As a result, the total response latency of functions can be decreased by up to 42.05\% (19.21\% on average). Compared with the state-of-the-art, FaaSLight achieves a 21.25\texttimes{} improvement in reducing the average total response latency.",Achievement,Successful,The approach proposed in Paper X emphasizes on the value item Successful which corresponds to the value Achievement. Through the application of FaaSLight; Software Users achieve success by reducing up to 78.95% of the code loading latency which in turn helps in reducing the cold-start latency. This allows users to operate the software applications successfully by decreasing the total response latency; thereby improving the functionality of the software.,"The justification for aligning 'Paper X' with the value item Successful and its corresponding value Achievement from a ""Software User"" perspective is based on the fact that the proposed approach, FaaSLight, significantly reduces code loading latency and cold-start latency. This reduction in latency ultimately leads to an improvement in the functionality and performance of the software applications. The ability to operate the software successfully by decreasing response latency aligns with the value of Achievement, as users are able to achieve their desired outcomes more effectively and efficiently.",Agreed-Justified,Agree,,Agree,,
Exp B,Exp H,1707,TOSEM,Software Deployment & Operations,FaaSLight: General Application-level Cold-start Latency Optimization for Function-as-a-Service in Serverless Computing,"Serverless computing is a popular cloud computing paradigm that frees developers from server management. Function-as-a-Service (FaaS) is the most popular implementation of serverless computing, representing applications as event-driven and stateless functions. However, existing studies report that functions of FaaS applications severely suffer from cold-start latency.In this article, we propose an approach, namely, FaaSLight, to accelerating the cold start for FaaS applications through application-level optimization. We first conduct a measurement study to investigate the possible root cause of the cold-start problem of FaaS. The result shows that application code loading latency is a significant overhead. Therefore, loading only indispensable code from FaaS applications can be an adequate solution. Based on this insight, we identify code related to application functionalities by constructing the function-level call graph and separate other code (i.e., optional code) from FaaS applications. The separated optional code can be loaded on demand to avoid the inaccurate identification of indispensable code causing application failure. In particular, a key principle guiding the design of FaaSLight is inherently general, i.e., platform- and language-agnostic. In practice, FaaSLight can be effectively applied to FaaS applications developed in different programming languages (Python and JavaScript), and can be seamlessly deployed on popular serverless platforms such as AWS Lambda and Google Cloud Functions, without having to modify the underlying OSes or hypervisors, nor introducing any additional manual engineering efforts to developers. The evaluation results on real-world FaaS applications show that FaaSLight can significantly reduce the code loading latency (up to 78.95\%, 28.78\% on average), thereby reducing the cold-start latency. As a result, the total response latency of functions can be decreased by up to 42.05\% (19.21\% on average). Compared with the state-of-the-art, FaaSLight achieves a 21.25\texttimes{} improvement in reducing the average total response latency.",Security,Sense of Belonging,Paper X‚Äö√Ñ√∂‚àö√ë‚àö‚àÇ‚Äö√†√∂‚àö√´‚Äö√†√∂‚Äö√†√á‚Äö√Ñ√∂‚àö‚Ä†‚àö‚àÇ‚Äö√†√∂‚àö¬¥‚Äö√Ñ√∂‚àö‚Ä†‚àö‚àÇ¬¨¬®‚Äö√Ñ¬¢s contribution to attenuating latency issues in Function-as-a-Service (FaaS) implementations can be seen as engendering a Sense of Belonging; a value item under the Security value. By providing a solution that improves the functionality and performance of FaaS applications; FaaSLight can facilitate smoother and more efficient interactions within software. This in turns fosters a sense of belonging for the Software Users as they feel comfortable and secure using the improved FaaS applications.,"Paper X's contribution to attenuating latency issues in Function-as-a-Service (FaaS) implementations aligns with the value item Sense of Belonging and its corresponding value Security from a ""Software User"" perspective because by improving the functionality and performance of FaaS applications, FaaSLight enables smoother and more efficient interactions within software. This fosters a sense of belonging for Software Users as they feel comfortable and secure using the improved FaaS applications, knowing that their interactions are reliable and their data is protected.",Agreed-Justified,Agree,,Agree,,
Exp E,Exp J,1712,TOSEM,Software Testing & QA,A Survey on Automated Driving System Testing: Landscapes and Trends,"Automated Driving Systems (ADS) have made great achievements in recent years thanks to the efforts from both academia and industry. A typical ADS is composed of multiple modules, including sensing, perception, planning, and control, which brings together the latest advances in different domains. Despite these achievements, safety assurance of ADS is of great significance, since unsafe behavior of ADS can bring catastrophic consequences. Testing has been recognized as an important system validation approach that aims to expose unsafe system behavior; however, in the context of ADS, it is extremely challenging to devise effective testing techniques, due to the high complexity and multidisciplinarity of the systems. There has been great much literature that focuses on the testing of ADS, and a number of surveys have also emerged to summarize the technical advances. Most of the surveys focus on the system-level testing performed within software simulators, and they thereby ignore the distinct features of different modules. In this article, we provide a comprehensive survey on the existing ADS testing literature, which takes into account both module-level and system-level testing. Specifically, we make the following contributions: (1) We survey the module-level testing techniques for ADS and highlight the technical differences affected by the features of different modules; (2) we also survey the system-level testing techniques, with focuses on the empirical studies that summarize the issues occurring in system development or deployment, the problems due to the collaborations between different modules, and the gap between ADS testing in simulators and the real world; and (3) we identify the challenges and opportunities in ADS testing, which pave the path to the future research in this field.",Security,National Security,The focus on safety assurance and exposing potential unsafe behaviors aligns with the value item 'National Security' under the value 'Security'.,"The justification for aligning 'Paper X' with the value item National Security and its corresponding value Security from a ""Software User"" perspective is based on the focus of the paper on safety assurance and exposing potential unsafe behaviors. As a software user, ensuring the security of automated driving systems is of utmost importance, as any unsafe behavior can have catastrophic consequences. The paper recognizes this significance and addresses the challenges of testing ADS to identify and mitigate potential risks, ultimately contributing to the enhancement of national security in the context of automated driving systems.",Agreed-Justified,Agree,,Agree,,
Exp F,Exp J,1713,TOSEM,AI & Machine Learning,QuoTe: Quality-oriented Testing for Deep Learning Systems,"Recently, there has been significant growth of interest in applying software engineering techniques for the quality assurance of deep learning (DL) systems. One popular direction is DL testingaEUR""that is, given a property of test, defects of DL systems are found either by fuzzing or guided search with the help of certain testing metrics. However, recent studies have revealed that the neuron coverage metrics, which are commonly used by most existing DL testing approaches, are not necessarily correlated with model quality (e.g., robustness, the most studied model property), and are also not an effective measurement on the confidence of the model quality after testing. In this work, we address this gap by proposing a novel testing framework called QuoTe (i.e., Quality-oriented Testing). A key part of QuoTe is a quantitative measurement on (1) the value of each test case in enhancing the model property of interest (often via retraining) and (2) the convergence quality of the model property improvement. QuoTe utilizes the proposed metric to automatically select or generate valuable test cases for improving model quality. The proposed metric is also a lightweight yet strong indicator of how well the improvement converged. Extensive experiments on both image and tabular datasets with a variety of model architectures confirm the effectiveness and efficiency of QuoTe in improving DL model qualityaEUR""that is, robustness and fairness. As a generic quality-oriented testing framework, future adaptations can be made to other domains (e.g., text) as well as other model properties.",Stimulation,Variation in Life,In the abstract; there is a mention of the utilization of 'Quality-oriented Testing' which involves a variety of model architectures; adaptions to other domains and model properties. This reflects an aspect of 'Variation in Life' under the value 'Stimulation' as it represents a variation in the model architectures; domains and properties the software user interacts with for quality assurance of Deep Learning systems.,"In 'Paper X', the utilization of 'Quality-oriented Testing' and the mention of adaptions to other domains and model properties align with the value item Variation in Life and its corresponding value Stimulation. This is because the introduction of different model architectures, domains, and properties for quality assurance in Deep Learning systems adds variety and excitement to the software user's experience, providing stimulation and a sense of exploration and diversity in their software usage.",Agreed-Clarified,Disagree,,Disagree,"In my perspective, it does not align with variation in life. It just talks about various architecture.",
Exp G,Exp K,1713,TOSEM,AI & Machine Learning,QuoTe: Quality-oriented Testing for Deep Learning Systems,"Recently, there has been significant growth of interest in applying software engineering techniques for the quality assurance of deep learning (DL) systems. One popular direction is DL testingaEUR""that is, given a property of test, defects of DL systems are found either by fuzzing or guided search with the help of certain testing metrics. However, recent studies have revealed that the neuron coverage metrics, which are commonly used by most existing DL testing approaches, are not necessarily correlated with model quality (e.g., robustness, the most studied model property), and are also not an effective measurement on the confidence of the model quality after testing. In this work, we address this gap by proposing a novel testing framework called QuoTe (i.e., Quality-oriented Testing). A key part of QuoTe is a quantitative measurement on (1) the value of each test case in enhancing the model property of interest (often via retraining) and (2) the convergence quality of the model property improvement. QuoTe utilizes the proposed metric to automatically select or generate valuable test cases for improving model quality. The proposed metric is also a lightweight yet strong indicator of how well the improvement converged. Extensive experiments on both image and tabular datasets with a variety of model architectures confirm the effectiveness and efficiency of QuoTe in improving DL model qualityaEUR""that is, robustness and fairness. As a generic quality-oriented testing framework, future adaptations can be made to other domains (e.g., text) as well as other model properties.",Achievement,Successful,The 'Paper X' abstract's key contribution is the proposal of a new testing framework focusing on improving model quality; like robustness and fairness. This aligns with the value item 'Successful' under 'Achievement' as the successful improvement of model quality is a measure of user's success in quality assurance tasks for Deep Learning systems.,"In the context of a ""Software User,"" the successful improvement of model quality, such as robustness and fairness, directly aligns with the value item ""Successful"" under the value of ""Achievement."" A software user aims to achieve successful outcomes in their quality assurance tasks for Deep Learning systems, and the contributions of 'Paper X' in proposing a new testing framework that effectively improves the model quality align with this value item. The user's success in enhancing the model's properties validates the achievement value as it demonstrates their ability to achieve their desired goals and objectives.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,1713,TOSEM,AI & Machine Learning,QuoTe: Quality-oriented Testing for Deep Learning Systems,"Recently, there has been significant growth of interest in applying software engineering techniques for the quality assurance of deep learning (DL) systems. One popular direction is DL testingaEUR""that is, given a property of test, defects of DL systems are found either by fuzzing or guided search with the help of certain testing metrics. However, recent studies have revealed that the neuron coverage metrics, which are commonly used by most existing DL testing approaches, are not necessarily correlated with model quality (e.g., robustness, the most studied model property), and are also not an effective measurement on the confidence of the model quality after testing. In this work, we address this gap by proposing a novel testing framework called QuoTe (i.e., Quality-oriented Testing). A key part of QuoTe is a quantitative measurement on (1) the value of each test case in enhancing the model property of interest (often via retraining) and (2) the convergence quality of the model property improvement. QuoTe utilizes the proposed metric to automatically select or generate valuable test cases for improving model quality. The proposed metric is also a lightweight yet strong indicator of how well the improvement converged. Extensive experiments on both image and tabular datasets with a variety of model architectures confirm the effectiveness and efficiency of QuoTe in improving DL model qualityaEUR""that is, robustness and fairness. As a generic quality-oriented testing framework, future adaptations can be made to other domains (e.g., text) as well as other model properties.",Security,National Security,The proposed 'Quality-oriented Testing' framework focuses on improving the robustness and fairness of Deep Learning models. This can be perceived as a contribution to 'National Security'; under the value 'Security'; in the context that improved robustness and fairness in DL systems may contribute to robust and fair digital services at a national level.,"The justification for aligning 'Paper X' with the value item National Security and its corresponding value Security is based on the idea that improved robustness and fairness in Deep Learning systems can contribute to more secure and reliable digital services at a national level. By enhancing the quality of DL models through the proposed QuoTe framework, the potential risks and vulnerabilities in these systems can be minimized, thereby safeguarding critical national infrastructure and sensitive data from potential threats and ensuring the security of software users.",Agreed-Justified,Disagree,More incline to Achievement: Successful as it is improves the fairness and robustness of the deep learning classifiers,Disagree,"The justification claims that the paper aligns with the value item of 'National Security', but the text neither explains how nor confirms this.
",
Exp B,Exp H,1717,TOSEM,AI & Machine Learning,ArchRepair: Block-Level Architecture-Oriented Repairing for Deep Neural Networks,"Over the past few years, deep neural networks (DNNs) have achieved tremendous success and have been continuously applied in many application domains. However, during the practical deployment in industrial tasks, DNNs are found to be erroneous-prone due to various reasons such as overfitting and lacking of robustness to real-world corruptions during practical usage. To address these challenges, many recent attempts have been made to repair DNNs for version updates under practical operational contexts by updating weights (i.e., network parameters) through retraining, fine-tuning, or direct weight fixing at a neural level. Nevertheless, existing solutions often neglect the effects of neural network architecture and weight relationships across neurons and layers. In this work, as the first attempt, we initiate to repair DNNs by jointly optimizing the architecture and weights at a higher (i.e., block level).We first perform empirical studies to investigate the limitation of whole network-level and layer-level repairing, which motivates us to explore a novel repairing direction for DNN repair at the block level. To this end, we need to further consider techniques to address two key technical challenges, i.e., block localization, where we should localize the targeted block that we need to fix; and how to perform joint architecture and weight repairing. Specifically, we first propose adversarial-aware spectrum analysis for vulnerable block localization that considers the neuronsaEUR(tm) status and weightsaEUR(tm) gradients in blocks during the forward and backward processes, which enables more accurate candidate block localization for repairing even under a few examples. Then, we further propose the architecture-oriented search-based repairing that relaxes the targeted block to a continuous repairing search space at higher deep feature levels. By jointly optimizing the architecture and weights in that space, we can identify a much better block architecture. We implement our proposed repairing techniques as a tool, named ArchRepair, and conduct extensive experiments to validate the proposed method. The results show that our method can not only repair but also enhance accuracy and robustness, outperforming the state-of-the-art DNN repair techniques.",Security,Healthy,The research in 'Paper X' directly impacts Software Users' value of 'Security' as it aims to 'repair' deficiencies and issues in neural networks (DNNs) which can include; but are not limited to; issues such as a lack of robustness and overfitting that can compromise the health (v6.1) of the software.,"The research in 'Paper X' directly aligns with the value item 'Healthy' and its corresponding value 'Security' from a ""Software User"" perspective because the paper specifically addresses the challenges and deficiencies in deep neural networks (DNNs), such as overfitting and lack of robustness, which can compromise the overall health and security of the software system. By proposing techniques to repair and enhance the accuracy and robustness of DNNs, the paper contributes to the software users' value of security, ensuring a healthier and more secure software experience.",Agreed-Justified,Disagree,reconcile with coder_2,Disagree,The justification doesn't align with the value item of the Healthy,
Exp C,Exp K,1717,TOSEM,AI & Machine Learning,ArchRepair: Block-Level Architecture-Oriented Repairing for Deep Neural Networks,"Over the past few years, deep neural networks (DNNs) have achieved tremendous success and have been continuously applied in many application domains. However, during the practical deployment in industrial tasks, DNNs are found to be erroneous-prone due to various reasons such as overfitting and lacking of robustness to real-world corruptions during practical usage. To address these challenges, many recent attempts have been made to repair DNNs for version updates under practical operational contexts by updating weights (i.e., network parameters) through retraining, fine-tuning, or direct weight fixing at a neural level. Nevertheless, existing solutions often neglect the effects of neural network architecture and weight relationships across neurons and layers. In this work, as the first attempt, we initiate to repair DNNs by jointly optimizing the architecture and weights at a higher (i.e., block level).We first perform empirical studies to investigate the limitation of whole network-level and layer-level repairing, which motivates us to explore a novel repairing direction for DNN repair at the block level. To this end, we need to further consider techniques to address two key technical challenges, i.e., block localization, where we should localize the targeted block that we need to fix; and how to perform joint architecture and weight repairing. Specifically, we first propose adversarial-aware spectrum analysis for vulnerable block localization that considers the neuronsaEUR(tm) status and weightsaEUR(tm) gradients in blocks during the forward and backward processes, which enables more accurate candidate block localization for repairing even under a few examples. Then, we further propose the architecture-oriented search-based repairing that relaxes the targeted block to a continuous repairing search space at higher deep feature levels. By jointly optimizing the architecture and weights in that space, we can identify a much better block architecture. We implement our proposed repairing techniques as a tool, named ArchRepair, and conduct extensive experiments to validate the proposed method. The results show that our method can not only repair but also enhance accuracy and robustness, outperforming the state-of-the-art DNN repair techniques.",Achievement,Intelligent,The development of 'ArchRepair' as a tool in 'Paper X' could assist Software Users in managing DNNs more intelligently (v4.1); as the tool helps in fixing vulnerable blocks and optimizing the architecture through a joint process; contributing to achieving the value 'Achievement'.,"In 'Paper X', the development of 'ArchRepair' as a tool aligns with the value item Intelligent and its corresponding value Achievement from a Software User perspective because the tool allows users to intelligently manage DNNs by fixing vulnerable blocks and optimizing the architecture through a joint process. By providing a solution that addresses the challenges of overfitting and lack of robustness, 'ArchRepair' enables Software Users to achieve their goals of improving the performance and accuracy of their DNN models, thus demonstrating an alignment with the value of Achievement.",Agreed-Justified,Agree,,Agree,,
Exp D,Exp K,1717,TOSEM,AI & Machine Learning,ArchRepair: Block-Level Architecture-Oriented Repairing for Deep Neural Networks,"Over the past few years, deep neural networks (DNNs) have achieved tremendous success and have been continuously applied in many application domains. However, during the practical deployment in industrial tasks, DNNs are found to be erroneous-prone due to various reasons such as overfitting and lacking of robustness to real-world corruptions during practical usage. To address these challenges, many recent attempts have been made to repair DNNs for version updates under practical operational contexts by updating weights (i.e., network parameters) through retraining, fine-tuning, or direct weight fixing at a neural level. Nevertheless, existing solutions often neglect the effects of neural network architecture and weight relationships across neurons and layers. In this work, as the first attempt, we initiate to repair DNNs by jointly optimizing the architecture and weights at a higher (i.e., block level).We first perform empirical studies to investigate the limitation of whole network-level and layer-level repairing, which motivates us to explore a novel repairing direction for DNN repair at the block level. To this end, we need to further consider techniques to address two key technical challenges, i.e., block localization, where we should localize the targeted block that we need to fix; and how to perform joint architecture and weight repairing. Specifically, we first propose adversarial-aware spectrum analysis for vulnerable block localization that considers the neuronsaEUR(tm) status and weightsaEUR(tm) gradients in blocks during the forward and backward processes, which enables more accurate candidate block localization for repairing even under a few examples. Then, we further propose the architecture-oriented search-based repairing that relaxes the targeted block to a continuous repairing search space at higher deep feature levels. By jointly optimizing the architecture and weights in that space, we can identify a much better block architecture. We implement our proposed repairing techniques as a tool, named ArchRepair, and conduct extensive experiments to validate the proposed method. The results show that our method can not only repair but also enhance accuracy and robustness, outperforming the state-of-the-art DNN repair techniques.",Stimulation,Excitement in Life,Upon successful repair of DNNs; users may experience an innovative; different and stimulating software experience; which contributes to providing excitement in life (v2.3) inherent in the 'Stimulation' value.,"The main contribution of 'Paper X' in repairing DNNs directly aligns with the value item Excitement in Life and its corresponding value Stimulation from a ""Software User"" perspective because successful repair of DNNs can lead to an enhanced and stimulating software experience, providing users with a sense of novelty and innovation, ultimately contributing to the excitement in their lives.",Agreed-Justified,Agree,,Agree,,
Exp E,Exp J,1718,TOSEM,AI & Machine Learning,Securing the Ethereum from Smart Ponzi Schemes: Identification Using Static Features,"Malware detection approaches have been extensively studied for traditional software systems. However, the development of blockchain technology has promoted the birth of a new type of software systemaEUR""decentralized applications. Composed of smart contracts, a type of application that implements the Ponzi scheme logic (called smart Ponzi schemes) has caused irreversible loss and hindered the development of blockchain technology. These smart contracts generally had a short life but involved a large amount of money. Whereas identification of these Ponzi schemes before causing financial loss has been significantly important, existing methods suffer from three main deficiencies, i.e., the insufficient dataset, the reliance on the transaction records, and the low accuracy. In this study, we first build a larger dataset. Then, a large number of features from multiple views, including bytecode, semantic, and developers, are extracted. These features are independent of the transaction records. Furthermore, we leveraged machine learning methods to build our identification model, i.e., Multi-view Cascade Ensemble model (MulCas). The experiment results show that MulCas can achieve higher performance and robustness in the scope of our dataset. Most importantly, the proposed method can identify smart Ponzi scheme at the creation time.",Security,Healthy,The main objective of the paper is to improve the security of software users; by detecting and preventing financial loss from the Ponzi scheme. This aligns directly with the value item Health under the value Security; which represents the software users‚Äö√Ñ√∂‚àö√ë‚àö¬• wish to have a safe and undisturbed usage of the software.,"In 'Paper X', the focus is on improving the security of software users by detecting and preventing financial loss from Ponzi schemes. This directly aligns with the value item Healthy under the value Security in Schwartz's Taxonomy. The software users, as individuals seeking a safe and undisturbed usage of the software, value the security of their financial resources, which is directly addressed by the main contributions of 'Paper X'.",Agreed-Justified,Agree,,Agree,,
Exp F,Exp J,1718,TOSEM,AI & Machine Learning,Securing the Ethereum from Smart Ponzi Schemes: Identification Using Static Features,"Malware detection approaches have been extensively studied for traditional software systems. However, the development of blockchain technology has promoted the birth of a new type of software systemaEUR""decentralized applications. Composed of smart contracts, a type of application that implements the Ponzi scheme logic (called smart Ponzi schemes) has caused irreversible loss and hindered the development of blockchain technology. These smart contracts generally had a short life but involved a large amount of money. Whereas identification of these Ponzi schemes before causing financial loss has been significantly important, existing methods suffer from three main deficiencies, i.e., the insufficient dataset, the reliance on the transaction records, and the low accuracy. In this study, we first build a larger dataset. Then, a large number of features from multiple views, including bytecode, semantic, and developers, are extracted. These features are independent of the transaction records. Furthermore, we leveraged machine learning methods to build our identification model, i.e., Multi-view Cascade Ensemble model (MulCas). The experiment results show that MulCas can achieve higher performance and robustness in the scope of our dataset. Most importantly, the proposed method can identify smart Ponzi scheme at the creation time.",Security,Social Order,The paper also contributes to maintaining the social order in the context of software use. Its approach reduces the opportunities for fraud and deception that disturb the order of the online community; aligning with the value item Social Order under Security.,"The paper's contribution to maintaining social order aligns with the value item of Social Order under Security in the context of software use. By detecting and identifying smart Ponzi schemes in decentralized applications, the paper reduces opportunities for fraud and deception, ultimately promoting a more secure and trustworthy online community. This directly aligns with the value of Social Order, as it helps to maintain a stable and harmonious software environment where users can feel safe and protected from financial harm.",Agreed-Justified,Agree,,Agree,,
Exp G,Exp K,1718,TOSEM,AI & Machine Learning,Securing the Ethereum from Smart Ponzi Schemes: Identification Using Static Features,"Malware detection approaches have been extensively studied for traditional software systems. However, the development of blockchain technology has promoted the birth of a new type of software systemaEUR""decentralized applications. Composed of smart contracts, a type of application that implements the Ponzi scheme logic (called smart Ponzi schemes) has caused irreversible loss and hindered the development of blockchain technology. These smart contracts generally had a short life but involved a large amount of money. Whereas identification of these Ponzi schemes before causing financial loss has been significantly important, existing methods suffer from three main deficiencies, i.e., the insufficient dataset, the reliance on the transaction records, and the low accuracy. In this study, we first build a larger dataset. Then, a large number of features from multiple views, including bytecode, semantic, and developers, are extracted. These features are independent of the transaction records. Furthermore, we leveraged machine learning methods to build our identification model, i.e., Multi-view Cascade Ensemble model (MulCas). The experiment results show that MulCas can achieve higher performance and robustness in the scope of our dataset. Most importantly, the proposed method can identify smart Ponzi scheme at the creation time.",Achievement,Capable,The paper's method potentially makes the software user capable of participating in the blockchain without the fear of being tricked; which aligns with the value item Capable under the value Achievement.,"The justification for aligning 'Paper X' with the value item Capable and its corresponding value Achievement from a ""Software User"" perspective is that the paper's approach aims to identify and detect smart Ponzi schemes in decentralized applications, which can potentially empower the software user to make informed decisions and participate in the blockchain ecosystem without the fear of being tricked. By enhancing the security and trustworthiness of decentralized applications, the paper contributes to the achievement of the user's capability in utilizing blockchain technology effectively and responsibly.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,1721,TOSEM,AI & Machine Learning,Fair Enough: Searching for Sufficient Measures of Fairness,"Testing machine learning software for ethical bias has become a pressing current concern. In response, recent research has proposed a plethora of new fairness metrics, for example, the dozens of fairness metrics in the IBM AIF360 toolkit. This raises the question: How can any fairness tool satisfy such a diverse range of goals? While we cannot completely simplify the task of fairness testing, we can certainly reduce the problem. This article shows that many of those fairness metrics effectively measure the same thing. Based on experiments using seven real-world datasets, we find that (a)&nbsp;26 classification metrics can be clustered into seven groups and (b)&nbsp;four dataset metrics can be clustered into three groups. Further, each reduced set may actually predict different things. Hence, it is no longer necessary (or even possible) to satisfy all fairness metrics. In summary, to simplify the fairness testing problem, we recommend the following steps: (1)&nbsp;determine what type of fairness is desirable (and we offer a handful of such types), then (2)&nbsp;lookup those types in our clusters, and then (3)&nbsp;just test for one item per cluster.For the purpose of reproducibility, our scripts and data are available at https://github.com/Repoanon ymous/Fairness_Metrics.",Universalism,Equality,"The paper discusses ethics in machine learning and proposes tools for fairness testing. This directly aligns with the value item ""Equality"" under the value ""Universalism"" as it aims to ensure ethical equity in machine learning software.","In the paper abstract, it is mentioned that the main contribution of 'Paper X' is proposing tools for fairness testing in machine learning software. Fairness testing aims to ensure ethical equity, which directly aligns with the value item ""Equality"" under the value ""Universalism"" in Schwartz's Taxonomy. By developing tools for fairness testing, the paper emphasizes the importance of equal treatment and ethical considerations in machine learning, which is in line with the value of Universalism that promotes justice, fairness, and equal rights for all individuals. Therefore, the alignment between 'Paper X' and the value item Equality and its corresponding value Universalism is evident from the perspective of a software user.",Agreed-Clarified,Agree,,Agree,,
Exp B,Exp H,1721,TOSEM,AI & Machine Learning,Fair Enough: Searching for Sufficient Measures of Fairness,"Testing machine learning software for ethical bias has become a pressing current concern. In response, recent research has proposed a plethora of new fairness metrics, for example, the dozens of fairness metrics in the IBM AIF360 toolkit. This raises the question: How can any fairness tool satisfy such a diverse range of goals? While we cannot completely simplify the task of fairness testing, we can certainly reduce the problem. This article shows that many of those fairness metrics effectively measure the same thing. Based on experiments using seven real-world datasets, we find that (a)&nbsp;26 classification metrics can be clustered into seven groups and (b)&nbsp;four dataset metrics can be clustered into three groups. Further, each reduced set may actually predict different things. Hence, it is no longer necessary (or even possible) to satisfy all fairness metrics. In summary, to simplify the fairness testing problem, we recommend the following steps: (1)&nbsp;determine what type of fairness is desirable (and we offer a handful of such types), then (2)&nbsp;lookup those types in our clusters, and then (3)&nbsp;just test for one item per cluster.For the purpose of reproducibility, our scripts and data are available at https://github.com/Repoanon ymous/Fairness_Metrics.",Universalism,Social Justice,"The paper emphasizes fairness in machine learning; contributing methods to measure fairness metrics. Thus; it aligns with the value item ""Social Justice"" under the value ""Universalism"" as it pertains to establishing justice in the use of machine learning software.","The main reason 'Paper X' aligns with the value item Social Justice and its corresponding value Universalism is because it addresses the concern of ethical bias in machine learning software and proposes methods to measure fairness metrics. By emphasizing fairness and striving to establish justice in the use of machine learning software, the paper directly aligns with the value of Social Justice within the broader value of Universalism, which includes principles such as equality, social justice, and protecting the rights of individuals.",Agreed-Justified,Agree,,Agree,,
Exp C,Exp K,1721,TOSEM,AI & Machine Learning,Fair Enough: Searching for Sufficient Measures of Fairness,"Testing machine learning software for ethical bias has become a pressing current concern. In response, recent research has proposed a plethora of new fairness metrics, for example, the dozens of fairness metrics in the IBM AIF360 toolkit. This raises the question: How can any fairness tool satisfy such a diverse range of goals? While we cannot completely simplify the task of fairness testing, we can certainly reduce the problem. This article shows that many of those fairness metrics effectively measure the same thing. Based on experiments using seven real-world datasets, we find that (a)&nbsp;26 classification metrics can be clustered into seven groups and (b)&nbsp;four dataset metrics can be clustered into three groups. Further, each reduced set may actually predict different things. Hence, it is no longer necessary (or even possible) to satisfy all fairness metrics. In summary, to simplify the fairness testing problem, we recommend the following steps: (1)&nbsp;determine what type of fairness is desirable (and we offer a handful of such types), then (2)&nbsp;lookup those types in our clusters, and then (3)&nbsp;just test for one item per cluster.For the purpose of reproducibility, our scripts and data are available at https://github.com/Repoanon ymous/Fairness_Metrics.",Achievement,Intelligent,"Though the paper's contribution is not directly benefiting users' achievement; the fairness testing proposed can inferentially make users feel intelligent. Users applying the fairness tool will effectively measure several metrics; giving them the feeling of competence. This aligns with the value item ""Intelligent"" under the value ""Achievement"".","In the context of software users, the alignment of 'Paper X' with the value item Intelligent and its corresponding value Achievement can be justified based on the fact that the fairness testing proposed in the paper allows users to effectively measure several metrics. By successfully utilizing the fairness tool and assessing the various metrics, users can feel a sense of competence and accomplishment, which indirectly contributes to their perception of being intelligent. Therefore, the alignment with the value item Intelligent and the value Achievement is based on the psychological satisfaction and confidence that users may experience while utilizing the fairness tool.",Agreed-Justified,Agree,,Agree,,
Exp D,Exp K,1724,TOSEM,AI & Machine Learning,TestSGD: Interpretable Testing of Neural Networks against Subtle Group Discrimination,"Discrimination has been shown in many machine learning applications, which calls for sufficient fairness testing before their deployment in ethic-relevant domains. One widely concerning type of discrimination, testing against group discrimination, mostly hidden, is much less studied, compared with identifying individual discrimination. In this work, we propose TestSGD, an interpretable testing approach that systematically identifies and measures hidden (which we call aEURoesubtleaEUR) group discrimination of a neural network characterized by conditions over combinations of the sensitive attributes. Specifically, given a neural network, TestSGD first automatically generates an interpretable rule set that categorizes the input space into two groups. Alongside, TestSGD also provides an estimated group discrimination score based on sampling the input space to measure the degree of the identified subtle group discrimination, which is guaranteed to be accurate up to an error bound. We evaluate TestSGD on multiple neural network models trained on popular datasets including both structured data and text data. The experiment results show that TestSGD is effective and efficient in identifying and measuring such subtle group discrimination that has never been revealed before. Furthermore, we show that the testing results of TestSGD can be used to mitigate such discrimination through retraining with negligible accuracy drop.",Security,Social Order,"The paper contributes an approach that systematically identifies and measures hidden group discrimination; which could help maintain social order among software users by ensuring fairness in machine learning applications. This aligns directly with the value item ""Social Order"" under the value ""Security"".","In the paper abstract, 'Paper X' proposes an approach called TestSGD that systematically identifies and measures hidden group discrimination in machine learning applications. By ensuring fairness in these applications, TestSGD can contribute to maintaining social order among software users. This directly aligns with the value item ""Social Order"" under the value ""Security"" as it promotes a sense of fairness and equality, creating a secure and orderly environment for software users.",Agreed-Justified,Agree,,Agree,,
Exp E,Exp J,1724,TOSEM,AI & Machine Learning,TestSGD: Interpretable Testing of Neural Networks against Subtle Group Discrimination,"Discrimination has been shown in many machine learning applications, which calls for sufficient fairness testing before their deployment in ethic-relevant domains. One widely concerning type of discrimination, testing against group discrimination, mostly hidden, is much less studied, compared with identifying individual discrimination. In this work, we propose TestSGD, an interpretable testing approach that systematically identifies and measures hidden (which we call aEURoesubtleaEUR) group discrimination of a neural network characterized by conditions over combinations of the sensitive attributes. Specifically, given a neural network, TestSGD first automatically generates an interpretable rule set that categorizes the input space into two groups. Alongside, TestSGD also provides an estimated group discrimination score based on sampling the input space to measure the degree of the identified subtle group discrimination, which is guaranteed to be accurate up to an error bound. We evaluate TestSGD on multiple neural network models trained on popular datasets including both structured data and text data. The experiment results show that TestSGD is effective and efficient in identifying and measuring such subtle group discrimination that has never been revealed before. Furthermore, we show that the testing results of TestSGD can be used to mitigate such discrimination through retraining with negligible accuracy drop.",Benevolence,Responsibility,"In proposing an approach that mitigates subtle group discrimination; the paper enhances users' responsibility towards promoting fair and unbiased practices in digital systems. This aligns with the value item ""Responsibility"" under the value ""Benevolence"".","The main contribution of 'Paper X' is the development of the TestSGD approach, which aims to identify and measure hidden group discrimination in neural networks. By addressing this issue, the paper promotes users' responsibility in ensuring fairness and non-discrimination in software systems. This alignment with the value item Responsibility and its corresponding value Benevolence reflects the paper's focus on promoting ethical practices and social responsibility within the software context, aligning with users' responsibility towards mitigating discrimination and fostering fairness.",Agreed-Clarified,Agree,,Agree,,
Exp F,Exp J,1724,TOSEM,AI & Machine Learning,TestSGD: Interpretable Testing of Neural Networks against Subtle Group Discrimination,"Discrimination has been shown in many machine learning applications, which calls for sufficient fairness testing before their deployment in ethic-relevant domains. One widely concerning type of discrimination, testing against group discrimination, mostly hidden, is much less studied, compared with identifying individual discrimination. In this work, we propose TestSGD, an interpretable testing approach that systematically identifies and measures hidden (which we call aEURoesubtleaEUR) group discrimination of a neural network characterized by conditions over combinations of the sensitive attributes. Specifically, given a neural network, TestSGD first automatically generates an interpretable rule set that categorizes the input space into two groups. Alongside, TestSGD also provides an estimated group discrimination score based on sampling the input space to measure the degree of the identified subtle group discrimination, which is guaranteed to be accurate up to an error bound. We evaluate TestSGD on multiple neural network models trained on popular datasets including both structured data and text data. The experiment results show that TestSGD is effective and efficient in identifying and measuring such subtle group discrimination that has never been revealed before. Furthermore, we show that the testing results of TestSGD can be used to mitigate such discrimination through retraining with negligible accuracy drop.",Universalism,Social Justice,"Through identifying and measuring subtle group discrimination in machine learning applications; Paper X directly contributes to ""Social Justice;"" which falls under the value ""Universalism"". The paper advocates for sufficient fairness testing before deployment; which corresponds to justice in a broad; universal sense.","In the context of a ""Software User"" perspective, the main contribution of 'Paper X' aligning with the value item Social Justice and its corresponding value Universalism can be justified by the fact that the paper focuses on identifying and measuring subtle group discrimination in machine learning applications. By advocating for sufficient fairness testing before deployment, the paper aims to address issues of justice in a broad and universal sense, ensuring that the software and its underlying algorithms do not discriminate against certain groups based on sensitive attributes. This aligns with the value of Universalism, which emphasizes equality, social justice, and the protection of individual and collective rights.",Agreed-Clarified,Agree,,Agree,,
Exp G,Exp K,1733,TOSEM,Software Project Management,An Empirical Study on GitHub Pull RequestsaEUR(tm) Reactions,"The pull request mechanism is commonly used to propose source code modifications and get feedback from the community before merging them into a software repository. On GitHub, practitioners can provide feedback on a pull request by either commenting on the pull request or simply reacting to it using a set of pre-defined GitHub reactions, i.e., aEURoeThumbs-upaEUR, aEURoeLaughaEUR, aEURoeHoorayaEUR, aEURoeHeartaEUR, aEURoeRocketaEUR, aEURoeThumbs-downaEUR, aEURoeConfusedaEUR, and aEURoeEyesaEUR. While a large number of prior studies investigated how to improve different software engineering activities (e.g., code review and integration) by investigating the feedback on pull requests, they focused only on pull requestsaEUR(tm) comments as a source of feedback. However, the GitHub reactions, according to our preliminary study, contain feedback that is not manifested within the comments of pull requests. In fact, our preliminary analysis of six popular projects shows that a median of 100\% of the practitioners who reacted to a pull request did not leave any comment suggesting that reactions can be a unique source of feedback to further improve the code review and integration process.To help future studies better leverage reactions as a feedback mechanism, we conduct an empirical study to understand the usage of GitHub reactions and understand their promises and limitations. We investigate in this article how reactions are used, when and who use them on what types of pull requests, and for what purposes. Our study considers a quantitative analysis on a set of 380 k reactions on 63 k pull requests of six popular open-source projects on GitHub and three qualitative analyses on a total number of 989 reactions from the same six projects. We find that the most common used GitHub reactions are the positive ones (i.e., aEURoeThumbs-upaEUR, aEURoeHoorayaEUR, aEURoeHeartaEUR, aEURoeRocketaEUR, and aEURoeLaughaEUR). We observe that reactors use positive reactions to express positive attitude (e.g., approval, appreciation, and excitement) on the proposed changes in pull requests. A median of just 1.95\% of the used reactions are negative ones, which are used by reactors who disagree with the proposed changes for six reasons, such as feature modifications that might have more downsides than upsides or the use of the wrong approach to address certain problems. Most (a median of 78.40\%) reactions on a pull request come before the closing of the corresponding pull requests. Interestingly, we observe that non-contributors (i.e., outsiders who potentially are the aEURoeend-usersaEUR of the software) are also active on reacting to pull requests. On top of that, we observe that core contributors, peripheral contributors, casual contributors and outsiders have different behaviors when reacting to pull requests. For instance, most core contributors react in the early stages of a pull request, while peripheral contributors, casual contributors and outsiders react around the closing time or, in some cases, after a pull request is merged. Contributors tend to react to the pull requestaEUR(tm)s source code, while outsiders are more concerned about the impact of the pull request on the end-user experience. Our findings shed light on common patterns of GitHub reactions usage on pull requests and provide taxonomies about the intention of reactors, which can inspire future studies better leverage pull requestsaEUR(tm) reactions.",Tradition,Moderation,"The paper abstract suggests a moderate approach to integrating pull requests from the community for code modification; as it does not completely rely on comments but also considers reactions as feedback. This aligns with the value item ""Moderation"" under the value ""Tradition"".","The justification for aligning 'Paper X' with the value item Moderation and its corresponding value Tradition is based on the approach described in the paper abstract. The paper acknowledges that reactions to pull requests, in addition to comments, provide valuable feedback for code review and integration. By considering reactions as a unique source of feedback, the paper suggests a moderate approach that balances the traditional reliance on comments with the inclusiveness of reactions. This aligns with the value of Tradition, as it promotes a balanced and cautious approach to incorporating feedback from the community in the software development process.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,1733,TOSEM,Software Project Management,An Empirical Study on GitHub Pull RequestsaEUR(tm) Reactions,"The pull request mechanism is commonly used to propose source code modifications and get feedback from the community before merging them into a software repository. On GitHub, practitioners can provide feedback on a pull request by either commenting on the pull request or simply reacting to it using a set of pre-defined GitHub reactions, i.e., aEURoeThumbs-upaEUR, aEURoeLaughaEUR, aEURoeHoorayaEUR, aEURoeHeartaEUR, aEURoeRocketaEUR, aEURoeThumbs-downaEUR, aEURoeConfusedaEUR, and aEURoeEyesaEUR. While a large number of prior studies investigated how to improve different software engineering activities (e.g., code review and integration) by investigating the feedback on pull requests, they focused only on pull requestsaEUR(tm) comments as a source of feedback. However, the GitHub reactions, according to our preliminary study, contain feedback that is not manifested within the comments of pull requests. In fact, our preliminary analysis of six popular projects shows that a median of 100\% of the practitioners who reacted to a pull request did not leave any comment suggesting that reactions can be a unique source of feedback to further improve the code review and integration process.To help future studies better leverage reactions as a feedback mechanism, we conduct an empirical study to understand the usage of GitHub reactions and understand their promises and limitations. We investigate in this article how reactions are used, when and who use them on what types of pull requests, and for what purposes. Our study considers a quantitative analysis on a set of 380 k reactions on 63 k pull requests of six popular open-source projects on GitHub and three qualitative analyses on a total number of 989 reactions from the same six projects. We find that the most common used GitHub reactions are the positive ones (i.e., aEURoeThumbs-upaEUR, aEURoeHoorayaEUR, aEURoeHeartaEUR, aEURoeRocketaEUR, and aEURoeLaughaEUR). We observe that reactors use positive reactions to express positive attitude (e.g., approval, appreciation, and excitement) on the proposed changes in pull requests. A median of just 1.95\% of the used reactions are negative ones, which are used by reactors who disagree with the proposed changes for six reasons, such as feature modifications that might have more downsides than upsides or the use of the wrong approach to address certain problems. Most (a median of 78.40\%) reactions on a pull request come before the closing of the corresponding pull requests. Interestingly, we observe that non-contributors (i.e., outsiders who potentially are the aEURoeend-usersaEUR of the software) are also active on reacting to pull requests. On top of that, we observe that core contributors, peripheral contributors, casual contributors and outsiders have different behaviors when reacting to pull requests. For instance, most core contributors react in the early stages of a pull request, while peripheral contributors, casual contributors and outsiders react around the closing time or, in some cases, after a pull request is merged. Contributors tend to react to the pull requestaEUR(tm)s source code, while outsiders are more concerned about the impact of the pull request on the end-user experience. Our findings shed light on common patterns of GitHub reactions usage on pull requests and provide taxonomies about the intention of reactors, which can inspire future studies better leverage pull requestsaEUR(tm) reactions.",Benevolence,Helpful,"The paper investigates the usage and impact of the GitHub reactions on pull requests; which can be seen as an attempt to improve the process and therefore help the community. This aligns with the value item ""Helpful"" under the value ""Benevolence"".","The paper's main objective is to investigate the usage of GitHub reactions on pull requests and understand their promises and limitations. By studying reactions and their impact on the code review and integration process, the paper aims to provide insights and improve the software development community. This aligns with the value item ""Helpful"" under the value ""Benevolence"" as the research contributes to the betterment and support of others in the software development context.",Agreed-Justified,Agree,,Agree,,
Exp B,Exp H,1733,TOSEM,Software Project Management,An Empirical Study on GitHub Pull RequestsaEUR(tm) Reactions,"The pull request mechanism is commonly used to propose source code modifications and get feedback from the community before merging them into a software repository. On GitHub, practitioners can provide feedback on a pull request by either commenting on the pull request or simply reacting to it using a set of pre-defined GitHub reactions, i.e., aEURoeThumbs-upaEUR, aEURoeLaughaEUR, aEURoeHoorayaEUR, aEURoeHeartaEUR, aEURoeRocketaEUR, aEURoeThumbs-downaEUR, aEURoeConfusedaEUR, and aEURoeEyesaEUR. While a large number of prior studies investigated how to improve different software engineering activities (e.g., code review and integration) by investigating the feedback on pull requests, they focused only on pull requestsaEUR(tm) comments as a source of feedback. However, the GitHub reactions, according to our preliminary study, contain feedback that is not manifested within the comments of pull requests. In fact, our preliminary analysis of six popular projects shows that a median of 100\% of the practitioners who reacted to a pull request did not leave any comment suggesting that reactions can be a unique source of feedback to further improve the code review and integration process.To help future studies better leverage reactions as a feedback mechanism, we conduct an empirical study to understand the usage of GitHub reactions and understand their promises and limitations. We investigate in this article how reactions are used, when and who use them on what types of pull requests, and for what purposes. Our study considers a quantitative analysis on a set of 380 k reactions on 63 k pull requests of six popular open-source projects on GitHub and three qualitative analyses on a total number of 989 reactions from the same six projects. We find that the most common used GitHub reactions are the positive ones (i.e., aEURoeThumbs-upaEUR, aEURoeHoorayaEUR, aEURoeHeartaEUR, aEURoeRocketaEUR, and aEURoeLaughaEUR). We observe that reactors use positive reactions to express positive attitude (e.g., approval, appreciation, and excitement) on the proposed changes in pull requests. A median of just 1.95\% of the used reactions are negative ones, which are used by reactors who disagree with the proposed changes for six reasons, such as feature modifications that might have more downsides than upsides or the use of the wrong approach to address certain problems. Most (a median of 78.40\%) reactions on a pull request come before the closing of the corresponding pull requests. Interestingly, we observe that non-contributors (i.e., outsiders who potentially are the aEURoeend-usersaEUR of the software) are also active on reacting to pull requests. On top of that, we observe that core contributors, peripheral contributors, casual contributors and outsiders have different behaviors when reacting to pull requests. For instance, most core contributors react in the early stages of a pull request, while peripheral contributors, casual contributors and outsiders react around the closing time or, in some cases, after a pull request is merged. Contributors tend to react to the pull requestaEUR(tm)s source code, while outsiders are more concerned about the impact of the pull request on the end-user experience. Our findings shed light on common patterns of GitHub reactions usage on pull requests and provide taxonomies about the intention of reactors, which can inspire future studies better leverage pull requestsaEUR(tm) reactions.",Security,Social Order,"The GitHub reactions to the pull requests can foster social order among the community by acting as an indicator of acceptance or disapproval; and thus influence decisions on code merges. This aligns with the value item ""Social Order"" under the value ""Security"".","By analyzing the GitHub reactions to pull requests, 'Paper X' contributes to social order by providing a means for the software user community to express their acceptance or disapproval of proposed code changes. This feedback mechanism helps ensure that only approved and secure modifications are merged into the software repository, aligning with the value item of ""Social Order"" under the value of ""Security"" from a software user's perspective.",Agreed-Clarified,Disagree,reconcile with coder_2,Disagree,The justification doesn't align with the value item of the Social Order,
Exp D,Exp K,1741,TOSEM,AI & Machine Learning,PatchCensor: Patch Robustness Certification for Transformers via Exhaustive Testing,"In the past few years, Transformer has been widely adopted in many domains and applications because of its impressive performance. Vision Transformer (ViT), a successful and well-known variant, attracts considerable attention from both industry and academia thanks to its record-breaking performance in various vision tasks. However, ViT is also highly nonlinear like other classical neural networks and could be easily fooled by both natural and adversarial perturbations. This limitation could pose a threat to the deployment of ViT in the real industrial environment, especially in safety-critical scenarios. How to improve the robustness of ViT is thus an urgent issue that needs to be addressed. Among all kinds of robustness, patch robustness is defined as giving a reliable output when a random patch in the input domain is perturbed. The perturbation could be natural corruption, such as part of the camera lens being blurred. It could also be a distribution shift, such as an object that does not exist in the training data suddenly appearing in the camera. And in the worst case, there could be a malicious adversarial patch attack that aims to fool the prediction of a machine learning model by arbitrarily modifying pixels within a restricted region of an input image. This kind of attack is also called physical attack, as it is believed to be more real than digital attack. Although there has been some work on patch robustness improvement of Convolutional Neural Network, related studies on its counterpart ViT are still at an early stage as ViT is usually much more complex with far more parameters. It is harder to assess and improve its robustness, not to mention to provide a provable guarantee. In this work, we propose PatchCensor, aiming to certify the patch robustness of ViT by applying exhaustive testing. We try to provide a provable guarantee by considering the worst patch attack scenarios. Unlike empirical defenses against adversarial patches that may be adaptively breached, certified robust approaches can provide a certified accuracy against arbitrary attacks under certain conditions. However, existing robustness certifications are mostly based on robust training, which often requires substantial training efforts and the sacrifice of model performance on normal samples. To bridge the gap, PatchCensor seeks to improve the robustness of the whole system by detecting abnormal inputs instead of training a robust model and asking it to give reliable results for every input, which may inevitably compromise accuracy. Specifically, each input is tested by voting over multiple inferences with different mutated attention masks, where at least one inference is guaranteed to exclude the abnormal patch. This can be seen as complete-coverage testing, which could provide a statistical guarantee on inference at the test time. Our comprehensive evaluation demonstrates that PatchCensor is able to achieve high certified accuracy (e.g.,&nbsp;67.1\% on ImageNet for 2\%-pixel adversarial patches), significantly outperforming state-of-the-art techniques while achieving similar clean accuracy (81.8\% on ImageNet). The clean accuracy is the same as vanilla ViT models. Meanwhile, our technique also supports flexible configurations to handle different adversarial patch sizes by simply changing the masking strategy.",Security,Social Order,"By improving ViTs robustness; the paper contributes to maintaining a secure and safe environment (digital and potentially physical if used in critical systems). This corresponds to the value item ""Social Order"" indicating a secure and safe environment under the value ""Security.""","In the context of a ""Software User,"" the main contribution of 'Paper X' lies in improving the robustness of ViT, specifically in terms of patch robustness. By addressing the vulnerability of ViT to adversarial attacks and perturbations, the paper aims to ensure the reliability and security of the system. This aligns with the value item ""Social Order"" as it pertains to maintaining a secure and safe environment, both in the digital realm and potentially in critical systems where ViT may be deployed. In essence, the paper's focus on enhancing the security and dependability of ViT contributes to upholding the value of ""Security"" within a software context.",Agreed-Justified,Agree,,Agree,,
Exp E,Exp J,1741,TOSEM,AI & Machine Learning,PatchCensor: Patch Robustness Certification for Transformers via Exhaustive Testing,"In the past few years, Transformer has been widely adopted in many domains and applications because of its impressive performance. Vision Transformer (ViT), a successful and well-known variant, attracts considerable attention from both industry and academia thanks to its record-breaking performance in various vision tasks. However, ViT is also highly nonlinear like other classical neural networks and could be easily fooled by both natural and adversarial perturbations. This limitation could pose a threat to the deployment of ViT in the real industrial environment, especially in safety-critical scenarios. How to improve the robustness of ViT is thus an urgent issue that needs to be addressed. Among all kinds of robustness, patch robustness is defined as giving a reliable output when a random patch in the input domain is perturbed. The perturbation could be natural corruption, such as part of the camera lens being blurred. It could also be a distribution shift, such as an object that does not exist in the training data suddenly appearing in the camera. And in the worst case, there could be a malicious adversarial patch attack that aims to fool the prediction of a machine learning model by arbitrarily modifying pixels within a restricted region of an input image. This kind of attack is also called physical attack, as it is believed to be more real than digital attack. Although there has been some work on patch robustness improvement of Convolutional Neural Network, related studies on its counterpart ViT are still at an early stage as ViT is usually much more complex with far more parameters. It is harder to assess and improve its robustness, not to mention to provide a provable guarantee. In this work, we propose PatchCensor, aiming to certify the patch robustness of ViT by applying exhaustive testing. We try to provide a provable guarantee by considering the worst patch attack scenarios. Unlike empirical defenses against adversarial patches that may be adaptively breached, certified robust approaches can provide a certified accuracy against arbitrary attacks under certain conditions. However, existing robustness certifications are mostly based on robust training, which often requires substantial training efforts and the sacrifice of model performance on normal samples. To bridge the gap, PatchCensor seeks to improve the robustness of the whole system by detecting abnormal inputs instead of training a robust model and asking it to give reliable results for every input, which may inevitably compromise accuracy. Specifically, each input is tested by voting over multiple inferences with different mutated attention masks, where at least one inference is guaranteed to exclude the abnormal patch. This can be seen as complete-coverage testing, which could provide a statistical guarantee on inference at the test time. Our comprehensive evaluation demonstrates that PatchCensor is able to achieve high certified accuracy (e.g.,&nbsp;67.1\% on ImageNet for 2\%-pixel adversarial patches), significantly outperforming state-of-the-art techniques while achieving similar clean accuracy (81.8\% on ImageNet). The clean accuracy is the same as vanilla ViT models. Meanwhile, our technique also supports flexible configurations to handle different adversarial patch sizes by simply changing the masking strategy.",Security,National Security,"The paper proposes a solution; PatchCensor; to certify the patch robustness of ViT; especially to fight against malicious adversarial patch attacks; contributing to a stronger assurance of digital security. This can be aligned with ""National Security"" in a digital context; within the ""Security"" value.","In the paper, PatchCensor is proposed as a solution to improve the patch robustness of Vision Transformer (ViT) and provide a provable guarantee against various types of attacks, including malicious adversarial patch attacks. By detecting abnormal inputs and excluding the malicious patch, PatchCensor enhances the security of ViT in the digital context. This aligns with the value item of National Security as it contributes to safeguarding digital systems against potential threats and vulnerabilities, providing a stronger assurance of security to software users.",Agreed-Clarified,Disagree,I see no link of digital secuirty with national secuirty,Agree,,
Exp F,Exp J,1742,TOSEM,AI & Machine Learning,Tiny Always-on and Fragile: Bias Propagation through Design Choices in On-device Machine Learning Workflows,"Billions of distributed, heterogeneous, and resource constrained IoT devices deploy on-device machine learning (ML) for private, fast, and offline inference on personal data. On-device ML is highly context dependent and sensitive to user, usage, hardware, and environment attributes. This sensitivity and the propensity toward bias in ML makes it important to study bias in on-device settings. Our study is one of the first investigations of bias in this emerging domain and lays important foundations for building fairer on-device ML. We apply a software engineering lens, investigating the propagation of bias through design choices in on-device ML workflows. We first identify reliability bias as a source of unfairness and propose a measure to quantify it. We then conduct empirical experiments for a keyword spotting task to show how complex and interacting technical design choices amplify and propagate reliability bias. Our results validate that design choices made during model training, like the sample rate and input feature type, and choices made to optimize models, like light-weight architectures, the pruning learning rate, and pruning sparsity, can result in disparate predictive performance across male and female groups. Based on our findings, we suggest low effort strategies for engineers to mitigate bias in on-device ML.",Achievement,Intelligent,"The paper's main contribution is an investigation into the propagation of bias in on-device machine learning workflows; a complex and innovative field; thereby aligning with the value item ""Intelligent"" and its corresponding value ""Achievement"".","In the paper's abstract, it is stated that the study investigates bias in on-device machine learning workflows, which are described as complex and innovative. By addressing bias in this emerging field, the paper contributes to advancing the field's knowledge and understanding, which aligns with the value item ""Intelligent"" and its corresponding value ""Achievement"".",Agreed-Justified,Disagree,,Disagree,It does not talk about intelligent.,
Exp G,Exp K,1742,TOSEM,AI & Machine Learning,Tiny Always-on and Fragile: Bias Propagation through Design Choices in On-device Machine Learning Workflows,"Billions of distributed, heterogeneous, and resource constrained IoT devices deploy on-device machine learning (ML) for private, fast, and offline inference on personal data. On-device ML is highly context dependent and sensitive to user, usage, hardware, and environment attributes. This sensitivity and the propensity toward bias in ML makes it important to study bias in on-device settings. Our study is one of the first investigations of bias in this emerging domain and lays important foundations for building fairer on-device ML. We apply a software engineering lens, investigating the propagation of bias through design choices in on-device ML workflows. We first identify reliability bias as a source of unfairness and propose a measure to quantify it. We then conduct empirical experiments for a keyword spotting task to show how complex and interacting technical design choices amplify and propagate reliability bias. Our results validate that design choices made during model training, like the sample rate and input feature type, and choices made to optimize models, like light-weight architectures, the pruning learning rate, and pruning sparsity, can result in disparate predictive performance across male and female groups. Based on our findings, we suggest low effort strategies for engineers to mitigate bias in on-device ML.",Universalism,Social Justice,"By identifying reliability bias in on-device workflows; the paper contributes to fairer on-device machine learning; which aligns with the value item ""Social Justice"" and its corresponding value ""Universalism"".","The alignment of 'Paper X' with the value item Social Justice and its corresponding value Universalism is justified by the paper's contribution in identifying reliability bias in on-device workflows, which directly leads to fairer on-device machine learning. This focus on fairness and equality in the context of software usage aligns with the principles of Social Justice, which emphasize the importance of inclusivity, equal opportunity, and addressing biases to create a more just and equitable society. By addressing bias in on-device ML, 'Paper X' is striving towards these values by promoting fairness and equal treatment for all software users.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,1742,TOSEM,AI & Machine Learning,Tiny Always-on and Fragile: Bias Propagation through Design Choices in On-device Machine Learning Workflows,"Billions of distributed, heterogeneous, and resource constrained IoT devices deploy on-device machine learning (ML) for private, fast, and offline inference on personal data. On-device ML is highly context dependent and sensitive to user, usage, hardware, and environment attributes. This sensitivity and the propensity toward bias in ML makes it important to study bias in on-device settings. Our study is one of the first investigations of bias in this emerging domain and lays important foundations for building fairer on-device ML. We apply a software engineering lens, investigating the propagation of bias through design choices in on-device ML workflows. We first identify reliability bias as a source of unfairness and propose a measure to quantify it. We then conduct empirical experiments for a keyword spotting task to show how complex and interacting technical design choices amplify and propagate reliability bias. Our results validate that design choices made during model training, like the sample rate and input feature type, and choices made to optimize models, like light-weight architectures, the pruning learning rate, and pruning sparsity, can result in disparate predictive performance across male and female groups. Based on our findings, we suggest low effort strategies for engineers to mitigate bias in on-device ML.",Self Direction,Freedom,"With on-device machine learning for private; fast and offline inference on personal data; the paper aligns with the value item ""Freedom"" and its corresponding value ""Self Direction""; as users have the freedom to run machine learning algorithms locally; privately and quickly.","The main contribution of 'Paper X' aligns with the value item Freedom and its corresponding value Self Direction from a ""Software User"" perspective because it enables users to have the freedom and autonomy to run machine learning algorithms locally on their own devices. This allows for private, fast, and offline inference on personal data, giving users the power to independently make decisions and control their own data without relying on external servers or platforms. By empowering users with the ability to leverage on-device machine learning, the paper aligns with the value of Self Direction, as it enables users to choose how they utilize their personal data and exercise control over their own software experiences.",Agreed-Justified,Agree,,Agree,,
Exp B,Exp H,1980,TSE,Security & Privacy,Incidents are Meant for Learning Not Repeating: Sharing Knowledge About Security Incidents in Cyber-Physical Systems,"Cyber-physical systems (CPSs) are part of many critical infrastructures such as industrial automation and transportation systems. Thus, security incidents targeting CPSs can have disruptive consequences to assets and people. As incidents tend to re-occur, sharing knowledge about these incidents can help organizations be more prepared to prevent, mitigate or investigate future incidents. This paper proposes a novel approach to enable representation and sharing of knowledge about CPS incidents across different organizations. To support sharing, we represent incident knowledge (incident patterns) capturing incident characteristics that can manifest again, such as incident activities or vulnerabilities exploited by offenders. Incident patterns are a more abstract representation of specific incident instances and, thus, are general enough to be applicable to various systems - different from the one in which the incident originally occurred. They can also avoid disclosing potentially sensitive information about an organization's assets and resources. We provide an automated technique to extract an incident pattern from a specific incident instance. To understand how an incident pattern can manifest again in other cyber-physical systems, we also provide an automated technique to instantiate incident patterns to specific systems. We demonstrate the feasibility of our approach in the application domain of smart buildings. We evaluate correctness, scalability, and performance using two substantive scenarios inspired by real-world systems and incidents.",Security,Healthy,The paper's main contribution is towards ensuring the security of Cyber-Physical Systems (CPS); which can protect systems' health that is a direct alignment with Healthy.,"My justification for aligning 'Paper X' with the value item Healthy and its corresponding value Security is based on the fact that the paper's main contribution is focused on addressing security incidents in Cyber-Physical Systems (CPSs). By preventing, mitigating, and investigating these incidents, the paper aims to ensure the health and well-being of these systems. A secure system is a healthy system, as it is protected from potential disruptions and threats. Therefore, the alignment with the value item Healthy and its corresponding value Security is evident.",Agreed-Justified,Agree,,Agree,,
Exp D,Exp K,1980,TSE,Security & Privacy,Incidents are Meant for Learning Not Repeating: Sharing Knowledge About Security Incidents in Cyber-Physical Systems,"Cyber-physical systems (CPSs) are part of many critical infrastructures such as industrial automation and transportation systems. Thus, security incidents targeting CPSs can have disruptive consequences to assets and people. As incidents tend to re-occur, sharing knowledge about these incidents can help organizations be more prepared to prevent, mitigate or investigate future incidents. This paper proposes a novel approach to enable representation and sharing of knowledge about CPS incidents across different organizations. To support sharing, we represent incident knowledge (incident patterns) capturing incident characteristics that can manifest again, such as incident activities or vulnerabilities exploited by offenders. Incident patterns are a more abstract representation of specific incident instances and, thus, are general enough to be applicable to various systems - different from the one in which the incident originally occurred. They can also avoid disclosing potentially sensitive information about an organization's assets and resources. We provide an automated technique to extract an incident pattern from a specific incident instance. To understand how an incident pattern can manifest again in other cyber-physical systems, we also provide an automated technique to instantiate incident patterns to specific systems. We demonstrate the feasibility of our approach in the application domain of smart buildings. We evaluate correctness, scalability, and performance using two substantive scenarios inspired by real-world systems and incidents.",Security,National Security,The proposed approach is directed towards preventing security incidents in CPSs such as industrial automation and transportation systems; which aligns with the value item National Security.,"My justification for aligning 'Paper X' with the value item National Security and its corresponding value Security is based on the fact that the paper specifically focuses on preventing security incidents in cyber-physical systems (CPSs) that are part of critical infrastructures. These incidents can have disruptive consequences to assets and people, highlighting the importance of ensuring security in these systems. By proposing a novel approach to represent and share knowledge about CPS incidents, the paper aims to help organizations be more prepared to prevent, mitigate, or investigate future incidents, ultimately contributing to the overall goal of national security. Therefore, the alignment with the value item National Security and its corresponding value Security is evident in the direct emphasis on preventing security incidents and safeguarding critical infrastructures.",Agreed-Clarified,Agree,,Agree,,
Exp E,Exp J,1983,TSE,AI & Machine Learning,An Empirical Study of Model-Agnostic Techniques for Defect Prediction Models,"Software analytics have empowered software organisations to support a wide range of improved decision-making and policy-making. However, such predictions made by software analytics to date have not been explained and justified. Specifically, current defect prediction models still fail to explain why models make such a prediction and fail to uphold the privacy laws in terms of the requirement to explain any decision made by an algorithm. In this paper, we empirically evaluate three model-agnostic techniques, i.e., two state-of-the-art Local Interpretability Model-agnostic Explanations technique (LIME) and BreakDown techniques, and our improvement of LIME with Hyper Parameter Optimisation (LIME-HPO). Through a case study of 32 highly-curated defect datasets that span across 9 open-source software systems, we conclude that (1) model-agnostic techniques are needed to explain individual predictions of defect models; (2) instance explanations generated by model-agnostic techniques are mostly overlapping (but not exactly the same) with the global explanation of defect models and reliable when they are re-generated; (3) model-agnostic techniques take less than a minute to generate instance explanations; and (4) more than half of the practitioners perceive that the contrastive explanations are necessary and useful to understand the predictions of defect models. Since the implementation of the studied model-agnostic techniques is available in both Python and R, we recommend model-agnostic techniques be used in the future.",Achievement,Intelligent,The paper contributes to enhancing software analytics through the usage of model-agnostic techniques; contributing to more intelligent decision-making and policy-making in software organisations. This aligns with the software user value item 'Intelligent' and its corresponding value 'Achievement'.,"In the paper abstract of 'Paper X', it is explicitly stated that the use of model-agnostic techniques in software analytics empowers software organizations to support improved decision-making and policy-making. By enhancing software analytics through these techniques, the paper contributes to the achievement of more intelligent decision-making and policy-making in software organizations. This directly aligns with the value item 'Intelligent' and its corresponding value 'Achievement' from the perspective of a software user, as it enables them to make more informed and intelligent choices within the software context.",Agreed-Justified,Agree,,Agree,,
Exp F,Exp J,1983,TSE,AI & Machine Learning,An Empirical Study of Model-Agnostic Techniques for Defect Prediction Models,"Software analytics have empowered software organisations to support a wide range of improved decision-making and policy-making. However, such predictions made by software analytics to date have not been explained and justified. Specifically, current defect prediction models still fail to explain why models make such a prediction and fail to uphold the privacy laws in terms of the requirement to explain any decision made by an algorithm. In this paper, we empirically evaluate three model-agnostic techniques, i.e., two state-of-the-art Local Interpretability Model-agnostic Explanations technique (LIME) and BreakDown techniques, and our improvement of LIME with Hyper Parameter Optimisation (LIME-HPO). Through a case study of 32 highly-curated defect datasets that span across 9 open-source software systems, we conclude that (1) model-agnostic techniques are needed to explain individual predictions of defect models; (2) instance explanations generated by model-agnostic techniques are mostly overlapping (but not exactly the same) with the global explanation of defect models and reliable when they are re-generated; (3) model-agnostic techniques take less than a minute to generate instance explanations; and (4) more than half of the practitioners perceive that the contrastive explanations are necessary and useful to understand the predictions of defect models. Since the implementation of the studied model-agnostic techniques is available in both Python and R, we recommend model-agnostic techniques be used in the future.",Security,Cleanliness,The paper emphasizes the implementation of techniques that adhere to privacy laws and requirements; ensuring clean decision-making that respects and upholds user privacy. This corresponds to the value item 'Cleanliness' and its corresponding value 'Security'.,"In the paper abstract, it is clearly stated that the current defect prediction models fail to uphold privacy laws and requirements by not explaining why a certain prediction is made. The paper addresses this issue by proposing model-agnostic techniques that provide explanations for individual predictions, ensuring transparency and privacy in decision-making. This aligns with the value item 'Cleanliness' from Schwartz's Taxonomy, as it pertains to adhering to laws and regulations, in this case, privacy laws. By ensuring clean decision-making that respects and upholds user privacy, the paper directly aligns with the value item 'Cleanliness' and its corresponding value 'Security' in a software context.",Agreed-Justified,Agree,,Agree,,
Exp G,Exp K,1983,TSE,AI & Machine Learning,An Empirical Study of Model-Agnostic Techniques for Defect Prediction Models,"Software analytics have empowered software organisations to support a wide range of improved decision-making and policy-making. However, such predictions made by software analytics to date have not been explained and justified. Specifically, current defect prediction models still fail to explain why models make such a prediction and fail to uphold the privacy laws in terms of the requirement to explain any decision made by an algorithm. In this paper, we empirically evaluate three model-agnostic techniques, i.e., two state-of-the-art Local Interpretability Model-agnostic Explanations technique (LIME) and BreakDown techniques, and our improvement of LIME with Hyper Parameter Optimisation (LIME-HPO). Through a case study of 32 highly-curated defect datasets that span across 9 open-source software systems, we conclude that (1) model-agnostic techniques are needed to explain individual predictions of defect models; (2) instance explanations generated by model-agnostic techniques are mostly overlapping (but not exactly the same) with the global explanation of defect models and reliable when they are re-generated; (3) model-agnostic techniques take less than a minute to generate instance explanations; and (4) more than half of the practitioners perceive that the contrastive explanations are necessary and useful to understand the predictions of defect models. Since the implementation of the studied model-agnostic techniques is available in both Python and R, we recommend model-agnostic techniques be used in the future.",Universalism,Social Justice,The contributions of the paper ensure fair and justified decision-making in software analytics by providing explanation and justification for the decisions. This aligns with the value item 'Social Justice' under the value 'Universalism'.,"The main contribution of 'Paper X' is the use of model-agnostic techniques to explain and justify predictions made by defect prediction models in software analytics. By providing clear and understandable explanations for these predictions, the paper ensures fairness and justice in the decision-making process. This aligns with the value item 'Social Justice' under the broader value of 'Universalism'.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,1987,TSE,Software Engineering Practices,How Gender-Biased Tools Shape Newcomer Experiences in OSS Projects,"Previous research has revealed that newcomer women are disproportionately affected by gender-biased barriers in open source software (OSS) projects. However, this research has focused mainly on social/cultural factors, neglecting the software tools and infrastructure. To shed light on how OSS tools and infrastructure might factor into OSS barriers to entry, we conducted two studies: (1) a field study with five teams of software professionals, who worked through five use cases to analyze the tools and infrastructure used in their OSS projects; and (2) a diary study with 22 newcomers (9 women and 13 men) to investigate whether the barriers matched the ones identified by the software professionals. The field study produced a bleak result: software professionals found gender biases in 73 percent of all the newcomer barriers they identified. Further, the diary study confirmed these results: Women newcomers encountered gender biases in 63 percent of barriers they faced. Fortunately, many kinds of barriers and biases revealed in these studies could potentially be ameliorated through changes to the OSS software environments and tools.",Universalism,Equality,The study of 'Paper X' analyzed gender biases in open source software (OSS) tools which aligns with the value item Equality due to its attempt of addressing gender disparities in the software usage.,"In 'Paper X', the analysis of gender biases in open source software tools directly aligns with the value item Equality and its corresponding value Universalism. By addressing gender disparities in the software usage, the study aims to promote equal opportunities and inclusivity for all software users, regardless of their gender. This aligns with the value of Universalism, which emphasizes the importance of fairness, justice, and equality for all individuals. Through its focus on gender biases and their impact on newcomer women in OSS projects, the paper contributes to creating a more equitable and inclusive software environment, thus aligning with the value item Equality.",Agreed-Justified,Agree,,Agree,,
Exp B,Exp H,1987,TSE,Software Engineering Practices,How Gender-Biased Tools Shape Newcomer Experiences in OSS Projects,"Previous research has revealed that newcomer women are disproportionately affected by gender-biased barriers in open source software (OSS) projects. However, this research has focused mainly on social/cultural factors, neglecting the software tools and infrastructure. To shed light on how OSS tools and infrastructure might factor into OSS barriers to entry, we conducted two studies: (1) a field study with five teams of software professionals, who worked through five use cases to analyze the tools and infrastructure used in their OSS projects; and (2) a diary study with 22 newcomers (9 women and 13 men) to investigate whether the barriers matched the ones identified by the software professionals. The field study produced a bleak result: software professionals found gender biases in 73 percent of all the newcomer barriers they identified. Further, the diary study confirmed these results: Women newcomers encountered gender biases in 63 percent of barriers they faced. Fortunately, many kinds of barriers and biases revealed in these studies could potentially be ameliorated through changes to the OSS software environments and tools.",Tradition,Respect for Tradition,The study analyzed traditional practices in OSS which contributes to the value item Respect for Tradition by trying to understand and possibly change current traditional practices that lead to gender disparity in OSS.,"In the analysis of 'Paper X', the alignment with the value item Respect for Tradition and its corresponding value Tradition is justified by the fact that the study specifically focuses on analyzing and potentially changing traditional practices in OSS projects. This emphasis on tradition demonstrates an acknowledgment and respect for existing practices while also recognizing the need for improvement and addressing gender disparities. By aligning with Respect for Tradition, the study acknowledges the significance of traditional practices in OSS and aims to bring about positive changes within this established framework.",Agreed-Clarified,Agree,,Agree,,
Exp D,Exp K,1993,TSE,Code Generation & Analysis,Automatic Detection Validation and Repair of Race Conditions in Interrupt-Driven Embedded Software,"Interrupt-driven programs are widely deployed in safety-critical embedded systems to perform hardware and resource dependent data operation tasks. The frequent use of interrupts in these systems can cause race conditions to occur due to interactions between application tasks and interrupt handlers (or two interrupt handlers). Numerous program analysis and testing techniques have been proposed to detect races in multithreaded programs. Little work, however, has addressed race condition problems related to hardware interrupts. In this paper, we present SDRacer, an automated framework that can detect, validate and repair race conditions in interrupt-driven embedded software. It uses a combination of static analysis and symbolic execution to generate input data for exercising the potential races. It then employs virtual platforms to dynamically validate these races by forcing the interrupts to occur at the potential racing points. Finally, it provides repair candidates to eliminate the detected races. We evaluate SDRacer on nine real-world embedded programs written in C language. The results show that SDRacer can precisely detect and successfully fix race conditions.",Achievement,Intelligent,The paper presents SDRacer; a tool that could help software users to detect; validate; and fix the problem in the software; which aligns with the value item Intelligent and its corresponding value Achievement.,"In the paper abstract, it is stated that SDRacer is an automated framework that detects, validates, and repairs race conditions in interrupt-driven embedded software. This contribution aligns with the value item Intelligent and its corresponding value Achievement because it demonstrates the capability of the tool to intelligently analyze and solve complex software issues, reflecting the achievement of successful problem-solving within the software context.",Agreed-Justified,Agree,,Agree,,
Exp E,Exp J,1993,TSE,Code Generation & Analysis,Automatic Detection Validation and Repair of Race Conditions in Interrupt-Driven Embedded Software,"Interrupt-driven programs are widely deployed in safety-critical embedded systems to perform hardware and resource dependent data operation tasks. The frequent use of interrupts in these systems can cause race conditions to occur due to interactions between application tasks and interrupt handlers (or two interrupt handlers). Numerous program analysis and testing techniques have been proposed to detect races in multithreaded programs. Little work, however, has addressed race condition problems related to hardware interrupts. In this paper, we present SDRacer, an automated framework that can detect, validate and repair race conditions in interrupt-driven embedded software. It uses a combination of static analysis and symbolic execution to generate input data for exercising the potential races. It then employs virtual platforms to dynamically validate these races by forcing the interrupts to occur at the potential racing points. Finally, it provides repair candidates to eliminate the detected races. We evaluate SDRacer on nine real-world embedded programs written in C language. The results show that SDRacer can precisely detect and successfully fix race conditions.",Security,Healthy,By repairing race conditions in interrupt-driven embedded software; the paper ensures that the software users have a secure and smooth experience; which aligns with the value item Healthy and its corresponding value Security.,"The justification for aligning 'Paper X' with the value item Healthy and its corresponding value Security is that by repairing race conditions in interrupt-driven embedded software, the paper contributes to ensuring the overall security and smooth operation of software that uses hardware interrupts. This directly aligns with the value item Healthy as software users can rely on the software to function properly without any unexpected disruptions or vulnerabilities that could compromise their system's security.",Agreed-Clarified,Agree,,Agree,,
Exp G,Exp K,1996,TSE,Data Management & Processing,Dominoes: An Interactive Exploratory Data Analysis Tool for Software Relationships,"Project comprehension questions, such as aEURoewhich modified artifacts can affect my work?aEUR and aEURoehow can I identify the developers who should be assigned to a given task?aEUR are difficult to answer, require an analysis of the project and its data, are context specific, and cannot always be pre-defined. Current research approaches are restricted to post hoc analyses over software repositories. Very few interactive exploratory tools exist since the large amount of data that need to be analyzed prohibits its exploration at interactive rates. Moreover, such analyses typically require the user to create complex scripts or queries to extract the desired information from data. Here we present Dominoes, a tool for interactive data exploration aimed at end users (i.e., project managers or developers). Dominoes allows users to interact with different types and units of data to investigate project relationships and view intermediate results as charts, tables, and graphs. Additionally, it allows users to save the derived data as well as their exploration paths for later use. In a scenario-based evaluation study, participants achieved a success rate of 86 percent in their explorations, with a mean time of 7.25 minutes for answering a set of (project) exploration questions.",Self Direction,Creativity,The Dominoes tool; as described in 'Paper X'; allows software users to interact with different types and units of data; demonstrating a value alignment with 'Creativity' under the 'Self Direction' value through the potential opportunity for users to explore data in innovative ways.,"In 'Paper X', the Dominoes tool enables software users to interact with diverse data types and units, providing them with the ability to investigate project relationships and view intermediate results. This capability aligns with the value item of Creativity under the value of Self Direction. By allowing software users to explore and manipulate data in innovative ways, the tool empowers them to exercise their creativity in analyzing projects and making informed decisions. The emphasis on user-driven exploration and the provision of various visualization options demonstrate a clear alignment with the value of Self Direction, as users have the freedom to choose their own goals and approach in interacting with the data.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,1996,TSE,Data Management & Processing,Dominoes: An Interactive Exploratory Data Analysis Tool for Software Relationships,"Project comprehension questions, such as aEURoewhich modified artifacts can affect my work?aEUR and aEURoehow can I identify the developers who should be assigned to a given task?aEUR are difficult to answer, require an analysis of the project and its data, are context specific, and cannot always be pre-defined. Current research approaches are restricted to post hoc analyses over software repositories. Very few interactive exploratory tools exist since the large amount of data that need to be analyzed prohibits its exploration at interactive rates. Moreover, such analyses typically require the user to create complex scripts or queries to extract the desired information from data. Here we present Dominoes, a tool for interactive data exploration aimed at end users (i.e., project managers or developers). Dominoes allows users to interact with different types and units of data to investigate project relationships and view intermediate results as charts, tables, and graphs. Additionally, it allows users to save the derived data as well as their exploration paths for later use. In a scenario-based evaluation study, participants achieved a success rate of 86 percent in their explorations, with a mean time of 7.25 minutes for answering a set of (project) exploration questions.",Achievement,Intelligent,The Dominoes tool; by allowing software users to interactively explore data and answer project questions; aligns with 'Intelligent' under the 'Achievement' value as it assists users in problem-solving; thereby demonstrating their intelligence.,"The justification for labeling 'Paper X' aligning with the value item Intelligent and its corresponding value Achievement is based on the fact that the Dominoes tool enables software users to actively explore data and find answers to project-related questions. This aligns with the value of Intelligence, as it demonstrates the user's ability to apply problem-solving skills and make informed decisions based on the data exploration process facilitated by the tool. By utilizing the Dominoes tool, software users can showcase their intelligence by effectively navigating and interpreting project data to achieve successful outcomes and reach desired goals within their software context.",Agreed-Justified,Agree,,Agree,,
Exp B,Exp H,1996,TSE,Data Management & Processing,Dominoes: An Interactive Exploratory Data Analysis Tool for Software Relationships,"Project comprehension questions, such as aEURoewhich modified artifacts can affect my work?aEUR and aEURoehow can I identify the developers who should be assigned to a given task?aEUR are difficult to answer, require an analysis of the project and its data, are context specific, and cannot always be pre-defined. Current research approaches are restricted to post hoc analyses over software repositories. Very few interactive exploratory tools exist since the large amount of data that need to be analyzed prohibits its exploration at interactive rates. Moreover, such analyses typically require the user to create complex scripts or queries to extract the desired information from data. Here we present Dominoes, a tool for interactive data exploration aimed at end users (i.e., project managers or developers). Dominoes allows users to interact with different types and units of data to investigate project relationships and view intermediate results as charts, tables, and graphs. Additionally, it allows users to save the derived data as well as their exploration paths for later use. In a scenario-based evaluation study, participants achieved a success rate of 86 percent in their explorations, with a mean time of 7.25 minutes for answering a set of (project) exploration questions.",Security,Healthy,The Dominoes tool promotes 'Health' under the 'Security' value; as it allows software users to save their exploration paths for later use; reducing the risk of mental fatigue and promoting good cognitive health by removing unnecessary repetitive tasks.,"In the context of the Dominoes tool, the alignment of 'Paper X' with the value item Healthy and its corresponding value Security refers to the promotion of mental and cognitive health for software users. By allowing users to save their exploration paths for later use, the tool reduces the risk of mental fatigue and eliminates unnecessary repetitive tasks. This feature contributes to maintaining good cognitive health by providing users with the opportunity to revisit and build upon their previous work without starting from scratch. Ultimately, the emphasis on Security in this context is closely tied to maintaining the well-being and efficiency of software users by promoting a healthier approach to project exploration and analysis.",Agreed-Justified,Agree,,Agree,,
Exp C,Exp K,1999,TSE,Software Development Methodologies,Effects of Mindfulness on Conceptual Modeling Performance: A Series of Experiments,"Context. Mindfulness is a meditation technique whose main goal is keeping the mind calm and educating attention by focusing only on one thing at a time, usually breathing. The reported benefits of its continued practice can be of interest for Software Engineering students and practitioners, especially in tasks like conceptual modeling, in which concentration and clearness of mind are crucial. Goal. In order to evaluate whether Software Engineering students enhance their conceptual modeling performance after several weeks of mindfulness practice, a series of three controlled experiments were carried out at the University of Seville during three consecutive academic years (2013aEUR""2016) involving 130 students. Method. In all the experiments, the subjects were divided into two groups. While the experimental group practiced mindfulness, the control group was trained in public speaking as a placebo treatment. All the subjects developed two conceptual models based on a transcript of an interview, one before and another one after the treatment. The results were compared in terms of conceptual modeling quality (measured as effectiveness, i.e., the percentage of model elements correctly identified) and productivity (measured as efficiency, i.e., the number of model elements correctly identified per unit of time). Results. The statistically significant results of the series of experiments revealed that the subjects who practiced mindfulness developed slightly better conceptual models (their quality was 8.16 percent higher) and they did it faster (they were 46.67 percent more productive) than the control group, even if they did not have a previous interest in meditation. Conclusions. The practice of mindfulness improves the performance of Software Engineering students in conceptual modeling, especially their productivity. Nevertheless, more experimentation is needed in order to confirm the outcomes in other Software Engineering tasks and populations.",Self Direction,Curiosity,The abstract mentions the use of mindfulness that is intended to aid in keeping the mind calm and hence fostering a clear mindset. This directly aligns with the 'Curiosity' value item within 'Self Direction'; as it involves learners (users) being curious about learning and applying a new technique (mindfulness) to improve their performance.,"In the abstract, it is explicitly mentioned that the practice of mindfulness improves the performance of Software Engineering students in conceptual modeling, specifically in terms of productivity. The value item 'Curiosity' within 'Self Direction' aligns with this because the use of mindfulness is a new technique that students are curious about learning and applying to enhance their performance. 'Self Direction' is about choosing one's own goals, and by exploring and adopting mindfulness, the students are taking initiative and directing themselves towards self-improvement.",Agreed-Justified,Agree,,Agree,,
Exp D,Exp K,1999,TSE,Software Development Methodologies,Effects of Mindfulness on Conceptual Modeling Performance: A Series of Experiments,"Context. Mindfulness is a meditation technique whose main goal is keeping the mind calm and educating attention by focusing only on one thing at a time, usually breathing. The reported benefits of its continued practice can be of interest for Software Engineering students and practitioners, especially in tasks like conceptual modeling, in which concentration and clearness of mind are crucial. Goal. In order to evaluate whether Software Engineering students enhance their conceptual modeling performance after several weeks of mindfulness practice, a series of three controlled experiments were carried out at the University of Seville during three consecutive academic years (2013aEUR""2016) involving 130 students. Method. In all the experiments, the subjects were divided into two groups. While the experimental group practiced mindfulness, the control group was trained in public speaking as a placebo treatment. All the subjects developed two conceptual models based on a transcript of an interview, one before and another one after the treatment. The results were compared in terms of conceptual modeling quality (measured as effectiveness, i.e., the percentage of model elements correctly identified) and productivity (measured as efficiency, i.e., the number of model elements correctly identified per unit of time). Results. The statistically significant results of the series of experiments revealed that the subjects who practiced mindfulness developed slightly better conceptual models (their quality was 8.16 percent higher) and they did it faster (they were 46.67 percent more productive) than the control group, even if they did not have a previous interest in meditation. Conclusions. The practice of mindfulness improves the performance of Software Engineering students in conceptual modeling, especially their productivity. Nevertheless, more experimentation is needed in order to confirm the outcomes in other Software Engineering tasks and populations.",Achievement,Intelligent,The abstract discusses how mindfulness improves the performance of software engineering students in conceptual modelling tasks; meaning it contributes to their intelligent usage of software. This aligns directly with the 'Intelligent' item of the value 'Achievement'.,"In the abstract of 'Paper X', it is stated that mindfulness practice enhances the performance of software engineering students in conceptual modeling tasks. This directly aligns with the value item Intelligent and its corresponding value Achievement from a ""Software User"" perspective because the improved performance in conceptual modeling indicates a higher level of intelligence and capability in utilizing software effectively. By practicing mindfulness, software users can enhance their ability to understand and create complex conceptual models, demonstrating their achievement in using software intelligently.",Agreed-Justified,Agree,,Agree,,
Exp E,Exp J,1999,TSE,Software Development Methodologies,Effects of Mindfulness on Conceptual Modeling Performance: A Series of Experiments,"Context. Mindfulness is a meditation technique whose main goal is keeping the mind calm and educating attention by focusing only on one thing at a time, usually breathing. The reported benefits of its continued practice can be of interest for Software Engineering students and practitioners, especially in tasks like conceptual modeling, in which concentration and clearness of mind are crucial. Goal. In order to evaluate whether Software Engineering students enhance their conceptual modeling performance after several weeks of mindfulness practice, a series of three controlled experiments were carried out at the University of Seville during three consecutive academic years (2013aEUR""2016) involving 130 students. Method. In all the experiments, the subjects were divided into two groups. While the experimental group practiced mindfulness, the control group was trained in public speaking as a placebo treatment. All the subjects developed two conceptual models based on a transcript of an interview, one before and another one after the treatment. The results were compared in terms of conceptual modeling quality (measured as effectiveness, i.e., the percentage of model elements correctly identified) and productivity (measured as efficiency, i.e., the number of model elements correctly identified per unit of time). Results. The statistically significant results of the series of experiments revealed that the subjects who practiced mindfulness developed slightly better conceptual models (their quality was 8.16 percent higher) and they did it faster (they were 46.67 percent more productive) than the control group, even if they did not have a previous interest in meditation. Conclusions. The practice of mindfulness improves the performance of Software Engineering students in conceptual modeling, especially their productivity. Nevertheless, more experimentation is needed in order to confirm the outcomes in other Software Engineering tasks and populations.",Security,Healthy,The study is about software engineering students using mindfulness to improve their conceptual modeling; which implies that the users (students) are improving their healthy use of the software. This aligns with the 'Healthy' value item within 'Security'.,"In the context of 'Paper X', the alignment with the value item 'Healthy' and its corresponding value 'Security' can be seen in the aspect of software users, specifically software engineering students, improving their conceptual modeling performance through mindfulness practice. By enhancing their mental clarity and concentration, the users can experience a sense of security in their ability to effectively utilize the software, leading to a healthier and more secure use of the software tools and techniques in their academic and professional endeavors. This alignment signifies the importance of promoting a secure and healthy software user experience through mindfulness practices.",Agreed-Clarified,Agree,,Agree,,
Exp F,Exp J,2003,TSE,Software Project Management,A Study of Bug Management Using the Stack Exchange Question and Answering Platform,"Traditional bug management systems, like Bugzilla, are widely used in open source and commercial projects. Stack Exchange uses its online question and answer (Q&A) platform to collect and manage bugs, which brings several new unique features that are not offered in traditional bug management systems. Users can edit bug reports, use different communication channels, and vote on bug reports, answers, and their associated comments. Understanding how these features manage bug reports can provide insights to the designers of traditional bug management systems, like whether a feature should be introduced? and how would users leverage such a feature? We performed a large-scale analysis of 19,151 bug reports of the bug management system of Stack Exchange and studied the in-place editing, the answering and commenting, and the voting features. We find that: 1) The three features are used actively. 2) 57 percent of the edits improved the quality of bug reports. 3) Commenting provides a channel for discussing bug-related information, while answering offers a channel for explaining the causes of a bug and bug-fix information. 4) Downvotes are made due to the disagreement of the reported aEURoebugaEUR being a real bug and the low quality of bug reports. Based on our findings, we provide suggestions for traditional bug management systems.",Stimulation,Variation in Life,The paper proposes improvements to the traditional bug management systems including the introduction of new features like editing; answering and voting. This could bring Variation in Life to the software users.,"In the context of a ""Software User,"" the introduction of features like editing, answering, and voting in bug management systems can align with the value item Variation in Life and its corresponding value Stimulation. These new features provide users with a dynamic and interactive experience in managing bugs, allowing them to actively participate in improving the quality of bug reports, discussing bug-related information, and exploring different perspectives through voting. This fosters a sense of variation and stimulation in the bug management process, making it more engaging and rewarding for the software users.",Agreed-Clarified,Agree,,Agree,,
Exp A,Exp H,2003,TSE,Software Project Management,A Study of Bug Management Using the Stack Exchange Question and Answering Platform,"Traditional bug management systems, like Bugzilla, are widely used in open source and commercial projects. Stack Exchange uses its online question and answer (Q&A) platform to collect and manage bugs, which brings several new unique features that are not offered in traditional bug management systems. Users can edit bug reports, use different communication channels, and vote on bug reports, answers, and their associated comments. Understanding how these features manage bug reports can provide insights to the designers of traditional bug management systems, like whether a feature should be introduced? and how would users leverage such a feature? We performed a large-scale analysis of 19,151 bug reports of the bug management system of Stack Exchange and studied the in-place editing, the answering and commenting, and the voting features. We find that: 1) The three features are used actively. 2) 57 percent of the edits improved the quality of bug reports. 3) Commenting provides a channel for discussing bug-related information, while answering offers a channel for explaining the causes of a bug and bug-fix information. 4) Downvotes are made due to the disagreement of the reported aEURoebugaEUR being a real bug and the low quality of bug reports. Based on our findings, we provide suggestions for traditional bug management systems.",Power,Social Power,The feature allowing users to vote on bug reports; enhancing the users' control and power over the system by deciding what is considered a valid bug or not aligns with the value item Social Power under the value Power.,"The feature of allowing users to vote on bug reports in 'Paper X' aligns with the value item Social Power because it empowers software users to have a say in determining the validity of a bug. By giving users the ability to vote, they have the power to influence the prioritization and resolution of bugs, thereby exerting control and making decisions within the bug management system. This aligns with the value of Power, as users are given the authority and recognition to shape the direction of the system based on their judgment and preferences.",Agreed-Clarified,Agree,,Agree,,
Exp C,Exp K,2005,TSE,Data Management & Processing,Quantitative Verification for Monitoring Event-Streaming Systems,"High-performance data streaming technologies are increasingly adopted in IT companies to support the integration of heterogeneous and possibly distributed applications. Compared with the traditional message queuing middleware, a streaming platform enables the implementation of event-streaming systems (ESS) which include not only complex queues but also pipelines that transform and react to the streams of data. By analysing the centralised data streams, one can evaluate the Quality-of-Service for other systems and components that produce or consume those streams. We consider the exploitation of probabilistic model checking as a performance monitoring technique for ESS systems. Probabilistic model checking is a mature, powerful verification technique with successful application in performance analysis. However, an ESS system may contain quantitative parameters that are determined by event streams observed in a certain period of time. In this paper, we present a novel theoretical framework called QV4M (meaning aEURoequantitative verification for monitoringaEUR) for monitoring ESS systems, which is based on two recent methods of probabilistic model checking. QV4M assumes the parameters in a probabilistic system model as random variables and infers the statistical significance for the probabilistic model checking output. We also present an empirical evaluation of computational time and data cost for QV4M.",Achievement,Successful,By introducing a system that can evaluate the Quality-of-Service of other systems; the paper contributes towards the user potentially being successful in their tasks; aligning with the value item 'Successful'.,"I apologize for the confusion. The justification for aligning 'Paper X' with the value item Successful and its corresponding value Achievement from a ""Software User"" perspective is based on the fact that the paper introduces a system for evaluating the Quality-of-Service of other systems. By ensuring that the Quality-of-Service is maintained at a high level, the user can potentially achieve their tasks successfully. Therefore, the alignment with the value item Successful reflects the ability of the paper's contributions to enable the user to accomplish their goals effectively within a software context.",Agreed-Justified,Agree,,Agree,,
Exp D,Exp K,2005,TSE,Data Management & Processing,Quantitative Verification for Monitoring Event-Streaming Systems,"High-performance data streaming technologies are increasingly adopted in IT companies to support the integration of heterogeneous and possibly distributed applications. Compared with the traditional message queuing middleware, a streaming platform enables the implementation of event-streaming systems (ESS) which include not only complex queues but also pipelines that transform and react to the streams of data. By analysing the centralised data streams, one can evaluate the Quality-of-Service for other systems and components that produce or consume those streams. We consider the exploitation of probabilistic model checking as a performance monitoring technique for ESS systems. Probabilistic model checking is a mature, powerful verification technique with successful application in performance analysis. However, an ESS system may contain quantitative parameters that are determined by event streams observed in a certain period of time. In this paper, we present a novel theoretical framework called QV4M (meaning aEURoequantitative verification for monitoringaEUR) for monitoring ESS systems, which is based on two recent methods of probabilistic model checking. QV4M assumes the parameters in a probabilistic system model as random variables and infers the statistical significance for the probabilistic model checking output. We also present an empirical evaluation of computational time and data cost for QV4M.",Security,Healthy,The paper's main contribution improves system performance; which indirectly can contribute towards the health of the overall system that a user interacts with; aligning with the value item 'Healthy'.,"In the context of software, the value item ""Healthy"" refers to the system's stability, reliability, and overall well-being. By improving system performance, 'Paper X' indirectly contributes to the health of the software system, ensuring that it operates smoothly and reliably. This alignment is essential for a software user as it enhances their experience, reduces frustration, and promotes a sense of trust in the software's functionality.",Agreed-Clarified,Agree,,Agree,,
Exp E,Exp J,2023,TSE,Security & Privacy,How Does Refactoring Impact Security When Improving Quality? A Security-Aware Refactoring Approach,"While state of the art of software refactoring research uses various quality attributes to identify refactoring opportunities and evaluate refactoring recommendations, the impact of refactoring on the security of software systems when improving other quality objectives is under-explored. It is critical to understand how a system is resistant to security risks after refactoring to improve quality metrics. For instance, refactoring is widely used to improve the reusability of code, however such an improvement may increase the attack surface due to the created abstractions. Increasing the spread of security-critical classes in the design to improve modularity may result in reducing the resilience of software systems to attacks. In this paper, we investigated the possible impact of improving different quality attributes (e.g., reusability, extendibility, etc.), from the QMOOD model, effectiveness on a set of 8 security metrics defined in the literature related to the data access. We also studied the impact of different refactorings on these static security metrics. Then, we proposed a multi-objective refactoring recommendation approach to find a balance between quality attributes and security based on the correlation results to guide the search. We evaluated our tool on 30 open source projects. We also collected the practitioner perceptions on the refactorings recommended by our tool in terms of the possible impact on both security and other quality attributes. Our results confirm that developers need to make trade-offs between security and other qualities when refactoring software systems due to the negative correlations between them.",Security,Healthy,"The paper contributes to the development of a multi-objective refactoring recommendation tool that aims to find a balance between software quality attributes and security. The focus on software security resonates with the value item ""Healthy"" and its corresponding value of ""Security."" Essentially; a software system that is well-secured against threats can be said to be 'healthy'.","In the context of software systems, security is crucial for ensuring the overall health and well-being of the system. By aligning with the value item ""Healthy"" and its corresponding value of ""Security,"" 'Paper X' acknowledges the significance of safeguarding software systems against security risks. The development of a multi-objective refactoring recommendation tool in the paper aims to find a balance between software quality attributes and security, showcasing the importance of integrating security measures into the overall health of the system. A well-secured software system can be considered healthy, as it ensures the protection of sensitive data and minimizes the potential for vulnerabilities and breaches.",Agreed-Justified,Agree,,Agree,,
Exp F,Exp J,2023,TSE,Security & Privacy,How Does Refactoring Impact Security When Improving Quality? A Security-Aware Refactoring Approach,"While state of the art of software refactoring research uses various quality attributes to identify refactoring opportunities and evaluate refactoring recommendations, the impact of refactoring on the security of software systems when improving other quality objectives is under-explored. It is critical to understand how a system is resistant to security risks after refactoring to improve quality metrics. For instance, refactoring is widely used to improve the reusability of code, however such an improvement may increase the attack surface due to the created abstractions. Increasing the spread of security-critical classes in the design to improve modularity may result in reducing the resilience of software systems to attacks. In this paper, we investigated the possible impact of improving different quality attributes (e.g., reusability, extendibility, etc.), from the QMOOD model, effectiveness on a set of 8 security metrics defined in the literature related to the data access. We also studied the impact of different refactorings on these static security metrics. Then, we proposed a multi-objective refactoring recommendation approach to find a balance between quality attributes and security based on the correlation results to guide the search. We evaluated our tool on 30 open source projects. We also collected the practitioner perceptions on the refactorings recommended by our tool in terms of the possible impact on both security and other quality attributes. Our results confirm that developers need to make trade-offs between security and other qualities when refactoring software systems due to the negative correlations between them.",Security,Social Order,"The paper emphasises how refactoring - improving the quality of the code - can alter a software's resilience to cyber attacks. This process ensures the software maintains a level of lawful and social order; resonating with the value of ""Security"" and its value item ""Social Order.""","In the given paper, the abstract highlights the impact of refactoring on the security of software systems. It explains how improving different quality attributes through refactoring can potentially affect the resilience of the software to attacks. By ensuring that the software maintains a level of social order and security in the face of cyber attacks, the paper aligns with the value item ""Social Order"" and its corresponding value ""Security"" from the perspective of a software user. This indicates that the paper's main contributions directly address the importance of security and social order in the context of software systems.",Agreed-Clarified,Agree,,Agree,,
Exp G,Exp K,2023,TSE,Security & Privacy,How Does Refactoring Impact Security When Improving Quality? A Security-Aware Refactoring Approach,"While state of the art of software refactoring research uses various quality attributes to identify refactoring opportunities and evaluate refactoring recommendations, the impact of refactoring on the security of software systems when improving other quality objectives is under-explored. It is critical to understand how a system is resistant to security risks after refactoring to improve quality metrics. For instance, refactoring is widely used to improve the reusability of code, however such an improvement may increase the attack surface due to the created abstractions. Increasing the spread of security-critical classes in the design to improve modularity may result in reducing the resilience of software systems to attacks. In this paper, we investigated the possible impact of improving different quality attributes (e.g., reusability, extendibility, etc.), from the QMOOD model, effectiveness on a set of 8 security metrics defined in the literature related to the data access. We also studied the impact of different refactorings on these static security metrics. Then, we proposed a multi-objective refactoring recommendation approach to find a balance between quality attributes and security based on the correlation results to guide the search. We evaluated our tool on 30 open source projects. We also collected the practitioner perceptions on the refactorings recommended by our tool in terms of the possible impact on both security and other quality attributes. Our results confirm that developers need to make trade-offs between security and other qualities when refactoring software systems due to the negative correlations between them.",Security,National Security,"By assessing the impact of refactoring on a set of 8 security metrics related to data access; the paper directly aligns with the value item ""National Security."" As most software systems are interconnected globally; maintaining the security of one software can affect other software systems; thereby contributing to national security. This aligns with the value ""Security.""","In the analysis of the paper abstract, it is clear that the main contribution of ""Paper X"" is assessing the impact of refactoring on security metrics related to data access. This aligns with the value item ""National Security"" as it recognizes that by improving the security of software systems, especially in interconnected global contexts, it contributes to the overall security and resilience of national systems. This alignment reflects the value of ""Security"" as it prioritizes the protection and safety of software systems, which is crucial for both individual users and larger entities such as nations.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2025,TSE,Data Management & Processing,ConEx: Efficient Exploration of Big-Data System Configurations for Better Performance,"Configuration space complexity makes the big-data software systems hard to configure well. Consider Hadoop, with over nine hundred parameters, developers often just use the default configurations provided with Hadoop distributions. The opportunity costs in lost performance are significant. Popular learning-based approaches to auto-tune software does not scale well for big-data systems because of the high cost of collecting training data. We present a new method based on a combination of Evolutionary Markov Chain Monte Carlo (EMCMC) sampling and cost reduction techniques to find better-performing configurations for big data systems. For cost reduction, we developed and experimentally tested and validated two approaches: using scaled-up big data jobs as proxies for the objective function for larger jobs and using a dynamic job similarity measure to infer that results obtained for one kind of big data problem will work well for similar problems. Our experimental results suggest that our approach promises to improve the performance of big data systems significantly and that it outperforms competing approaches based on random sampling, basic genetic algorithms (GA), and predictive model learning. Our experimental results support the conclusion that our approach strongly demonstrates the potential to improve the performance of big data systems significantly and frugally.",Achievement,Successful,Paper X contributes by developing a new method for finding better-performing configurations for big data systems. This directly aligns with the value item; Successful; under the value Achievement as it promises to improve the performance of these systems significantly for the software users.,"The justification for aligning 'Paper X' with the value item Successful and its corresponding value Achievement from a ""Software User"" perspective is based on the fact that the paper presents a new method that promises to improve the performance of big data systems significantly. By finding better-performing configurations, the software users can achieve success in terms of achieving their goals and desired outcomes with the use of these systems. This aligns with the value item Successful under the value Achievement as it directly contributes to the software users' ability to be successful in utilizing big data systems.",Agreed-Justified,Agree,,Agree,,
Exp B,Exp H,2025,TSE,Data Management & Processing,ConEx: Efficient Exploration of Big-Data System Configurations for Better Performance,"Configuration space complexity makes the big-data software systems hard to configure well. Consider Hadoop, with over nine hundred parameters, developers often just use the default configurations provided with Hadoop distributions. The opportunity costs in lost performance are significant. Popular learning-based approaches to auto-tune software does not scale well for big-data systems because of the high cost of collecting training data. We present a new method based on a combination of Evolutionary Markov Chain Monte Carlo (EMCMC) sampling and cost reduction techniques to find better-performing configurations for big data systems. For cost reduction, we developed and experimentally tested and validated two approaches: using scaled-up big data jobs as proxies for the objective function for larger jobs and using a dynamic job similarity measure to infer that results obtained for one kind of big data problem will work well for similar problems. Our experimental results suggest that our approach promises to improve the performance of big data systems significantly and that it outperforms competing approaches based on random sampling, basic genetic algorithms (GA), and predictive model learning. Our experimental results support the conclusion that our approach strongly demonstrates the potential to improve the performance of big data systems significantly and frugally.",Stimulation,Variation in Life,Paper X presents an approach to variability in big data system performance. This aligns with Variation in Life of the values Stimulation for software users as the method introduces changes in system performance.,"The justification for labeling 'Paper X' as aligned with the value item Variation in Life and its corresponding value Stimulation is based on the fact that the paper presents a method that introduces changes in system performance for big data systems. These changes in performance can be seen as a form of variation in the software user's experience, providing stimulation and potentially new and exciting challenges. By offering a new way to configure and optimize big data systems, the paper contributes to the software user's desire for stimulation by enabling them to explore different performance possibilities in their software usage.",Agreed-Justified,Agree,,Agree,,
Exp D,Exp K,2029,TSE,Security & Privacy,Formal Verification of Masking Countermeasures for Arithmetic Programs,"Cryptographic algorithms are widely used to protect data privacy in many aspects of daily lives from smart card to cyber-physical systems. Unfortunately, programs implementing cryptographic algorithms may be vulnerable to practical power side-channel attacks, which may infer private data via statistical analysis of the correlation between power consumptions of an electronic device and private data. To thwart these attacks, several masking schemes have been proposed, giving rise to effective countermeasures for reducing the statistical correlation between private data and power consumptions. However, programs that rely on secure masking schemes are not secure a priori. Indeed, designing effective masking programs is a labor intensive and error-prone task. Although some techniques have been proposed for formally verifying masking countermeasures and for quantifying masking strength, they are currently limited to Boolean programs and suffer from low accuracy. In this work, we propose an approach for formally verifying masking countermeasures of arithmetic programs. Our approach is more accurate for arithmetic programs and more scalable for Boolean programs comparing to the existing approaches. It is essentially a synergistic integration of type inference and model-counting based methods, armed with domain specific heuristics. The type inference system allows a fast deduction of leakage-freeness of most intermediate computations, the model-counting based methods accounts for completeness, namely, to eliminate spurious flaws, and the heuristics facilitate both type inference and model-counting based reasoning, which improve scalability and efficiency in practice. In case that the program does contain leakage, we provide a method to quantify its masking strength. A distuiguished feature of our type sytem lies in its support of compositonal reasoning when verifying programs with procedure calls, so the need of inlining procedures can be significantly reduced. We have implemented our methods in a verification tool QMVerif which has been extensively evaluated on cryptographic benchmarks including full AES, DES and MAC-Keccak. The experimental results demonstrate the effectiveness and efficiency of our approach, especially for compositional reasoning. In particular, our tool is able to automatically prove leakage-freeness of arithmetic programs for which only manual proofs exist so far; it is also significantly faster than the state-of-the-art tools: EasyCrypt on common arithmetic programs, QMSInfer, SC Sniffer and maskVerif on Boolean programs.",Security,Healthy,The paper proposes an approach to improving the secure masking schemes of cryptographic algorithms to enhance data privacy in software; thereby contributing to maintaining healthy operations of the software user's electronic device as they would be protected from power side-channel attacks.,"My justification for aligning 'Paper X' with the value item Healthy and its corresponding value Security is based on the paper's proposal to improve the secure masking schemes of cryptographic algorithms. By enhancing data privacy in software, these improvements contribute to maintaining the overall healthy operations of the software user's electronic device. This is achieved by protecting the device from power side-channel attacks, which can pose a threat to the security and integrity of the user's data.",Agreed-Reconciled,Agree,,Disagree,Value healthy concerns Human health not software health,
Exp E,Exp J,2029,TSE,Security & Privacy,Formal Verification of Masking Countermeasures for Arithmetic Programs,"Cryptographic algorithms are widely used to protect data privacy in many aspects of daily lives from smart card to cyber-physical systems. Unfortunately, programs implementing cryptographic algorithms may be vulnerable to practical power side-channel attacks, which may infer private data via statistical analysis of the correlation between power consumptions of an electronic device and private data. To thwart these attacks, several masking schemes have been proposed, giving rise to effective countermeasures for reducing the statistical correlation between private data and power consumptions. However, programs that rely on secure masking schemes are not secure a priori. Indeed, designing effective masking programs is a labor intensive and error-prone task. Although some techniques have been proposed for formally verifying masking countermeasures and for quantifying masking strength, they are currently limited to Boolean programs and suffer from low accuracy. In this work, we propose an approach for formally verifying masking countermeasures of arithmetic programs. Our approach is more accurate for arithmetic programs and more scalable for Boolean programs comparing to the existing approaches. It is essentially a synergistic integration of type inference and model-counting based methods, armed with domain specific heuristics. The type inference system allows a fast deduction of leakage-freeness of most intermediate computations, the model-counting based methods accounts for completeness, namely, to eliminate spurious flaws, and the heuristics facilitate both type inference and model-counting based reasoning, which improve scalability and efficiency in practice. In case that the program does contain leakage, we provide a method to quantify its masking strength. A distuiguished feature of our type sytem lies in its support of compositonal reasoning when verifying programs with procedure calls, so the need of inlining procedures can be significantly reduced. We have implemented our methods in a verification tool QMVerif which has been extensively evaluated on cryptographic benchmarks including full AES, DES and MAC-Keccak. The experimental results demonstrate the effectiveness and efficiency of our approach, especially for compositional reasoning. In particular, our tool is able to automatically prove leakage-freeness of arithmetic programs for which only manual proofs exist so far; it is also significantly faster than the state-of-the-art tools: EasyCrypt on common arithmetic programs, QMSInfer, SC Sniffer and maskVerif on Boolean programs.",Security,National Security,Through enhancing the security of arithmetic programs; which are used in cyber-physical systems; the paper indirectly contributes to values related to National Security.,"My justification for aligning 'Paper X' with the value item National Security and its corresponding value Security is based on the fact that the paper focuses on enhancing the security of arithmetic programs, which are used in cyber-physical systems. By addressing vulnerabilities and providing countermeasures for power side-channel attacks, the paper indirectly contributes to ensuring the security of these systems, which in turn has implications for national security.",Agreed-Justified,Agree,,Agree,,
Exp F,Exp J,2029,TSE,Security & Privacy,Formal Verification of Masking Countermeasures for Arithmetic Programs,"Cryptographic algorithms are widely used to protect data privacy in many aspects of daily lives from smart card to cyber-physical systems. Unfortunately, programs implementing cryptographic algorithms may be vulnerable to practical power side-channel attacks, which may infer private data via statistical analysis of the correlation between power consumptions of an electronic device and private data. To thwart these attacks, several masking schemes have been proposed, giving rise to effective countermeasures for reducing the statistical correlation between private data and power consumptions. However, programs that rely on secure masking schemes are not secure a priori. Indeed, designing effective masking programs is a labor intensive and error-prone task. Although some techniques have been proposed for formally verifying masking countermeasures and for quantifying masking strength, they are currently limited to Boolean programs and suffer from low accuracy. In this work, we propose an approach for formally verifying masking countermeasures of arithmetic programs. Our approach is more accurate for arithmetic programs and more scalable for Boolean programs comparing to the existing approaches. It is essentially a synergistic integration of type inference and model-counting based methods, armed with domain specific heuristics. The type inference system allows a fast deduction of leakage-freeness of most intermediate computations, the model-counting based methods accounts for completeness, namely, to eliminate spurious flaws, and the heuristics facilitate both type inference and model-counting based reasoning, which improve scalability and efficiency in practice. In case that the program does contain leakage, we provide a method to quantify its masking strength. A distuiguished feature of our type sytem lies in its support of compositonal reasoning when verifying programs with procedure calls, so the need of inlining procedures can be significantly reduced. We have implemented our methods in a verification tool QMVerif which has been extensively evaluated on cryptographic benchmarks including full AES, DES and MAC-Keccak. The experimental results demonstrate the effectiveness and efficiency of our approach, especially for compositional reasoning. In particular, our tool is able to automatically prove leakage-freeness of arithmetic programs for which only manual proofs exist so far; it is also significantly faster than the state-of-the-art tools: EasyCrypt on common arithmetic programs, QMSInfer, SC Sniffer and maskVerif on Boolean programs.",Achievement,Capable,Though this value is normally about software users; the 'capability' of software itself to maintain user data security and privacy is relevant and important for users in this context,"In the context of 'Paper X', the alignment with the value item Capable and its corresponding value Achievement is justified based on the explicit contributions mentioned in the abstract. The paper aims to address the vulnerability of programs implementing cryptographic algorithms to power side-channel attacks, by proposing an approach to formally verify masking countermeasures and quantify masking strength. By ensuring the capability of software in maintaining user data security and privacy, the paper aligns with the value of Achievement for software users, as it provides a solution to enhance the overall capability of software systems in protecting sensitive information.",Agreed-Justified,Agree,,Agree,,
Exp G,Exp K,2035,TSE,Mobile & IoT,Why My App Crashes? Understanding and Benchmarking Framework-Specific Exceptions of Android Apps,"Mobile apps have become ubiquitous. Ensuring their correctness and reliability is important. However, many apps still suffer from occasional to frequent crashes, weakening their competitive edge. Large-scale, deep analyses of the characteristics of real-world app crashes can provide useful insights to both developers and researchers. However, such studies are difficult and yet to be carried out aEUR"" this work fills this gap. We collected 16,245 and 8,760 unique exceptions from 2,486 open-source and 3,230 commercial Android apps, respectively, and observed that the exceptions thrown from Android framework (termed aEURoeframework-specific exceptionsaEUR) account for the majority. With one-year effort, we (1) extensively investigated these framework-specific exceptions, and (2) further conducted an online survey of 135 professional app developers about how they analyze, test, reproduce and fix these exceptions. Specifically, we aim to understand the framework-specific exceptions from several perspectives: (i) their characteristics (e.g., manifestation locations, fault taxonomy), (ii) the developersaEUR(tm) testing practices, (iii) existing bug detection techniquesaEUR(tm) effectiveness, (iv) their reproducibility and (v) bug fixes. To enable follow-up research (e.g., bug understanding, detection, localization and repairing), we further systematically constructed, DroidDefects, the first comprehensive and largest benchmark of Android app exception bugs. This benchmark contains 33 reproducible exceptions (with test cases, stack traces, faulty and fixed app versions, bug types, etc.), and 3,696 ground-truth exceptions (real faults manifested by automated testing tools), which cover the apps with different complexities and diverse exception types. Based on our findings, we also built two prototype tools: Stoat+, an optimized dynamic testing tool, which quickly uncovered three previously-unknown, fixed crashes in Gmail and Google+; ExLocator, an exception localization tool, which can locate the root causes of specific exception types. Our dataset, benchmark and tools are publicly available on https://github.com/tingsu/droiddefects.",Achievement,Successful,The paper aims to improve the reliability and correctness of mobile apps; ensuring that the apps perform as expected and hence contributing to successful usage experiences for the 'Software User'. This aligns with the value item 'Successful' and its corresponding value 'Achievement'.,"My justification for labeling 'Paper X' as aligning with the value item Successful and its corresponding value Achievement from a ""Software User"" perspective is based on the fact that the paper explicitly states that the main contribution is to improve the correctness and reliability of mobile apps. By addressing and analyzing app crashes, the paper aims to enhance the overall user experience, leading to successful usage experiences for the software user. This aligns with the value item Successful, as users' successful utilization of the app is directly dependent on its reliability and correctness, which is the focus of the paper.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2035,TSE,Mobile & IoT,Why My App Crashes? Understanding and Benchmarking Framework-Specific Exceptions of Android Apps,"Mobile apps have become ubiquitous. Ensuring their correctness and reliability is important. However, many apps still suffer from occasional to frequent crashes, weakening their competitive edge. Large-scale, deep analyses of the characteristics of real-world app crashes can provide useful insights to both developers and researchers. However, such studies are difficult and yet to be carried out aEUR"" this work fills this gap. We collected 16,245 and 8,760 unique exceptions from 2,486 open-source and 3,230 commercial Android apps, respectively, and observed that the exceptions thrown from Android framework (termed aEURoeframework-specific exceptionsaEUR) account for the majority. With one-year effort, we (1) extensively investigated these framework-specific exceptions, and (2) further conducted an online survey of 135 professional app developers about how they analyze, test, reproduce and fix these exceptions. Specifically, we aim to understand the framework-specific exceptions from several perspectives: (i) their characteristics (e.g., manifestation locations, fault taxonomy), (ii) the developersaEUR(tm) testing practices, (iii) existing bug detection techniquesaEUR(tm) effectiveness, (iv) their reproducibility and (v) bug fixes. To enable follow-up research (e.g., bug understanding, detection, localization and repairing), we further systematically constructed, DroidDefects, the first comprehensive and largest benchmark of Android app exception bugs. This benchmark contains 33 reproducible exceptions (with test cases, stack traces, faulty and fixed app versions, bug types, etc.), and 3,696 ground-truth exceptions (real faults manifested by automated testing tools), which cover the apps with different complexities and diverse exception types. Based on our findings, we also built two prototype tools: Stoat+, an optimized dynamic testing tool, which quickly uncovered three previously-unknown, fixed crashes in Gmail and Google+; ExLocator, an exception localization tool, which can locate the root causes of specific exception types. Our dataset, benchmark and tools are publicly available on https://github.com/tingsu/droiddefects.",Security,Social Order,The paper collected and analyzed data from real-world app crashes and proposed solutions to these issues. This could promote social order by reducing instances of software failure and improving overall software reliability. This aligns with the value item 'Social Order' and its corresponding value 'Security'.,"The paper's focus on analyzing real-world app crashes and providing solutions aligns with the value item of Social Order and its corresponding value of Security. By identifying and addressing the causes of crashes, the paper contributes to reducing instances of software failure and improving overall software reliability. This ultimately promotes social order, as users can rely on stable and secure mobile apps, leading to a more secure and smoothly functioning software ecosystem.",Agreed-Clarified,Disagree,More incline to Achievement: Capable as it capable of identifying app crashing and provide solutions,Disagree,"The justification claims that the paper aligns with the value 'Security', but doesn't align with the value item of the healthy
",
Exp B,Exp H,2035,TSE,Mobile & IoT,Why My App Crashes? Understanding and Benchmarking Framework-Specific Exceptions of Android Apps,"Mobile apps have become ubiquitous. Ensuring their correctness and reliability is important. However, many apps still suffer from occasional to frequent crashes, weakening their competitive edge. Large-scale, deep analyses of the characteristics of real-world app crashes can provide useful insights to both developers and researchers. However, such studies are difficult and yet to be carried out aEUR"" this work fills this gap. We collected 16,245 and 8,760 unique exceptions from 2,486 open-source and 3,230 commercial Android apps, respectively, and observed that the exceptions thrown from Android framework (termed aEURoeframework-specific exceptionsaEUR) account for the majority. With one-year effort, we (1) extensively investigated these framework-specific exceptions, and (2) further conducted an online survey of 135 professional app developers about how they analyze, test, reproduce and fix these exceptions. Specifically, we aim to understand the framework-specific exceptions from several perspectives: (i) their characteristics (e.g., manifestation locations, fault taxonomy), (ii) the developersaEUR(tm) testing practices, (iii) existing bug detection techniquesaEUR(tm) effectiveness, (iv) their reproducibility and (v) bug fixes. To enable follow-up research (e.g., bug understanding, detection, localization and repairing), we further systematically constructed, DroidDefects, the first comprehensive and largest benchmark of Android app exception bugs. This benchmark contains 33 reproducible exceptions (with test cases, stack traces, faulty and fixed app versions, bug types, etc.), and 3,696 ground-truth exceptions (real faults manifested by automated testing tools), which cover the apps with different complexities and diverse exception types. Based on our findings, we also built two prototype tools: Stoat+, an optimized dynamic testing tool, which quickly uncovered three previously-unknown, fixed crashes in Gmail and Google+; ExLocator, an exception localization tool, which can locate the root causes of specific exception types. Our dataset, benchmark and tools are publicly available on https://github.com/tingsu/droiddefects.",Self Direction,Privacy,The research and its artefacts; provided at the public GitHub repository; contribute to the privacy of 'Software User' since it is helping to uncover and fix security faults that could potentially leak private data. It aligns with 'Privacy' from 'Self Direction'.,"The justification for aligning 'Paper X' with the value item Privacy and its corresponding value Self Direction is based on the fact that the research and its artefacts, made publicly available on the GitHub repository, contribute to the privacy of software users. By uncovering and fixing security faults that could potentially leak private data, the paper directly addresses the value of self-directed individuals who seek to protect their personal information and have control over their online privacy.",Agreed-Justified,Agree,,Agree,,
Exp C,Exp K,2039,TSE,AI & Machine Learning,Chatbot4QR: Interactive Query Refinement for Technical Question Retrieval,"Technical Q&A sites (e.g., Stack Overflow (SO)) are important resources for developers to search for knowledge about technical problems. Search engines provided in Q&A sites and information retrieval approaches (e.g., word embedding-based) have limited capabilities to retrieve relevant questions when queries are imprecisely specified, such as missing important technical details (e.g., the useraEUR(tm)s preferred programming languages). Although many automatic query expansion approaches have been proposed to improve the quality of queries by expanding queries with relevant terms, the information missed in a query is not identified. Moreover, without user involvement, the existing query expansion approaches may introduce unexpected terms and lead to undesired results. In this paper, we propose an interactive query refinement approach for question retrieval, named Chatbot4QR, which can assist users in recognizing and clarifying technical details missed in queries and thus retrieve more relevant questions for users. Chatbot4QR automatically detects missing technical details in a query and generates several clarification questions (CQs) to interact with the user to capture their overlooked technical details. To ensure the accuracy of CQs, we design a heuristic-based approach for CQ generation after building two kinds of technical knowledge bases: a manually categorized result of 1,841 technical tags in SO and the multiple version-frequency information of the tags. We develop a Chatbot4QR prototype that uses 1.88 million SO questions as the repository for question retrieval. To evaluate Chatbot4QR, we conduct six user studies with 25 participants on 50 experimental queries. The results are as follows. (1) On average 60.8 percent of the CQs generated for a query are useful for helping the participants recognize missing technical details. (2) Chatbot4QR can rapidly respond to the participants after receiving a query within approximately 1.3 seconds. (3) The refined queries contribute to retrieving more relevant SO questions than nine baseline approaches. For more than 70 percent of the participants who have preferred techniques on the query tasks, Chatbot4QR significantly outperforms the state-of-the-art word embedding-based retrieval approach with an improvement of at least 54.6 percent in terms of two measurements: Pre$@$@k and NDCG$@$@k. (4) For 48-88 percent of the assigned query tasks, the participants obtain more desired results after interacting with Chatbot4QR than directly searching from Web search engines (e.g., the SO search engine and Google) using the original queries.",Stimulation,Variation in Life,In the abstract; it's noted that the software Chatbot4QR aims to enhance user experience by retrieving more relevant questions. This variation in the information retrieval experience aligns with the value item 'Variation in Life' and its corresponding value 'Stimulation'.,"On a closer examination of the abstract, it is clear that the main contribution of 'Paper X' is the development of a software tool called Chatbot4QR that aims to enhance the user experience by retrieving more relevant questions. By incorporating an interactive query refinement approach, the software recognizes and clarifies technical details missed in queries and generates clarification questions to capture overlooked information. This interactive and dynamic nature of the software introduces variation in the information retrieval experience, providing users with different and stimulating interactions. Therefore, aligning with the value item of Variation in Life and its corresponding value Stimulation from a ""Software User"" perspective.",Agreed-Clarified,Agree,,Agree,,
Exp D,Exp K,2039,TSE,AI & Machine Learning,Chatbot4QR: Interactive Query Refinement for Technical Question Retrieval,"Technical Q&A sites (e.g., Stack Overflow (SO)) are important resources for developers to search for knowledge about technical problems. Search engines provided in Q&A sites and information retrieval approaches (e.g., word embedding-based) have limited capabilities to retrieve relevant questions when queries are imprecisely specified, such as missing important technical details (e.g., the useraEUR(tm)s preferred programming languages). Although many automatic query expansion approaches have been proposed to improve the quality of queries by expanding queries with relevant terms, the information missed in a query is not identified. Moreover, without user involvement, the existing query expansion approaches may introduce unexpected terms and lead to undesired results. In this paper, we propose an interactive query refinement approach for question retrieval, named Chatbot4QR, which can assist users in recognizing and clarifying technical details missed in queries and thus retrieve more relevant questions for users. Chatbot4QR automatically detects missing technical details in a query and generates several clarification questions (CQs) to interact with the user to capture their overlooked technical details. To ensure the accuracy of CQs, we design a heuristic-based approach for CQ generation after building two kinds of technical knowledge bases: a manually categorized result of 1,841 technical tags in SO and the multiple version-frequency information of the tags. We develop a Chatbot4QR prototype that uses 1.88 million SO questions as the repository for question retrieval. To evaluate Chatbot4QR, we conduct six user studies with 25 participants on 50 experimental queries. The results are as follows. (1) On average 60.8 percent of the CQs generated for a query are useful for helping the participants recognize missing technical details. (2) Chatbot4QR can rapidly respond to the participants after receiving a query within approximately 1.3 seconds. (3) The refined queries contribute to retrieving more relevant SO questions than nine baseline approaches. For more than 70 percent of the participants who have preferred techniques on the query tasks, Chatbot4QR significantly outperforms the state-of-the-art word embedding-based retrieval approach with an improvement of at least 54.6 percent in terms of two measurements: Pre$@$@k and NDCG$@$@k. (4) For 48-88 percent of the assigned query tasks, the participants obtain more desired results after interacting with Chatbot4QR than directly searching from Web search engines (e.g., the SO search engine and Google) using the original queries.",Achievement,Intelligent,Chatbot4QR is designed to assist users in recognizing and clarifying technical details that they might have overlooked. This support to boost user understanding and problem-solving capability resonates with the value item 'Intelligent' under the value 'Achievement'.,"Chatbot4QR's contribution of assisting users in recognizing and clarifying technical details aligns with the value item 'Intelligent' under the value 'Achievement'. By enabling users to gain a deeper understanding of their technical problems and effectively solve them, the paper's approach empowers software users to achieve greater levels of intelligence in navigating technical challenges and accomplishing their software-related goals.",Agreed-Justified,Agree,,Agree,,
Exp E,Exp J,2039,TSE,AI & Machine Learning,Chatbot4QR: Interactive Query Refinement for Technical Question Retrieval,"Technical Q&A sites (e.g., Stack Overflow (SO)) are important resources for developers to search for knowledge about technical problems. Search engines provided in Q&A sites and information retrieval approaches (e.g., word embedding-based) have limited capabilities to retrieve relevant questions when queries are imprecisely specified, such as missing important technical details (e.g., the useraEUR(tm)s preferred programming languages). Although many automatic query expansion approaches have been proposed to improve the quality of queries by expanding queries with relevant terms, the information missed in a query is not identified. Moreover, without user involvement, the existing query expansion approaches may introduce unexpected terms and lead to undesired results. In this paper, we propose an interactive query refinement approach for question retrieval, named Chatbot4QR, which can assist users in recognizing and clarifying technical details missed in queries and thus retrieve more relevant questions for users. Chatbot4QR automatically detects missing technical details in a query and generates several clarification questions (CQs) to interact with the user to capture their overlooked technical details. To ensure the accuracy of CQs, we design a heuristic-based approach for CQ generation after building two kinds of technical knowledge bases: a manually categorized result of 1,841 technical tags in SO and the multiple version-frequency information of the tags. We develop a Chatbot4QR prototype that uses 1.88 million SO questions as the repository for question retrieval. To evaluate Chatbot4QR, we conduct six user studies with 25 participants on 50 experimental queries. The results are as follows. (1) On average 60.8 percent of the CQs generated for a query are useful for helping the participants recognize missing technical details. (2) Chatbot4QR can rapidly respond to the participants after receiving a query within approximately 1.3 seconds. (3) The refined queries contribute to retrieving more relevant SO questions than nine baseline approaches. For more than 70 percent of the participants who have preferred techniques on the query tasks, Chatbot4QR significantly outperforms the state-of-the-art word embedding-based retrieval approach with an improvement of at least 54.6 percent in terms of two measurements: Pre$@$@k and NDCG$@$@k. (4) For 48-88 percent of the assigned query tasks, the participants obtain more desired results after interacting with Chatbot4QR than directly searching from Web search engines (e.g., the SO search engine and Google) using the original queries.",Benevolence,Helpful,The primary function of the software is to expand query refinement capabilities of Q&A sites such as Stack Overflow; making the information retrieval process more efficient for users. This aligns with the value item 'Helpful' because the software enables users to easily find information and potentially share this with others; which corresponds to the 'Benevolence' value.,"The justification for aligning 'Paper X' with the value item Helpful and its corresponding value Benevolence from a ""Software User"" perspective is based on the main contribution of the paper, which is the development of an interactive query refinement approach called Chatbot4QR. This approach aims to assist users in recognizing and clarifying technical details missed in their queries, thereby retrieving more relevant questions for the users. By improving the quality of queries and enabling users to find information more efficiently, the software aligns with the value item Helpful. Furthermore, this alignment with the value item Helpful corresponds to the value of Benevolence as it allows users to not only benefit themselves but potentially share the acquired knowledge and help others in the software community.",Agreed-Justified,Agree,,Agree,,
Exp F,Exp J,2040,TSE,Code Generation & Analysis,Conditional Quantitative Program Analysis,"Standards for certifying safety-critical systems have evolved to permit the inclusion of evidence generated by program analysis and verification techniques. The past decade has witnessed the development of several program analyses that are capable of computing guarantees on bounds for the probability of failure. This paper develops a novel program analysis framework, CQA, that combines evidence from different underlying analyses to compute bounds on failure probability. It reports on an evaluation of different CQA-enabled analyses and implementations of state-of-the-art quantitative analyses to evaluate their relative strengths and weaknesses. To conduct this evaluation, we filter an existing verification benchmark to reflect certification evidence generation challenges. Our evaluation across the resulting set of 136 C programs, totaling more than 385k SLOC, each with a probability of failure below $10^{-4}$10-4, demonstrates how CQA extends the state-of-the-art. The CQA infrastructure, including tools, subjects, and generated data is publicly available at bitbucket.org/mgerrard/cqa.",Security,Social Order,"The paper contributes a novel program analysis framework that is designed to compute guarantees on bounds for the probability of software failure. This advancement aligns with the value item ""Social Order"" under the value ""Security"" as it works towards creating a safer digital environment for users.","The contribution of the paper aligns with the value item ""Social Order"" and its corresponding value ""Security"" as it provides a program analysis framework that computes guarantees on the probability of software failure. By ensuring the safety and reliability of safety-critical systems, the paper's approach contributes to maintaining social order and creating a secure digital environment for software users.",Agreed-Clarified,Agree,,Agree,,
Exp G,Exp K,2040,TSE,Code Generation & Analysis,Conditional Quantitative Program Analysis,"Standards for certifying safety-critical systems have evolved to permit the inclusion of evidence generated by program analysis and verification techniques. The past decade has witnessed the development of several program analyses that are capable of computing guarantees on bounds for the probability of failure. This paper develops a novel program analysis framework, CQA, that combines evidence from different underlying analyses to compute bounds on failure probability. It reports on an evaluation of different CQA-enabled analyses and implementations of state-of-the-art quantitative analyses to evaluate their relative strengths and weaknesses. To conduct this evaluation, we filter an existing verification benchmark to reflect certification evidence generation challenges. Our evaluation across the resulting set of 136 C programs, totaling more than 385k SLOC, each with a probability of failure below $10^{-4}$10-4, demonstrates how CQA extends the state-of-the-art. The CQA infrastructure, including tools, subjects, and generated data is publicly available at bitbucket.org/mgerrard/cqa.",Security,Healthy,"These computed guarantees on bounds for the failure probability of software systems align with the value item ""Healthy"" under the value ""Security"" as it ensures that the software system used by the user is able to perform without compromising its functions and without potential hazards.","In aligning 'Paper X' with the value item Healthy and its corresponding value Security from a ""Software User"" perspective, it can be argued that the computed guarantees on bounds for the failure probability of software systems ensure that the software system used by the user is protected from potential hazards and able to function without compromising its intended functions. This alignment highlights the importance of security and reliability in software systems, which ultimately contributes to maintaining the overall health and well-being of the user's software experience.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2040,TSE,Code Generation & Analysis,Conditional Quantitative Program Analysis,"Standards for certifying safety-critical systems have evolved to permit the inclusion of evidence generated by program analysis and verification techniques. The past decade has witnessed the development of several program analyses that are capable of computing guarantees on bounds for the probability of failure. This paper develops a novel program analysis framework, CQA, that combines evidence from different underlying analyses to compute bounds on failure probability. It reports on an evaluation of different CQA-enabled analyses and implementations of state-of-the-art quantitative analyses to evaluate their relative strengths and weaknesses. To conduct this evaluation, we filter an existing verification benchmark to reflect certification evidence generation challenges. Our evaluation across the resulting set of 136 C programs, totaling more than 385k SLOC, each with a probability of failure below $10^{-4}$10-4, demonstrates how CQA extends the state-of-the-art. The CQA infrastructure, including tools, subjects, and generated data is publicly available at bitbucket.org/mgerrard/cqa.",Achievement,Successful,"The paper reports an evaluation of the framework's relative strengths and weaknesses; demonstrating an approach to successful application of the theoretical framework in practical scenarios. These are tied to the value item ""Successful"" under the value ""Achievement""; embodying a user's aspiration for the software to excel and perform at its best.","In the paper abstract, the authors mention that they evaluate different program analysis techniques and implementations to assess their relative strengths and weaknesses. By doing so, they are demonstrating an approach that can be successful in the practical use of the framework. This aligns with the value item ""Successful"" under the value ""Achievement"" because it represents a user's desire for the software to excel and perform at its best. Therefore, the main contributions of 'Paper X' directly align with the value item ""Successful"" and the value of ""Achievement"" from the perspective of a software user.",Agreed-Justified,Agree,,Agree,,
Exp B,Exp H,2056,TSE,Security & Privacy,An Empirical Study of C++ Vulnerabilities in Crowd-Sourced Code Examples,"Software developers share programming solutions in Q&A sites like Stack Overflow, Stack Exchange, Android forum, and so on. The reuse of crowd-sourced code snippets can facilitate rapid prototyping. However, recent research shows that the shared code snippets may be of low quality and can even contain vulnerabilities. This paper aims to understand the nature and the prevalence of security vulnerabilities in crowd-sourced code examples. To achieve this goal, we investigate security vulnerabilities in the C++ code snippets shared on Stack Overflow over a period of 10 years. In collaborative sessions involving multiple human coders, we manually assessed each code snippet for security vulnerabilities following CWE (Common Weakness Enumeration) guidelines. From the 72,483 reviewed code snippets used in at least one project hosted on GitHub, we found a total of 99 vulnerable code snippets categorized into 31 types. Many of the investigated code snippets are still not corrected on Stack Overflow. The 99 vulnerable code snippets found in Stack Overflow were reused in a total of 2859 GitHub projects. To help improve the quality of code snippets shared on Stack Overflow, we developed a browser extension that allows Stack Overflow users to be notified for vulnerabilities in code snippets when they see them on the platform.",Stimulation,Variation in Life,The study adds Variation in Life (v2.2) under the value Stimulation (v2) by informing us about the diversity of security vulnerabilities present in the shared code snippets; indicating the variety of challenges that users may face in using software projects that incorporate these snippets.,"The justification for aligning the main contributions of 'Paper X' with the value item Variation in Life and its corresponding value Stimulation from a ""Software User"" perspective is based on the fact that the study reveals the diversity of security vulnerabilities in shared code snippets. This indicates the range of challenges that users may encounter when using software projects that incorporate these snippets, providing a variety of situations and opportunities for problem-solving and intellectual engagement, which correlates with the value of Stimulation.",Agreed-Clarified,Agree,,Agree,,
Exp C,Exp K,2056,TSE,Security & Privacy,An Empirical Study of C++ Vulnerabilities in Crowd-Sourced Code Examples,"Software developers share programming solutions in Q&A sites like Stack Overflow, Stack Exchange, Android forum, and so on. The reuse of crowd-sourced code snippets can facilitate rapid prototyping. However, recent research shows that the shared code snippets may be of low quality and can even contain vulnerabilities. This paper aims to understand the nature and the prevalence of security vulnerabilities in crowd-sourced code examples. To achieve this goal, we investigate security vulnerabilities in the C++ code snippets shared on Stack Overflow over a period of 10 years. In collaborative sessions involving multiple human coders, we manually assessed each code snippet for security vulnerabilities following CWE (Common Weakness Enumeration) guidelines. From the 72,483 reviewed code snippets used in at least one project hosted on GitHub, we found a total of 99 vulnerable code snippets categorized into 31 types. Many of the investigated code snippets are still not corrected on Stack Overflow. The 99 vulnerable code snippets found in Stack Overflow were reused in a total of 2859 GitHub projects. To help improve the quality of code snippets shared on Stack Overflow, we developed a browser extension that allows Stack Overflow users to be notified for vulnerabilities in code snippets when they see them on the platform.",Security,Healthy,"The paper contributes to users' Security (v6) especially in being Healthy (v6.1) - ""healthy"" here referring to the health of the software that they are using. This is because the main aim of the research is to expose and understand the prevalence of security vulnerabilities in reused code snippets; which can lead to less secure software.","The justification for aligning 'Paper X' with the value item Healthy and its corresponding value Security is based on the fact that the research aims to understand the prevalence of security vulnerabilities in code snippets shared on Stack Overflow. By identifying and assessing these vulnerabilities, the paper contributes to ensuring the security of software for users. This aligns with the value of Security (v6) and being Healthy (v6.1) as it emphasizes the importance of a secure and safe software environment for users.",Agreed-Reconciled,Agree,"if a software is secure, users are not worried and feel relax when using it",Disagree,Value healthy concerns Human health not software health,
Exp D,Exp K,2056,TSE,Security & Privacy,An Empirical Study of C++ Vulnerabilities in Crowd-Sourced Code Examples,"Software developers share programming solutions in Q&A sites like Stack Overflow, Stack Exchange, Android forum, and so on. The reuse of crowd-sourced code snippets can facilitate rapid prototyping. However, recent research shows that the shared code snippets may be of low quality and can even contain vulnerabilities. This paper aims to understand the nature and the prevalence of security vulnerabilities in crowd-sourced code examples. To achieve this goal, we investigate security vulnerabilities in the C++ code snippets shared on Stack Overflow over a period of 10 years. In collaborative sessions involving multiple human coders, we manually assessed each code snippet for security vulnerabilities following CWE (Common Weakness Enumeration) guidelines. From the 72,483 reviewed code snippets used in at least one project hosted on GitHub, we found a total of 99 vulnerable code snippets categorized into 31 types. Many of the investigated code snippets are still not corrected on Stack Overflow. The 99 vulnerable code snippets found in Stack Overflow were reused in a total of 2859 GitHub projects. To help improve the quality of code snippets shared on Stack Overflow, we developed a browser extension that allows Stack Overflow users to be notified for vulnerabilities in code snippets when they see them on the platform.",Universalism,A World at Peace,By aiming to improve the level of code security; 'Paper X' is contributing to A World at Peace (v10.7) under the value Universalism (v10); as it seeks to create a safer digital environment for users by reducing the potential for security breaches.,"In aligning with the value item A World at Peace and its corresponding value Universalism, 'Paper X' contributes to creating a safer digital environment for software users by improving the level of code security. By addressing and mitigating security vulnerabilities in crowd-sourced code snippets, the paper aims to reduce the potential for security breaches, thereby promoting a more secure and peaceful digital ecosystem. This aligns with the value of Universalism as it emphasizes the importance of protecting and ensuring the well-being of all individuals within a broader societal context, in this case, the software user community.",Agreed-Clarified,Agree,,Agree,,
Exp E,Exp J,2057,TSE,Security & Privacy,The Best Laid Plans or Lack Thereof: Security Decision-Making of Different Stakeholder Groups,"Cyber security requirements are influenced by the priorities and decisions of a range of stakeholders. Board members and Chief Information Security Officers (CISOs) determine strategic priorities. Managers have responsibility for resource allocation and project management. Legal professionals concern themselves with regulatory compliance. Little is understood about how the security decision-making approaches of these different stakeholders contrast, and if particular groups of stakeholders have a better appreciation of security requirements during decision-making. Are risk analysts better decision makers than CISOs? Do security experts exhibit more effective strategies than board members? This paper explores the effect that different experience and diversity of expertise has on the quality of a team's cyber security decision-making and whether teams with members from more varied backgrounds perform better than those with more focused, homogeneous skill sets. Using data from 208 sessions and 948 players of a tabletop game run in the wild by a major national organization over 16 months, we explore how choices are affected by player background (e.g., cyber security experts versus risk analysts, board-level decision makers versus technical experts) and different team make-ups (homogeneous teams of security experts versus various mixes). We find that no group of experts makes significantly better game decisions than anyone else, and that their biases lead them to not fully comprehend what they are defending or how the defenses work.",Achievement,Intelligent,"The paper is focused on cyber security decision-making; implicitly aligning with the value item ""Intelligent"" under the ""Achievement"" value as it assumes that smarter decisions will lead to better security outcomes for users.","In the context of the abstract, the alignment of 'Paper X' with the value item Intelligent and its corresponding value Achievement is justified based on the assumption that smarter decisions, informed by the findings of the paper, will lead to better security outcomes for software users. The paper aims to explore the effect of different stakeholders' decision-making approaches on the quality of cyber security decision-making. By identifying how different teams with varied backgrounds perform in terms of decision-making, the paper indirectly suggests that teams with more knowledgeable and intelligent stakeholders are more likely to make effective decisions that enhance the security of software and protect users. Therefore, the alignment with the value item Intelligent and its corresponding value Achievement is substantiated by the focus on improving decision-making and the implicit connection between intelligence and achieving better security outcomes for software users.",Agreed-Clarified,Agree,,Agree,,
Exp F,Exp J,2057,TSE,Security & Privacy,The Best Laid Plans or Lack Thereof: Security Decision-Making of Different Stakeholder Groups,"Cyber security requirements are influenced by the priorities and decisions of a range of stakeholders. Board members and Chief Information Security Officers (CISOs) determine strategic priorities. Managers have responsibility for resource allocation and project management. Legal professionals concern themselves with regulatory compliance. Little is understood about how the security decision-making approaches of these different stakeholders contrast, and if particular groups of stakeholders have a better appreciation of security requirements during decision-making. Are risk analysts better decision makers than CISOs? Do security experts exhibit more effective strategies than board members? This paper explores the effect that different experience and diversity of expertise has on the quality of a team's cyber security decision-making and whether teams with members from more varied backgrounds perform better than those with more focused, homogeneous skill sets. Using data from 208 sessions and 948 players of a tabletop game run in the wild by a major national organization over 16 months, we explore how choices are affected by player background (e.g., cyber security experts versus risk analysts, board-level decision makers versus technical experts) and different team make-ups (homogeneous teams of security experts versus various mixes). We find that no group of experts makes significantly better game decisions than anyone else, and that their biases lead them to not fully comprehend what they are defending or how the defenses work.",Security,Sense of Belonging,"By exploring how decisions are made and their impact on overall security; the paper is contributing to the ""Sense of Belonging"" under ""Security"" value since effective security decisions can create a safer online environment; making users feel more at ease; thereby fostering a sense of belonging.","In the context of a software user, the main contribution of 'Paper X' is the exploration of how different stakeholders' decision-making approaches and diverse expertise affect the quality of cyber security decision-making. By studying the choices and biases of board members, CISOs, managers, and legal professionals, the paper provides insights into the effectiveness of security decision-making. This directly aligns with the value item ""Sense of Belonging"" and its corresponding value ""Security"" from Schwartz's Taxonomy. Effective security decisions contribute to a safer online environment, which in turn makes software users feel more at ease and fosters a sense of belonging. Users are more likely to trust and have a sense of belonging to a software system that prioritizes their security and privacy, making this alignment between 'Paper X' and the value item Sense of Belonging and the value Security highly relevant in the software user context.",Agreed-Clarified,Agree,,Agree,,
Exp G,Exp K,2057,TSE,Security & Privacy,The Best Laid Plans or Lack Thereof: Security Decision-Making of Different Stakeholder Groups,"Cyber security requirements are influenced by the priorities and decisions of a range of stakeholders. Board members and Chief Information Security Officers (CISOs) determine strategic priorities. Managers have responsibility for resource allocation and project management. Legal professionals concern themselves with regulatory compliance. Little is understood about how the security decision-making approaches of these different stakeholders contrast, and if particular groups of stakeholders have a better appreciation of security requirements during decision-making. Are risk analysts better decision makers than CISOs? Do security experts exhibit more effective strategies than board members? This paper explores the effect that different experience and diversity of expertise has on the quality of a team's cyber security decision-making and whether teams with members from more varied backgrounds perform better than those with more focused, homogeneous skill sets. Using data from 208 sessions and 948 players of a tabletop game run in the wild by a major national organization over 16 months, we explore how choices are affected by player background (e.g., cyber security experts versus risk analysts, board-level decision makers versus technical experts) and different team make-ups (homogeneous teams of security experts versus various mixes). We find that no group of experts makes significantly better game decisions than anyone else, and that their biases lead them to not fully comprehend what they are defending or how the defenses work.",Security,National Security,"The study's main contribution of understanding the effectiveness of different decision-making approaches in cyber security aligns with the ""National Security"" under the ""Security"" value as this knowledge can be used to fortify a nation's cyber infrastructure; this directly aligns with a user's desire for a secure national cyber space.","The justification for aligning 'Paper X' with the value item National Security and its corresponding value Security from a ""Software User"" perspective is based on the fact that the study aims to understand the effectiveness of different decision-making approaches in cyber security. This understanding can be utilized to strengthen a nation's cyber infrastructure, which directly relates to the user's desire for a secure national cyber space. By identifying and evaluating different strategies and biases in decision-making, the study contributes to enhancing the security measures and ultimately ensures the protection of a user's personal and national data in a software context.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2058,TSE,Software Testing & QA,PerfJIT: Test-Level Just-in-Time Prediction for Performance Regression Introducing Commits,"Performance issues may compromise user experiences, increase the cost resources, and cause field failures. One of the most prevalent performance issues is performance regression. Due to the importance and challenges in performance regression detection, prior research proposes various automated approaches that detect performance regressions. However, the performance regression detection is conducted after the system is built and deployed. Hence, large amounts of resources are still required to locate and fix performance regressions. In our paper, we propose an approach that automatically predicts whether a test would manifest performance regressions given a code commit. In particular, we extract both traditional metrics and performance-related metrics from the code changes that are associated with each test. For each commit, we build random forest classifiers that are trained from all prior commits to predict in this commit whether each test would manifest performance regression. We conduct case studies on three open-source systems (Hadoop, Cassandra, and OpenJPA). Our results show that our approach can predict tests that manifest performance regressions in a commit with high AUC values (on average 0.86). Our approach can drastically reduce the testing time needed to detect performance regressions. In addition, we find that our approach could be used to detect the introduction of six out of nine real-life performance issues from the subject systems during our studied period. Finally, we find that traditional metrics that are associated with size and code change histories are the most important factors in our models. Our approach and the study results can be leveraged by practitioners to effectively cope with performance regressions in a timely and proactive manner.",Self Direction,Choosing Own Goals,"The paper contributes an automated approach that predicts performance regressions; thereby enabling the users to choose appropriate software or checkpoint versions. This aligns with the value item ""Choosing Own Goals"" under the value ""Self Direction.""",The justification for aligning 'Paper X' with the value item Choosing Own Goals and its corresponding value Self Direction is that the paper's automated approach empowers software users to have control over their software experiences by enabling them to choose appropriate software or checkpoint versions based on the predicted performance regressions. This aligns with the value item because it allows users to make independent decisions and exercise their freedom to determine their own goals and preferences in the software context.,Agreed-Clarified,Agree,,Agree,,
Exp B,Exp H,2058,TSE,Software Testing & QA,PerfJIT: Test-Level Just-in-Time Prediction for Performance Regression Introducing Commits,"Performance issues may compromise user experiences, increase the cost resources, and cause field failures. One of the most prevalent performance issues is performance regression. Due to the importance and challenges in performance regression detection, prior research proposes various automated approaches that detect performance regressions. However, the performance regression detection is conducted after the system is built and deployed. Hence, large amounts of resources are still required to locate and fix performance regressions. In our paper, we propose an approach that automatically predicts whether a test would manifest performance regressions given a code commit. In particular, we extract both traditional metrics and performance-related metrics from the code changes that are associated with each test. For each commit, we build random forest classifiers that are trained from all prior commits to predict in this commit whether each test would manifest performance regression. We conduct case studies on three open-source systems (Hadoop, Cassandra, and OpenJPA). Our results show that our approach can predict tests that manifest performance regressions in a commit with high AUC values (on average 0.86). Our approach can drastically reduce the testing time needed to detect performance regressions. In addition, we find that our approach could be used to detect the introduction of six out of nine real-life performance issues from the subject systems during our studied period. Finally, we find that traditional metrics that are associated with size and code change histories are the most important factors in our models. Our approach and the study results can be leveraged by practitioners to effectively cope with performance regressions in a timely and proactive manner.",Achievement,Successful,"The automated approach proposed in the paper; which predicts performance regressions; can contribute to a user's success in carrying out their tasks without software hiccups; which aligns with the ""Successful"" item under the ""Achievement"" value.","The justification for aligning 'Paper X' with the value item ""Successful"" and its corresponding value ""Achievement"" from a ""Software User"" perspective is based on the fact that the automated approach proposed in the paper aims to predict and prevent performance regressions. By doing so, it enables users to carry out their tasks without experiencing software hiccups, thus contributing to their success in achieving their goals. A successful software user is one who can effectively utilize the software to accomplish their tasks without any hindrance or disruptions. By addressing performance issues and regression, 'Paper X' directly aligns with the value of Achievement by enabling users to achieve their goals efficiently with a high level of performance and effectiveness.",Agreed-Justified,Agree,,Agree,,
Exp D,Exp K,2059,TSE,Algorithms,Probabilistic Preference Planning Problem for Markov Decision Processes,"The classical planning problem aims to find a sequence of permitted actions leading a system to a designed state, i.e., to achieve the systemaEUR(tm)s task. However, in many realistic cases we also have requirements on how to complete the task, indicating that some behaviors and situations are more preferred than others. In this paper, we present the probabilistic preference-based planning problem ($\mathrm{P4}$P4) for Markov decision processes, where the preferences are defined based on an enriched probabilistic LTL-style logic. We first recall $\mathrm{\mathrm{P4} {}Solver}$P4 Solver , an SMT-based planner computing the preferred plan by reducing the problem to a quadratic programming one previously developed to solve $\mathrm{P4}$P4. To improve computational efficiency and scalability, we then introduce a new encoding of the probabilistic preference-based planning problem as a multi-objective model checking one, and propose the corresponding planner $\mathrm{\mathrm{P4} {}Solver} _{{MO}}$P4 Solver MO. We illustrate the efficacy of both planners on some selected case studies to show that the model checking-based algorithm is considerably more efficient than the quadratic-programming-based one.",Achievement,Intelligent,The paper introduces a new model in order to enhance the efficiency and scalability of the software which aligns with the value item 'Intelligent' and its corresponding value 'Achievement'.,"The justification for labeling 'Paper X' as aligning with the value item Intelligent and its corresponding value Achievement is based on the fact that the paper presents a new model that improves the efficiency and scalability of the software. This achievement demonstrates an intelligent approach to solving the problem of preference-based planning, as it introduces a novel and more effective method. By addressing the limitations of previous approaches, the paper contributes to achieving higher standards and success in software development, aligning with the value of Achievement within the context of a Software User.",Agreed-Justified,Agree,,Agree,,
Exp E,Exp J,2059,TSE,Algorithms,Probabilistic Preference Planning Problem for Markov Decision Processes,"The classical planning problem aims to find a sequence of permitted actions leading a system to a designed state, i.e., to achieve the systemaEUR(tm)s task. However, in many realistic cases we also have requirements on how to complete the task, indicating that some behaviors and situations are more preferred than others. In this paper, we present the probabilistic preference-based planning problem ($\mathrm{P4}$P4) for Markov decision processes, where the preferences are defined based on an enriched probabilistic LTL-style logic. We first recall $\mathrm{\mathrm{P4} {}Solver}$P4 Solver , an SMT-based planner computing the preferred plan by reducing the problem to a quadratic programming one previously developed to solve $\mathrm{P4}$P4. To improve computational efficiency and scalability, we then introduce a new encoding of the probabilistic preference-based planning problem as a multi-objective model checking one, and propose the corresponding planner $\mathrm{\mathrm{P4} {}Solver} _{{MO}}$P4 Solver MO. We illustrate the efficacy of both planners on some selected case studies to show that the model checking-based algorithm is considerably more efficient than the quadratic-programming-based one.",Achievement,Capable,The abstract mentions increasing computational efficiency; aligning with the value item 'Capable' and its corresponding value 'Achievement'.,"In the abstract of 'Paper X', it is mentioned that the authors introduce a new encoding of the probabilistic preference-based planning problem to improve computational efficiency and scalability. This directly aligns with the value item 'Capable' from Schwartz's Taxonomy and its corresponding value 'Achievement' because improving computational efficiency and scalability demonstrates the ability to accomplish tasks successfully and achieve desired outcomes, which is valued by users of software.",Agreed-Clarified,Agree,,Agree,,
Exp F,Exp J,2059,TSE,Algorithms,Probabilistic Preference Planning Problem for Markov Decision Processes,"The classical planning problem aims to find a sequence of permitted actions leading a system to a designed state, i.e., to achieve the systemaEUR(tm)s task. However, in many realistic cases we also have requirements on how to complete the task, indicating that some behaviors and situations are more preferred than others. In this paper, we present the probabilistic preference-based planning problem ($\mathrm{P4}$P4) for Markov decision processes, where the preferences are defined based on an enriched probabilistic LTL-style logic. We first recall $\mathrm{\mathrm{P4} {}Solver}$P4 Solver , an SMT-based planner computing the preferred plan by reducing the problem to a quadratic programming one previously developed to solve $\mathrm{P4}$P4. To improve computational efficiency and scalability, we then introduce a new encoding of the probabilistic preference-based planning problem as a multi-objective model checking one, and propose the corresponding planner $\mathrm{\mathrm{P4} {}Solver} _{{MO}}$P4 Solver MO. We illustrate the efficacy of both planners on some selected case studies to show that the model checking-based algorithm is considerably more efficient than the quadratic-programming-based one.",Security,Healthy,The abstract talks about algorithm efficiency and improving computational health aligning with the value item 'Healthy' and its corresponding value 'Security'.,"In the abstract, it is mentioned that the paper aims to improve computational efficiency and scalability, which can be interpreted as ensuring the well-being and stability of the software system. This aligns with the value item 'Healthy' and its corresponding value 'Security' as it indicates a focus on the stability and security of the software system, which is important for software users.",Agreed-Justified,Agree,,Agree,,
Exp G,Exp K,2062,TSE,Security & Privacy,Vuln4Real: A Methodology for Counting Actually Vulnerable Dependencies,"Vulnerable dependencies are a known problem in todayaEUR(tm)s free open-source software ecosystems because FOSS libraries are highly interconnected, and developers do not always update their dependencies. Our paper proposes Vuln4Real, the methodology for counting actually vulnerable dependencies, that addresses the over-inflation problem of academic and industrial approaches for reporting vulnerable dependencies in FOSS software, and therefore, caters to the needs of industrial practice for correct allocation of development and audit resources. To understand the industrial impact of a more precise methodology, we considered the 500 most popular FOSS Java libraries used by SAP in its own software. Our analysis included 25767 distinct library instances in Maven. We found that the proposed methodology has visible impacts on both ecosystem view and the individual library developer view of the situation of software dependencies: Vuln4Real significantly reduces the number of false alerts for deployed code (dependencies wrongly flagged as vulnerable), provides meaningful insights on the exposure to third-parties (and hence vulnerabilities) of a library, and automatically predicts when dependency maintenance starts lagging, so it may not receive updates for arising issues.",Security,Social Order,The paper presents a methodology that significantly reduces the number of false alerts for deployed code in software; thus encouraging a more efficient and orderly process in handling software dependencies which aligns with the value item 'Social Order' under the 'Security' value.,"In 'Paper X', the proposed methodology aims to reduce false alerts for vulnerable dependencies in software, which ultimately improves the security and stability of the software. By implementing this methodology, software users can have a more orderly and secure software environment, aligning with the value item of 'Social Order' under the broader value of 'Security' from a user perspective.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2062,TSE,Security & Privacy,Vuln4Real: A Methodology for Counting Actually Vulnerable Dependencies,"Vulnerable dependencies are a known problem in todayaEUR(tm)s free open-source software ecosystems because FOSS libraries are highly interconnected, and developers do not always update their dependencies. Our paper proposes Vuln4Real, the methodology for counting actually vulnerable dependencies, that addresses the over-inflation problem of academic and industrial approaches for reporting vulnerable dependencies in FOSS software, and therefore, caters to the needs of industrial practice for correct allocation of development and audit resources. To understand the industrial impact of a more precise methodology, we considered the 500 most popular FOSS Java libraries used by SAP in its own software. Our analysis included 25767 distinct library instances in Maven. We found that the proposed methodology has visible impacts on both ecosystem view and the individual library developer view of the situation of software dependencies: Vuln4Real significantly reduces the number of false alerts for deployed code (dependencies wrongly flagged as vulnerable), provides meaningful insights on the exposure to third-parties (and hence vulnerabilities) of a library, and automatically predicts when dependency maintenance starts lagging, so it may not receive updates for arising issues.",Security,Family Security,The method proposed by the paper provides meaningful insights on the exposure to third-parties of a library; contributing to 'Family Security' (As in this context; 'Family' can be considered as a unit in software terms; like a group of software libraries working together).,"In the context of software, the term ""Family Security"" refers to the safety and protection of a group of software libraries working together. The main contribution of 'Paper X' is the methodology of Vuln4Real, which provides meaningful insights into the exposure of a library to vulnerabilities caused by third-party dependencies. By reducing the number of false alerts for vulnerable dependencies and predicting when dependency maintenance starts lagging, the methodology ensures that software libraries are secure and free from potential risks. This directly aligns with the value of Security, as it addresses the concern of software users in maintaining a secure software environment for their applications. Therefore, the alignment between 'Paper X' and the value item Family Security from the perspective of a software user is evident.",Agreed-Clarified,Agree,reconcile with coder_2,Disagree,"The justification claiming that the paper aligns with the value 'Security' because of 'The method proposed by the paper provides meaningful insights on the exposure to third-parties of a library,but is not supported Family security according abstract",
Exp B,Exp H,2062,TSE,Security & Privacy,Vuln4Real: A Methodology for Counting Actually Vulnerable Dependencies,"Vulnerable dependencies are a known problem in todayaEUR(tm)s free open-source software ecosystems because FOSS libraries are highly interconnected, and developers do not always update their dependencies. Our paper proposes Vuln4Real, the methodology for counting actually vulnerable dependencies, that addresses the over-inflation problem of academic and industrial approaches for reporting vulnerable dependencies in FOSS software, and therefore, caters to the needs of industrial practice for correct allocation of development and audit resources. To understand the industrial impact of a more precise methodology, we considered the 500 most popular FOSS Java libraries used by SAP in its own software. Our analysis included 25767 distinct library instances in Maven. We found that the proposed methodology has visible impacts on both ecosystem view and the individual library developer view of the situation of software dependencies: Vuln4Real significantly reduces the number of false alerts for deployed code (dependencies wrongly flagged as vulnerable), provides meaningful insights on the exposure to third-parties (and hence vulnerabilities) of a library, and automatically predicts when dependency maintenance starts lagging, so it may not receive updates for arising issues.",Achievement,Capable,The methodology introduced by the paper aims to predict the irregularities in software dependencies' updates; which can enable users to prevent issues arrising from unmaintained dependencies. This aligns with 'Capable'; under the 'Achievement' value.,"The methodology proposed by 'Paper X' aligns with the value item Capable and the corresponding value Achievement from a ""Software User"" perspective because it enables users to accurately assess the vulnerability of software dependencies and predict when maintenance starts lagging, allowing them to take proactive measures and prevent potential issues arising from unmaintained dependencies. This empowers users to have a sense of capability and accomplishment in successfully managing their software ecosystem and ensuring the security and reliability of their software applications.",Agreed-Justified,Agree,,Agree,,
Exp C,Exp K,2064,TSE,Software Development Methodologies,Comparing Block-Based Programming Models for Two-Armed Robots,"Modern industrial robots can work alongside human workers and coordinate with other robots. This means they can perform complex tasks, but doing so requires complex programming. Therefore, robots are typically programmed by experts, but there are not enough to meet the growing demand for robots. To reduce the need for experts, researchers have tried to make robot programming accessible to factory workers without programming experience. However, none of that previous work supports coordinating multiple robot arms that work on the same task. In this paper we present four block-based programming language designs that enable end-users to program two-armed robots. We analyze the benefits and trade-offs of each design on expressiveness and user cognition, and evaluate the designs based on a survey of 273 professional participants of whom 110 had no previous programming experience. We further present an interactive experiment based on a prototype implementation of the design we deem best. This experiment confirmed that novices can successfully use our prototype to complete realistic robotics tasks. This work contributes to making coordinated programming of robots accessible to end-users. It further explores how visual programming elements can make traditionally challenging programming tasks more beginner-friendly.",Self Direction,Creativity,Paper X introduces a creative approach to disseminate programming knowledge of industrial robots among factory workers; aligning with Creativity under the value Self-Direction.,"In 'Paper X', the authors propose a block-based programming language design that enables end-users, specifically factory workers without programming experience, to program two-armed robots. This approach addresses the need for experts in robot programming and aims to make robot programming accessible to a wider range of users. By introducing a new approach and designing a prototype that allows novices to successfully complete realistic robotics tasks, the paper demonstrates a creative solution for disseminating programming knowledge and empowering end-users. This aligns with the value of Creativity under the overarching value of Self-Direction, as it encourages individuals to explore their own potential and pursue their own goals in the domain of robotics programming.",Agreed-Justified,Agree,,Agree,,
Exp D,Exp K,2064,TSE,Software Development Methodologies,Comparing Block-Based Programming Models for Two-Armed Robots,"Modern industrial robots can work alongside human workers and coordinate with other robots. This means they can perform complex tasks, but doing so requires complex programming. Therefore, robots are typically programmed by experts, but there are not enough to meet the growing demand for robots. To reduce the need for experts, researchers have tried to make robot programming accessible to factory workers without programming experience. However, none of that previous work supports coordinating multiple robot arms that work on the same task. In this paper we present four block-based programming language designs that enable end-users to program two-armed robots. We analyze the benefits and trade-offs of each design on expressiveness and user cognition, and evaluate the designs based on a survey of 273 professional participants of whom 110 had no previous programming experience. We further present an interactive experiment based on a prototype implementation of the design we deem best. This experiment confirmed that novices can successfully use our prototype to complete realistic robotics tasks. This work contributes to making coordinated programming of robots accessible to end-users. It further explores how visual programming elements can make traditionally challenging programming tasks more beginner-friendly.",Achievement,Successful,The prototype developed on the design enables novices to successfully complete realistic robotics tasks; reinforcing Software User's Success under the value Achievement.,"In the paper abstract, it is explicitly stated that the prototype developed based on the block-based programming language designs enables novices to successfully complete realistic robotics tasks. This aligns with the value item Successful and its corresponding value Achievement from a ""Software User"" perspective because it demonstrates that the main contribution of 'Paper X' is empowering software users to achieve success in programming complex tasks without the need for expert programming skills.",Agreed-Justified,Agree,,Agree,,
Exp E,Exp J,2064,TSE,Software Development Methodologies,Comparing Block-Based Programming Models for Two-Armed Robots,"Modern industrial robots can work alongside human workers and coordinate with other robots. This means they can perform complex tasks, but doing so requires complex programming. Therefore, robots are typically programmed by experts, but there are not enough to meet the growing demand for robots. To reduce the need for experts, researchers have tried to make robot programming accessible to factory workers without programming experience. However, none of that previous work supports coordinating multiple robot arms that work on the same task. In this paper we present four block-based programming language designs that enable end-users to program two-armed robots. We analyze the benefits and trade-offs of each design on expressiveness and user cognition, and evaluate the designs based on a survey of 273 professional participants of whom 110 had no previous programming experience. We further present an interactive experiment based on a prototype implementation of the design we deem best. This experiment confirmed that novices can successfully use our prototype to complete realistic robotics tasks. This work contributes to making coordinated programming of robots accessible to end-users. It further explores how visual programming elements can make traditionally challenging programming tasks more beginner-friendly.",Conformity,Self-Discipline,The focus of Paper X on enabling factory workers without prior programming experience to learn complex programming tasks emphasises Self-Discipline under the value Conformity.,"In 'Paper X', the main contribution is the development of block-based programming language designs that enable factory workers without programming experience to program two-armed robots. This emphasis on enabling non-experts to learn complex programming tasks aligns with the value item Self-Discipline and its corresponding value Conformity from a ""Software User"" perspective. Self-discipline is demonstrated by the willingness of these non-experts to learn and engage in the programming process, while conformity refers to their adherence to the prescribed programming methods and guidelines set by the experts. By providing accessible programming tools to factory workers, 'Paper X' promotes self-discipline and conformity within the context of robotic programming.",Agreed-Clarified,Agree,,Agree,,
Exp F,Exp J,2066,TSE,Security & Privacy,Detecting Software Security Vulnerabilities Via Requirements Dependency Analysis,"Cyber attacks targeting software applications have a tremendous impact on our daily life. For example, attackers have utilized vulnerabilities of web applications to steal and gain unauthorized use of sensitive data stored in these systems. Previous studies indicate that security testing is highly precise, and therefore is widely applied to validate individual security requirements. However, dependencies between security requirements may cause additional vulnerabilities. Manual dependency detection faces scalability challenges, e.g., a previous study shows that the pairwise dependency analysis of 40 requirements would take around 12 hours. In this article, we present a novel approach which integrates the interdependency among high-level security requirements, such as those documented in policies, regulations, and standards. We then use automated requirements tracing methods to identify product-level security requirements and their dependencies. Our manual analysis of HIPAA and FIPS 200 leads to the identification of five types of high-level security requirements dependencies, which further inform the automated tracing methods and guide the designs of system-level security tests. Experimental results on five projects in healthcare and education domains show the significant recall improvements at 81 percent. Our case study on a deployed production system uncovers four previously unknown vulnerabilities by using the detected requirements dependencies as test paths, demonstrating our approach's value in connecting requirements engineering with security testing.",Security,Healthy,The paper develops a novel approach to enhance the security of software applications; which aligns with the value item 'Healthy' (ensuring the healthy functioning of the system by avoiding cyber attacks) and its corresponding value 'Security'.,"In the paper, the authors present a novel approach to enhance the security of software applications. This aligns with the value item 'Healthy' in Schwartz's Taxonomy, as ensuring the healthy functioning of the system involves protecting it from cyber attacks and vulnerabilities. This directly relates to the value of 'Security' in the context of a software user, as it aims to safeguard sensitive data and prevent unauthorized access. By addressing and mitigating security vulnerabilities, the paper contributes to maintaining the overall health and security of software systems, which is an essential concern for software users.",Agreed-Clarified,Agree,,Agree,,
Exp G,Exp K,2066,TSE,Security & Privacy,Detecting Software Security Vulnerabilities Via Requirements Dependency Analysis,"Cyber attacks targeting software applications have a tremendous impact on our daily life. For example, attackers have utilized vulnerabilities of web applications to steal and gain unauthorized use of sensitive data stored in these systems. Previous studies indicate that security testing is highly precise, and therefore is widely applied to validate individual security requirements. However, dependencies between security requirements may cause additional vulnerabilities. Manual dependency detection faces scalability challenges, e.g., a previous study shows that the pairwise dependency analysis of 40 requirements would take around 12 hours. In this article, we present a novel approach which integrates the interdependency among high-level security requirements, such as those documented in policies, regulations, and standards. We then use automated requirements tracing methods to identify product-level security requirements and their dependencies. Our manual analysis of HIPAA and FIPS 200 leads to the identification of five types of high-level security requirements dependencies, which further inform the automated tracing methods and guide the designs of system-level security tests. Experimental results on five projects in healthcare and education domains show the significant recall improvements at 81 percent. Our case study on a deployed production system uncovers four previously unknown vulnerabilities by using the detected requirements dependencies as test paths, demonstrating our approach's value in connecting requirements engineering with security testing.",Security,Social Order,The paper's methodology focuses on integrating high-level security requirements and their interdependencies; which aligns with the value item 'Social Order' (establishing an organized; systematic process of software security) and its corresponding value 'Security'.,"The justification for aligning 'Paper X' with the value item Social Order and its corresponding value Security is that the paper's approach of integrating high-level security requirements and identifying their dependencies contributes to establishing an organized and systematic process of software security. By addressing and tracing these dependencies, the paper aims to ensure the integrity and stability of software systems, promoting a sense of security for software users. This aligns with the value item Social Order, as it emphasizes the importance of maintaining a structured and well-regulated environment, which in this case, pertains to the security aspects of software applications.",Agreed-Justified,Disagree,Agree with coder_2,Disagree,Value healthy concerns Human health not software health,
Exp A,Exp H,2066,TSE,Security & Privacy,Detecting Software Security Vulnerabilities Via Requirements Dependency Analysis,"Cyber attacks targeting software applications have a tremendous impact on our daily life. For example, attackers have utilized vulnerabilities of web applications to steal and gain unauthorized use of sensitive data stored in these systems. Previous studies indicate that security testing is highly precise, and therefore is widely applied to validate individual security requirements. However, dependencies between security requirements may cause additional vulnerabilities. Manual dependency detection faces scalability challenges, e.g., a previous study shows that the pairwise dependency analysis of 40 requirements would take around 12 hours. In this article, we present a novel approach which integrates the interdependency among high-level security requirements, such as those documented in policies, regulations, and standards. We then use automated requirements tracing methods to identify product-level security requirements and their dependencies. Our manual analysis of HIPAA and FIPS 200 leads to the identification of five types of high-level security requirements dependencies, which further inform the automated tracing methods and guide the designs of system-level security tests. Experimental results on five projects in healthcare and education domains show the significant recall improvements at 81 percent. Our case study on a deployed production system uncovers four previously unknown vulnerabilities by using the detected requirements dependencies as test paths, demonstrating our approach's value in connecting requirements engineering with security testing.",Security,National Security,The paper's main contribution involves the uncovering of previously unknown vulnerabilities in a deployed production system; which aligns with the value item 'National Security' (assuming the system in question has a significant scale/importance) and its corresponding value 'Security'.,"The main contribution of ""Paper X"" involves the identification of previously unknown vulnerabilities in a deployed production system. The concept of national security involves protecting a nation's critical infrastructure, which can include software systems that are crucial for various sectors. By uncovering these vulnerabilities, the paper directly aligns with the value item of National Security and its corresponding value of Security, as it highlights the importance of ensuring the security and resilience of software systems used by a nation, ultimately contributing to the overall national security.",Agreed-Justified,Disagree,"Related to Achievement: capable more, as it automatically identifies vulnerabilities",Agree,,
Exp B,Exp H,2071,TSE,AI & Machine Learning,Towards Security Threats of Deep Learning Systems: A Survey,"Deep learning has gained tremendous success and great popularity in the past few years. However, deep learning systems are suffering several inherent weaknesses, which can threaten the security of learning models. Deep learningaEUR(tm)s wide use further magnifies the impact and consequences. To this end, lots of research has been conducted with the purpose of exhaustively identifying intrinsic weaknesses and subsequently proposing feasible mitigation. Yet few are clear about how these weaknesses are incurred and how effective these attack approaches are in assaulting deep learning. In order to unveil the security weaknesses and aid in the development of a robust deep learning system, we undertake an investigation on attacks towards deep learning, and analyze these attacks to conclude some findings in multiple views. In particular, we focus on four types of attacks associated with security threats of deep learning: model extraction attack, model inversion attack, poisoning attack and adversarial attack. For each type of attack, we construct its essential workflow as well as adversary capabilities and attack goals. Pivot metrics are devised for comparing the attack approaches, by which we perform quantitative and qualitative analyses. From the analysis, we have identified significant and indispensable factors in an attack vector, e.g., how to reduce queries to target models, what distance should be used for measuring perturbation. We shed light on 18 findings covering these approachesaEUR(tm) merits and demerits, success probability, deployment complexity and prospects. Moreover, we discuss other potential security weaknesses and possible mitigation which can inspire relevant research in this area.",Security,Social Order,The research focuses on analyzing and investigating security weaknesses in deep learning systems; aiming to develop a more robust system which aligns directly with the value of Security and its item; Social Order (represented as an orderly; secure digital environment for software users).,"In 'Paper X', the main focus is on the analysis and investigation of security weaknesses in deep learning systems, with the goal of developing a more robust system. This aligns directly with the value of Security and its item, Social Order, as it aims to ensure an orderly and secure digital environment for software users. By addressing the inherent weaknesses of deep learning systems, the paper contributes to creating a sense of security and stability for users, which is crucial in the software context.",Agreed-Clarified,Disagree,reconcile with coder_2,Disagree,The justification doesn't align with the value item of the social order,
Exp C,Exp K,2071,TSE,AI & Machine Learning,Towards Security Threats of Deep Learning Systems: A Survey,"Deep learning has gained tremendous success and great popularity in the past few years. However, deep learning systems are suffering several inherent weaknesses, which can threaten the security of learning models. Deep learningaEUR(tm)s wide use further magnifies the impact and consequences. To this end, lots of research has been conducted with the purpose of exhaustively identifying intrinsic weaknesses and subsequently proposing feasible mitigation. Yet few are clear about how these weaknesses are incurred and how effective these attack approaches are in assaulting deep learning. In order to unveil the security weaknesses and aid in the development of a robust deep learning system, we undertake an investigation on attacks towards deep learning, and analyze these attacks to conclude some findings in multiple views. In particular, we focus on four types of attacks associated with security threats of deep learning: model extraction attack, model inversion attack, poisoning attack and adversarial attack. For each type of attack, we construct its essential workflow as well as adversary capabilities and attack goals. Pivot metrics are devised for comparing the attack approaches, by which we perform quantitative and qualitative analyses. From the analysis, we have identified significant and indispensable factors in an attack vector, e.g., how to reduce queries to target models, what distance should be used for measuring perturbation. We shed light on 18 findings covering these approachesaEUR(tm) merits and demerits, success probability, deployment complexity and prospects. Moreover, we discuss other potential security weaknesses and possible mitigation which can inspire relevant research in this area.",Security,Healthy,The study is designed to identify potential threats to deep learning systems; once mitigated these threats result in a 'healthier'; more secure system for users aligning with value item Healthy (observed in context of a secure digital environment) under the value Security.,"In the context of a software user, the alignment of 'Paper X' with the value item Healthy and its corresponding value Security can be justified based on the fact that the paper aims to identify and mitigate potential threats to deep learning systems. By addressing these weaknesses and strengthening the security of the learning models, the paper contributes to creating a more secure and reliable software environment for users. This aligns with the value item Healthy, as it emphasizes the importance of maintaining a secure and protected digital ecosystem for users to interact with.",Agreed-Clarified,Agree,,Agree,,
Exp D,Exp K,2071,TSE,AI & Machine Learning,Towards Security Threats of Deep Learning Systems: A Survey,"Deep learning has gained tremendous success and great popularity in the past few years. However, deep learning systems are suffering several inherent weaknesses, which can threaten the security of learning models. Deep learningaEUR(tm)s wide use further magnifies the impact and consequences. To this end, lots of research has been conducted with the purpose of exhaustively identifying intrinsic weaknesses and subsequently proposing feasible mitigation. Yet few are clear about how these weaknesses are incurred and how effective these attack approaches are in assaulting deep learning. In order to unveil the security weaknesses and aid in the development of a robust deep learning system, we undertake an investigation on attacks towards deep learning, and analyze these attacks to conclude some findings in multiple views. In particular, we focus on four types of attacks associated with security threats of deep learning: model extraction attack, model inversion attack, poisoning attack and adversarial attack. For each type of attack, we construct its essential workflow as well as adversary capabilities and attack goals. Pivot metrics are devised for comparing the attack approaches, by which we perform quantitative and qualitative analyses. From the analysis, we have identified significant and indispensable factors in an attack vector, e.g., how to reduce queries to target models, what distance should be used for measuring perturbation. We shed light on 18 findings covering these approachesaEUR(tm) merits and demerits, success probability, deployment complexity and prospects. Moreover, we discuss other potential security weaknesses and possible mitigation which can inspire relevant research in this area.",Achievement,Capable,By identifying and proposing solutions to security weaknesses in deep learning; the paper potentially enables users to better utilize these systems successfully; relating to the Achievement value and its Capable value item (in the context of user's ability to successfully use the software).,"Through identifying and addressing security weaknesses in deep learning, ""Paper X"" contributes to the achievement value of users within a software context by enabling them to successfully utilize these systems. By proposing feasible mitigation strategies, the paper enhances users' capability to protect their learning models and ensure the reliability and effectiveness of their deep learning systems. This alignment with the capable value item directly addresses the user's ability to achieve their goals and successfully utilize the software, highlighting the relevance of ""Paper X"" in enhancing user achievement in the context of software usage.",Agreed-Justified,Agree,,Agree,,
Exp E,Exp J,2074,TSE,Software Development Methodologies,Human Values in Software Engineering: Contrasting Case Studies of Practice,"The growing diffusion of software in society and its influence on people demands from its creators that their work carefully considers human values such as transparency, social responsibility, and equality. But how do software practitioners address human values in software engineering practice? We interviewed 31 software practitioners from two organizations, each having a strong values framework, with the aim to understand: (a) practitionersaEUR(tm) perceptions of human values and their role in software engineering; (b) practices that practitioners use to address human values in software; and (c) challenges they face during this process. We report our findings from two contrasting case organizations on how practitioners aEURoeengineeraEUR values in their unique organizational settings. We found evidence that organizational culture significantly contributes to how values are addressed in software. We summarize recommendations from the practitioners to support proactive engineering of values-conscious software.",Self Direction,Independent,The paper explores how software practitioners address human values like transparency; social responsibility; and equality in their work and thus contributes to the independent judgement and actions of the software user.,"The paper specifically addresses the issue of software practitioners addressing human values such as transparency, social responsibility, and equality. These values align with the value item Independent and its corresponding value Self Direction from Schwartz's Taxonomy. By focusing on these values, the paper emphasizes the importance of independent judgment and actions for software users. It highlights the need for software to consider and accommodate these values, thus empowering users to make informed decisions and take control over their software experiences.",Agreed-Clarified,Agree,,Agree,,
Exp F,Exp J,2074,TSE,Software Development Methodologies,Human Values in Software Engineering: Contrasting Case Studies of Practice,"The growing diffusion of software in society and its influence on people demands from its creators that their work carefully considers human values such as transparency, social responsibility, and equality. But how do software practitioners address human values in software engineering practice? We interviewed 31 software practitioners from two organizations, each having a strong values framework, with the aim to understand: (a) practitionersaEUR(tm) perceptions of human values and their role in software engineering; (b) practices that practitioners use to address human values in software; and (c) challenges they face during this process. We report our findings from two contrasting case organizations on how practitioners aEURoeengineeraEUR values in their unique organizational settings. We found evidence that organizational culture significantly contributes to how values are addressed in software. We summarize recommendations from the practitioners to support proactive engineering of values-conscious software.",Benevolence,Honesty,The paper reports on software practitioners' perceptions of human values and their role in software engineering; aligning with the value item Honesty in providing a true account of the actual practices and beliefs in software creation.,"In ""Paper X"", the alignment with the value item Honesty and its corresponding value Benevolence from a ""Software User"" perspective is justified by the fact that the paper reports on software practitioners' perceptions of human values and their role in software engineering. This aligns with Honesty as it provides a true account of the practices and beliefs in software creation, which promotes the value of Benevolence by ensuring that software developers act in the best interest of users and society.",Agreed-Clarified,Agree,,Agree,,
Exp G,Exp K,2074,TSE,Software Development Methodologies,Human Values in Software Engineering: Contrasting Case Studies of Practice,"The growing diffusion of software in society and its influence on people demands from its creators that their work carefully considers human values such as transparency, social responsibility, and equality. But how do software practitioners address human values in software engineering practice? We interviewed 31 software practitioners from two organizations, each having a strong values framework, with the aim to understand: (a) practitionersaEUR(tm) perceptions of human values and their role in software engineering; (b) practices that practitioners use to address human values in software; and (c) challenges they face during this process. We report our findings from two contrasting case organizations on how practitioners aEURoeengineeraEUR values in their unique organizational settings. We found evidence that organizational culture significantly contributes to how values are addressed in software. We summarize recommendations from the practitioners to support proactive engineering of values-conscious software.",Universalism,Equality,The important emphasis on equality in software engineering; as stated in the paper; aligns with the value item Equality under the value Universalism; indicating a commitment to ensuring fairness in software access and use.,"In 'Paper X', the abstract mentions that the growing diffusion of software in society demands that software creators consider human values such as equality. This aligns with the value item Equality and its corresponding value Universalism because the paper emphasizes the importance of ensuring fairness in software access and use, indicating a commitment to promoting equality in the software context.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2078,TSE,AI & Machine Learning,A Wizard of Oz Study Simulating API Usage Dialogues With a Virtual Assistant,"Virtual Assistant technology is rapidly proliferating to improve productivity in a variety of tasks. While several virtual assistants for everyday tasks are well-known (e.g., Siri, Cortana, Alexa), assistants for specialty tasks such as software engineering are rarer. One key reason software engineering assistants are rare is that very few experimental datasets are available and suitable for training the AI that is the bedrock of current virtual assistants. In this paper, we present a set of Wizard of Oz experiments that we designed to build a dataset for creating a virtual assistant. Our target is a hypothetical virtual assistant for helping programmers use APIs. In our experiments, we recruited 30 professional programmers to complete programming tasks using two APIs. The programmers interacted with a simulated virtual assistant for help aEUR"" the programmers were not aware that the assistant was actually operated by human experts. We then annotated the dialogue acts in the corpus along four dimensions: illocutionary intent, API information type(s), backward-facing function, and traceability to specific API components. We observed a diverse range of interactions that will facilitate the development of dialogue strategies for virtual assistants for API usage.",Achievement,Intelligent,"The paper focuses on creating a virtual assistant to help programmers use APIs more proficiently which is directly aligned with the value item ""Intelligent"" under the value ""Achievement"". This is because an intelligent user would use assistance to better their skills and gain more knowledge.","The justification provided aligning 'Paper X' with the value item Intelligent and its corresponding value Achievement from a ""Software User"" perspective is that the creation of a virtual assistant to help programmers use APIs more proficiently demonstrates intelligence in the software user's desire to improve their skills and gain knowledge. By utilizing the virtual assistant as a tool for achieving their goals, the software user showcases their ambition and capability to succeed in their software engineering tasks, thus aligning with the value of Achievement.",Agreed-Justified,Agree,,Agree,,
Exp B,Exp H,2078,TSE,AI & Machine Learning,A Wizard of Oz Study Simulating API Usage Dialogues With a Virtual Assistant,"Virtual Assistant technology is rapidly proliferating to improve productivity in a variety of tasks. While several virtual assistants for everyday tasks are well-known (e.g., Siri, Cortana, Alexa), assistants for specialty tasks such as software engineering are rarer. One key reason software engineering assistants are rare is that very few experimental datasets are available and suitable for training the AI that is the bedrock of current virtual assistants. In this paper, we present a set of Wizard of Oz experiments that we designed to build a dataset for creating a virtual assistant. Our target is a hypothetical virtual assistant for helping programmers use APIs. In our experiments, we recruited 30 professional programmers to complete programming tasks using two APIs. The programmers interacted with a simulated virtual assistant for help aEUR"" the programmers were not aware that the assistant was actually operated by human experts. We then annotated the dialogue acts in the corpus along four dimensions: illocutionary intent, API information type(s), backward-facing function, and traceability to specific API components. We observed a diverse range of interactions that will facilitate the development of dialogue strategies for virtual assistants for API usage.",Achievement,Successful,"The paper aims to assist software users in successfully completing programming tasks with the help of a virtual assistant. This speaks to the value item ""Successful"" under the value ""Achievement""; as it empowers the user to succeed in their tasks.","In the context of the abstract, the paper introduces a virtual assistant designed to help programmers successfully complete programming tasks using APIs. By providing this assistance, the paper aligns with the value item ""Successful"" under the value ""Achievement"" from the perspective of a software user. The virtual assistant empowers the user to achieve their goals and succeed in their programming tasks, ultimately contributing to their sense of accomplishment and fulfillment.",Agreed-Justified,Agree,,Agree,,
Exp C,Exp K,2078,TSE,AI & Machine Learning,A Wizard of Oz Study Simulating API Usage Dialogues With a Virtual Assistant,"Virtual Assistant technology is rapidly proliferating to improve productivity in a variety of tasks. While several virtual assistants for everyday tasks are well-known (e.g., Siri, Cortana, Alexa), assistants for specialty tasks such as software engineering are rarer. One key reason software engineering assistants are rare is that very few experimental datasets are available and suitable for training the AI that is the bedrock of current virtual assistants. In this paper, we present a set of Wizard of Oz experiments that we designed to build a dataset for creating a virtual assistant. Our target is a hypothetical virtual assistant for helping programmers use APIs. In our experiments, we recruited 30 professional programmers to complete programming tasks using two APIs. The programmers interacted with a simulated virtual assistant for help aEUR"" the programmers were not aware that the assistant was actually operated by human experts. We then annotated the dialogue acts in the corpus along four dimensions: illocutionary intent, API information type(s), backward-facing function, and traceability to specific API components. We observed a diverse range of interactions that will facilitate the development of dialogue strategies for virtual assistants for API usage.",Stimulation,Daring,"By recruiting programmers to interact with a simulated virtual assistant; the paper fosters ""Daring""; a value item under ""Stimulation"". This reflects the users' willingness to interact with a new; unknown assistant and potentially enhance their software usage and development experience.","In 'Paper X', the recruitment of programmers to interact with a simulated virtual assistant for help in using APIs fosters the value item of ""Daring"" and its corresponding value ""Stimulation"". This is because the programmers are willing to engage with a new and unknown assistant, which indicates their openness to explore new technologies and potentially enhance their software usage and development experience. This aligns with the value of seeking excitement and novel experiences, which is an important aspect of stimulation.",Agreed-Justified,Agree,,Agree,,
Exp E,Exp J,2091,TSE,Software Development Methodologies,The Effects of Human Aspects on the Requirements Engineering Process: A Systematic Literature Review,"Requirements Engineering (RE) requires the collaboration of various roles in SE, such as requirements engineers, stakeholders and other developers, and it is thus a very highly human dependent process in software engineering (SE). Identifying how aEURoehuman aspectsaEUR aEUR"" such as personality, motivation, emotions, communication, gender, culture and geographic distribution aEUR"" might impact on the RE process would assist us in better supporting successful RE. The main objective of this paper is to systematically review primary studies that have investigated the effects of various human aspects on the RE process. We wanted to identify if any critical human aspects have been found, and what might be the relationships between different human aspects impacting the RE process. A systematic literature review (SLR) was conducted and identified 474 initial primary research studies. These were eventually filtered down to 74 relevant, high-quality primary studies. No primary study to date was found to focus on identifying what are the most influential human aspects on the RE process. Among the studied human aspects, the effects of communication have been considered in many studies of RE. Other human aspects such as personality, motivation and gender have mainly been investigated to date in relation to more general SE studies that include RE as one phase. Findings show that studying more than one human aspect together is beneficial, as this reveals relationships between various human aspects and how they together impact the RE process. However, the majority of these studied combinations of human aspects are unique. From 56.8 percent of studies that identified the effects of human aspects on RE, 40.5 percent identified the positive impact, 30.9 percent negative, 26.2 percent identified both impacts whereas 2.3 percent mentioned that there was no impact. This implies that a variety of human aspects positively or negatively affects the RE process and a well-defined theoretical analysis on the effects of different human aspects on RE remains to be defined and practically evaluated. The findings of this SLR help researchers who are investigating the impact of various human aspects on the RE process by identifying well-studied research areas, and highlight new areas that should be focused on in future research.",Tradition,Humility,The paper's focus on understanding the impacts of human aspects such as motivation; emotion; and gender requires a degree of humility; as it requires acknowledging the significance of these aspects in the highly technical field of software engineering.,"The justification for aligning 'Paper X' with the value item Humility and its corresponding value Tradition is based on the acknowledgement that the study of human aspects in the field of software engineering requires a humble approach. The paper aims to understand the impacts of various human aspects on the requirements engineering process, which involves collaboration between different roles. By recognizing the significance of these human aspects and their potential impact on software engineering, the paper promotes an attitude of humility and respect for the complexity of the field. This aligns with the value of Tradition, which emphasizes the importance of respecting and valuing established practices and knowledge. Therefore, the alignment between 'Paper X' and Humility/Tradition is justified by the recognition of the need for humility in studying and understanding the impacts of human aspects in software engineering.",Agreed-Clarified,Agree,,Agree,,
Exp F,Exp J,2091,TSE,Software Development Methodologies,The Effects of Human Aspects on the Requirements Engineering Process: A Systematic Literature Review,"Requirements Engineering (RE) requires the collaboration of various roles in SE, such as requirements engineers, stakeholders and other developers, and it is thus a very highly human dependent process in software engineering (SE). Identifying how aEURoehuman aspectsaEUR aEUR"" such as personality, motivation, emotions, communication, gender, culture and geographic distribution aEUR"" might impact on the RE process would assist us in better supporting successful RE. The main objective of this paper is to systematically review primary studies that have investigated the effects of various human aspects on the RE process. We wanted to identify if any critical human aspects have been found, and what might be the relationships between different human aspects impacting the RE process. A systematic literature review (SLR) was conducted and identified 474 initial primary research studies. These were eventually filtered down to 74 relevant, high-quality primary studies. No primary study to date was found to focus on identifying what are the most influential human aspects on the RE process. Among the studied human aspects, the effects of communication have been considered in many studies of RE. Other human aspects such as personality, motivation and gender have mainly been investigated to date in relation to more general SE studies that include RE as one phase. Findings show that studying more than one human aspect together is beneficial, as this reveals relationships between various human aspects and how they together impact the RE process. However, the majority of these studied combinations of human aspects are unique. From 56.8 percent of studies that identified the effects of human aspects on RE, 40.5 percent identified the positive impact, 30.9 percent negative, 26.2 percent identified both impacts whereas 2.3 percent mentioned that there was no impact. This implies that a variety of human aspects positively or negatively affects the RE process and a well-defined theoretical analysis on the effects of different human aspects on RE remains to be defined and practically evaluated. The findings of this SLR help researchers who are investigating the impact of various human aspects on the RE process by identifying well-studied research areas, and highlight new areas that should be focused on in future research.",Benevolence,Responsibility,The paper contributes towards the responsibility of software by improving the RE process through better understanding of human aspects. The responsible handling of these aspects in software design can directly affect the users' experiences.,"In 'Paper X,' the authors aim to understand how various human aspects can impact the requirements engineering (RE) process in software engineering. This understanding contributes towards the responsibility of software by improving the RE process through a better comprehension of these human aspects. By considering personality, motivation, emotions, communication, gender, culture, and geographic distribution, the authors acknowledge the importance of taking into account diverse user needs and experiences. This aligns with the value item Responsibility and its corresponding value Benevolence from Schwartz's Taxonomy, as it emphasizes the responsible handling of human aspects in software design to ensure users' wellbeing and satisfaction.",Agreed-Justified,Agree,,Agree,,
Exp G,Exp K,2095,TSE,Software Project Management,Are You Still Working on This? An Empirical Study on Pull Request Abandonment,"The great success of numerous community-based open source software (OSS) is based on volunteers continuously submitting contributions, but ensuring sustainability is a persistent challenge in OSS communities. Although the motivations behind and barriers to OSS contributorsaEUR(tm) joining and retention have been extensively studied, the impacts of, reasons for and solutions to contribution abandonment at the individual level have not been well studied, especially for pull-based development. To bridge this gap, we present an empirical study on pull request abandonment based on a sizable dataset. We manually examine 321 abandoned pull requests on GitHub and then quantify the manual observations by surveying 710 OSS developers. We find that while the lack of integratorsaEUR(tm) responsiveness and the lack of contributorsaEUR(tm) time and interest remain the main reasons that deter contributors from participation, limitations during the processes of patch updating and consensus reaching can also cause abandonment. We also show the significant impacts of pull request abandonment on project management and maintenance. Moreover, we elucidate the strategies used by project integrators to cope with abandoned pull requests and highlight the need for a practical handover mechanism. We discuss the actionable suggestions and implications for OSS practitioners and tool builders, which can help to upgrade the infrastructure and optimize the mechanisms of OSS communities.",Self Direction,Independent,The paper discusses the motivation and barriers of individual contributors to open source communities; which relates to the value item Independent and its corresponding value Self Direction.,"The paper's examination of the motivations and barriers that individual contributors face in open source communities aligns with the value item Independent and its corresponding value Self Direction from a ""Software User"" perspective because it addresses the importance of individuals being able to make independent choices and pursue their own goals within the context of contributing to open source projects. By understanding the factors that influence individual contributors' decisions to join and remain in these communities, the paper acknowledges the value of autonomy and self-direction in software development.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2095,TSE,Software Project Management,Are You Still Working on This? An Empirical Study on Pull Request Abandonment,"The great success of numerous community-based open source software (OSS) is based on volunteers continuously submitting contributions, but ensuring sustainability is a persistent challenge in OSS communities. Although the motivations behind and barriers to OSS contributorsaEUR(tm) joining and retention have been extensively studied, the impacts of, reasons for and solutions to contribution abandonment at the individual level have not been well studied, especially for pull-based development. To bridge this gap, we present an empirical study on pull request abandonment based on a sizable dataset. We manually examine 321 abandoned pull requests on GitHub and then quantify the manual observations by surveying 710 OSS developers. We find that while the lack of integratorsaEUR(tm) responsiveness and the lack of contributorsaEUR(tm) time and interest remain the main reasons that deter contributors from participation, limitations during the processes of patch updating and consensus reaching can also cause abandonment. We also show the significant impacts of pull request abandonment on project management and maintenance. Moreover, we elucidate the strategies used by project integrators to cope with abandoned pull requests and highlight the need for a practical handover mechanism. We discuss the actionable suggestions and implications for OSS practitioners and tool builders, which can help to upgrade the infrastructure and optimize the mechanisms of OSS communities.",Benevolence,Helpful,The paper discusses the contribution abandonment in open source projects; which necessitates other contributors' help to continue the work. This aligns with the value item Helpful and its corresponding value Benevolence.,"The paper's focus on the issue of contribution abandonment in open source projects directly aligns with the value item Helpful and its corresponding value Benevolence from a ""Software User"" perspective. By investigating the reasons for contribution abandonment and offering strategies to cope with it, the paper emphasizes the importance of helping others in the software community to ensure the sustainability of open source projects. This aligns with the value of Benevolence, which promotes the idea of being helpful and supportive towards others.",Agreed-Clarified,Agree,,Agree,,
Exp B,Exp H,2095,TSE,Software Project Management,Are You Still Working on This? An Empirical Study on Pull Request Abandonment,"The great success of numerous community-based open source software (OSS) is based on volunteers continuously submitting contributions, but ensuring sustainability is a persistent challenge in OSS communities. Although the motivations behind and barriers to OSS contributorsaEUR(tm) joining and retention have been extensively studied, the impacts of, reasons for and solutions to contribution abandonment at the individual level have not been well studied, especially for pull-based development. To bridge this gap, we present an empirical study on pull request abandonment based on a sizable dataset. We manually examine 321 abandoned pull requests on GitHub and then quantify the manual observations by surveying 710 OSS developers. We find that while the lack of integratorsaEUR(tm) responsiveness and the lack of contributorsaEUR(tm) time and interest remain the main reasons that deter contributors from participation, limitations during the processes of patch updating and consensus reaching can also cause abandonment. We also show the significant impacts of pull request abandonment on project management and maintenance. Moreover, we elucidate the strategies used by project integrators to cope with abandoned pull requests and highlight the need for a practical handover mechanism. We discuss the actionable suggestions and implications for OSS practitioners and tool builders, which can help to upgrade the infrastructure and optimize the mechanisms of OSS communities.",Achievement,Successful,The paper discusses the project management and maintenance in the context of contribution abandonment; which emphasizes the need for software users to be successful in their contributions. This aligns with the value item Successful; and its corresponding value Achievement.,"The paper discusses the impact of pull request abandonment on project management and maintenance. This implies that for software users who are involved in contributing to open-source projects, their success in making contributions is dependent on the project being effectively managed and maintained. This aligns with the value item Successful, as software users want their contributions to be successful and have a positive impact on the project. Therefore, the paper's focus on project management and maintenance directly aligns with the value item Successful and its corresponding value Achievement from a software user's perspective.",Agreed-Justified,Agree,,Agree,,
Exp C,Exp K,2096,TSE,Emerging Technologies,DefectChecker: Automated Smart Contract Defect Detection by Analyzing EVM Bytecode,"Smart contracts are Turing-complete programs running on the blockchain. They are immutable and cannot be modified, even when bugs are detected. Therefore, ensuring smart contracts are bug-free and well-designed before deploying them to the blockchain is extremely important. A contract defect is an error, flaw or fault in a smart contract that causes it to produce an incorrect or unexpected result, or to behave in unintended ways. Detecting and removing contract defects can avoid potential bugs and make programs more robust. Our previous work defined 20 contract defects for smart contracts and divided them into five impact levels. According to our classification, contract defects with seriousness level between 1-3 can lead to unwanted behaviors, e.g., a contract being controlled by attackers. In this paper, we propose DefectChecker, a symbolic execution-based approach and tool to detect eight contract defects that can cause unwanted behaviors of smart contracts on the Ethereum blockchain platform. DefectChecker can detect contract defects from smart contractsaEUR(tm) bytecode. We verify the performance of DefectChecker by applying it to an open-source dataset. Our evaluation results show that DefectChecker obtains a high F-score (88.8 percent in the whole dataset) and only requires 0.15s to analyze one smart contract on average. We also applied DefectChecker to 165,621 distinct smart contracts on the Ethereum platform. We found that 25,815 of these smart contracts contain at least one of the contract defects that belongs to impact level 1-3, including some real-world attacks.",Self Direction,Freedom,The paper contributes to the value item 'Freedom' which is part of the 'Self Direction' value as it ensures that software users have the freedom from misuse or control of smart contracts by unauthorized entities by detecting and removing defects in them.,"In 'Paper X', the proposed approach and tool, DefectChecker, directly align with the value item Freedom and its corresponding value Self Direction from a ""Software User"" perspective. By detecting and removing contract defects, DefectChecker ensures that software users have the freedom from misuse or control of smart contracts by unauthorized entities. This aligns with the value item Freedom, as it empowers software users to have independent control over their smart contracts and protects them from unwanted behaviors or attacks.",Agreed-Clarified,Agree,,Agree,,
Exp E,Exp J,2096,TSE,Emerging Technologies,DefectChecker: Automated Smart Contract Defect Detection by Analyzing EVM Bytecode,"Smart contracts are Turing-complete programs running on the blockchain. They are immutable and cannot be modified, even when bugs are detected. Therefore, ensuring smart contracts are bug-free and well-designed before deploying them to the blockchain is extremely important. A contract defect is an error, flaw or fault in a smart contract that causes it to produce an incorrect or unexpected result, or to behave in unintended ways. Detecting and removing contract defects can avoid potential bugs and make programs more robust. Our previous work defined 20 contract defects for smart contracts and divided them into five impact levels. According to our classification, contract defects with seriousness level between 1-3 can lead to unwanted behaviors, e.g., a contract being controlled by attackers. In this paper, we propose DefectChecker, a symbolic execution-based approach and tool to detect eight contract defects that can cause unwanted behaviors of smart contracts on the Ethereum blockchain platform. DefectChecker can detect contract defects from smart contractsaEUR(tm) bytecode. We verify the performance of DefectChecker by applying it to an open-source dataset. Our evaluation results show that DefectChecker obtains a high F-score (88.8 percent in the whole dataset) and only requires 0.15s to analyze one smart contract on average. We also applied DefectChecker to 165,621 distinct smart contracts on the Ethereum platform. We found that 25,815 of these smart contracts contain at least one of the contract defects that belongs to impact level 1-3, including some real-world attacks.",Security,Healthy,The work also aligns with the value item 'Healthy' under the 'Security' value as it ensures the healthy functioning of smart contracts; a core part of the software's operation; by identifying and resolving defects and potential bugs.,"The alignment of 'Paper X' with the value item 'Healthy' and its corresponding value 'Security' is justified by the fact that the paper focuses on detecting and removing contract defects in smart contracts. By ensuring the bug-free and well-designed functioning of smart contracts, it contributes to the security and healthy operation of the software. This directly aligns with the value of security, which in turn promotes the overall health and reliability of the software system for the software users.",Agreed-Justified,Agree,,Agree,,
Exp F,Exp J,2105,TSE,Security & Privacy,A Study of C/C++ Code Weaknesses on Stack Overflow,"Stack Overflow hosts millions of solutions that aim to solve developersaEUR(tm) programming issues. In this crowdsourced question answering process, Stack Overflow becomes a code hosting website where developers actively share its code. However, code snippets on Stack Overflow may contain security vulnerabilities, and if shared carelessly, such snippets can introduce security problems in software systems. In this paper, we empirically study the prevalence of the Common Weakness Enumeration aEUR"" CWE, in code snippets of C/C++ related answers. We explore the characteristics of $Code_w$Codew, i.e., code snippets that have CWE instances, in terms of the types of weaknesses, the evolution of $Code_w$Codew, and who contributed such code snippets. We find that: 1) 36 percent (i.e., 32 out of 89) CWE types are detected in $Code_w$Codew on Stack Overflow. Particularly, CWE-119, i.e., improper restriction of operations within the bounds of a memory buffer, is common in both answer code snippets and real-world software systems. Furthermore, the proportion of $Code_w$Codew doubled from 2008 to 2018 after normalizing by the total number of C/C++ snippets in each year. 2) In general, code revisions are associated with a reduction in the number of code weaknesses. However, the majority of $Code_w$Codew had weaknesses introduced in the first version of the code, and these $Code_w$Codew were never revised since then. Only 7.5 percent of users who contributed C/C++ code snippets posted or edited code with weaknesses. Users contributed less code with CWE weakness when they were more active (i.e., they either revised more code snippets or had a higher reputation). We also find that some users tended to have the same CWE type repeatedly in their various code snippets. Our empirical study provides insights to users who share code snippets on Stack Overflow so that they are aware of the potential security issues. To understand the community feedback about improving code weaknesses by answer revisions, we also conduct a qualitative study and find that 62.5 percent of our suggested revisions are adopted by the community. Stack Overflow can perform CWE scanning for all the code that is hosted on its platform. Further research is needed to improve the quality of the crowdsourced knowledge on Stack Overflow.",Security,Healthy,The paper's main aim is to identify and mitigate security vulnerabilities in code snippets on Stack Overflow; demonstrating a commitment towards keeping the software healthy and free from damaging issues.,"In 'Paper X,' the main contribution is focused on identifying and addressing security vulnerabilities in code snippets on Stack Overflow. By doing so, the paper aims to ensure the overall health and security of the software. This aligns with the value item ""Healthy"" from Schwartz's Taxonomy, as it demonstrates a commitment towards maintaining the well-being and integrity of the software system. This commitment to security and health can be seen as a reflection of the value of ""Security,"" which emphasizes the importance of protecting the software system from potential threats and damages. Therefore, the alignment with the value item Healthy and its corresponding value Security is evident in the paper's objective to mitigate security vulnerabilities in software systems and ensure their healthy functioning.",Agreed-Justified,Agree,,Agree,,
Exp G,Exp K,2105,TSE,Security & Privacy,A Study of C/C++ Code Weaknesses on Stack Overflow,"Stack Overflow hosts millions of solutions that aim to solve developersaEUR(tm) programming issues. In this crowdsourced question answering process, Stack Overflow becomes a code hosting website where developers actively share its code. However, code snippets on Stack Overflow may contain security vulnerabilities, and if shared carelessly, such snippets can introduce security problems in software systems. In this paper, we empirically study the prevalence of the Common Weakness Enumeration aEUR"" CWE, in code snippets of C/C++ related answers. We explore the characteristics of $Code_w$Codew, i.e., code snippets that have CWE instances, in terms of the types of weaknesses, the evolution of $Code_w$Codew, and who contributed such code snippets. We find that: 1) 36 percent (i.e., 32 out of 89) CWE types are detected in $Code_w$Codew on Stack Overflow. Particularly, CWE-119, i.e., improper restriction of operations within the bounds of a memory buffer, is common in both answer code snippets and real-world software systems. Furthermore, the proportion of $Code_w$Codew doubled from 2008 to 2018 after normalizing by the total number of C/C++ snippets in each year. 2) In general, code revisions are associated with a reduction in the number of code weaknesses. However, the majority of $Code_w$Codew had weaknesses introduced in the first version of the code, and these $Code_w$Codew were never revised since then. Only 7.5 percent of users who contributed C/C++ code snippets posted or edited code with weaknesses. Users contributed less code with CWE weakness when they were more active (i.e., they either revised more code snippets or had a higher reputation). We also find that some users tended to have the same CWE type repeatedly in their various code snippets. Our empirical study provides insights to users who share code snippets on Stack Overflow so that they are aware of the potential security issues. To understand the community feedback about improving code weaknesses by answer revisions, we also conduct a qualitative study and find that 62.5 percent of our suggested revisions are adopted by the community. Stack Overflow can perform CWE scanning for all the code that is hosted on its platform. Further research is needed to improve the quality of the crowdsourced knowledge on Stack Overflow.",Security,Social Order,By working to minimise the security problems in software systems; the paper showcase concern for maintaining social order; as it discourages careless sharing of susceptible code snippets that can disrupt software functioning and user experience.,"The justification for aligning 'Paper X' with the value item Social Order and its corresponding value Security from a ""Software User"" perspective is that the paper addresses the issue of security vulnerabilities present in code snippets shared on Stack Overflow, which can potentially disrupt the functioning of software systems and compromise the user experience. By actively studying and identifying weaknesses in the code snippets, the paper contributes to minimizing these security problems, ultimately promoting social order within the software context. It emphasizes the importance of responsible and cautious sharing of code snippets to maintain the integrity and stability of software systems, aligning with the value of Security and its manifestation in Social Order.",Agreed-Justified,Agree,,Agree,,
Exp B,Exp H,2112,TSE,Software Engineering Practices,Engineering Impacts of Anonymous Author Code Review: A Field Experiment,"Code review is a powerful technique to ensure high quality software and spread knowledge of best coding practices between engineers. Unfortunately, code reviewers may have biases about authors of the code they are reviewing, which can lead to inequitable experiences and outcomes. In principle, anonymous author code review can reduce the impact of such biases by withholding an author's identity from a reviewer. In this paper, to understand the engineering effects of using author anonymous code review in a practical setting, we applied the technique to 5217 code reviews performed by 300 software engineers at Google. Our results suggest that during anonymous author code review, reviewers can frequently guess authorsaEUR(tm) identities; that focus is reduced on reviewer-author power dynamics; and that the practice poses a barrier to offline, high-bandwidth conversations. Based on our findings, we recommend that those who choose to implement anonymous author code review should reveal the time zone of the author by default, have a break-the-glass option for revealing author identity, and reveal author identity directly after the review.",Power,Authority,The paper revolves around the concept of author anonymous code review that directly influences the power dynamics of reviewers and authors. It aligns with the value item Authority and the corresponding value Power.,"In 'Paper X', the concept of author anonymous code review is explored, which directly impacts the power dynamics between code reviewers and authors. By implementing anonymous author code review, the authority of reviewers is reduced, as their judgment and biases towards specific authors are minimized. This aligns with the value item Authority and its corresponding value Power, as the paper aims to address and mitigate the influence of power dynamics within the code review process.",Agreed-Clarified,Agree,,Agree,,
Exp D,Exp K,2112,TSE,Software Engineering Practices,Engineering Impacts of Anonymous Author Code Review: A Field Experiment,"Code review is a powerful technique to ensure high quality software and spread knowledge of best coding practices between engineers. Unfortunately, code reviewers may have biases about authors of the code they are reviewing, which can lead to inequitable experiences and outcomes. In principle, anonymous author code review can reduce the impact of such biases by withholding an author's identity from a reviewer. In this paper, to understand the engineering effects of using author anonymous code review in a practical setting, we applied the technique to 5217 code reviews performed by 300 software engineers at Google. Our results suggest that during anonymous author code review, reviewers can frequently guess authorsaEUR(tm) identities; that focus is reduced on reviewer-author power dynamics; and that the practice poses a barrier to offline, high-bandwidth conversations. Based on our findings, we recommend that those who choose to implement anonymous author code review should reveal the time zone of the author by default, have a break-the-glass option for revealing author identity, and reveal author identity directly after the review.",Universalism,Social Justice,The paper explores the potential of anonymous author code review to reduce biases and ensuring fairness - aligning with the value item Social Justice and its corresponding value Universalism.,"The justification for aligning 'Paper X' with the value item Social Justice and its corresponding value Universalism is based on the fact that the paper addresses the potential for bias reduction and fairness through the use of anonymous author code review. By ensuring that code reviewers cannot identify the authors, the paper aims to create an environment where equal treatment and opportunities are provided to all software users, aligning with the principles of social justice and the universalistic value of fairness and equality.",Agreed-Clarified,Agree,,Agree,,
Exp E,Exp J,2118,TSE,Algorithms,A Compositional Approach for Complex Event Pattern Modeling and Transformation to Colored Petri Nets with Black Sequencing Transitions,"Prioritized Colored Petri Nets (PCPNs) are a well-known extension of plain Petri nets in which transitions can have priorities and the tokens on the places carry data information. In this paper, we propose an extension of the PCPN model with black sequencing transitions (BPCPN). This extension allows us to easily model the ordered firing of the same transition using an ordered set of tokens on one of its precondition places. Black sequencing transitions are then presented as a shorthand notation in order to model the processing of a flow of events, represented by one of their precondition places. We then show how black sequencing transitions can be encoded into PCPNs, and their application to model Complex Event Processing (CEP), defining a compositional approach to translate some of the most relevant event pattern operators. We have developed MEdit4CEP-BPCPN, an extension of the MEdit4CEP tool, to provide tool support for this novel technique, thus allowing end users to easily define event patterns and obtain an automatic translation into BPCPNs. This can, in turn, be transformed into a corresponding PCPN, and then be immediately used in CPN Tools. Finally, a health case study concerning the monitoring of pregnant women is considered to illustrate how the event patterns are created and how the BPCPN and PCPN models are obtained by using the MEdit4CEP-BPCPN tool.",Self Direction,Choosing Own Goals,Paper X contributes by allowing end users to define event patterns and obtain an automatic translation into BPCPNs; thus providing software users the capability to choose their own goals aligning with the value item Choosing Own Goals and its corresponding value Self Direction.,"In 'Paper X', the main contribution is the development of the MEdit4CEP-BPCPN tool, which allows software users to define event patterns and obtain an automatic translation into BPCPNs. This directly aligns with the value item Choosing Own Goals and its corresponding value Self Direction, as it empowers software users to have autonomy and control in setting their own goals and defining their own event patterns within the software context. The tool provides users the ability to customize and tailor their workflows, reflecting the self-directed nature of choosing and pursuing their own goals.",Agreed-Clarified,Agree,,Agree,,
Exp F,Exp J,2118,TSE,Algorithms,A Compositional Approach for Complex Event Pattern Modeling and Transformation to Colored Petri Nets with Black Sequencing Transitions,"Prioritized Colored Petri Nets (PCPNs) are a well-known extension of plain Petri nets in which transitions can have priorities and the tokens on the places carry data information. In this paper, we propose an extension of the PCPN model with black sequencing transitions (BPCPN). This extension allows us to easily model the ordered firing of the same transition using an ordered set of tokens on one of its precondition places. Black sequencing transitions are then presented as a shorthand notation in order to model the processing of a flow of events, represented by one of their precondition places. We then show how black sequencing transitions can be encoded into PCPNs, and their application to model Complex Event Processing (CEP), defining a compositional approach to translate some of the most relevant event pattern operators. We have developed MEdit4CEP-BPCPN, an extension of the MEdit4CEP tool, to provide tool support for this novel technique, thus allowing end users to easily define event patterns and obtain an automatic translation into BPCPNs. This can, in turn, be transformed into a corresponding PCPN, and then be immediately used in CPN Tools. Finally, a health case study concerning the monitoring of pregnant women is considered to illustrate how the event patterns are created and how the BPCPN and PCPN models are obtained by using the MEdit4CEP-BPCPN tool.",Security,Healthy,The paper uses a health case study concerning the monitoring of pregnant women; illustrating how the tool helps to maintain the health of the users thus aligning with the value item Healthy and its corresponding value Security.,"In the given abstract, 'Paper X' explicitly mentions a health case study concerning the monitoring of pregnant women. This indicates that the main contribution of the paper is related to ensuring the health and well-being of users within a software context. By providing a tool that allows for the easy definition of event patterns and the translation into PCPNs, the paper aligns with the value item Healthy and its corresponding value Security. This alignment is based on the direct evidence provided in the abstract, which highlights the focus on user health in the software application.",Agreed-Clarified,Agree,,Agree,,
Exp A,Exp H,2121,TSE,Security & Privacy,Formal Equivalence Checking for Mobile Malware Detection and Family Classification,"Several techniques to overcome the weaknesses of the current signature based detection approaches adopted by free and commercial antimalware have been proposed by industrial and research communities. These techniques are mainly supervised machine learning based, requiring optimal class balance to generate good predictive models. In this paper, we propose a method to infer mobile application maliciousness by detecting the belonging family, exploiting formal equivalence checking. We introduce a set of heuristics to reduce the number of mobile application comparisons and we define a metric reflecting the application maliciousness. Real-world experiments on 35 Android malware families (ranging from 2010 to 2018) confirm the effectiveness of the proposed method in mobile malware detection and family identification.",Security,Healthy,The paper proposes a method for detecting mobile application maliciousness which contributes to the healthy operation of the user's mobile device; aligning with the value item Health and its corresponding value Security.,"In the paper abstract, the method proposed aims to detect mobile application maliciousness, which directly contributes to the security of the user's mobile device. By identifying and mitigating potential threats, it ensures a more secure and healthy operation of the device. Therefore, it aligns with the value item Healthy and its corresponding value Security from a ""Software User"" perspective.",Agreed-Justified,Disagree,"Related to Achievement: capable more, as it automatically identifies maliciousness",Disagree,The justification doesn't align with the value item of the healthy,
Exp B,Exp H,2121,TSE,Security & Privacy,Formal Equivalence Checking for Mobile Malware Detection and Family Classification,"Several techniques to overcome the weaknesses of the current signature based detection approaches adopted by free and commercial antimalware have been proposed by industrial and research communities. These techniques are mainly supervised machine learning based, requiring optimal class balance to generate good predictive models. In this paper, we propose a method to infer mobile application maliciousness by detecting the belonging family, exploiting formal equivalence checking. We introduce a set of heuristics to reduce the number of mobile application comparisons and we define a metric reflecting the application maliciousness. Real-world experiments on 35 Android malware families (ranging from 2010 to 2018) confirm the effectiveness of the proposed method in mobile malware detection and family identification.",Security,Social Order,The method's ability to identify malicious mobile applications contributes to maintaining social order in the digital realm among users; aligning with the value item Social Order and its corresponding value Security.,"The identification of malicious mobile applications through the proposed method in 'Paper X' contributes to maintaining social order by promoting a secure digital environment for software users. By detecting and categorizing malware families, the method helps to protect users from potential threats and ensures the integrity and safety of their devices and personal information. This aligns directly with the value item Social Order and its corresponding value Security, as it addresses the need for a stable and protected software ecosystem where users can confidently engage with mobile applications without the risk of security breaches and disruptions.",Agreed-Clarified,Agree,,Agree,,
Exp C,Exp K,2121,TSE,Security & Privacy,Formal Equivalence Checking for Mobile Malware Detection and Family Classification,"Several techniques to overcome the weaknesses of the current signature based detection approaches adopted by free and commercial antimalware have been proposed by industrial and research communities. These techniques are mainly supervised machine learning based, requiring optimal class balance to generate good predictive models. In this paper, we propose a method to infer mobile application maliciousness by detecting the belonging family, exploiting formal equivalence checking. We introduce a set of heuristics to reduce the number of mobile application comparisons and we define a metric reflecting the application maliciousness. Real-world experiments on 35 Android malware families (ranging from 2010 to 2018) confirm the effectiveness of the proposed method in mobile malware detection and family identification.",Security,National Security,The proposed detection method contributes to national security by mitigating the potential damages caused by cyber threats; aligning with the value item National Security and its corresponding value Security.,"The proposed method in 'Paper X' contributes to national security by effectively detecting and identifying mobile malware families. By reducing the number of mobile application comparisons and defining a metric reflecting application maliciousness, the method helps in mitigating the potential damages caused by cyber threats. This directly aligns with the value item National Security and its corresponding value Security, as it aims to protect software users from malicious activities and ensure the overall security of mobile applications and devices.",Agreed-Clarified,Agree,,Agree,,
Exp D,Exp K,2134,TSE,Mobile & IoT,A Survey of Performance Optimization for Mobile Applications,"To ensure user satisfaction and success of mobile applications, it is important to provide highly performant applications. This is particularly important for resource-constrained systems such as mobile devices. Thereby, non-functional performance characteristics, such as energy and memory consumption, play an important role for user satisfaction. This paper provides a comprehensive survey of non-functional performance optimization for Android applications. We collected 156 unique publications, published between 2008 and 2020, that focus on the optimization of performance of mobile applications. We target our search at four performance characteristics: responsiveness, launch time, memory and energy consumption. For each performance characteristic, we categorize optimization approaches based on the method used in the corresponding publications. Furthermore, we identify research gaps in the literature for future work.",Achievement,Successful,The paper is focused on ensuring user satisfaction and success; which aligns with the value item Successful and its corresponding value Achievement.,"The paper explicitly states that its goal is to ensure user satisfaction and success of mobile applications. By optimizing non-functional performance characteristics such as energy and memory consumption, the paper aims to improve the overall performance of Android applications. This aligns with the value item Successful and its corresponding value Achievement because achieving user satisfaction and success can be seen as a form of accomplishment and attainment in using the software.",Agreed-Justified,Agree,,Agree,,
Exp E,Exp J,2134,TSE,Mobile & IoT,A Survey of Performance Optimization for Mobile Applications,"To ensure user satisfaction and success of mobile applications, it is important to provide highly performant applications. This is particularly important for resource-constrained systems such as mobile devices. Thereby, non-functional performance characteristics, such as energy and memory consumption, play an important role for user satisfaction. This paper provides a comprehensive survey of non-functional performance optimization for Android applications. We collected 156 unique publications, published between 2008 and 2020, that focus on the optimization of performance of mobile applications. We target our search at four performance characteristics: responsiveness, launch time, memory and energy consumption. For each performance characteristic, we categorize optimization approaches based on the method used in the corresponding publications. Furthermore, we identify research gaps in the literature for future work.",Security,Healthy,The paper addresses the importance of performance characteristics like energy and memory consumption on mobile devices; which aligns with the value item Healthy; referring to the healthy operation of devices; and its corresponding value Security.,"The justification for aligning 'Paper X' with the value item Healthy and its corresponding value Security from a ""Software User"" perspective is based on the notion that the paper emphasizes the significance of optimizing performance characteristics like energy and memory consumption for mobile devices. By prioritizing these optimizations, the paper contributes to maintaining the healthy operation of devices, which in turn enhances the overall security of the software user's experience.",Agreed-Justified,Agree,,Agree,,
Exp G,Exp K,2138,TSE,Software Project Management,On the Relationship Between the DeveloperaEUR(tm)s Perceptible Race and Ethnicity and the Evaluation of Contributions in OSS,"Context: Open Source Software (OSS) projects are typically the result of collective efforts performed by developers with different backgrounds. Although the quality of developersaEUR(tm) contributions should be the only factor influencing the evaluation of the contributions to OSS projects, recent studies have shown that diversity issues are correlated with the acceptance or rejection of developersaEUR(tm) contributions. Objective: This paper assists this emerging state-of-the-art body on diversity research with the first empirical study that analyzes how developersaEUR(tm) perceptible race and ethnicity relates to the evaluation of the contributions in OSS. We also want to create awareness of the racial and ethnic diversity in OSS projects. Methodology: We performed a large-scale quantitative study of OSS projects in GitHub. We extracted the developersaEUR(tm) perceptible race and ethnicity from their names in GitHub using the Name-Prism tool and applied regression modeling of contributions (i.e, pull requests) data from GHTorrent and GitHub. Results: We observed that (1) among the developers whose perceptible race and ethnicity was captured by the tool, only 16.56 percent were perceptible as Non-White developers; (2) contributions from perceptible White developers have about 6aEUR""10 percent higher odds of being accepted when compared to contributions from perceptible Non-White developers; and (3) submitters with perceptible non-white races and ethnicities are more likely to get their pull requests accepted when the integrator is estimated to be from their same race and ethnicity rather than when the integrator is estimated to be White. Conclusion: Our initial analysis shows a low number of Non-White developers participating in OSS. Furthermore, the results from our regression analysis lead us to believe that there may exist differences between the evaluation of the contributions from different perceptible races and ethnicities. Thus, our findings reinforce the need for further studies on racial and ethnic diversity in software engineering to foster healthier OSS communities.",Universalism,Equality,The paper promotes equality by investigating the racial and ethnic disparities in the acceptance of software contributions in OSS projects. This aligns with the value item Equality and its corresponding value Universalism.,"The paper's investigation into racial and ethnic disparities in the acceptance of software contributions aligns with the value item Equality and its corresponding value Universalism, as it emphasizes the importance of treating all contributors fairly and promoting inclusivity within the OSS community, regardless of race or ethnicity. This aligns with the broader value of Universalism, which values equality and justice for all individuals, and highlights the significance of creating a software environment that is accessible and fair to everyone.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2138,TSE,Software Project Management,On the Relationship Between the DeveloperaEUR(tm)s Perceptible Race and Ethnicity and the Evaluation of Contributions in OSS,"Context: Open Source Software (OSS) projects are typically the result of collective efforts performed by developers with different backgrounds. Although the quality of developersaEUR(tm) contributions should be the only factor influencing the evaluation of the contributions to OSS projects, recent studies have shown that diversity issues are correlated with the acceptance or rejection of developersaEUR(tm) contributions. Objective: This paper assists this emerging state-of-the-art body on diversity research with the first empirical study that analyzes how developersaEUR(tm) perceptible race and ethnicity relates to the evaluation of the contributions in OSS. We also want to create awareness of the racial and ethnic diversity in OSS projects. Methodology: We performed a large-scale quantitative study of OSS projects in GitHub. We extracted the developersaEUR(tm) perceptible race and ethnicity from their names in GitHub using the Name-Prism tool and applied regression modeling of contributions (i.e, pull requests) data from GHTorrent and GitHub. Results: We observed that (1) among the developers whose perceptible race and ethnicity was captured by the tool, only 16.56 percent were perceptible as Non-White developers; (2) contributions from perceptible White developers have about 6aEUR""10 percent higher odds of being accepted when compared to contributions from perceptible Non-White developers; and (3) submitters with perceptible non-white races and ethnicities are more likely to get their pull requests accepted when the integrator is estimated to be from their same race and ethnicity rather than when the integrator is estimated to be White. Conclusion: Our initial analysis shows a low number of Non-White developers participating in OSS. Furthermore, the results from our regression analysis lead us to believe that there may exist differences between the evaluation of the contributions from different perceptible races and ethnicities. Thus, our findings reinforce the need for further studies on racial and ethnic diversity in software engineering to foster healthier OSS communities.",Universalism,Social Justice,By highlighting the ethnic and racial diversity issue within OSS communities; the paper advances the discourse on Social Justice; echoing the value item in Schwartz's Taxonomy and its related value Universalism.,"The main contribution of 'Paper X' in aligning with the value item Social Justice and its corresponding value Universalism is by bringing attention to and addressing the lack of racial and ethnic diversity within OSS communities. This aligns with the value of Universalism as it promotes equality, broad-mindedness, and social justice, which are all crucial in creating inclusive and diverse software communities. By highlighting this issue, the paper directly addresses the value of Social Justice and contributes to fostering healthier and more inclusive OSS communities.",Agreed-Clarified,Agree,,Agree,,
Exp C,Exp K,2142,TSE,Mobile & IoT,Emerging App Issue Identification via Online Joint Sentiment-Topic Tracing,"Millions of mobile apps are available in app stores, such as Apple's App Store and Google Play. For a mobile app, it would be increasingly challenging to stand out from the enormous competitors and become prevalent among users. Good user experience and well-designed functionalities are the keys to a successful app. To achieve this, popular apps usually schedule their updates frequently. If we can capture the critical app issues faced by users in a timely and accurate manner, developers can make timely updates, and good user experience can be ensured. There exist prior studies on analyzing reviews for detecting emerging app issues. These studies are usually based on topic modeling or clustering techniques. However, the short-length characteristics and sentiment of user reviews have not been considered. In this paper, we propose a novel emerging issue detection approach named MERIT to take into consideration the two aforementioned characteristics. Specifically, we propose an Adaptive Online Biterm Sentiment-Topic (AOBST) model for jointly modeling topics and corresponding sentiments that takes into consideration app versions. Based on the AOBST model, we infer the topics negatively reflected in user reviews for one app version, and automatically interpret the meaning of the topics with most relevant phrases and sentences. Experiments on popular apps from Google Play and Apple's App Store demonstrate the effectiveness of MERIT in identifying emerging app issues, improving the state-of-the-art method by 22.3 percent in terms of F1-score. In terms of efficiency, MERIT can return results within acceptable time.",Achievement,Successful,The paper proposes a novel method (MERIT) for detecting critical issues in mobile apps using user reviews; aiming to improve the user experience and the success of the app. This aligns with the value item Successful and its corresponding value Achievement.,"In the given paper abstract, the main contribution of 'Paper X' is to propose a novel method (MERIT) for detecting critical issues in mobile apps using user reviews. By capturing these issues in a timely and accurate manner, developers can make timely updates and ensure a good user experience. Since the paper explicitly states that good user experience and well-designed functionalities are the keys to a successful app, it can be inferred that the main objective of 'Paper X' aligns with the value item Successful and its corresponding value Achievement from the perspective of a software user.",Agreed-Justified,Agree,,Agree,,
Exp D,Exp K,2142,TSE,Mobile & IoT,Emerging App Issue Identification via Online Joint Sentiment-Topic Tracing,"Millions of mobile apps are available in app stores, such as Apple's App Store and Google Play. For a mobile app, it would be increasingly challenging to stand out from the enormous competitors and become prevalent among users. Good user experience and well-designed functionalities are the keys to a successful app. To achieve this, popular apps usually schedule their updates frequently. If we can capture the critical app issues faced by users in a timely and accurate manner, developers can make timely updates, and good user experience can be ensured. There exist prior studies on analyzing reviews for detecting emerging app issues. These studies are usually based on topic modeling or clustering techniques. However, the short-length characteristics and sentiment of user reviews have not been considered. In this paper, we propose a novel emerging issue detection approach named MERIT to take into consideration the two aforementioned characteristics. Specifically, we propose an Adaptive Online Biterm Sentiment-Topic (AOBST) model for jointly modeling topics and corresponding sentiments that takes into consideration app versions. Based on the AOBST model, we infer the topics negatively reflected in user reviews for one app version, and automatically interpret the meaning of the topics with most relevant phrases and sentences. Experiments on popular apps from Google Play and Apple's App Store demonstrate the effectiveness of MERIT in identifying emerging app issues, improving the state-of-the-art method by 22.3 percent in terms of F1-score. In terms of efficiency, MERIT can return results within acceptable time.",Security,Social Order,By ensuring good user experience through timely and accurate detection of app issues; the paper contributes to maintaining order and stability in software use; which aligns with the value item Social Order and its corresponding value Security.,"In ""Paper X,"" the main contributions involve the development of a novel approach for detecting emerging app issues through the analysis of user reviews. By addressing these issues in a timely and accurate manner, developers can ensure good user experience. This directly aligns with the value item of Social Order from Schwartz's Taxonomy, as maintaining order and stability in software use contributes to a sense of security for software users.",Agreed-Clarified,Agree,,Agree,,
Exp E,Exp J,2142,TSE,Mobile & IoT,Emerging App Issue Identification via Online Joint Sentiment-Topic Tracing,"Millions of mobile apps are available in app stores, such as Apple's App Store and Google Play. For a mobile app, it would be increasingly challenging to stand out from the enormous competitors and become prevalent among users. Good user experience and well-designed functionalities are the keys to a successful app. To achieve this, popular apps usually schedule their updates frequently. If we can capture the critical app issues faced by users in a timely and accurate manner, developers can make timely updates, and good user experience can be ensured. There exist prior studies on analyzing reviews for detecting emerging app issues. These studies are usually based on topic modeling or clustering techniques. However, the short-length characteristics and sentiment of user reviews have not been considered. In this paper, we propose a novel emerging issue detection approach named MERIT to take into consideration the two aforementioned characteristics. Specifically, we propose an Adaptive Online Biterm Sentiment-Topic (AOBST) model for jointly modeling topics and corresponding sentiments that takes into consideration app versions. Based on the AOBST model, we infer the topics negatively reflected in user reviews for one app version, and automatically interpret the meaning of the topics with most relevant phrases and sentences. Experiments on popular apps from Google Play and Apple's App Store demonstrate the effectiveness of MERIT in identifying emerging app issues, improving the state-of-the-art method by 22.3 percent in terms of F1-score. In terms of efficiency, MERIT can return results within acceptable time.",Conformity,Obedience,The paper's proposed model; by considering the opinion of users and their feedback (reviews); obeys the necessity to listen to user voice for better app performance. This complies with the value item Obedience and its corresponding value Conformity.,"Sure, I apologize for any confusion. The justification for aligning 'Paper X' with the value item Obedience and its corresponding value Conformity is based on the fact that the proposed model in the paper focuses on listening to user feedback and opinions through the analysis of user reviews. By incorporating user feedback into the development process and making timely updates based on these insights, the paper aligns with the value of Conformity, as it demonstrates a willingness to conform to the expectations and desires of users in order to improve the app experience. This aligns with the value item Obedience, as it represents the act of adhering to and following the preferences and needs of users, which is essential for a software user's perspective.",Agreed-Justified,Agree,,Agree,,
Exp F,Exp J,2143,TSE,Software Testing & QA,A Systematical Study on Application Performance Management Libraries for Apps,"Being able to automatically detect the performance issues in apps can significantly improve appsaEUR(tm) quality as well as having a positive influence on user satisfaction. Application Performance Management (APM) libraries are used to locate the appsaEUR(tm) performance bottleneck, monitor their behaviors at runtime, and identify potential security risks. Although app developers have been exploiting application performance management (APM) tools to capture these potential performance issues, most of them do not fully understand the internals of these APM tools and the effect on their apps. To fill this gap, in this paper, we conduct the first systematic study on APMs for apps by scrutinizing 25 widely-used APMs for Android apps and develop a framework named APMHunter for exploring the usage of APMs in Android apps. Using APMHunter, we conduct a large-scale empirical study on 500,000 Android apps to explore the usage patterns of APMs and discover the potential misuses of APMs. We obtain two major findings: 1) some APMs still employ deprecated permissions and approaches, which makes APMs fail to perform as expected; 2) inappropriate use of APMs can cause privacy leaks. Thus, our study suggests that both APM vendors and developers should design and use APMs scrupulously.",Power,Social Recognition,The paper contributes to enhancing the user experience by improving the performance and quality of apps; which would increase user satisfaction and thus could lead to greater social recognition for the user.,"The justification for labeling 'Paper X' as aligning with the value item Social Recognition and its corresponding value Power from a ""Software User"" perspective is based on the premise that improving the performance and quality of apps can lead to increased user satisfaction. This, in turn, could potentially result in greater recognition and influence for the user within their social circle or community. By focusing on enhancing the user experience through app performance improvement, the paper indirectly aligns with the value item Social Recognition and its corresponding value Power by suggesting that users who have access to better-performing apps may gain social recognition or influence as a result.",Agreed-Justified,Agree,,Agree,,
Exp G,Exp K,2143,TSE,Software Testing & QA,A Systematical Study on Application Performance Management Libraries for Apps,"Being able to automatically detect the performance issues in apps can significantly improve appsaEUR(tm) quality as well as having a positive influence on user satisfaction. Application Performance Management (APM) libraries are used to locate the appsaEUR(tm) performance bottleneck, monitor their behaviors at runtime, and identify potential security risks. Although app developers have been exploiting application performance management (APM) tools to capture these potential performance issues, most of them do not fully understand the internals of these APM tools and the effect on their apps. To fill this gap, in this paper, we conduct the first systematic study on APMs for apps by scrutinizing 25 widely-used APMs for Android apps and develop a framework named APMHunter for exploring the usage of APMs in Android apps. Using APMHunter, we conduct a large-scale empirical study on 500,000 Android apps to explore the usage patterns of APMs and discover the potential misuses of APMs. We obtain two major findings: 1) some APMs still employ deprecated permissions and approaches, which makes APMs fail to perform as expected; 2) inappropriate use of APMs can cause privacy leaks. Thus, our study suggests that both APM vendors and developers should design and use APMs scrupulously.",Security,Healthy,The paper contributes to the health aspect by identifying potential security risks in apps; which would lend to a more secure; and thus healthier; user experience.,"The paper's contribution to the value item Healthy is evidenced through its identification of potential security risks in apps. By addressing these risks, the paper aims to improve the overall security of the apps, thereby creating a more secure environment for software users. This alignment directly contributes to the value of Security, as a more secure app environment ultimately promotes the well-being and safety of users.",Agreed-Justified,Disagree,Agree with coder_2,Disagree,Too generic. Value healthy concerns Human health not software health,
Exp A,Exp H,2143,TSE,Software Testing & QA,A Systematical Study on Application Performance Management Libraries for Apps,"Being able to automatically detect the performance issues in apps can significantly improve appsaEUR(tm) quality as well as having a positive influence on user satisfaction. Application Performance Management (APM) libraries are used to locate the appsaEUR(tm) performance bottleneck, monitor their behaviors at runtime, and identify potential security risks. Although app developers have been exploiting application performance management (APM) tools to capture these potential performance issues, most of them do not fully understand the internals of these APM tools and the effect on their apps. To fill this gap, in this paper, we conduct the first systematic study on APMs for apps by scrutinizing 25 widely-used APMs for Android apps and develop a framework named APMHunter for exploring the usage of APMs in Android apps. Using APMHunter, we conduct a large-scale empirical study on 500,000 Android apps to explore the usage patterns of APMs and discover the potential misuses of APMs. We obtain two major findings: 1) some APMs still employ deprecated permissions and approaches, which makes APMs fail to perform as expected; 2) inappropriate use of APMs can cause privacy leaks. Thus, our study suggests that both APM vendors and developers should design and use APMs scrupulously.",Security,Family Security,By identifying and addressing potential misuses of APMs; the paper is indirectly contributing to the security of the user's family; as it would protect the user's personal data from being leaked.,"The paper directly aligns with the value item Family Security and its corresponding value Security from a ""Software User"" perspective by identifying potential misuses of APMs and addressing them. By ensuring that APMs do not cause privacy leaks, the paper contributes to the security of the user's family by protecting their personal data.",Agreed-Justified,Disagree,More incline to Self direction: Privacy as it takes care of privacy-related information,Disagree,The justification doesn't align with the value item of the Family security,
Exp B,Exp H,2145,TSE,AI & Machine Learning,A Comparison of Natural Language Understanding Platforms for Chatbots in Software Engineering,"Chatbots are envisioned to dramatically change the future of Software Engineering, allowing practitioners to chat and inquire about their software projects and interact with different services using natural language. At the heart of every chatbot is a Natural Language Understanding (NLU) component that enables the chatbot to understand natural language input. Recently, many NLU platforms were provided to serve as an off-the-shelf NLU component for chatbots, however, selecting the best NLU for Software Engineering chatbots remains an open challenge. Therefore, in this paper, we evaluate four of the most commonly used NLUs, namely IBM Watson, Google Dialogflow, Rasa, and Microsoft LUIS to shed light on which NLU should be used in Software Engineering based chatbots. Specifically, we examine the NLUsaEUR(tm) performance in classifying intents, confidence scores stability, and extracting entities. To evaluate the NLUs, we use two datasets that reflect two common tasks performed by Software Engineering practitioners, 1) the task of chatting with the chatbot to ask questions about software repositories 2) the task of asking development questions on Q&A forums (e.g., Stack Overflow). According to our findings, IBM Watson is the best performing NLU when considering the three aspects (intents classification, confidence scores, and entity extraction). However, the results from each individual aspect show that, in intents classification, IBM Watson performs the best with an F1-measure$>$>84%, but in confidence scores, Rasa comes on top with a median confidence score higher than 0.91. Our results also show that all NLUs, except for Dialogflow, generally provide trustable confidence scores. For entity extraction, Microsoft LUIS and IBM Watson outperform other NLUs in the two SE tasks. Our results provide guidance to software engineering practitioners when deciding which NLU to use in their chatbots.",Achievement,Intelligent,The paper evaluates four popular NLU platforms and identifies the NLU platform that best understands natural language inputs; improving the intelligent interaction between chatbots and software users.,"I apologize for any confusion. The justification for aligning 'Paper X' with the value item Intelligent and its corresponding value Achievement is that the paper specifically evaluates and compares four widely used NLU platforms in order to determine the best one for understanding natural language inputs. This evaluation and selection process demonstrates a focus on achieving optimal performance and intelligence in chatbot interactions, thereby aligning with the values of intelligence and achievement from a software user perspective.",Agreed-Clarified,Agree,,Agree,,
Exp C,Exp K,2145,TSE,AI & Machine Learning,A Comparison of Natural Language Understanding Platforms for Chatbots in Software Engineering,"Chatbots are envisioned to dramatically change the future of Software Engineering, allowing practitioners to chat and inquire about their software projects and interact with different services using natural language. At the heart of every chatbot is a Natural Language Understanding (NLU) component that enables the chatbot to understand natural language input. Recently, many NLU platforms were provided to serve as an off-the-shelf NLU component for chatbots, however, selecting the best NLU for Software Engineering chatbots remains an open challenge. Therefore, in this paper, we evaluate four of the most commonly used NLUs, namely IBM Watson, Google Dialogflow, Rasa, and Microsoft LUIS to shed light on which NLU should be used in Software Engineering based chatbots. Specifically, we examine the NLUsaEUR(tm) performance in classifying intents, confidence scores stability, and extracting entities. To evaluate the NLUs, we use two datasets that reflect two common tasks performed by Software Engineering practitioners, 1) the task of chatting with the chatbot to ask questions about software repositories 2) the task of asking development questions on Q&A forums (e.g., Stack Overflow). According to our findings, IBM Watson is the best performing NLU when considering the three aspects (intents classification, confidence scores, and entity extraction). However, the results from each individual aspect show that, in intents classification, IBM Watson performs the best with an F1-measure$>$>84%, but in confidence scores, Rasa comes on top with a median confidence score higher than 0.91. Our results also show that all NLUs, except for Dialogflow, generally provide trustable confidence scores. For entity extraction, Microsoft LUIS and IBM Watson outperform other NLUs in the two SE tasks. Our results provide guidance to software engineering practitioners when deciding which NLU to use in their chatbots.",Achievement,Successful,The paper's findings suggest IBM Watson offers the best overall performance in classifying intents; maintaining confidence scores; and extracting entities; contributing to users' successful use of software engineering chatbots.,"Based on the paper's findings, IBM Watson performs the best in classifying intents, maintaining confidence scores, and extracting entities, which directly contributes to the successful use of software engineering chatbots by users. This alignment with the value item ""Successful"" and its corresponding value ""Achievement"" indicates that the main contributions of 'Paper X' focus on providing users with a chatbot that enables them to achieve their goals effectively and efficiently in the software engineering context.",Agreed-Justified,Agree,,Agree,,
Exp D,Exp K,2145,TSE,AI & Machine Learning,A Comparison of Natural Language Understanding Platforms for Chatbots in Software Engineering,"Chatbots are envisioned to dramatically change the future of Software Engineering, allowing practitioners to chat and inquire about their software projects and interact with different services using natural language. At the heart of every chatbot is a Natural Language Understanding (NLU) component that enables the chatbot to understand natural language input. Recently, many NLU platforms were provided to serve as an off-the-shelf NLU component for chatbots, however, selecting the best NLU for Software Engineering chatbots remains an open challenge. Therefore, in this paper, we evaluate four of the most commonly used NLUs, namely IBM Watson, Google Dialogflow, Rasa, and Microsoft LUIS to shed light on which NLU should be used in Software Engineering based chatbots. Specifically, we examine the NLUsaEUR(tm) performance in classifying intents, confidence scores stability, and extracting entities. To evaluate the NLUs, we use two datasets that reflect two common tasks performed by Software Engineering practitioners, 1) the task of chatting with the chatbot to ask questions about software repositories 2) the task of asking development questions on Q&A forums (e.g., Stack Overflow). According to our findings, IBM Watson is the best performing NLU when considering the three aspects (intents classification, confidence scores, and entity extraction). However, the results from each individual aspect show that, in intents classification, IBM Watson performs the best with an F1-measure$>$>84%, but in confidence scores, Rasa comes on top with a median confidence score higher than 0.91. Our results also show that all NLUs, except for Dialogflow, generally provide trustable confidence scores. For entity extraction, Microsoft LUIS and IBM Watson outperform other NLUs in the two SE tasks. Our results provide guidance to software engineering practitioners when deciding which NLU to use in their chatbots.",Self Direction,Independent,The paper analyzes several NLU platforms offering chatbot services; contributing to users' independent selection of the best-performing NLU for their software engineering chatbots.,"In the paper, the authors evaluate different NLUs for software engineering chatbots, aiming to guide software engineering practitioners in selecting the most suitable NLU. This aligns with the value item of Independent in Schwartz's Taxonomy because it allows software users to have the freedom and autonomy to choose the NLU that best meets their specific needs and preferences, empowering them to make their own decisions in the software development process. Consequently, the paper's contribution directly aligns with the value of Self Direction as it enables software users to independently select the NLU for their chatbots.",Agreed-Justified,Agree,,Agree,,
Exp E,Exp J,2148,TSE,Software Testing & QA,Context-Aware Personalized Crowdtesting Task Recommendation,"Crowdsourced software testing (short for crowdtesting) is a special type of crowdsourcing. It requires that crowdworkers master appropriate skill-sets and commit significant effort for completing a task. Abundant uncertainty may arise during a crowdtesting process due to imperfect information between the task requester and crowdworkers. For example, a worker frequently chooses tasks in an ad hoc manner in crowdtesting context, and an inappropriate task selection may lead to the worker's failing to detect any bugs, and significant testing effort unpaid and wasted. Recent studies have explored methods for supporting task requesters to make informed decisions on task pricing, worker recommendation, and so on. Unfortunately, very few study offers decision making support from the crowdworkersaEUR(tm) perspectives. We motivate this study through a pilot study, revealing the large portion (74 percent) of unpaid crowdworkersaEUR(tm) effort due to the inappropriate task choice. Drawn from our previous work on context-aware crowdworker recommendations, we advocate a more effective alternative to manual task selection would be to provide contextualized and personalized task recommendation considering the diverse distribution of worker preference and expertise, with objectives to increase their winning chances and to potentially reduce the frequency of unpaid crowd work. This paper proposes a context-aware personalized task recommendation approach PTRec, consisting of a testing context model and a learning-based task recommendation model to aid dynamic worker decision in selecting crowdtesting tasks. The testing context model is constructed in two perspectives, i.e., process context and resource context, to capture the in-process progress-oriented information and crowdworkersaEUR(tm) characteristics respectively. Built on top of this context model, the learning-based task recommendation model extracts 60 features automatically, and employs random forest learner to generate dynamic and personalized task recommendation which matches workersaEUR(tm) expertise and interest. The evaluation is conducted on 636 crowdtesting tasks involving 2,404 crowdworkers from one of the largest crowdtesting platforms, and results show our approach can achieve an average precision of 82 percent, average recall of 84 percent, and save an estimated average of 81 percent effort originally spent on exploring, significantly outperforming four commonly-used and state-of-the-art baselines. This indicates its potential in recommending proper tasks to workers so as to improve bug detection efficiency and increase their monetary earnings.",Achievement,Capable,The paper proposing an approach for workers to select appropriate crowdtesting tasks aligns with 'Capable' under 'Achievement'; as the approach enables workers to utilize their specific expertise and interest effectively in the tasks; improving their competence and success rate.,"In 'Paper X', the proposed approach for personalized task recommendations aligns with the value item Capable under Achievement in Schwartz's Taxonomy. This alignment is evident as the approach allows crowdworkers to effectively utilize their specific expertise and interest in selecting appropriate tasks, thereby enhancing their competence and increasing their chances of success in completing the tasks. The paper's focus on improving worker decision-making and increasing their monetary earnings directly supports the value of Achievement, as it enables workers to showcase their capabilities and achieve tangible results in their crowdtesting efforts.",Agreed-Clarified,Agree,,Agree,,
Exp F,Exp J,2148,TSE,Software Testing & QA,Context-Aware Personalized Crowdtesting Task Recommendation,"Crowdsourced software testing (short for crowdtesting) is a special type of crowdsourcing. It requires that crowdworkers master appropriate skill-sets and commit significant effort for completing a task. Abundant uncertainty may arise during a crowdtesting process due to imperfect information between the task requester and crowdworkers. For example, a worker frequently chooses tasks in an ad hoc manner in crowdtesting context, and an inappropriate task selection may lead to the worker's failing to detect any bugs, and significant testing effort unpaid and wasted. Recent studies have explored methods for supporting task requesters to make informed decisions on task pricing, worker recommendation, and so on. Unfortunately, very few study offers decision making support from the crowdworkersaEUR(tm) perspectives. We motivate this study through a pilot study, revealing the large portion (74 percent) of unpaid crowdworkersaEUR(tm) effort due to the inappropriate task choice. Drawn from our previous work on context-aware crowdworker recommendations, we advocate a more effective alternative to manual task selection would be to provide contextualized and personalized task recommendation considering the diverse distribution of worker preference and expertise, with objectives to increase their winning chances and to potentially reduce the frequency of unpaid crowd work. This paper proposes a context-aware personalized task recommendation approach PTRec, consisting of a testing context model and a learning-based task recommendation model to aid dynamic worker decision in selecting crowdtesting tasks. The testing context model is constructed in two perspectives, i.e., process context and resource context, to capture the in-process progress-oriented information and crowdworkersaEUR(tm) characteristics respectively. Built on top of this context model, the learning-based task recommendation model extracts 60 features automatically, and employs random forest learner to generate dynamic and personalized task recommendation which matches workersaEUR(tm) expertise and interest. The evaluation is conducted on 636 crowdtesting tasks involving 2,404 crowdworkers from one of the largest crowdtesting platforms, and results show our approach can achieve an average precision of 82 percent, average recall of 84 percent, and save an estimated average of 81 percent effort originally spent on exploring, significantly outperforming four commonly-used and state-of-the-art baselines. This indicates its potential in recommending proper tasks to workers so as to improve bug detection efficiency and increase their monetary earnings.",Power,Social Recognition,The task recommendation approach proposed can improves the visibility of the crowdworkers' skills and match with the proper tasks; enhancing their social recognition; which aligns with the 'Social Recognition' under 'Power'.,"In the paper abstract, it is stated that the proposed task recommendation approach aims to improve the visibility of crowdworkers' skills and match them with appropriate tasks. This enhancement in visibility and matching can ultimately lead to increased social recognition for the crowdworkers. This aligns with the value item of Social Recognition under the value of Power in Schwartz's Taxonomy. By providing workers with recommendations that better align with their expertise and interests, the paper's main contribution directly addresses the aspect of social recognition in the context of software testing from a ""Software User"" perspective.",Agreed-Clarified,Agree,,Agree,,
Exp G,Exp K,2148,TSE,Software Testing & QA,Context-Aware Personalized Crowdtesting Task Recommendation,"Crowdsourced software testing (short for crowdtesting) is a special type of crowdsourcing. It requires that crowdworkers master appropriate skill-sets and commit significant effort for completing a task. Abundant uncertainty may arise during a crowdtesting process due to imperfect information between the task requester and crowdworkers. For example, a worker frequently chooses tasks in an ad hoc manner in crowdtesting context, and an inappropriate task selection may lead to the worker's failing to detect any bugs, and significant testing effort unpaid and wasted. Recent studies have explored methods for supporting task requesters to make informed decisions on task pricing, worker recommendation, and so on. Unfortunately, very few study offers decision making support from the crowdworkersaEUR(tm) perspectives. We motivate this study through a pilot study, revealing the large portion (74 percent) of unpaid crowdworkersaEUR(tm) effort due to the inappropriate task choice. Drawn from our previous work on context-aware crowdworker recommendations, we advocate a more effective alternative to manual task selection would be to provide contextualized and personalized task recommendation considering the diverse distribution of worker preference and expertise, with objectives to increase their winning chances and to potentially reduce the frequency of unpaid crowd work. This paper proposes a context-aware personalized task recommendation approach PTRec, consisting of a testing context model and a learning-based task recommendation model to aid dynamic worker decision in selecting crowdtesting tasks. The testing context model is constructed in two perspectives, i.e., process context and resource context, to capture the in-process progress-oriented information and crowdworkersaEUR(tm) characteristics respectively. Built on top of this context model, the learning-based task recommendation model extracts 60 features automatically, and employs random forest learner to generate dynamic and personalized task recommendation which matches workersaEUR(tm) expertise and interest. The evaluation is conducted on 636 crowdtesting tasks involving 2,404 crowdworkers from one of the largest crowdtesting platforms, and results show our approach can achieve an average precision of 82 percent, average recall of 84 percent, and save an estimated average of 81 percent effort originally spent on exploring, significantly outperforming four commonly-used and state-of-the-art baselines. This indicates its potential in recommending proper tasks to workers so as to improve bug detection efficiency and increase their monetary earnings.",Self Direction,Independent,The personalized task recommendation approach lets users be independent in task selection; aligned with 'Independent' under 'Self Direction' as it empowers crowdworkers to make their own choices based on their expertise and interests.,"The personalized task recommendation approach proposed in 'Paper X' aligns with the value item Independent and its corresponding value Self Direction from a ""Software User"" perspective because it enables crowdworkers to have control and autonomy in selecting their tasks based on their own expertise and interests. This empowers the crowdworkers to make independent choices, allowing them to have a sense of freedom and self-determination in their work within a software context.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2152,TSE,Accessibility & User Experience,Interacto: A Modern User Interaction Processing Model,"Since most software systems provide their users with interactive features, building user interfaces (UI) is one of the core software engineering tasks. It consists in designing, implementing and testing ever more sophisticated and versatile ways for users to interact with software systems, and safely connecting these interactions with commands querying or modifying their state. However, most UI frameworks still rely on a low level model, the bare bone UI event processing model. This model was suitable for the rather simple UIs of the early 80's (menus, buttons, keyboards, mouse clicks), but now exhibits major software engineering flaws for modern, highly interactive UIs. These flaws include lack of separation of concerns, weak modularity and thus low reusability of code for advanced interactions, as well as low testability. To mitigate these flaws, we propose Interacto as a high level user interaction processing model. By reifying the concept of user interaction, Interacto makes it easy to design, implement and test modular and reusable advanced user interactions, and to connect them to commands with built-in undo/redo support. To demonstrate its applicability and generality, we briefly present two open source implementations of Interacto for Java/JavaFX and TypeScript/Angular. We evaluate Interacto interest (1) on a real world case study, where it has been used since 2013, and with (2) a controlled experiment with 44 master students, comparing it with traditionnal UI frameworks.",Self Direction,Freedom,The paper introduces a new user interaction processing model; Interacto; for more sophisticated and versatile interactions with software systems; which provides users the Freedom to interact with the software in a more flexible way.,"The main contribution of 'Paper X' aligns with the value item Freedom and its corresponding value Self Direction from a ""Software User"" perspective because the proposed Interacto model provides users with the freedom to interact with software systems in a more flexible and versatile way. By reifying the concept of user interaction and allowing for modular and reusable advanced interactions, Interacto gives users the ability to independently navigate and explore different functionalities and features within the software, ultimately empowering them to choose their own goals and exercise their own creativity in how they interact with the system. This aligns directly with the value of Self Direction, as users are given the autonomy and independence to control and customize their interactions according to their needs and preferences.",Agreed-Justified,Agree,,Agree,,
Exp B,Exp H,2152,TSE,Accessibility & User Experience,Interacto: A Modern User Interaction Processing Model,"Since most software systems provide their users with interactive features, building user interfaces (UI) is one of the core software engineering tasks. It consists in designing, implementing and testing ever more sophisticated and versatile ways for users to interact with software systems, and safely connecting these interactions with commands querying or modifying their state. However, most UI frameworks still rely on a low level model, the bare bone UI event processing model. This model was suitable for the rather simple UIs of the early 80's (menus, buttons, keyboards, mouse clicks), but now exhibits major software engineering flaws for modern, highly interactive UIs. These flaws include lack of separation of concerns, weak modularity and thus low reusability of code for advanced interactions, as well as low testability. To mitigate these flaws, we propose Interacto as a high level user interaction processing model. By reifying the concept of user interaction, Interacto makes it easy to design, implement and test modular and reusable advanced user interactions, and to connect them to commands with built-in undo/redo support. To demonstrate its applicability and generality, we briefly present two open source implementations of Interacto for Java/JavaFX and TypeScript/Angular. We evaluate Interacto interest (1) on a real world case study, where it has been used since 2013, and with (2) a controlled experiment with 44 master students, comparing it with traditionnal UI frameworks.",Stimulation,Daring,By introducing Interacto and providing users with more complex and varied interaction options; Interacto dares to push the boundaries of User Interfaces; aligning with the Daring value item under Stimulation.,"In 'Paper X', the authors propose Interacto as a high-level user interaction processing model that enables the design, implementation, and testing of advanced and versatile user interactions in software systems. This aligns with the value item Daring under Stimulation, as Interacto pushes the boundaries of traditional UI frameworks by providing users with more complex and varied interaction options. By daring to challenge the limitations of the bare bone UI event processing model, Interacto contributes to the stimulation of software users by offering innovative and exciting ways for them to interact with software systems.",Agreed-Justified,Agree,,Agree,,
Exp C,Exp K,2152,TSE,Accessibility & User Experience,Interacto: A Modern User Interaction Processing Model,"Since most software systems provide their users with interactive features, building user interfaces (UI) is one of the core software engineering tasks. It consists in designing, implementing and testing ever more sophisticated and versatile ways for users to interact with software systems, and safely connecting these interactions with commands querying or modifying their state. However, most UI frameworks still rely on a low level model, the bare bone UI event processing model. This model was suitable for the rather simple UIs of the early 80's (menus, buttons, keyboards, mouse clicks), but now exhibits major software engineering flaws for modern, highly interactive UIs. These flaws include lack of separation of concerns, weak modularity and thus low reusability of code for advanced interactions, as well as low testability. To mitigate these flaws, we propose Interacto as a high level user interaction processing model. By reifying the concept of user interaction, Interacto makes it easy to design, implement and test modular and reusable advanced user interactions, and to connect them to commands with built-in undo/redo support. To demonstrate its applicability and generality, we briefly present two open source implementations of Interacto for Java/JavaFX and TypeScript/Angular. We evaluate Interacto interest (1) on a real world case study, where it has been used since 2013, and with (2) a controlled experiment with 44 master students, comparing it with traditionnal UI frameworks.",Achievement,Successful,The paper shows that using Interacto in software systems can contribute to the success of software user interaction; aligning with the Successful value item under Achievement.,"The main contributions of 'Paper X' in proposing the Interacto user interaction processing model directly align with the value item Successful and its corresponding value Achievement from a ""Software User"" perspective. By introducing a higher level model for designing and implementing advanced user interactions, Interacto enables software users to have more successful interactions with software systems. This aligns with the value of Achievement, as it helps users to feel intelligent, capable, and influential in their interactions, ultimately contributing to their overall success in using the software.",Agreed-Justified,Agree,,Agree,,
Exp D,Exp K,2153,TSE,Data Management & Processing,LogAssist: Assisting Log Analysis Through Log Summarization,"Logs contain valuable information about the runtime behaviors of software systems. Thus, practitioners rely on logs for various tasks such as debugging, system comprehension, and anomaly detection. However, logs are difficult to analyze due to their unstructured nature and large size. In this paper, we propose a novel approach called LogAssist that assists practitioners with log analysis. LogAssist provides an organized and concise view of logs by first grouping logs into event sequences (i.e., workflows), which better illustrate the system runtime execution paths. Then, LogAssist compresses the log events in workflows by hiding consecutive events and applying n-gram modeling to identify common event sequences. We evaluated LogAssist on logs generated by one enterprise and two open source systems. We find that LogAssist can reduce the number of log events that practitioners need to investigate by up to 99 percent. Through a user study with 19 participants, we find that LogAssist can assist practitioners by reducing the time required for log analysis tasks by an average of 40 percent. The participants also rated LogAssist an average of 4.53 out of 5 for improving their experiences of performing log analysis. Finally, we document our experiences and lessons learned from developing and adopting LogAssist in practice. We believe that LogAssist and our reported experiences may lay the basis for future analysis and interactive exploration on logs.",Achievement,Intelligent,"Paper X contributes LogAssist; a tool that helps software users understand logs and reduce the complexity of their analysis tasks. This aligns with the value item ""Intelligent"" under the value ""Achievement""; as users are empowered to comprehend and operate on intricate log data intelligently.","The main contribution of 'Paper X', LogAssist, directly aligns with the value item Intelligent and its corresponding value Achievement from a ""Software User"" perspective. LogAssist empowers software users to comprehend and operate on intricate log data intelligently by providing an organized and concise view of logs, reducing the complexity of log analysis tasks. With LogAssist, users can efficiently investigate and understand the runtime behaviors of software systems, which aligns with the value of achievement as users are able to achieve their goals of system comprehension and anomaly detection through intelligent log analysis.",Agreed-Justified,Agree,,Agree,,
Exp E,Exp J,2153,TSE,Data Management & Processing,LogAssist: Assisting Log Analysis Through Log Summarization,"Logs contain valuable information about the runtime behaviors of software systems. Thus, practitioners rely on logs for various tasks such as debugging, system comprehension, and anomaly detection. However, logs are difficult to analyze due to their unstructured nature and large size. In this paper, we propose a novel approach called LogAssist that assists practitioners with log analysis. LogAssist provides an organized and concise view of logs by first grouping logs into event sequences (i.e., workflows), which better illustrate the system runtime execution paths. Then, LogAssist compresses the log events in workflows by hiding consecutive events and applying n-gram modeling to identify common event sequences. We evaluated LogAssist on logs generated by one enterprise and two open source systems. We find that LogAssist can reduce the number of log events that practitioners need to investigate by up to 99 percent. Through a user study with 19 participants, we find that LogAssist can assist practitioners by reducing the time required for log analysis tasks by an average of 40 percent. The participants also rated LogAssist an average of 4.53 out of 5 for improving their experiences of performing log analysis. Finally, we document our experiences and lessons learned from developing and adopting LogAssist in practice. We believe that LogAssist and our reported experiences may lay the basis for future analysis and interactive exploration on logs.",Achievement,Capable,"LogAssist enables the users to analyze large and unstructured logs more efficiently; demonstrating their capability in managing difficult tasks. This relates to the value item ""Capable"" under ""Achievement"".","In the abstract of 'Paper X', it is explicitly stated that LogAssist reduces the number of log events that practitioners need to investigate by up to 99 percent and reduces the time required for log analysis tasks by an average of 40 percent. These contributions directly align with the value item Capable under the value Achievement. By enabling users to efficiently analyze large and unstructured logs, LogAssist empowers them to successfully tackle challenging tasks and achieve their goals, demonstrating their capability and competence in the software context.",Agreed-Justified,Agree,,Agree,,
Exp F,Exp J,2153,TSE,Data Management & Processing,LogAssist: Assisting Log Analysis Through Log Summarization,"Logs contain valuable information about the runtime behaviors of software systems. Thus, practitioners rely on logs for various tasks such as debugging, system comprehension, and anomaly detection. However, logs are difficult to analyze due to their unstructured nature and large size. In this paper, we propose a novel approach called LogAssist that assists practitioners with log analysis. LogAssist provides an organized and concise view of logs by first grouping logs into event sequences (i.e., workflows), which better illustrate the system runtime execution paths. Then, LogAssist compresses the log events in workflows by hiding consecutive events and applying n-gram modeling to identify common event sequences. We evaluated LogAssist on logs generated by one enterprise and two open source systems. We find that LogAssist can reduce the number of log events that practitioners need to investigate by up to 99 percent. Through a user study with 19 participants, we find that LogAssist can assist practitioners by reducing the time required for log analysis tasks by an average of 40 percent. The participants also rated LogAssist an average of 4.53 out of 5 for improving their experiences of performing log analysis. Finally, we document our experiences and lessons learned from developing and adopting LogAssist in practice. We believe that LogAssist and our reported experiences may lay the basis for future analysis and interactive exploration on logs.",Achievement,Successful,"With LogAssist; users could achieve a reduction of time spent on log analysis tasks by 40 percent with enhanced experience. This high success rate in performing tasks aligns with the value item ""Successful"" under ""Achievement"".","The justification for aligning 'Paper X' with the value item Successful and its corresponding value Achievement from a ""Software User"" perspective is that the paper explicitly states that LogAssist can assist practitioners by reducing the time required for log analysis tasks by an average of 40 percent. This significant reduction in time spent on log analysis tasks can be seen as a successful achievement for software users as it improves their efficiency and productivity. The main focus of the paper is on the practical benefits and positive experiences of using LogAssist, which aligns with the value of achieving successful outcomes in software-related tasks.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2154,TSE,Software Engineering Practices,Broken External Links on Stack Overflow,"Stack Overflow hosts valuable programming-related knowledge with 11,926,354 links that reference to the third-party websites. The links that reference to the resources hosted outside the Stack Overflow websites extend the Stack Overflow knowledge base substantially. However, with the rapid development of programming-related knowledge, many resources hosted on the Internet are not available anymore. Based on our analysis of the Stack Overflow data that was released on Jun. 2, 2019, 14.2 percent of the links on Stack Overflow are broken links. The broken links on Stack Overflow can obstruct viewers from obtaining desired programming-related knowledge, and potentially damage the reputation of the Stack Overflow as viewers might regard the posts with broken links as obsolete. In this paper, we characterize the broken links on Stack Overflow. 65 percent of the broken links in our sampled questions are used to show examples, e.g., code examples. 70 percent of the broken links in our sampled answers are used to provide supporting information, e.g., explaining a certain concept and describing a step to solve a problem. Only 1.67 percent of the posts with broken links are highlighted as such by viewers in the postsaEUR(tm) comments. Only 5.8 percent of the posts with broken links removed the broken links. Viewers cannot fully rely on the vote scores to detect broken links, as broken links are common across posts with different vote scores. The websites that host resources that can be maintained by their users are referenced by broken links the most on Stack Overflow aEUR"" a prominent example of such websites is GitHub. The posts and comments related to the web technologies, i.e., JavaScript, HTML, CSS, and jQuery, are associated with more broken links. Based on our findings, we shed lights for future directions and provide recommendations for practitioners and researchers.",Stimulation,Variation in Life,By providing recommendations for practitioners and researchers; the paper introduces variety and change in tackling the issue of broken links which aligns with Variation in Life in the value set Stimulation.,"Sure, let me clarify my justification. The paper's recommendations for practitioners and researchers introduce variety and change in addressing the issue of broken links on Stack Overflow. This aligns with the value item Variation in Life in the value set Stimulation because it promotes new approaches, strategies, and potential solutions in the software context. By encouraging different methods and perspectives, the paper stimulates creativity and curiosity among software users, enhancing their experience and engagement with the platform.",Agreed-Justified,Agree,,Agree,,
Exp B,Exp H,2154,TSE,Software Engineering Practices,Broken External Links on Stack Overflow,"Stack Overflow hosts valuable programming-related knowledge with 11,926,354 links that reference to the third-party websites. The links that reference to the resources hosted outside the Stack Overflow websites extend the Stack Overflow knowledge base substantially. However, with the rapid development of programming-related knowledge, many resources hosted on the Internet are not available anymore. Based on our analysis of the Stack Overflow data that was released on Jun. 2, 2019, 14.2 percent of the links on Stack Overflow are broken links. The broken links on Stack Overflow can obstruct viewers from obtaining desired programming-related knowledge, and potentially damage the reputation of the Stack Overflow as viewers might regard the posts with broken links as obsolete. In this paper, we characterize the broken links on Stack Overflow. 65 percent of the broken links in our sampled questions are used to show examples, e.g., code examples. 70 percent of the broken links in our sampled answers are used to provide supporting information, e.g., explaining a certain concept and describing a step to solve a problem. Only 1.67 percent of the posts with broken links are highlighted as such by viewers in the postsaEUR(tm) comments. Only 5.8 percent of the posts with broken links removed the broken links. Viewers cannot fully rely on the vote scores to detect broken links, as broken links are common across posts with different vote scores. The websites that host resources that can be maintained by their users are referenced by broken links the most on Stack Overflow aEUR"" a prominent example of such websites is GitHub. The posts and comments related to the web technologies, i.e., JavaScript, HTML, CSS, and jQuery, are associated with more broken links. Based on our findings, we shed lights for future directions and provide recommendations for practitioners and researchers.",Security,Healthy,The paper's effort in examining broken links and providing recommendations indicates caring for the health of information environment in Stack Overflow which can be associated with the value item Healthy under the value Security.,"In the context of a ""Software User,"" the alignment of 'Paper X' with the value item Healthy and its corresponding value Security can be justified based on the paper's focus on broken links in Stack Overflow. The presence of broken links can hinder the ability of users to access accurate and reliable programming-related knowledge, thus posing a potential security risk by hindering the user's ability to obtain the desired information. By examining and characterizing the broken links, the paper highlights the importance of ensuring a healthy information environment within Stack Overflow, ultimately contributing to the value of security for software users.",Agreed-Justified,Disagree,reconcile with coder_2,Disagree,"The justification doesn't align with the value item of the healthy
",
Exp D,Exp K,2156,TSE,AI & Machine Learning,Deep Learning Based Vulnerability Detection: Are We There Yet?,"Automated detection of software vulnerabilities is a fundamental problem in software security. Existing program analysis techniques either suffer from high false positives or false negatives. Recent progress in Deep Learning (DL) has resulted in a surge of interest in applying DL for automated vulnerability detection. Several recent studies have demonstrated promising results achieving an accuracy of up to 95 percent at detecting vulnerabilities. In this paper, we ask, aEURoehow well do the state-of-the-art DL-based techniques perform in a real-world vulnerability prediction scenario?aEUR To our surprise, we find that their performance drops by more than 50 percent. A systematic investigation of what causes such precipitous performance drop reveals that existing DL-based vulnerability prediction approaches suffer from challenges with the training data (e.g., data duplication, unrealistic distribution of vulnerable classes, etc.) and with the model choices (e.g., simple token-based models). As a result, these approaches often do not learn features related to the actual cause of the vulnerabilities. Instead, they learn unrelated artifacts from the dataset (e.g., specific variable/function names, etc.). Leveraging these empirical findings, we demonstrate how a more principled approach to data collection and model design, based on realistic settings of vulnerability prediction, can lead to better solutions. The resulting tools perform significantly better than the studied baselineaEUR""up to 33.57 percent boost in precision and 128.38 percent boost in recall compared to the best performing model in the literature. Overall, this paper elucidates existing DL-based vulnerability prediction systemsaEUR(tm) potential issues and draws a roadmap for future DL-based vulnerability prediction research.",Security,National Security,The paper contributes to improving the security of software by efficiently detecting vulnerabilities; which ultimately increases national security by ensuring that digital infrastructures are less prone to cyber-attacks.,"The justification for aligning 'Paper X' with the value item National Security and its corresponding value Security from a ""Software User"" perspective is based on the paper's focus on detecting vulnerabilities in software. By effectively identifying and addressing these vulnerabilities, the security of software systems can be significantly improved. This increased security ultimately contributes to national security by reducing the risk of cyber-attacks targeting digital infrastructures. Thus, the paper's main contribution aligns with the value of Security and its corresponding value item National Security.",Agreed-Clarified,Agree,,Agree,,
Exp E,Exp J,2156,TSE,AI & Machine Learning,Deep Learning Based Vulnerability Detection: Are We There Yet?,"Automated detection of software vulnerabilities is a fundamental problem in software security. Existing program analysis techniques either suffer from high false positives or false negatives. Recent progress in Deep Learning (DL) has resulted in a surge of interest in applying DL for automated vulnerability detection. Several recent studies have demonstrated promising results achieving an accuracy of up to 95 percent at detecting vulnerabilities. In this paper, we ask, aEURoehow well do the state-of-the-art DL-based techniques perform in a real-world vulnerability prediction scenario?aEUR To our surprise, we find that their performance drops by more than 50 percent. A systematic investigation of what causes such precipitous performance drop reveals that existing DL-based vulnerability prediction approaches suffer from challenges with the training data (e.g., data duplication, unrealistic distribution of vulnerable classes, etc.) and with the model choices (e.g., simple token-based models). As a result, these approaches often do not learn features related to the actual cause of the vulnerabilities. Instead, they learn unrelated artifacts from the dataset (e.g., specific variable/function names, etc.). Leveraging these empirical findings, we demonstrate how a more principled approach to data collection and model design, based on realistic settings of vulnerability prediction, can lead to better solutions. The resulting tools perform significantly better than the studied baselineaEUR""up to 33.57 percent boost in precision and 128.38 percent boost in recall compared to the best performing model in the literature. Overall, this paper elucidates existing DL-based vulnerability prediction systemsaEUR(tm) potential issues and draws a roadmap for future DL-based vulnerability prediction research.",Achievement,Successful,The paper discusses how its methods lead to a significant increase in the accuracy of vulnerability detection; which aligns with the 'Achievement' value as it enables software users to be more successful in maintaining a secure digital environment.,"The justification for labeling 'Paper X' as aligning with the value item Successful and its corresponding value Achievement from a ""Software User"" perspective is that the paper clearly states that its methods lead to a significant increase in the accuracy of vulnerability detection. This directly aligns with the value of Achievement as it enables software users to be more successful in maintaining a secure digital environment. By improving the vulnerability prediction systems, users can achieve the goal of ensuring the security of their software and achieving success in their endeavors.",Agreed-Justified,Agree,,Agree,,
Exp F,Exp J,2159,TSE,Software Project Management,Emotions and Perceived Productivity of Software Developers at the Workplace,"Emotions are known to impact cognitive skills, thus influencing job performance. This is also true for software development, which requires creativity and problem-solving abilities. In this paper, we report the results of a field study involving professional developers from five different companies. We provide empirical evidence that a link exists between emotions and perceived productivity at the workplace. Furthermore, we present a taxonomy of triggers for developersaEUR(tm) positive and negative emotions, based on the qualitative analysis of participantsaEUR(tm) self-reported answers collected through daily experience sampling. Finally, we experiment with a minimal set of non-invasive biometric sensors that we use as input for emotion detection. We found that positive emotional valence, neutral arousal, and high dominance are prevalent. We also found a positive correlation between emotional valence and perceived productivity, with a stronger correlation in the afternoon. Both social and individual breaks emerge as useful for restoring a positive mood. Furthermore, we found that a minimum set of non-invasive biometric sensors can be used as a predictor for emotions, provided that training is performed on an individual basis. While promising, our classifier performance is not yet robust enough for practical usage. Further data collection is required to strengthen the classifier, by also implementing individual fine-tuning of emotion models.",Self Direction,Independent,Paper X contributes to research on how emotions impact employee productivity; and how these emotions can be detected and predicted using biometric sensors. This aligns with the value item Independent and its corresponding value Self Direction; as understanding and controlling one's emotional responses allows for greater independence in individual thought and decision-making.,"In 'Paper X', the focus is on understanding and managing emotions in the workplace, particularly in the context of software development. By studying the link between emotions and job performance, the paper acknowledges the importance of individual control and self-direction in managing one's emotional responses. This aligns with the value item Independent and its corresponding value Self Direction from Schwartz's Taxonomy. In the software context, being able to understand and regulate emotions can lead to greater independence in decision-making and individual thought processes, allowing software users to navigate their work environment with autonomy.",Agreed-Clarified,Disagree,,Disagree,It is not aligned with independence. It talks about the relationship between emotions and productivity.,
Exp G,Exp K,2159,TSE,Software Project Management,Emotions and Perceived Productivity of Software Developers at the Workplace,"Emotions are known to impact cognitive skills, thus influencing job performance. This is also true for software development, which requires creativity and problem-solving abilities. In this paper, we report the results of a field study involving professional developers from five different companies. We provide empirical evidence that a link exists between emotions and perceived productivity at the workplace. Furthermore, we present a taxonomy of triggers for developersaEUR(tm) positive and negative emotions, based on the qualitative analysis of participantsaEUR(tm) self-reported answers collected through daily experience sampling. Finally, we experiment with a minimal set of non-invasive biometric sensors that we use as input for emotion detection. We found that positive emotional valence, neutral arousal, and high dominance are prevalent. We also found a positive correlation between emotional valence and perceived productivity, with a stronger correlation in the afternoon. Both social and individual breaks emerge as useful for restoring a positive mood. Furthermore, we found that a minimum set of non-invasive biometric sensors can be used as a predictor for emotions, provided that training is performed on an individual basis. While promising, our classifier performance is not yet robust enough for practical usage. Further data collection is required to strengthen the classifier, by also implementing individual fine-tuning of emotion models.",Stimulation,Variation in Life,The paper's focus on understanding and predicting emotional patterns can contribute to the variation in life for software users; as it might help users adjust their workflow or routine based on their emotions and predicted productivity levels. This aligns with the value item Variation in Life and its corresponding value Stimulation.,"In the context of a software user, the alignment of 'Paper X' with the value item Variation in Life and its corresponding value Stimulation is justified by the fact that the paper's exploration of emotional patterns and their impact on productivity can potentially introduce variation and stimulation in a user's software experience. By understanding and predicting emotional states, users may be able to adjust their workflow or routine to optimize their productivity and enhance their overall software experience, thereby introducing variation and stimulation into their daily lives.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2159,TSE,Software Project Management,Emotions and Perceived Productivity of Software Developers at the Workplace,"Emotions are known to impact cognitive skills, thus influencing job performance. This is also true for software development, which requires creativity and problem-solving abilities. In this paper, we report the results of a field study involving professional developers from five different companies. We provide empirical evidence that a link exists between emotions and perceived productivity at the workplace. Furthermore, we present a taxonomy of triggers for developersaEUR(tm) positive and negative emotions, based on the qualitative analysis of participantsaEUR(tm) self-reported answers collected through daily experience sampling. Finally, we experiment with a minimal set of non-invasive biometric sensors that we use as input for emotion detection. We found that positive emotional valence, neutral arousal, and high dominance are prevalent. We also found a positive correlation between emotional valence and perceived productivity, with a stronger correlation in the afternoon. Both social and individual breaks emerge as useful for restoring a positive mood. Furthermore, we found that a minimum set of non-invasive biometric sensors can be used as a predictor for emotions, provided that training is performed on an individual basis. While promising, our classifier performance is not yet robust enough for practical usage. Further data collection is required to strengthen the classifier, by also implementing individual fine-tuning of emotion models.",Achievement,Capable,By understanding and potentially controlling emotions through the use of biometric sensors; software users may be able to increase their own productivity. This aligns with the value item Capable under the value Achievement; which focuses on the desire to demonstrate capabilities and competence.,"The justification for aligning 'Paper X' with the value item Capable and its corresponding value Achievement from a ""Software User"" perspective is based on the premise that by understanding and potentially controlling emotions through the use of biometric sensors, software users can enhance their own productivity. This aligns with the value item Capable, as it pertains to the pursuit of demonstrating capabilities and competence. By leveraging this understanding of emotions and their impact, software users can take proactive measures to optimize their performance, thereby aligning with the value of Achievement in seeking to excel in their software-related tasks.",Agreed-Clarified,Agree,,Agree,,
Exp C,Exp K,2163,TSE,Accessibility & User Experience,Including Everyone Everywhere: Understanding Opportunities and Challenges of Geographic Gender-Inclusion in OSS,"The gender gap is a significant concern facing the software industry as the development becomes more geographically distributed. Widely shared reports indicate that gender differences may be specific to each region. However, how complete can these reports be with little to no research reflective of the Open Source Software (OSS) process and communities software is now commonly developed in? Our study presents a multi-region geographical analysis of gender inclusion on GitHub. This mixed-methods approach includes quantitatively investigating differences in gender inclusion in projects across geographic regions and investigate these trends over time using data from contributions to 21,456 project repositories. We also qualitatively understand the unique experiences of developers contributing to these projects through a survey that is strategically targeted to developers in various regions worldwide. Our findings indicate that gender diversity is low across all parts of the world, with no substantial difference across regions. However, there has been statistically significant improvement in diversity worldwide since 2014, with certain regions such as Africa improving at faster pace. We also find that most motivations and barriers to contributions (e.g., lack of resources to contribute and poor working environment) were shared across regions, however, some insightful differences, such as how to make projects more inclusive, did arise. From these findings, we derive and present implications for tools that can foster inclusion in open source software communities and empower contributions from everyone, everywhere.",Universalism,Equality,The paper contributes to investigating and understanding gender diversity in open source software development practices across different geographic regions and works towards creating equal opportunities for all genders to contribute. This directly aligns with the value item 'Equality' in the 'Universalism' value of Schwartz's Taxonomy.,"In the given justification, the alignment of 'Paper X' with the value item Equality and its corresponding value Universalism from a ""Software User"" perspective is based on the fact that the paper aims to investigate and understand gender diversity in open source software development practices across different geographic regions. By working towards creating equal opportunities for all genders to contribute, the paper aligns with the value of Equality, as it promotes fairness and equal treatment of individuals within the open source software community. This focus on inclusivity and equal representation supports the value of Universalism, which emphasizes the importance of broadmindedness, equality, and social justice.",Agreed-Justified,Agree,,Agree,,
Exp D,Exp K,2163,TSE,Accessibility & User Experience,Including Everyone Everywhere: Understanding Opportunities and Challenges of Geographic Gender-Inclusion in OSS,"The gender gap is a significant concern facing the software industry as the development becomes more geographically distributed. Widely shared reports indicate that gender differences may be specific to each region. However, how complete can these reports be with little to no research reflective of the Open Source Software (OSS) process and communities software is now commonly developed in? Our study presents a multi-region geographical analysis of gender inclusion on GitHub. This mixed-methods approach includes quantitatively investigating differences in gender inclusion in projects across geographic regions and investigate these trends over time using data from contributions to 21,456 project repositories. We also qualitatively understand the unique experiences of developers contributing to these projects through a survey that is strategically targeted to developers in various regions worldwide. Our findings indicate that gender diversity is low across all parts of the world, with no substantial difference across regions. However, there has been statistically significant improvement in diversity worldwide since 2014, with certain regions such as Africa improving at faster pace. We also find that most motivations and barriers to contributions (e.g., lack of resources to contribute and poor working environment) were shared across regions, however, some insightful differences, such as how to make projects more inclusive, did arise. From these findings, we derive and present implications for tools that can foster inclusion in open source software communities and empower contributions from everyone, everywhere.",Benevolence,A Spiritual Life,The paper's contribution involves deriving implications for tools that foster inclusion in open source software communities; thus contributing to the spiritual life of software users who are seeking inclusive and diverse software environments. This aligns with the value item 'A Spiritual Life' and its corresponding value 'Benevolence'.,"In the paper abstract, 'Paper X' aims to investigate gender inclusion in open source software communities and present implications for tools that can foster inclusion. The contribution of the paper aligns with the value item 'A Spiritual Life' and its corresponding value 'Benevolence' because it focuses on creating a more inclusive and diverse software environment, which can be seen as a positive and beneficial contribution to software users who value benevolence and seek a spiritual connection with the software they interact with.",Agreed-Justified,Agree,,Agree,,
Exp E,Exp J,2168,TSE,Code Generation & Analysis,Trimmer: An Automated System for Configuration-Based Software Debloating,"Software bloat has negative implications for security, reliability, and performance. To counter bloat, we propose Trimmer, a static analysis-based system for pruning unused functionality. Trimmer removes code that is unused with respect to user-provided command-line arguments and application-specific configuration files. Trimmer uses concrete memory tracking and a custom inter-procedural constant propagation analysis that facilitates dead code elimination. Our system supports both context-sensitive and context-insensitive constant propagation. We show that context-sensitive constant propagation is important for effective software pruning in most applications. We introduce sparse constant propagation that performs constant propagation only for configuration-hosting variables and show that it performs better (higher code size reductions) compared to constant propagation for all program variables. Overall, our results show that Trimmer reduces binary sizes for real-world programs with reasonable analysis times. Across 20 evaluated programs, we observe a mean binary size reduction of 22.7 percent and a maximum reduction of 62.7 percent. For 5 programs, we observe performance speedups ranging from 5 to 53 percent. Moreover, we show that winnowing software applications can reduce the program attack surface by removing code that contains exploitable vulnerabilities. We find that debloating using Trimmer removes CVEs in 4 applications.",Security,Healthy,The paper proposes a system; Trimmer; that by reducing software bloat; can potentially enhance the software's reliability and performance; thus indirectly contributing to user's health by reducing the likelihood of stress and frustration from poorly performing software.,"The justification for labeling 'Paper X' as aligning with the value item Healthy and its corresponding value Security from a ""Software User"" perspective is that by reducing software bloat, the proposed system Trimmer can improve the reliability and performance of software. This improvement can indirectly contribute to the user's health by reducing stress and frustration that may arise from using poorly performing software. When software functions smoothly and efficiently, users are less likely to experience negative emotions or negative impacts on their overall well-being.",Agreed-Clarified,Agree,,Agree,,
Exp F,Exp J,2168,TSE,Code Generation & Analysis,Trimmer: An Automated System for Configuration-Based Software Debloating,"Software bloat has negative implications for security, reliability, and performance. To counter bloat, we propose Trimmer, a static analysis-based system for pruning unused functionality. Trimmer removes code that is unused with respect to user-provided command-line arguments and application-specific configuration files. Trimmer uses concrete memory tracking and a custom inter-procedural constant propagation analysis that facilitates dead code elimination. Our system supports both context-sensitive and context-insensitive constant propagation. We show that context-sensitive constant propagation is important for effective software pruning in most applications. We introduce sparse constant propagation that performs constant propagation only for configuration-hosting variables and show that it performs better (higher code size reductions) compared to constant propagation for all program variables. Overall, our results show that Trimmer reduces binary sizes for real-world programs with reasonable analysis times. Across 20 evaluated programs, we observe a mean binary size reduction of 22.7 percent and a maximum reduction of 62.7 percent. For 5 programs, we observe performance speedups ranging from 5 to 53 percent. Moreover, we show that winnowing software applications can reduce the program attack surface by removing code that contains exploitable vulnerabilities. We find that debloating using Trimmer removes CVEs in 4 applications.",Security,Social Order,By reducing software bloat and eliminating code that contains exploitable vulnerabilities; Trimmer is aligning with the value of 'Security' and its value item 'Social Order'; as it can contribute to a safer user experience and potentially reduce cybercrime.,"By reducing software bloat and eliminating code that contains exploitable vulnerabilities, Trimmer can contribute to enhancing the security of software applications. This, in turn, can promote a sense of social order by reducing the potential for cybercrime and creating a safer user experience for individuals utilizing the software.",Agreed-Clarified,Agree,,Agree,,
Exp G,Exp K,2168,TSE,Code Generation & Analysis,Trimmer: An Automated System for Configuration-Based Software Debloating,"Software bloat has negative implications for security, reliability, and performance. To counter bloat, we propose Trimmer, a static analysis-based system for pruning unused functionality. Trimmer removes code that is unused with respect to user-provided command-line arguments and application-specific configuration files. Trimmer uses concrete memory tracking and a custom inter-procedural constant propagation analysis that facilitates dead code elimination. Our system supports both context-sensitive and context-insensitive constant propagation. We show that context-sensitive constant propagation is important for effective software pruning in most applications. We introduce sparse constant propagation that performs constant propagation only for configuration-hosting variables and show that it performs better (higher code size reductions) compared to constant propagation for all program variables. Overall, our results show that Trimmer reduces binary sizes for real-world programs with reasonable analysis times. Across 20 evaluated programs, we observe a mean binary size reduction of 22.7 percent and a maximum reduction of 62.7 percent. For 5 programs, we observe performance speedups ranging from 5 to 53 percent. Moreover, we show that winnowing software applications can reduce the program attack surface by removing code that contains exploitable vulnerabilities. We find that debloating using Trimmer removes CVEs in 4 applications.",Security,National Security,By reducing the program attack surface; Trimmer provides an added layer of security; which directly aligns with 'National Security'; given the global nature of software products and threats; upholding the value behind 'Security'.,"By reducing the program attack surface, Trimmer removes code that contains exploitable vulnerabilities, thereby providing enhanced security for software users. This aligns with the value of National Security because software products are globally accessible and can be targeted by threats from anywhere, making it important to uphold the value of Security to protect users' data and systems.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2174,TSE,AI & Machine Learning,Automatic Fairness Testing of Neural Classifiers Through Adversarial Sampling,"Although deep learning has demonstrated astonishing performance in many applications, there are still concerns about its dependability. One desirable property of deep learning applications with societal impact is fairness (i.e., non-discrimination). Unfortunately, discrimination might be intrinsically embedded into the models due to the discrimination in the training data. As a countermeasure, fairness testing systemically identifies discriminatory samples, which can be used to retrain the model and improve the modelaEUR(tm)s fairness. Existing fairness testing approaches however have two major limitations. First, they only work well on traditional machine learning models and have poor performance (e.g., effectiveness and efficiency) on deep learning models. Second, they only work on simple structured (e.g., tabular) data and are not applicable for domains such as text. In this work, we bridge the gap by proposing a scalable and effective approach for systematically searching for discriminatory samples while extending existing fairness testing approaches to address a more challenging domain, i.e., text classification. Compared with state-of-the-art methods, our approach only employs lightweight procedures like gradient computation and clustering, which is significantly more scalable and effective. Experimental results show that on average, our approach explores the search space much more effectively (9.62 and 2.38 times more than the state-of-the-art methods respectively on tabular and text datasets) and generates much more discriminatory samples (24.95 and 2.68 times) within a same reasonable time. Moreover, the retrained models reduce discrimination by 57.2 and 60.2 percent respectively on average.",Security,Social Order,The paper contributes to improving the dependability and fairness of deep learning applications; which aligns with the value item Social Order and its corresponding value Security.,"The alignment of 'Paper X' with the value item Social Order and its corresponding value Security is justified by the paper's contribution towards improving the fairness and dependability of deep learning applications. Fairness ensures that discrimination is minimized, promoting a just and equitable society. Security, on the other hand, entails the protection of individuals' rights and well-being. By addressing the issue of discrimination embedded in deep learning models, the paper directly aligns with the value item Social Order, as it aims to create a more equitable and secure environment within the software context for the software users.",Agreed-Clarified,Agree,,Disagree,The justification doesn't align with the value item of the social order,
Exp B,Exp H,2174,TSE,AI & Machine Learning,Automatic Fairness Testing of Neural Classifiers Through Adversarial Sampling,"Although deep learning has demonstrated astonishing performance in many applications, there are still concerns about its dependability. One desirable property of deep learning applications with societal impact is fairness (i.e., non-discrimination). Unfortunately, discrimination might be intrinsically embedded into the models due to the discrimination in the training data. As a countermeasure, fairness testing systemically identifies discriminatory samples, which can be used to retrain the model and improve the modelaEUR(tm)s fairness. Existing fairness testing approaches however have two major limitations. First, they only work well on traditional machine learning models and have poor performance (e.g., effectiveness and efficiency) on deep learning models. Second, they only work on simple structured (e.g., tabular) data and are not applicable for domains such as text. In this work, we bridge the gap by proposing a scalable and effective approach for systematically searching for discriminatory samples while extending existing fairness testing approaches to address a more challenging domain, i.e., text classification. Compared with state-of-the-art methods, our approach only employs lightweight procedures like gradient computation and clustering, which is significantly more scalable and effective. Experimental results show that on average, our approach explores the search space much more effectively (9.62 and 2.38 times more than the state-of-the-art methods respectively on tabular and text datasets) and generates much more discriminatory samples (24.95 and 2.68 times) within a same reasonable time. Moreover, the retrained models reduce discrimination by 57.2 and 60.2 percent respectively on average.",Security,Healthy,The paper contributes to enhancing the overall health and dependability of the deep learning models by identifying and rectifying discriminatory samples; which aligns with the value item Healthy and its corresponding value Security.,"In the context of a ""Software User,"" the alignment of 'Paper X' with the value item Healthy and its corresponding value Security can be justified as the paper focuses on enhancing the dependability of deep learning models by addressing the issue of discrimination in training data. By systematically identifying and rectifying discriminatory samples, the paper aims to create fairer models, which contributes to a healthier and more secure software environment. This aligns with the value item Healthy as it promotes fairness and non-discrimination in the use of deep learning applications, ultimately leading to a more secure experience for software users.",Agreed-Justified,Disagree,reconcile with coder_2,Disagree,"The justification doesn't align with the value item of the healthy
",
Exp C,Exp K,2174,TSE,AI & Machine Learning,Automatic Fairness Testing of Neural Classifiers Through Adversarial Sampling,"Although deep learning has demonstrated astonishing performance in many applications, there are still concerns about its dependability. One desirable property of deep learning applications with societal impact is fairness (i.e., non-discrimination). Unfortunately, discrimination might be intrinsically embedded into the models due to the discrimination in the training data. As a countermeasure, fairness testing systemically identifies discriminatory samples, which can be used to retrain the model and improve the modelaEUR(tm)s fairness. Existing fairness testing approaches however have two major limitations. First, they only work well on traditional machine learning models and have poor performance (e.g., effectiveness and efficiency) on deep learning models. Second, they only work on simple structured (e.g., tabular) data and are not applicable for domains such as text. In this work, we bridge the gap by proposing a scalable and effective approach for systematically searching for discriminatory samples while extending existing fairness testing approaches to address a more challenging domain, i.e., text classification. Compared with state-of-the-art methods, our approach only employs lightweight procedures like gradient computation and clustering, which is significantly more scalable and effective. Experimental results show that on average, our approach explores the search space much more effectively (9.62 and 2.38 times more than the state-of-the-art methods respectively on tabular and text datasets) and generates much more discriminatory samples (24.95 and 2.68 times) within a same reasonable time. Moreover, the retrained models reduce discrimination by 57.2 and 60.2 percent respectively on average.",Universalism,Social Justice,The paper's contribution to ensure non-discrimination in deep learning models aligns with the value item Social Justice and its corresponding value Universalism.,"In the abstract, 'Paper X' explicitly states that one desirable property of deep learning applications with societal impact is fairness and non-discrimination, and it proposes a scalable and effective approach for systematically searching for discriminatory samples in text classification models. This directly aligns with the value item Social Justice, which is a component of the value Universalism. By addressing discrimination and promoting fairness in deep learning models, the paper's contributions are aligned with the value item Social Justice and its corresponding value of Universalism from a ""Software User"" perspective.",Agreed-Clarified,Agree,,Agree,,
Exp D,Exp K,2175,TSE,Security & Privacy,Identifying Challenges for OSS Vulnerability Scanners - A Study & Test Suite,"The use of vulnerable open-source dependencies is a known problem in today's software development. Several vulnerability scanners to detect known-vulnerable dependencies appeared in the last decade, however, there exists no case study investigating the impact of development practices, e.g., forking, patching, re-bundling, on their performance. This paper studies (i) types of modifications that may affect vulnerable open-source dependencies and (ii) their impact on the performance of vulnerability scanners. Through an empirical study on 7,024 Java projects developed at SAP, we identified four types of modifications: re-compilation, re-bundling, metadata-removal and re-packaging. In particular, we found that more than 87 percent (56 percent, resp.) of the vulnerable Java classes considered occur in Maven Central in re-bundled (re-packaged, resp.) form. We assessed the impact of these modifications on the performance of the open-source vulnerability scanners OWASP Dependency-Check (OWASP) and Eclipse Steady, GitHub Security Alerts, and three commercial scanners. The results show that none of the scanners is able to handle all the types of modifications identified. Finally, we present Achilles, a novel test suite with 2,505 test cases that allow replicating the modifications on open-source dependencies.",Security,Sense of Belonging,The paper presents analysis software vulnerability which can directly contribute to a sense of belonging in a software context as users would feel safer interacting with the software which is not vulnerable and hence; would foster a sense of security and friendship especially in collaborative software use.,"In ""Paper X,"" the identification and analysis of software vulnerabilities directly align with the value item Sense of Belonging and its corresponding value Security from a ""Software User"" perspective. By addressing vulnerabilities, the paper ensures that users can feel safer while interacting with the software, which in turn fosters a sense of security and trust. This sense of security can contribute to a feeling of belonging, especially in collaborative software use where users rely on each other's code and depend on the overall security of the system. Ultimately, the paper's focus on vulnerability detection and mitigation directly contributes to a user's sense of belonging and the value of security within a software context.",Agreed-Clarified,Agree,,Agree,,
Exp E,Exp J,2175,TSE,Security & Privacy,Identifying Challenges for OSS Vulnerability Scanners - A Study & Test Suite,"The use of vulnerable open-source dependencies is a known problem in today's software development. Several vulnerability scanners to detect known-vulnerable dependencies appeared in the last decade, however, there exists no case study investigating the impact of development practices, e.g., forking, patching, re-bundling, on their performance. This paper studies (i) types of modifications that may affect vulnerable open-source dependencies and (ii) their impact on the performance of vulnerability scanners. Through an empirical study on 7,024 Java projects developed at SAP, we identified four types of modifications: re-compilation, re-bundling, metadata-removal and re-packaging. In particular, we found that more than 87 percent (56 percent, resp.) of the vulnerable Java classes considered occur in Maven Central in re-bundled (re-packaged, resp.) form. We assessed the impact of these modifications on the performance of the open-source vulnerability scanners OWASP Dependency-Check (OWASP) and Eclipse Steady, GitHub Security Alerts, and three commercial scanners. The results show that none of the scanners is able to handle all the types of modifications identified. Finally, we present Achilles, a novel test suite with 2,505 test cases that allow replicating the modifications on open-source dependencies.",Security,Healthy,The research focuses on the health of software which can be seen as an alignment to the 'Healthy' value item of 'Security' since a software without vulnerabilities is equivalent to a healthy software from a user's perspective.,"In the context of software, the term ""healthy"" refers to the absence of vulnerabilities or weaknesses that could potentially compromise the security of the software. In the case of 'Paper X', it specifically addresses the problem of vulnerable open-source dependencies and investigates the impact of development practices on the performance of vulnerability scanners. By identifying modifications that affect vulnerable dependencies and assessing their impact on scanner performance, the paper aims to improve the overall security of software. This aligns with the value item ""Healthy"" from the perspective of a software user, as a user would value software that is secure and free from vulnerabilities. Therefore, the focus on improving software security directly corresponds to the value item of ""Healthy"" and its corresponding value of ""Security"" from the standpoint of a software user.",Agreed-Clarified,Agree,,Agree,,
Exp F,Exp J,2175,TSE,Security & Privacy,Identifying Challenges for OSS Vulnerability Scanners - A Study & Test Suite,"The use of vulnerable open-source dependencies is a known problem in today's software development. Several vulnerability scanners to detect known-vulnerable dependencies appeared in the last decade, however, there exists no case study investigating the impact of development practices, e.g., forking, patching, re-bundling, on their performance. This paper studies (i) types of modifications that may affect vulnerable open-source dependencies and (ii) their impact on the performance of vulnerability scanners. Through an empirical study on 7,024 Java projects developed at SAP, we identified four types of modifications: re-compilation, re-bundling, metadata-removal and re-packaging. In particular, we found that more than 87 percent (56 percent, resp.) of the vulnerable Java classes considered occur in Maven Central in re-bundled (re-packaged, resp.) form. We assessed the impact of these modifications on the performance of the open-source vulnerability scanners OWASP Dependency-Check (OWASP) and Eclipse Steady, GitHub Security Alerts, and three commercial scanners. The results show that none of the scanners is able to handle all the types of modifications identified. Finally, we present Achilles, a novel test suite with 2,505 test cases that allow replicating the modifications on open-source dependencies.",Security,National Security,The paper's main contributions include the improvement of vulnerability scanners which is a direct contribution to the 'National Security' value item (in a software context; it could be seen as 'Software Security'); as detecting and dealing with vulnerabilities can prevent major security issues thus securing the products of the software nation.,"The paper's main contribution lies in analyzing the impact of development practices on the performance of vulnerability scanners, specifically for vulnerable open-source dependencies. By improving the scanners' ability to detect and handle modifications to these dependencies, such as re-bundling and re-packaging, the paper directly aligns with the value item of National Security. This alignment is based on the understanding that addressing vulnerabilities in software can ultimately enhance security measures and protect the nation's software assets from potential threats.",Agreed-Justified,Agree,,Agree,,
Exp G,Exp K,2185,TSE,Software Development Methodologies,Socio-Technical Grounded Theory for Software Engineering,"Grounded Theory (GT), a sociological research method designed to study social phenomena, is increasingly being used to investigate the human and social aspects of software engineering (SE). However, being written by and for sociologists, GT is often challenging for a majority of SE researchers to understand and apply. Additionally, SE researchers attempting ad hoc adaptations of traditional GT guidelines for modern socio-technical (ST) contexts often struggle in the absence of clear and relevant guidelines to do so, resulting in poor quality studies. To overcome these research community challenges and leverage modern research opportunities, this paper presents Socio-Technical Grounded Theory (STGT) designed to ease application and achieve quality outcomes. It defines what exactly is meant by an ST research context and presents the STGT guidelines that expand GT's philosophical foundations, provide increased clarity and flexibility in its methodological steps and procedures, define possible scope and contexts of application, encourage frequent reporting of a variety of interim, preliminary, and mature outcomes, and introduce nuanced evaluation guidelines for different outcomes. It is hoped that the SE research community and related ST disciplines such as computer science, data science, artificial intelligence, information systems, human computer/robot/AI interaction, human-centered emerging technologies (and increasingly other disciplines being transformed by rapid digitalisation and AI-based augmentation), will benefit from applying STGT to conduct quality research studies and systematically produce rich findings and mature theories with confidence.",Achievement,Intelligent,The paper aims to increase the understanding of software users by refining a research method,"In 'Paper X', the main contribution is the introduction of Socio-Technical Grounded Theory (STGT) as a research method to study social aspects of software engineering. By refining the traditional Grounded Theory (GT) guidelines and providing clarity and flexibility, STGT aims to enhance the understanding of software users. This aligns with the value item Intelligent and its corresponding value Achievement because the paper seeks to improve the quality of research studies and produce rich findings and mature theories, demonstrating a focus on intellectual growth and successful outcomes in understanding software users' perspectives.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2185,TSE,Software Development Methodologies,Socio-Technical Grounded Theory for Software Engineering,"Grounded Theory (GT), a sociological research method designed to study social phenomena, is increasingly being used to investigate the human and social aspects of software engineering (SE). However, being written by and for sociologists, GT is often challenging for a majority of SE researchers to understand and apply. Additionally, SE researchers attempting ad hoc adaptations of traditional GT guidelines for modern socio-technical (ST) contexts often struggle in the absence of clear and relevant guidelines to do so, resulting in poor quality studies. To overcome these research community challenges and leverage modern research opportunities, this paper presents Socio-Technical Grounded Theory (STGT) designed to ease application and achieve quality outcomes. It defines what exactly is meant by an ST research context and presents the STGT guidelines that expand GT's philosophical foundations, provide increased clarity and flexibility in its methodological steps and procedures, define possible scope and contexts of application, encourage frequent reporting of a variety of interim, preliminary, and mature outcomes, and introduce nuanced evaluation guidelines for different outcomes. It is hoped that the SE research community and related ST disciplines such as computer science, data science, artificial intelligence, information systems, human computer/robot/AI interaction, human-centered emerging technologies (and increasingly other disciplines being transformed by rapid digitalisation and AI-based augmentation), will benefit from applying STGT to conduct quality research studies and systematically produce rich findings and mature theories with confidence.",Conformity,Self-Discipline,"The paper proposes a new theory; termed as Socio-Technical Grounded Theory (STGT) which requires discipline from software users to adapt and follow. Hence; it corresponds with the value item ""Self-Discipline"" from the ""Conformity"" value category.","In the 'Paper X' abstract, the authors introduce Socio-Technical Grounded Theory (STGT) as a new approach for conducting research in software engineering. They mention that STGT provides guidelines and methodological steps for researchers to follow, and encourages the frequent reporting of various outcomes. This implies that software users, in order to effectively apply STGT, would need to have self-discipline in following these guidelines and procedures. Therefore, the alignment with the value item of ""Self-Discipline"" from the ""Conformity"" value category is evident as software users are expected to conform to the guidelines and requirements set forth by STGT.",Agreed-Justified,Agree,,Agree,,
Exp B,Exp H,2185,TSE,Software Development Methodologies,Socio-Technical Grounded Theory for Software Engineering,"Grounded Theory (GT), a sociological research method designed to study social phenomena, is increasingly being used to investigate the human and social aspects of software engineering (SE). However, being written by and for sociologists, GT is often challenging for a majority of SE researchers to understand and apply. Additionally, SE researchers attempting ad hoc adaptations of traditional GT guidelines for modern socio-technical (ST) contexts often struggle in the absence of clear and relevant guidelines to do so, resulting in poor quality studies. To overcome these research community challenges and leverage modern research opportunities, this paper presents Socio-Technical Grounded Theory (STGT) designed to ease application and achieve quality outcomes. It defines what exactly is meant by an ST research context and presents the STGT guidelines that expand GT's philosophical foundations, provide increased clarity and flexibility in its methodological steps and procedures, define possible scope and contexts of application, encourage frequent reporting of a variety of interim, preliminary, and mature outcomes, and introduce nuanced evaluation guidelines for different outcomes. It is hoped that the SE research community and related ST disciplines such as computer science, data science, artificial intelligence, information systems, human computer/robot/AI interaction, human-centered emerging technologies (and increasingly other disciplines being transformed by rapid digitalisation and AI-based augmentation), will benefit from applying STGT to conduct quality research studies and systematically produce rich findings and mature theories with confidence.",Security,Social Order,"Through the presentation of STGT; the paper might indirectly promote a social order among software users; by ensuring clear and relevant guidelines for modern socio-technical contexts. It aligns with value item ""Social Order"" from the ""Security"" value category.","In the paper abstract, it is stated that Socio-Technical Grounded Theory (STGT) provides clear and relevant guidelines for modern socio-technical contexts. By doing so, it can help researchers in the software engineering field to conduct quality research studies and produce mature theories with confidence. This emphasis on clear guidelines for conducting research and producing quality outcomes aligns with the value item of Social Order within the Security value category in Schwartz's Taxonomy. Social order refers to the desire for a clear structure and guidelines that ensure stability and predictability in society. Therefore, the alignment between the main contributions of the paper and the value item of Social Order is evident in promoting a sense of security and stability within the software context.",Agreed-Justified,Agree,,Agree,,
Exp C,Exp K,2190,TSE,Software Testing & QA,Mutation Analysis for Cyber-Physical Systems: Scalable Solutions and Results in the Space Domain,"On-board embedded software developed for spaceflight systems (space software) must adhere to stringent software quality assurance procedures. For example, verification and validation activities are typically performed and assessed by third party organizations. To further minimize the risk of human mistakes, space agencies, such as the European Space Agency (ESA), are looking for automated solutions for the assessment of software testing activities, which play a crucial role in this context. Though space software is our focus here, it should be noted that such software shares the above considerations, to a large extent, with embedded software in many other types of cyber-physical systems. Over the years, mutation analysis has shown to be a promising solution for the automated assessment of test suites; it consists of measuring the quality of a test suite in terms of the percentage of injected faults leading to a test failure. A number of optimization techniques, addressing scalability and accuracy problems, have been proposed to facilitate the industrial adoption of mutation analysis. However, to date, two major problems prevent space agencies from enforcing mutation analysis in space software development. First, there is uncertainty regarding the feasibility of applying mutation analysis optimization techniques in their context. Second, most of the existing techniques either can break the real-time requirements common in embedded software or cannot be applied when the software is tested in Software Validation Facilities, including CPU emulators and sensor simulators. In this paper, we enhance mutation analysis optimization techniques to enable their applicability to embedded software and propose a pipeline that successfully integrates them to address scalability and accuracy issues in this context, as described above. Further, we report on the largest study involving embedded software systems in the mutation analysis literature. Our research is part of a research project funded by ESA ESTEC involving private companies (GomSpace Luxembourg and LuxSpace) in the space sector. These industry partners provided the case studies reported in this paper; they include an on-board software system managing a microsatellite currently on-orbit, a set of libraries used in deployed cubesats, and a mathematical library certified by ESA.",Achievement,Intelligent,The paper presents an enhancement to mutation analysis optimization techniques that enables their applicability to embedded software; contributing to the intelligent engagement of software users. The work done among private companies in the space sector also demonstrates an intention towards successful outcomes in space software development which aligns with the value item Intelligence and its corresponding value Achievement.,"The enhancement to mutation analysis optimization techniques in 'Paper X' directly aligns with the value item Intelligent and its corresponding value Achievement from a ""Software User"" perspective because it introduces improvements that enable more intelligent engagement and interactions for software users. By enhancing the accuracy and scalability of mutation analysis, the paper contributes to the achievement of successful outcomes in space software development, which requires intelligent decision-making and problem-solving abilities. Additionally, the collaboration between private companies in the space sector demonstrates a commitment to achieving high levels of intelligence and accomplishment in the field of software development.",Agreed-Justified,Agree,,Agree,,
Exp E,Exp J,2190,TSE,Software Testing & QA,Mutation Analysis for Cyber-Physical Systems: Scalable Solutions and Results in the Space Domain,"On-board embedded software developed for spaceflight systems (space software) must adhere to stringent software quality assurance procedures. For example, verification and validation activities are typically performed and assessed by third party organizations. To further minimize the risk of human mistakes, space agencies, such as the European Space Agency (ESA), are looking for automated solutions for the assessment of software testing activities, which play a crucial role in this context. Though space software is our focus here, it should be noted that such software shares the above considerations, to a large extent, with embedded software in many other types of cyber-physical systems. Over the years, mutation analysis has shown to be a promising solution for the automated assessment of test suites; it consists of measuring the quality of a test suite in terms of the percentage of injected faults leading to a test failure. A number of optimization techniques, addressing scalability and accuracy problems, have been proposed to facilitate the industrial adoption of mutation analysis. However, to date, two major problems prevent space agencies from enforcing mutation analysis in space software development. First, there is uncertainty regarding the feasibility of applying mutation analysis optimization techniques in their context. Second, most of the existing techniques either can break the real-time requirements common in embedded software or cannot be applied when the software is tested in Software Validation Facilities, including CPU emulators and sensor simulators. In this paper, we enhance mutation analysis optimization techniques to enable their applicability to embedded software and propose a pipeline that successfully integrates them to address scalability and accuracy issues in this context, as described above. Further, we report on the largest study involving embedded software systems in the mutation analysis literature. Our research is part of a research project funded by ESA ESTEC involving private companies (GomSpace Luxembourg and LuxSpace) in the space sector. These industry partners provided the case studies reported in this paper; they include an on-board software system managing a microsatellite currently on-orbit, a set of libraries used in deployed cubesats, and a mathematical library certified by ESA.",Security,Healthy,The paper aims at improving the health of the software systems through enhancing mutation analysis optimization techniques and proposing a pipeline for scalability and accuracy issues. Given that this would also minimize the risk of human mistakes; this fundamentally aligns with the value item Healthy and its corresponding value Security.,"The justification for aligning 'Paper X' with the value item Healthy and its corresponding value Security is based on the fact that the paper's main contributions focus on improving the health and quality of software systems. By enhancing mutation analysis optimization techniques and proposing a pipeline for scalability and accuracy issues, the paper aims to minimize the risk of human mistakes in software testing activities. This directly aligns with the value item Healthy, as it emphasizes the importance of ensuring the well-being and soundness of software systems. Furthermore, by enhancing the reliability and robustness of software through automated assessment, the paper contributes to the overall security of the software systems, aligning with the value item Security.",Agreed-Justified,Agree,,Agree,,
Exp F,Exp J,2191,TSE,Software Project Management,Pots of Gold at the End of the Rainbow: What is Success for Open Source Contributors?,"Success in Open Source Software (OSS) is often perceived as an exclusively code-centric endeavor. This perception can exclude a variety of individuals with a diverse set of skills and backgrounds, in turn helping exacerbate the current diversity & inclusion imbalance in OSS. Because one's perspective of success can affect one's personal, professional, and life choices, to support a diverse class of individuals we must first understand how OSS contributors understand success. Thus far, research has used a uni-dimensional, code-centric lens to define success. In this paper, we challenge this status quo to reveal OSS contributorsaEUR(tm) multifaceted definitions of success. We do so through interviews with 27 OSS contributors whose communities recognize them as successful, and a follow-up open survey with 193 OSS contributors. Our study provides nuanced definitions of success perceptions in OSS, which might help devise strategies to attract and retain a diverse set of contributors, helping them attain their unique aEURoepot of gold at the end of the rainbowaEUR.",Achievement,Successful,The study aims at understanding open source software (OSS) contributors' success definitions to help them attain their goals; which aligns with 'Successful' value item under 'Achievement' value.,"The justification for aligning 'Paper X' with the value item Successful and its corresponding value Achievement from a ""Software User"" perspective is based on the fact that the study examines how OSS contributors define success and aims to support them in achieving their goals. By understanding their perspectives and providing strategies for success, the paper directly addresses the value of Achievement and aligns with the goal of being Successful. This alignment is significant from the perspective of a ""Software User"" as it highlights the importance of diverse contributors and their success in shaping and improving open source software.",Agreed-Justified,Agree,,Agree,,
Exp G,Exp K,2191,TSE,Software Project Management,Pots of Gold at the End of the Rainbow: What is Success for Open Source Contributors?,"Success in Open Source Software (OSS) is often perceived as an exclusively code-centric endeavor. This perception can exclude a variety of individuals with a diverse set of skills and backgrounds, in turn helping exacerbate the current diversity & inclusion imbalance in OSS. Because one's perspective of success can affect one's personal, professional, and life choices, to support a diverse class of individuals we must first understand how OSS contributors understand success. Thus far, research has used a uni-dimensional, code-centric lens to define success. In this paper, we challenge this status quo to reveal OSS contributorsaEUR(tm) multifaceted definitions of success. We do so through interviews with 27 OSS contributors whose communities recognize them as successful, and a follow-up open survey with 193 OSS contributors. Our study provides nuanced definitions of success perceptions in OSS, which might help devise strategies to attract and retain a diverse set of contributors, helping them attain their unique aEURoepot of gold at the end of the rainbowaEUR.",Benevolence,Helpful,The paper's goal is to devise strategies that would help in attracting and retaining diverse contributors; thus promoting collaborative well-being; aligning with the 'Helpful' value item under 'Benevolence' value.,"The main contribution of 'Paper X' aligns with the value item Helpful and its corresponding value Benevolence from a ""Software User"" perspective because the paper seeks to understand and support a diverse class of individuals in the context of Open Source Software (OSS). By investigating how OSS contributors perceive success and providing nuanced definitions of success perceptions in OSS, the paper aims to devise strategies that would attract and retain a diverse set of contributors, thus promoting a collaborative and supportive environment aligned with the value of being helpful and benevolent towards others.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2191,TSE,Software Project Management,Pots of Gold at the End of the Rainbow: What is Success for Open Source Contributors?,"Success in Open Source Software (OSS) is often perceived as an exclusively code-centric endeavor. This perception can exclude a variety of individuals with a diverse set of skills and backgrounds, in turn helping exacerbate the current diversity & inclusion imbalance in OSS. Because one's perspective of success can affect one's personal, professional, and life choices, to support a diverse class of individuals we must first understand how OSS contributors understand success. Thus far, research has used a uni-dimensional, code-centric lens to define success. In this paper, we challenge this status quo to reveal OSS contributorsaEUR(tm) multifaceted definitions of success. We do so through interviews with 27 OSS contributors whose communities recognize them as successful, and a follow-up open survey with 193 OSS contributors. Our study provides nuanced definitions of success perceptions in OSS, which might help devise strategies to attract and retain a diverse set of contributors, helping them attain their unique aEURoepot of gold at the end of the rainbowaEUR.",Universalism,Equality,By focusing on diversity and inclusion in the OSS community; the paper directs towards 'Equality'; a value item under 'Universalism'.,"In 'Paper X', the main contribution is focusing on the diversity and inclusion imbalance in the OSS community. This aligns with the value item Equality and its corresponding value Universalism because by addressing the existing imbalance and striving for diversity and inclusion, the paper promotes equality among contributors, regardless of their backgrounds or skills. This directly supports the value of Universalism, which emphasizes broadmindedness, equality, and social justice.",Agreed-Justified,Agree,,Agree,,
Exp B,Exp H,2192,TSE,Accessibility & User Experience,Accessible or Not? An Empirical Investigation of Android App Accessibility,"Mobile apps provide new opportunities to people with disabilities to act independently in the world. Following the law of the US, EU, mobile OS vendors such as Google and Apple have included accessibility features in their mobile systems and provide a set of guidelines and toolsets for ensuring mobile app accessibility. Motivated by this trend, researchers have conducted empirical studies by using the inaccessibility issue rate of each page (i.e., screen level) to represent the characteristics of mobile app accessibility. However, there still lacks an empirical investigation directly focusing on the issues themselves (i.e., issue level) to unveil more fine-grained findings, due to the lack of an effective issue detection method and a relatively comprehensive dataset of issues. To fill in this literature gap, we first propose an automated app page exploration tool, named Xbot, to facilitate app accessibility testing and automatically collect accessibility issues by leveraging the instrumentation technique and static program analysis. Owing to the relatively high activity coverage (around 80%) achieved by Xbot when exploring apps, Xbot achieves better performance on accessibility issue collection than existing testing tools such as Google Monkey. With Xbot, we are able to collect a relatively comprehensive accessibility issue dataset and finally collect 86,767 issues from 2,270 unique apps including both closed-source and open-source apps, based on which we further carry out an empirical study from the perspective of accessibility issues themselves to investigate novel characteristics of accessibility issues. Specifically, we extensively investigate these issues by checking 1) the overall severity of issues with multiple criteria, 2) the in-depth relation between issue types and app categories, GUI component types, 3) the frequent issue patterns quantitatively, and 4) the fixing status of accessibility issues. Finally, we highlight some insights to the community and hope to raise the attention to maintaining mobile app accessibility for users especially the elderly and disabled.",Stimulation,Excitement in Life,The paper contributes to developing an app exploration tool; Xbot that automatically identifies and catalogs accessibility issues. This aligns with the value item Excitement in Life and its corresponding value Stimulation; as this tool's work can significantly contribute to creating more accessible and exciting mobile applications for users; especially those with disabilities.,"The development of the app exploration tool, Xbot, as described in 'Paper X', aligns with the value item Excitement in Life and its corresponding value Stimulation because it introduces a new and innovative approach to identifying and cataloging accessibility issues in mobile apps. By automating the process and achieving a high activity coverage, Xbot provides a more efficient and effective way to ensure mobile app accessibility. This directly contributes to creating more exciting and stimulating mobile applications for all users, especially those with disabilities, as it enables developers to address and fix accessibility issues, thereby enhancing the overall user experience and enjoyment of using mobile apps.",Agreed-Justified,Agree,,Agree,,
Exp C,Exp K,2192,TSE,Accessibility & User Experience,Accessible or Not? An Empirical Investigation of Android App Accessibility,"Mobile apps provide new opportunities to people with disabilities to act independently in the world. Following the law of the US, EU, mobile OS vendors such as Google and Apple have included accessibility features in their mobile systems and provide a set of guidelines and toolsets for ensuring mobile app accessibility. Motivated by this trend, researchers have conducted empirical studies by using the inaccessibility issue rate of each page (i.e., screen level) to represent the characteristics of mobile app accessibility. However, there still lacks an empirical investigation directly focusing on the issues themselves (i.e., issue level) to unveil more fine-grained findings, due to the lack of an effective issue detection method and a relatively comprehensive dataset of issues. To fill in this literature gap, we first propose an automated app page exploration tool, named Xbot, to facilitate app accessibility testing and automatically collect accessibility issues by leveraging the instrumentation technique and static program analysis. Owing to the relatively high activity coverage (around 80%) achieved by Xbot when exploring apps, Xbot achieves better performance on accessibility issue collection than existing testing tools such as Google Monkey. With Xbot, we are able to collect a relatively comprehensive accessibility issue dataset and finally collect 86,767 issues from 2,270 unique apps including both closed-source and open-source apps, based on which we further carry out an empirical study from the perspective of accessibility issues themselves to investigate novel characteristics of accessibility issues. Specifically, we extensively investigate these issues by checking 1) the overall severity of issues with multiple criteria, 2) the in-depth relation between issue types and app categories, GUI component types, 3) the frequent issue patterns quantitatively, and 4) the fixing status of accessibility issues. Finally, we highlight some insights to the community and hope to raise the attention to maintaining mobile app accessibility for users especially the elderly and disabled.",Security,Healthy,The paper contributes an automated app page exploration tool to facilitate app accessibility testing; raising attention to mobile app accessibility for users. This aligns with the value item Healthy and corresponding value Security; as improving app accessibility is essential to enhance user's digital health and well-being; especially for the elderly and disabled.,"In the context of a ""Software User,"" the main contribution of 'Paper X' aligns with the value item Healthy and its corresponding value Security from Schwartz's Taxonomy. By proposing an automated app page exploration tool to improve mobile app accessibility, the paper aims to enhance the digital health and well-being of users, particularly the elderly and disabled. The improved accessibility of apps contributes to a sense of security for users, as they can navigate and interact with technology more easily, promoting their overall health and quality of life.",Agreed-Justified,Agree,,Agree,,
Exp D,Exp K,2192,TSE,Accessibility & User Experience,Accessible or Not? An Empirical Investigation of Android App Accessibility,"Mobile apps provide new opportunities to people with disabilities to act independently in the world. Following the law of the US, EU, mobile OS vendors such as Google and Apple have included accessibility features in their mobile systems and provide a set of guidelines and toolsets for ensuring mobile app accessibility. Motivated by this trend, researchers have conducted empirical studies by using the inaccessibility issue rate of each page (i.e., screen level) to represent the characteristics of mobile app accessibility. However, there still lacks an empirical investigation directly focusing on the issues themselves (i.e., issue level) to unveil more fine-grained findings, due to the lack of an effective issue detection method and a relatively comprehensive dataset of issues. To fill in this literature gap, we first propose an automated app page exploration tool, named Xbot, to facilitate app accessibility testing and automatically collect accessibility issues by leveraging the instrumentation technique and static program analysis. Owing to the relatively high activity coverage (around 80%) achieved by Xbot when exploring apps, Xbot achieves better performance on accessibility issue collection than existing testing tools such as Google Monkey. With Xbot, we are able to collect a relatively comprehensive accessibility issue dataset and finally collect 86,767 issues from 2,270 unique apps including both closed-source and open-source apps, based on which we further carry out an empirical study from the perspective of accessibility issues themselves to investigate novel characteristics of accessibility issues. Specifically, we extensively investigate these issues by checking 1) the overall severity of issues with multiple criteria, 2) the in-depth relation between issue types and app categories, GUI component types, 3) the frequent issue patterns quantitatively, and 4) the fixing status of accessibility issues. Finally, we highlight some insights to the community and hope to raise the attention to maintaining mobile app accessibility for users especially the elderly and disabled.",Universalism,Protecting the Environment,The paper contributes to the advancement of mobile app accessibility; paying special attention to people with disabilities. This aligns with the value item Protecting the Environment and its corresponding value Universalism; as improving accessibility can be seen as an effort to protect and improve the digital environment for this group of users.,"I apologize for any confusion. The alignment between 'Paper X' and the value item Protecting the Environment and its corresponding value Universalism is not based on a literal interpretation of protecting the physical environment. Rather, it is an analogy emphasizing the importance of creating an inclusive and accessible digital environment to support and uplift individuals with disabilities. By improving mobile app accessibility, the paper contributes to protecting the digital environment for this user group, aligning with the value of Universalism, which promotes equality and inclusivity.",Agreed-Clarified,Agree,,Agree,,
Exp E,Exp J,2194,TSE,Software Project Management,Exploring the Use of Chatrooms by Developers: An Empirical Study on Slack and Gitter,"Communication is critical for the software development teams to maintain project awareness, facilitate project co-ordination and avoid misunderstandings. The features offered in the chatrooms, such as private messaging, group conversations, and code sharing help accommodate the communication needs of the software development teams. Therefore, chatrooms have been increasingly adopted among the developers. Since the last study on Slack performed by (Lin et al. 2016), the audience of Slack has more than doubled possibly leading to an evolution of the ways Slack is used; while another rich community formed around Gitter and remains unstudied. In this paper, we perform an investigative study using qualitative and quantitative techniques to gain insights on the use of popular modern chatrooms, specifically Slack and Gitter. Based on the survey responses from 163 developers, the interviews with 21 developers, and the chatroom data collected from 11 Slack and 770 Gitter rooms, we are able to uncover the reasons behind the use of Slack and Gitter, the perceived impact on the associated projects, and the quality determinants of the two chatrooms. We find that the developers seek knowledge from the chatrooms to obtain timely feedback from experts, and in return share their expertise to build the project community and their reputations. Furthermore, it is perceived by the Gitter developers that the chatrooms have an impact on prioritizing the new features and the bug fixes. In Slack, the most reported impact concerns an increased project awareness, in terms of a better tracking of the work progress. As reported on the developersaEUR(tm) survey, both Slack and Gitter chat services have a visible impact on mentoring developers, and sharing the best practices. In terms of quality determinants, a non-ephemeral history and a better history management (e.g., advanced search) could be keys for both chat services to reach their full potential.",Achievement,Intelligent,The chatrooms like Slack and Gitter allow developers to seek knowledge from experts and share their expertise; which contributes to their Intelligence; a value item under the Achievement value.,"In the abstract of 'Paper X', it is explicitly stated that developers use chatrooms like Slack and Gitter to seek knowledge from experts and share their expertise. This activity of seeking knowledge and sharing expertise can be seen as contributing to the intelligence of the developers, as it allows them to acquire new information and enhance their skills in the software development domain. Therefore, aligning the main contributions of 'Paper X' with the value item Intelligent and its corresponding value Achievement from a ""Software User"" perspective is justified based on the direct evidence provided in the abstract.",Agreed-Clarified,Agree,,Agree,,
Exp F,Exp J,2194,TSE,Software Project Management,Exploring the Use of Chatrooms by Developers: An Empirical Study on Slack and Gitter,"Communication is critical for the software development teams to maintain project awareness, facilitate project co-ordination and avoid misunderstandings. The features offered in the chatrooms, such as private messaging, group conversations, and code sharing help accommodate the communication needs of the software development teams. Therefore, chatrooms have been increasingly adopted among the developers. Since the last study on Slack performed by (Lin et al. 2016), the audience of Slack has more than doubled possibly leading to an evolution of the ways Slack is used; while another rich community formed around Gitter and remains unstudied. In this paper, we perform an investigative study using qualitative and quantitative techniques to gain insights on the use of popular modern chatrooms, specifically Slack and Gitter. Based on the survey responses from 163 developers, the interviews with 21 developers, and the chatroom data collected from 11 Slack and 770 Gitter rooms, we are able to uncover the reasons behind the use of Slack and Gitter, the perceived impact on the associated projects, and the quality determinants of the two chatrooms. We find that the developers seek knowledge from the chatrooms to obtain timely feedback from experts, and in return share their expertise to build the project community and their reputations. Furthermore, it is perceived by the Gitter developers that the chatrooms have an impact on prioritizing the new features and the bug fixes. In Slack, the most reported impact concerns an increased project awareness, in terms of a better tracking of the work progress. As reported on the developersaEUR(tm) survey, both Slack and Gitter chat services have a visible impact on mentoring developers, and sharing the best practices. In terms of quality determinants, a non-ephemeral history and a better history management (e.g., advanced search) could be keys for both chat services to reach their full potential.",Benevolence,Helpful,Developers using these chatrooms are not only seeking information but also sharing their expertise to build the project community; thus being Helpful; which aligns with the Benevolence value.,"In the context of 'Paper X', the alignment with the value item ""Helpful"" and its corresponding value ""Benevolence"" is evident as developers using chatrooms are not only seeking knowledge and feedback but also actively sharing their expertise to build the project community. This act of sharing and helping others aligns with the value of Benevolence, as it emphasizes kindness, altruism, and a willingness to support and assist others.",Agreed-Clarified,Agree,,Agree,,
Exp G,Exp K,2194,TSE,Software Project Management,Exploring the Use of Chatrooms by Developers: An Empirical Study on Slack and Gitter,"Communication is critical for the software development teams to maintain project awareness, facilitate project co-ordination and avoid misunderstandings. The features offered in the chatrooms, such as private messaging, group conversations, and code sharing help accommodate the communication needs of the software development teams. Therefore, chatrooms have been increasingly adopted among the developers. Since the last study on Slack performed by (Lin et al. 2016), the audience of Slack has more than doubled possibly leading to an evolution of the ways Slack is used; while another rich community formed around Gitter and remains unstudied. In this paper, we perform an investigative study using qualitative and quantitative techniques to gain insights on the use of popular modern chatrooms, specifically Slack and Gitter. Based on the survey responses from 163 developers, the interviews with 21 developers, and the chatroom data collected from 11 Slack and 770 Gitter rooms, we are able to uncover the reasons behind the use of Slack and Gitter, the perceived impact on the associated projects, and the quality determinants of the two chatrooms. We find that the developers seek knowledge from the chatrooms to obtain timely feedback from experts, and in return share their expertise to build the project community and their reputations. Furthermore, it is perceived by the Gitter developers that the chatrooms have an impact on prioritizing the new features and the bug fixes. In Slack, the most reported impact concerns an increased project awareness, in terms of a better tracking of the work progress. As reported on the developersaEUR(tm) survey, both Slack and Gitter chat services have a visible impact on mentoring developers, and sharing the best practices. In terms of quality determinants, a non-ephemeral history and a better history management (e.g., advanced search) could be keys for both chat services to reach their full potential.",Universalism,Broadmindedness,By facilitating the sharing of expert knowledge and best practices through chatrooms; the paper contributes to encouraging Broadmindedness among developers; which is a value item under Universalism.,"The main contribution of 'Paper X' is to uncover the reasons behind the use of Slack and Gitter chatrooms in software development teams, as well as the impact and quality determinants of these chat services. The paper highlights that developers seek knowledge from the chatrooms to obtain timely feedback from experts and share their own expertise, fostering an environment of knowledge sharing and collaboration. This aligns with the value item Broadmindedness, as the paper promotes open-mindedness and receptiveness to different perspectives and expertise from the software user's viewpoint. By encouraging developers to be open to diverse knowledge and ideas, the paper contributes to the value of Universalism, which emphasizes the importance of equality, broadmindedness, and social justice.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2199,TSE,Software Development Methodologies,How Templated Requirements Specifications Inhibit Creativity in Software Engineering,"Desiderata is a general term for stakeholder needs, desires or preferences. Recent experiments demonstrate that presenting desiderata as templated requirements specifications leads to less creative solutions. However, these experiments do not establish how the presentation of desiderata affects design creativity. This study, therefore, aims to explore the cognitive mechanisms by which presenting desiderata as templated requirements specifications reduces creativity during software design. Forty-two software designers, organized into 21 pairs, participated in a dialog-based protocol study. Their interactions were transcribed and the transcripts were analyzed in two ways: (1) using inductive process coding and (2) using an a-priori coding scheme focusing on fixation and critical thinking. Process coding shows that participants exhibited seven categories of behavior: making design moves, uncritically accepting, rejecting, grouping, questioning, assuming and considering quality criteria. Closed coding shows that participants tend to accept given requirements and priority levels while rejecting newer, more innovative design ideas. Overall, the results suggest that designers fixate on desiderata presented as templated requirements specifications, hindering critical thinking. More precisely, requirements fixation mediates the negative relationship between specification formality and creativity.",Self Direction,Creativity,The study contributes to understanding how requirement specifications can affect creativity in software design; which can then translate into software that allows users to express their creativity.,"In 'Paper X', the authors investigate how presenting desiderata as templated requirements specifications reduces creativity in software design. By exploring the cognitive mechanisms behind this reduction in creativity, the study provides insights into how requirement specifications can hinder the software design process. Aligning with the value item Creativity and its corresponding value Self Direction, this understanding can lead to the development of software that allows users to have more freedom and expression of their own creativity in using the software.",Agreed-Justified,Agree,,Agree,,
Exp B,Exp H,2199,TSE,Software Development Methodologies,How Templated Requirements Specifications Inhibit Creativity in Software Engineering,"Desiderata is a general term for stakeholder needs, desires or preferences. Recent experiments demonstrate that presenting desiderata as templated requirements specifications leads to less creative solutions. However, these experiments do not establish how the presentation of desiderata affects design creativity. This study, therefore, aims to explore the cognitive mechanisms by which presenting desiderata as templated requirements specifications reduces creativity during software design. Forty-two software designers, organized into 21 pairs, participated in a dialog-based protocol study. Their interactions were transcribed and the transcripts were analyzed in two ways: (1) using inductive process coding and (2) using an a-priori coding scheme focusing on fixation and critical thinking. Process coding shows that participants exhibited seven categories of behavior: making design moves, uncritically accepting, rejecting, grouping, questioning, assuming and considering quality criteria. Closed coding shows that participants tend to accept given requirements and priority levels while rejecting newer, more innovative design ideas. Overall, the results suggest that designers fixate on desiderata presented as templated requirements specifications, hindering critical thinking. More precisely, requirements fixation mediates the negative relationship between specification formality and creativity.",Stimulation,Variation in Life,By understanding what stimulates and hinders creativity during the design process; the study can contribute to the creation of software that offers variety and stimulation to the user.,"By exploring the cognitive mechanisms through which presenting desiderata as templated requirements specifications reduces creativity during software design, the study in 'Paper X' can provide valuable insights into understanding what stimulates and hinders creativity during the design process. This understanding can then be leveraged to create software that offers variety and stimulation to the user. By aligning with the value item Variation in Life and its corresponding value Stimulation, 'Paper X' emphasizes the importance of incorporating diverse and creative elements in software design to enhance the user's experience and provide a more stimulating and satisfying software environment.",Agreed-Justified,Agree,,Agree,,
Exp C,Exp K,2199,TSE,Software Development Methodologies,How Templated Requirements Specifications Inhibit Creativity in Software Engineering,"Desiderata is a general term for stakeholder needs, desires or preferences. Recent experiments demonstrate that presenting desiderata as templated requirements specifications leads to less creative solutions. However, these experiments do not establish how the presentation of desiderata affects design creativity. This study, therefore, aims to explore the cognitive mechanisms by which presenting desiderata as templated requirements specifications reduces creativity during software design. Forty-two software designers, organized into 21 pairs, participated in a dialog-based protocol study. Their interactions were transcribed and the transcripts were analyzed in two ways: (1) using inductive process coding and (2) using an a-priori coding scheme focusing on fixation and critical thinking. Process coding shows that participants exhibited seven categories of behavior: making design moves, uncritically accepting, rejecting, grouping, questioning, assuming and considering quality criteria. Closed coding shows that participants tend to accept given requirements and priority levels while rejecting newer, more innovative design ideas. Overall, the results suggest that designers fixate on desiderata presented as templated requirements specifications, hindering critical thinking. More precisely, requirements fixation mediates the negative relationship between specification formality and creativity.",Hedonism,Pleasure,The study investigates how software design practices can potentially hinder creativity; which can limit the potential for user pleasure in the software experience. By addressing these issues; the paper could contribute to creating software that maximizes user pleasure and enjoyment.,"The justification for aligning 'Paper X' with the value item Pleasure and its corresponding value Hedonism from a ""Software User"" perspective is based on the premise that software design practices can impact creativity, which in turn affects the potential for user pleasure in the software experience. By exploring how presenting desiderata as templated requirements specifications hinders critical thinking and creative solutions, the paper aims to address these issues and contribute to the creation of software that maximizes user pleasure and enjoyment. In doing so, it directly aligns with the value item Pleasure and its corresponding value Hedonism, as it focuses on promoting the enjoyment and indulgence of users in the software context.",Agreed-Clarified,Agree,,Agree,,
Exp D,Exp K,2200,TSE,Security & Privacy,"Back to the Past aEUR"" Analysing Backporting Practices in Package Dependency Networks","The practice of backporting aims to bring the benefits of a bug or vulnerability fix from a higher to a lower release of a software package. When such a package adheres to semantic versioning, backports can be recognised as new releases in a lower major train. This is particularly useful in case a substantial number of software packages continues to depend on that lower major train. In this article, we study the backporting practices in four popular package distributions, namely Cargo, npm, Packagist and RubyGems. We observe that many dependent packages could benefit from backports provided by their dependencies. In particular, we find that a majority of security vulnerabilities affect more than one major train but are only fixed in the highest one, letting thousands of dependent packages exposed to the vulnerability. Despite that, we find that backporting updates is quite infrequent, and mostly practised by long-lived and more active packages for a variety of reasons.",Security,Healthy,The research in the paper essentially aims at improving the health of software packages by addressing vulnerabilities through the practice of backporting. This aligns with the value 'Security' and its value item 'Healthy.',"The justification for aligning 'Paper X' with the value item Healthy and its corresponding value Security is that the research focuses on addressing vulnerabilities in software packages through the practice of backporting. By fixing bugs and vulnerabilities, the software packages become healthier and more secure. This directly aligns with the software user's perspective of desiring a secure and stable software environment, thereby enhancing the value of Security.",Agreed-Clarified,Disagree,,Disagree,Software health is not necessarily Human health,
Exp E,Exp J,2200,TSE,Security & Privacy,"Back to the Past aEUR"" Analysing Backporting Practices in Package Dependency Networks","The practice of backporting aims to bring the benefits of a bug or vulnerability fix from a higher to a lower release of a software package. When such a package adheres to semantic versioning, backports can be recognised as new releases in a lower major train. This is particularly useful in case a substantial number of software packages continues to depend on that lower major train. In this article, we study the backporting practices in four popular package distributions, namely Cargo, npm, Packagist and RubyGems. We observe that many dependent packages could benefit from backports provided by their dependencies. In particular, we find that a majority of security vulnerabilities affect more than one major train but are only fixed in the highest one, letting thousands of dependent packages exposed to the vulnerability. Despite that, we find that backporting updates is quite infrequent, and mostly practised by long-lived and more active packages for a variety of reasons.",Security,National Security,By studying and improving backporting practices; this paper aids in securing software packages which could be seen as contributing to 'National Security' within a software context as it makes the software environment safer for users.,"In the context of software, the concept of ""National Security"" refers to ensuring the safety and protection of the software environment used by individuals or organizations within a country. By studying and improving backporting practices, 'Paper X' aims to address the issue of security vulnerabilities in software packages by bringing bug or vulnerability fixes from a higher to a lower release. This directly aligns with the value item of ""National Security"" as it contributes to making the software environment safer for users, ensuring the overall security and integrity of the software ecosystem within a national context.",Agreed-Clarified,Agree,,Agree,,
Exp G,Exp K,2205,TSE,Mobile & IoT,Research on Third-Party Libraries in Android Apps: A Taxonomy and Systematic Literature Review,"Third-party libraries (TPLs) have been widely used in mobile apps, which play an essential part in the entire Android ecosystem. However, TPL is a double-edged sword. On the one hand, it can ease the development of mobile apps. On the other hand, it also brings security risks such as privacy leaks or increased attack surfaces (e.g., by introducing over-privileged permissions) to mobile apps. Although there are already many studies for characterizing third-party libraries, including automated detection, security and privacy analysis of TPLs, TPL attributes analysis, etc., what strikes us odd is that there is no systematic study to summarize those studiesaEUR(tm) endeavors. To this end, we conduct the first systematic literature review on Android TPL-related research. Following a well-defined systematic literature review protocol, we collected 74 primary research papers closely related to Android third-party library from 2012 to 2020. After carefully examining these studies, we designed a taxonomy of TPL-related research studies and conducted a systematic study to summarize current solutions, limitations, challenges and possible implications of new research directions related to third-party library analysis. We hope that these contributions can give readers a clear overview of existing TPL-related studies and inspire them to go beyond the current status quo by advancing the discipline with innovative approaches.",Security,Social Order,The paper examines potential security risks introduced by third-party libraries in Android apps; addressing concerns about security and social order for users.,"The paper's focus on identifying and analyzing the potential security risks introduced by third-party libraries directly aligns with the value item ""Social Order"" and its corresponding value ""Security"" from a ""Software User"" perspective. By addressing these security concerns, the paper aims to contribute to maintaining a sense of order and safety for users in the software context, emphasizing the importance of protecting their personal data and ensuring their privacy.",Agreed-Clarified,Agree,,Agree,,
Exp A,Exp H,2205,TSE,Mobile & IoT,Research on Third-Party Libraries in Android Apps: A Taxonomy and Systematic Literature Review,"Third-party libraries (TPLs) have been widely used in mobile apps, which play an essential part in the entire Android ecosystem. However, TPL is a double-edged sword. On the one hand, it can ease the development of mobile apps. On the other hand, it also brings security risks such as privacy leaks or increased attack surfaces (e.g., by introducing over-privileged permissions) to mobile apps. Although there are already many studies for characterizing third-party libraries, including automated detection, security and privacy analysis of TPLs, TPL attributes analysis, etc., what strikes us odd is that there is no systematic study to summarize those studiesaEUR(tm) endeavors. To this end, we conduct the first systematic literature review on Android TPL-related research. Following a well-defined systematic literature review protocol, we collected 74 primary research papers closely related to Android third-party library from 2012 to 2020. After carefully examining these studies, we designed a taxonomy of TPL-related research studies and conducted a systematic study to summarize current solutions, limitations, challenges and possible implications of new research directions related to third-party library analysis. We hope that these contributions can give readers a clear overview of existing TPL-related studies and inspire them to go beyond the current status quo by advancing the discipline with innovative approaches.",Security,Healthy,By discussing potential vulnerabilities that can lead to privacy leaks; the paper is implicitly focused on maintaining the health of the user's digital space.,"The paper's focus on potential vulnerabilities and privacy leaks aligns with the value item of Healthy and its corresponding value of Security because the paper aims to address and mitigate security risks in third-party libraries used in mobile apps. By highlighting the potential threats and suggesting possible solutions, the paper indirectly contributes to maintaining the health and security of users' digital environment, which ultimately aligns with the value item of Healthy and the value of Security from a user's perspective.",Agreed-Justified,Disagree,reconcile with coder_2,Disagree,"The justification doesn't align with the value item of the healthy
",
Exp C,Exp K,2214,TSE,Security & Privacy,Pluto: Exposing Vulnerabilities in Inter-Contract Scenarios,"Attacks on smart contracts have caused considerable losses to digital assets. Many techniques based on symbolic execution, fuzzing, and static analysis are used to detect contract vulnerabilities. Most of the current analyzers only consider vulnerability detection intra-contract scenarios. However, Ethereum contracts usually interact with others by calling their functions. A bug hidden in a path that depends on information from external contract calls is defined as an inter-contract vulnerability. Failure to deal with this kind of bug can result in potential false negatives and false positives. In this work, we propose Pluto, which supports vulnerability detection in inter-contract scenarios. It first builds an Inter-contract Control Flow Graph (ICFG) to extract semantic information among contract calls. Afterward, it symbolically explores the ICFG and deduces Inter-Contract Path Constraints (ICPC) to check the reachability of execution paths more accurately. Finally, Pluto detects whether there is a vulnerability based on some predefined rules. For evaluation, we compare Pluto with five state-of-the-art tools, including Oyente, Mythril, Securify, ILF, and Clairvoyance on a labeled benchmark and 39,443 real-world Ethereum smart contracts. The result shows that other tools can only detect 10% of the inter-contract vulnerabilities, while Pluto can detect 80% of them on the labeled dataset. Beyond that, Pluto has detected 451 confirmed vulnerabilities on real-world contracts, including 36 vulnerabilities in inter-contract scenarios. Two bugs have been assigned with unique CVE identifiers by the US National Vulnerability Database (NVD). On average, Pluto costs 16.9 seconds to analyze a contract, which is as fast as the state-of-the-art tools.",Stimulation,Excitement in Life,The software user would experience a more secure and exciting interaction with smart contracts due to the Pluto tool; it can define and detect an inter-contract vulnerability leading to an excitement in life.,"In the context of a software user, the alignment with the value item ""Excitement in Life"" and its corresponding value ""Stimulation"" can be justified based on the abstract of 'Paper X'. The presence of an inter-contract vulnerability in smart contracts can lead to potential false negatives and false positives, resulting in financial losses. By introducing Pluto, a tool that detects inter-contract vulnerabilities more accurately, the software user can experience a heightened sense of excitement and stimulation as they can have a more secure and reliable interaction with smart contracts, thereby reducing the risks associated with potential vulnerabilities.",Agreed-Clarified,Agree,,Agree,,
Exp D,Exp K,2217,TSE,Security & Privacy,The Importance of Security Is in the Eye of the Beholder: Cultural Organizational and Personal Factors Affecting the Implementation of Security by Design,"Security by design is a recommended approach, addressing end-to-end security and privacy in the design of software systems. To realize this approach, proactive security behavior is required from software developers. This research follows results from previous studies that suggest that personal and organizational characteristics influence security-related behaviors during the software design process. The research is aimed at gaining an in-depth understanding of proactive security behavior and the factors affecting it. Leveraging organization climate theory from organizational psychology, we propose a theoretical model, detailing different factors and their relations with proactive security behavior and test it in empirical settings. The empirical study was conducted in collaboration with an internationally distributed information technology enterprise and included a survey questionnaire completed by 499 software developers working in seven countries. The results of the survey confirm the moderation-mediation relations in the proposed model, revealing that organizational security climate and security self-efficacy are both positively associated with proactive security behavior, organizational security climate is positively associated with security self-efficacy, and cultures promoting individualism moderate the relationship between organizational security climate and security self-efficacy, thus impeding proactive security behavior. The body of knowledge of organizational psychology points to directions that can effectively be activated for improvement.",Security,Healthy,The paper explores a proactive approach to addressing end-to-end security and privacy design in software which aligns with the value item Healthy within the value Security; indicating a healthy software environment for the user.,"In the paper's abstract, it is mentioned that security by design is proposed as an approach to ensure end-to-end security and privacy in software systems. This aligns with the value item Healthy within the value Security. By addressing security and privacy concerns in the design process, the paper aims to create a healthy software environment for the user, ensuring their information and interactions are protected. This proactive approach to security directly aligns with the value item Healthy and its corresponding value Security, as it prioritizes the well-being and protection of the software user.",Agreed-Clarified,Disagree,,Disagree,Software health is not necessarily Human health,
Exp E,Exp J,2217,TSE,Security & Privacy,The Importance of Security Is in the Eye of the Beholder: Cultural Organizational and Personal Factors Affecting the Implementation of Security by Design,"Security by design is a recommended approach, addressing end-to-end security and privacy in the design of software systems. To realize this approach, proactive security behavior is required from software developers. This research follows results from previous studies that suggest that personal and organizational characteristics influence security-related behaviors during the software design process. The research is aimed at gaining an in-depth understanding of proactive security behavior and the factors affecting it. Leveraging organization climate theory from organizational psychology, we propose a theoretical model, detailing different factors and their relations with proactive security behavior and test it in empirical settings. The empirical study was conducted in collaboration with an internationally distributed information technology enterprise and included a survey questionnaire completed by 499 software developers working in seven countries. The results of the survey confirm the moderation-mediation relations in the proposed model, revealing that organizational security climate and security self-efficacy are both positively associated with proactive security behavior, organizational security climate is positively associated with security self-efficacy, and cultures promoting individualism moderate the relationship between organizational security climate and security self-efficacy, thus impeding proactive security behavior. The body of knowledge of organizational psychology points to directions that can effectively be activated for improvement.",Security,Family Security,The paper's contribution to an understanding of proactive security behavior aligns with the value item Family Security; as security in software can be understood as a form of security for the user's 'family' of data.,"In 'Paper X', the contribution of understanding proactive security behavior aligns with the value item Family Security and its corresponding value Security from a ""Software User"" perspective because ensuring security in software systems is essential for protecting the user's personal data, information, and privacy. By designing software systems with security by design principles and addressing end-to-end security and privacy, the paper directly aligns with the value of Security. This, in turn, contributes to providing a sense of Family Security for the software user, as their sensitive information and data will be safeguarded within the software system.",Agreed-Clarified,Agree,,Agree,,
Exp F,Exp J,2217,TSE,Security & Privacy,The Importance of Security Is in the Eye of the Beholder: Cultural Organizational and Personal Factors Affecting the Implementation of Security by Design,"Security by design is a recommended approach, addressing end-to-end security and privacy in the design of software systems. To realize this approach, proactive security behavior is required from software developers. This research follows results from previous studies that suggest that personal and organizational characteristics influence security-related behaviors during the software design process. The research is aimed at gaining an in-depth understanding of proactive security behavior and the factors affecting it. Leveraging organization climate theory from organizational psychology, we propose a theoretical model, detailing different factors and their relations with proactive security behavior and test it in empirical settings. The empirical study was conducted in collaboration with an internationally distributed information technology enterprise and included a survey questionnaire completed by 499 software developers working in seven countries. The results of the survey confirm the moderation-mediation relations in the proposed model, revealing that organizational security climate and security self-efficacy are both positively associated with proactive security behavior, organizational security climate is positively associated with security self-efficacy, and cultures promoting individualism moderate the relationship between organizational security climate and security self-efficacy, thus impeding proactive security behavior. The body of knowledge of organizational psychology points to directions that can effectively be activated for improvement.",Achievement,Successful,The paper's focus on empowering software developers with proactive security behavior aligns with the value item Successful within the value Achievement; here the success is for the user in the terms of using secure software. Based on our value definition; it is only a contributory success for the user.,"The alignment of 'Paper X' with the value item Successful and its corresponding value Achievement from a ""Software User"" perspective is justified based on the paper's focus on empowering software developers with proactive security behavior. This aligns with the user's goal of using secure software, which can be considered a contributory success for the user. By addressing end-to-end security and privacy in the design of software systems, the paper aims to ensure that software users can trust the security measures implemented, leading to a successful user experience.",Agreed-Justified,Agree,,Agree,,
Exp G,Exp K,2219,TSE,Security & Privacy,Android Custom Permissions Demystified: A Comprehensive Security Evaluation,"Permission is the fundamental security mechanism for protecting user data and privacy on Android. Given its importance, security researchers have studied the design and usage of permissions from various aspects. However, most of the previous research focused on the security issues of system permissions. Overlooked by many researchers, an app can use custom permissions to share its resources and capabilities with other apps. However, the security implications of using custom permissions have not been fully understood. In this paper, we systematically evaluate the design and implementation of Android custom permissions. Notably, we built an automatic fuzzing tool, called CuPerFuzzer+, to detect custom permission related vulnerabilities existing in the Android OS. CuPerFuzzer+ treats the operations of the permission mechanism as a black-box and executes massive targeted test cases to trigger privilege escalation. In the experiments, CuPerFuzzer+ discovered 5,932 effective cases with 47 critical paths successfully. Through investigating these vulnerable cases and analyzing the source code of Android OS, we further identified a series of severe design shortcomings lying in the Android permission framework, including dangling custom permission, inconsistent permission-group mapping, custom permission elevating, inconsistent permission definition, dormant permission group, and inconsistent permission type. Exploiting these shortcomings, a malicious app can access unauthorized platform resources. On top of these observations, we propose three general design guidelines to secure custom permissions. Our findings have been acknowledged by the Android security team and assigned CVE-2020-0418, CVE-2021-0306, CVE-2021-0307, and CVE-2021-0317.",Security,Healthy,The paper focuses on software security vulnerabilities which is an important aspect of ensuring that end users are healthy from cyber threats. This aligns with the value item 'Healthy' and its corresponding value 'Security'.,"The justification for labeling 'Paper X' with the value item Healthy and its corresponding value Security from a ""Software User"" perspective is that the paper explicitly states that it evaluates the design and implementation of Android custom permissions to detect vulnerabilities. By addressing security issues and vulnerabilities, the paper directly contributes to ensuring the security and well-being of software users. Ensuring the security of software is crucial for protecting user data, privacy, and overall system health, aligning with the value item Healthy and its corresponding value Security from Schwartz's Taxonomy.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2219,TSE,Security & Privacy,Android Custom Permissions Demystified: A Comprehensive Security Evaluation,"Permission is the fundamental security mechanism for protecting user data and privacy on Android. Given its importance, security researchers have studied the design and usage of permissions from various aspects. However, most of the previous research focused on the security issues of system permissions. Overlooked by many researchers, an app can use custom permissions to share its resources and capabilities with other apps. However, the security implications of using custom permissions have not been fully understood. In this paper, we systematically evaluate the design and implementation of Android custom permissions. Notably, we built an automatic fuzzing tool, called CuPerFuzzer+, to detect custom permission related vulnerabilities existing in the Android OS. CuPerFuzzer+ treats the operations of the permission mechanism as a black-box and executes massive targeted test cases to trigger privilege escalation. In the experiments, CuPerFuzzer+ discovered 5,932 effective cases with 47 critical paths successfully. Through investigating these vulnerable cases and analyzing the source code of Android OS, we further identified a series of severe design shortcomings lying in the Android permission framework, including dangling custom permission, inconsistent permission-group mapping, custom permission elevating, inconsistent permission definition, dormant permission group, and inconsistent permission type. Exploiting these shortcomings, a malicious app can access unauthorized platform resources. On top of these observations, we propose three general design guidelines to secure custom permissions. Our findings have been acknowledged by the Android security team and assigned CVE-2020-0418, CVE-2021-0306, CVE-2021-0307, and CVE-2021-0317.",Security,Social Order,The paper identifies shortcomings in Android's permission framework and proposes design guidelines to secure permissions. This contributes to maintaining the social order within the software user community by ensuring software users can safely share resources and capabilities without unauthorized access. This aligns with the value item 'Social Order' under the value 'Security'.,"In 'Paper X', the authors discuss the design and implementation of Android custom permissions and identify severe design shortcomings in the Android permission framework. By proposing design guidelines to secure permissions, the paper contributes to maintaining the social order within the software user community by ensuring that software users can safely share resources and capabilities without unauthorized access. This directly aligns with the value item 'Social Order' and its corresponding value 'Security' from Schwartz's Taxonomy.",Agreed-Justified,Disagree,More incline to Self direction: Privacy as it takes care of privacy-related information,Disagree,"The justification doesn't align with the value item of the social order
",
Exp B,Exp H,2219,TSE,Security & Privacy,Android Custom Permissions Demystified: A Comprehensive Security Evaluation,"Permission is the fundamental security mechanism for protecting user data and privacy on Android. Given its importance, security researchers have studied the design and usage of permissions from various aspects. However, most of the previous research focused on the security issues of system permissions. Overlooked by many researchers, an app can use custom permissions to share its resources and capabilities with other apps. However, the security implications of using custom permissions have not been fully understood. In this paper, we systematically evaluate the design and implementation of Android custom permissions. Notably, we built an automatic fuzzing tool, called CuPerFuzzer+, to detect custom permission related vulnerabilities existing in the Android OS. CuPerFuzzer+ treats the operations of the permission mechanism as a black-box and executes massive targeted test cases to trigger privilege escalation. In the experiments, CuPerFuzzer+ discovered 5,932 effective cases with 47 critical paths successfully. Through investigating these vulnerable cases and analyzing the source code of Android OS, we further identified a series of severe design shortcomings lying in the Android permission framework, including dangling custom permission, inconsistent permission-group mapping, custom permission elevating, inconsistent permission definition, dormant permission group, and inconsistent permission type. Exploiting these shortcomings, a malicious app can access unauthorized platform resources. On top of these observations, we propose three general design guidelines to secure custom permissions. Our findings have been acknowledged by the Android security team and assigned CVE-2020-0418, CVE-2021-0306, CVE-2021-0307, and CVE-2021-0317.",Universalism,Protecting the Environment,The paper‚Äö√Ñ√∂‚àö√ë‚àö‚àÇ‚Äö√†√∂‚àö√´‚Äö√†√∂‚Äö√†√á‚Äö√Ñ√∂‚àö‚Ä†‚àö‚àÇ‚Äö√†√∂‚àö¬¥‚Äö√Ñ√∂‚àö‚Ä†‚àö‚àÇ¬¨¬®‚Äö√Ñ¬¢s focus on protecting user data and privacy directly aligns with Schwartz‚Äö√Ñ√∂‚àö√ë‚àö‚àÇ‚Äö√†√∂‚àö√´‚Äö√†√∂‚Äö√†√á‚Äö√Ñ√∂‚àö‚Ä†‚àö‚àÇ‚Äö√†√∂‚àö¬¥‚Äö√Ñ√∂‚àö‚Ä†‚àö‚àÇ¬¨¬®‚Äö√Ñ¬¢s value item 'Protecting the Environment'. It is important to note that in this context; 'Protecting the Environment' refers to safeguarding the software environment for the user; not the natural environment. This corresponds to the value 'Universalism'.,"I apologize for any confusion. To clarify, the alignment of 'Paper X' with the value item Protecting the Environment and its corresponding value Universalism is based on the understanding that protecting the software environment is equivalent to protecting the user's digital environment. The paper aims to secure the custom permissions of the Android OS, which is essential for safeguarding user data and privacy. By addressing design shortcomings and proposing guidelines, the paper contributes to creating a more secure and protected digital environment for software users, aligning with the value of Universalism in Schwartz's Taxonomy.",Agreed-Justified,Agree,,Agree,,
Exp D,Exp K,2225,TSE,Security & Privacy,Automated Use-After-Free Detection and Exploit Mitigation: How Far Have We Gone?,"C/C++ programs frequently encounter memory errors, such as Use-After-Free (UAF), buffer overflow, and integer overflow. Among these memory errors, UAF vulnerabilities are increasingly being exploited by attackers to disrupt critical software systems, leading to serious consequences, such as remote code execution and data breaches. Researchers have proposed dozens of approaches to detect UAFs in testing environments and to mitigate UAF exploit in production environments. However, to the best of our knowledge, no comprehensive studies have evaluated and compared these approaches. In this paper, we shed light on the current UAF detection and exploit mitigation approaches and provide a systematic overview, comprehensive comparison, and evaluation. Specifically, we evaluate the effectiveness and efficiency of publicly available UAF detection and exploit mitigation tools. The experimental results show that static UAF detectors are suitable for detecting intra-procedural UAFs but are not sufficient to detect inter-procedural UAFs in real-world programs. Dynamic UAF detectors are still the first choice for detecting inter-procedural UAFs. Our evaluation also demonstrates that the runtime overhead of existing UAF exploit mitigation tools is relatively stable whereas the memory overhead may vary dramatically with respect to different programs. Finally, we envision potential valuable future research directions.",Security,Sense of Belonging,By addressing the software related issues and improving its efficiency; the users' sense of belonging to the software might increase; aligning with the value item 'Sense of belonging' under the value 'Security'.,"In the context of a software user, the sense of belonging refers to the feeling of connection and comfort that a user experiences when using a software system. In the case of 'Paper X', the main contributions mentioned in the abstract aim to address memory errors and vulnerabilities in software programs, which can have serious consequences such as data breaches. By improving the software's efficiency and addressing these security issues, users are likely to feel more secure and confident in using the software, thus enhancing their sense of belonging to it. This aligns with the value item 'Sense of Belonging' under the value 'Security' in Schwartz's Taxonomy.",Agreed-Clarified,Agree,,Agree,,
Exp E,Exp J,2225,TSE,Security & Privacy,Automated Use-After-Free Detection and Exploit Mitigation: How Far Have We Gone?,"C/C++ programs frequently encounter memory errors, such as Use-After-Free (UAF), buffer overflow, and integer overflow. Among these memory errors, UAF vulnerabilities are increasingly being exploited by attackers to disrupt critical software systems, leading to serious consequences, such as remote code execution and data breaches. Researchers have proposed dozens of approaches to detect UAFs in testing environments and to mitigate UAF exploit in production environments. However, to the best of our knowledge, no comprehensive studies have evaluated and compared these approaches. In this paper, we shed light on the current UAF detection and exploit mitigation approaches and provide a systematic overview, comprehensive comparison, and evaluation. Specifically, we evaluate the effectiveness and efficiency of publicly available UAF detection and exploit mitigation tools. The experimental results show that static UAF detectors are suitable for detecting intra-procedural UAFs but are not sufficient to detect inter-procedural UAFs in real-world programs. Dynamic UAF detectors are still the first choice for detecting inter-procedural UAFs. Our evaluation also demonstrates that the runtime overhead of existing UAF exploit mitigation tools is relatively stable whereas the memory overhead may vary dramatically with respect to different programs. Finally, we envision potential valuable future research directions.",Security,Social Order,By tackling the memory errors in a systematic manner and providing better solutions; the abstract points out the improvement in social order; aligning with the value item 'Social order' under the value 'Security'.,"In the abstract of 'Paper X', it is mentioned that memory errors, including UAF vulnerabilities, can have serious consequences such as data breaches and disruption of critical software systems. The paper aims to evaluate and compare approaches for detecting and mitigating UAFs, which can ultimately lead to improved security and order within software systems. This aligns with the value item 'Social Order' and its corresponding value 'Security' from Schwartz's Taxonomy, as the paper addresses the need for better solutions to prevent memory errors and enhance the overall security and stability of software systems.",Agreed-Clarified,Agree,,Agree,,
Exp G,Exp K,2228,TSE,AI & Machine Learning,CAGFuzz: Coverage-Guided Adversarial Generative Fuzzing Testing for Image-Based Deep Learning Systems,"Deep Neural Network (DNN) driven technologies have been extensively employed in various aspects of our life. Nevertheless, the applied DNN always fails to detect erroneous behaviors, which may lead to serious problems. Several approaches have been proposed to enhance adversarial examples for automatically testing deep learning (DL) systems, such as image-based DL systems. However, the approaches contain the following two limitations. First, existing approaches only take into account small perturbations on adversarial examples, they design and generate adversarial examples for a certain particular DNN model. This might hamper the transferability of the examples for other DNN models. Second, they only use shallow features (e.g., pixel-level features) to judge the differences between the generated adversarial examples and the original examples. The deep features, which contain high-level semantic information, such as image object categories and scene semantics, are completely neglected. To address these two problems, we propose CAGFuzz, a Coverage-guided Adversarial Generative Fuzzing testing approach for image-based DL systems. CAGFuzz is able to generate adversarial examples for mainstream DNN models to discover their potential errors. First, we train an Adversarial Example Generator (AEG) based on general datasets. AEG only considers the data characteristics to alleviate the transferability problem. Second, we extract the deep features of the original and adversarial examples, and constrain the adversarial examples by cosine similarity to ensure that the deep features of the adversarial examples remain unchanged. Finally, we use the adversarial examples to retrain the models. Based on several standard datasets, we design a set of dedicated experiments to evaluate CAGFuzz. The experimental results show that CAGFuzz can detect more hidden errors, enhance the accuracy of the target DNN models, and generate adversarial examples with higher transferability.",Achievement,Successful,The approach proposed; aims to detect more hidden errors and enhance the accuracy of the DNN models; which aligns with the value item 'Successful' as it directly aids the software users in achieving successful outcomes with the utilization of image-based DL systems.,"The justification for aligning 'Paper X' with the value item 'Successful' from a ""Software User"" perspective is that the proposed approach aims to detect more hidden errors and enhance the accuracy of the DNN models. This directly benefits software users by increasing the likelihood of successful outcomes when using image-based DL systems. By addressing the limitations of existing approaches and generating adversarial examples for mainstream DNN models, the paper contributes to the achievement of successful outcomes for software users in the context of deep learning systems.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2228,TSE,AI & Machine Learning,CAGFuzz: Coverage-Guided Adversarial Generative Fuzzing Testing for Image-Based Deep Learning Systems,"Deep Neural Network (DNN) driven technologies have been extensively employed in various aspects of our life. Nevertheless, the applied DNN always fails to detect erroneous behaviors, which may lead to serious problems. Several approaches have been proposed to enhance adversarial examples for automatically testing deep learning (DL) systems, such as image-based DL systems. However, the approaches contain the following two limitations. First, existing approaches only take into account small perturbations on adversarial examples, they design and generate adversarial examples for a certain particular DNN model. This might hamper the transferability of the examples for other DNN models. Second, they only use shallow features (e.g., pixel-level features) to judge the differences between the generated adversarial examples and the original examples. The deep features, which contain high-level semantic information, such as image object categories and scene semantics, are completely neglected. To address these two problems, we propose CAGFuzz, a Coverage-guided Adversarial Generative Fuzzing testing approach for image-based DL systems. CAGFuzz is able to generate adversarial examples for mainstream DNN models to discover their potential errors. First, we train an Adversarial Example Generator (AEG) based on general datasets. AEG only considers the data characteristics to alleviate the transferability problem. Second, we extract the deep features of the original and adversarial examples, and constrain the adversarial examples by cosine similarity to ensure that the deep features of the adversarial examples remain unchanged. Finally, we use the adversarial examples to retrain the models. Based on several standard datasets, we design a set of dedicated experiments to evaluate CAGFuzz. The experimental results show that CAGFuzz can detect more hidden errors, enhance the accuracy of the target DNN models, and generate adversarial examples with higher transferability.",Security,Healthy,With the use of CAGFuzz and reduction in erroneous behaviors of DNN models; software users can expect a reliable and less erroneous tool which aligns with the value item 'Healthy'; indicating that the usage of the tool is envisioned to be trouble-free and smooth as a reflection of well-being in the context of software use.,"By reducing erroneous behaviors of DNN models through the use of CAGFuzz, 'Paper X' aligns with the value item 'Healthy' from a ""Software User"" perspective. This alignment implies that the software users can expect a reliable and error-free tool, promoting a sense of well-being and smooth experience in the context of software use. The focus of 'Paper X' is on enhancing the accuracy of DNN models and detecting hidden errors, which directly contributes to the security and reliability of the software, ultimately benefiting the users' overall experience.",Agreed-Justified,Disagree,"The abstract is more concerned with achievement: successful, as it successfully identify malicious bahivours",Disagree,"The justification doesn't align with the value item of the healthy
",
Exp B,Exp H,2229,TSE,AI & Machine Learning,AI-Enabled Automation for Completeness Checking of Privacy Policies,"Technological advances in information sharing have raised concerns about data protection. Privacy policies contain privacy-related requirements about how the personal data of individuals will be handled by an organization or a software system (e.g., a web service or an app). In Europe, privacy policies are subject to compliance with the General Data Protection Regulation (GDPR). A prerequisite for GDPR compliance checking is to verify whether the content of a privacy policy is complete according to the provisions of GDPR. Incomplete privacy policies might result in large fines on violating organization as well as incomplete privacy-related software specifications. Manual completeness checking is both time-consuming and error-prone. In this paper, we propose AI-based automation for the completeness checking of privacy policies. Through systematic qualitative methods, we first build two artifacts to characterize the privacy-related provisions of GDPR, namely a conceptual model and a set of completeness criteria. Then, we develop an automated solution on top of these artifacts by leveraging a combination of natural language processing and supervised machine learning. Specifically, we identify the GDPR-relevant information content in privacy policies and subsequently check them against the completeness criteria. To evaluate our approach, we collected 234 real privacy policies from the fund industry. Over a set of 48 unseen privacy policies, our approach detected 300 of the total of 334 violations of some completeness criteria correctly, while producing 23 false positives. The approach thus has a precision of 92.9% and recall of 89.8%. Compared to a baseline that applies keyword search only, our approach results in an improvement of 24.5% in precision and 38% in recall.",Self Direction,Privacy,The paper addresses the issue of data protection privacy policies and their alignment with GDPR regulations by proposing an AI-based automation for completeness checking of privacy policies. This contributes to the value item Privacy (v1.6) by improving data privacy in software.,"The paper's proposed AI-based automation for completeness checking of privacy policies directly aligns with the value item Privacy (v1.6) by improving data privacy in software. By leveraging natural language processing and supervised machine learning, the approach identifies GDPR-relevant information content in privacy policies and checks them against completeness criteria. This ensures that privacy-related requirements and provisions in privacy policies are properly implemented, leading to enhanced data protection and privacy for software users.",Agreed-Justified,Agree,,Agree,,
Exp D,Exp K,2229,TSE,AI & Machine Learning,AI-Enabled Automation for Completeness Checking of Privacy Policies,"Technological advances in information sharing have raised concerns about data protection. Privacy policies contain privacy-related requirements about how the personal data of individuals will be handled by an organization or a software system (e.g., a web service or an app). In Europe, privacy policies are subject to compliance with the General Data Protection Regulation (GDPR). A prerequisite for GDPR compliance checking is to verify whether the content of a privacy policy is complete according to the provisions of GDPR. Incomplete privacy policies might result in large fines on violating organization as well as incomplete privacy-related software specifications. Manual completeness checking is both time-consuming and error-prone. In this paper, we propose AI-based automation for the completeness checking of privacy policies. Through systematic qualitative methods, we first build two artifacts to characterize the privacy-related provisions of GDPR, namely a conceptual model and a set of completeness criteria. Then, we develop an automated solution on top of these artifacts by leveraging a combination of natural language processing and supervised machine learning. Specifically, we identify the GDPR-relevant information content in privacy policies and subsequently check them against the completeness criteria. To evaluate our approach, we collected 234 real privacy policies from the fund industry. Over a set of 48 unseen privacy policies, our approach detected 300 of the total of 334 violations of some completeness criteria correctly, while producing 23 false positives. The approach thus has a precision of 92.9% and recall of 89.8%. Compared to a baseline that applies keyword search only, our approach results in an improvement of 24.5% in precision and 38% in recall.",Stimulation,Variation in Life,The paper proposes a novel AI-based automation which brings Variation in Life (v2.2) as it changes users' current experience by providing a more efficient and error-free solution to check privacy policy completeness,"In the paper, 'Paper X' proposes an AI-based automation solution for completeness checking of privacy policies. This solution offers a more efficient and error-free method compared to manual checking. By introducing this automation, it brings a variation in the life of software users by changing the way they handle and ensure compliance with privacy policies. This aligns with the value item ""Variation in Life"" (v2.2) from Schwartz's Taxonomy, as it introduces a new and improved approach that alters the software users' experiences in dealing with privacy-related requirements.",Agreed-Justified,Agree,,Agree,,
Exp E,Exp J,2230,TSE,Software Engineering Practices,Systematic Evaluation and Usability Analysis of Formal Methods Tools for Railway Signaling System Design,"Formal methods and supporting tools have a long record of success in the development of safety-critical systems. However, no single tool has emerged as the dominant solution for system design. Each tool differs from the others in terms of the modeling language used, its verification capabilities and other complementary features, and each development context has peculiar needs that require different tools. This is particularly problematic for the railway industry, in which formal methods are highly recommended by the norms, but no actual guidance is provided for the selection of tools. To guide companies in the selection of the most appropriate formal methods tools to adopt in their contexts, a clear assessment of the features of the currently available tools is required. To address this goal, this paper considers a set of 13 formal methods tools that have been used for the early design of railway systems, and it presents a systematic evaluation of such tools and a preliminary usability analysis of a subset of 7 tools, involving railway practitioners. The results are discussed considering the most desired aspects by industry and earlier related studies. While the focus is on the railway signaling domain, the overall methodology can be applied to similar contexts. Our study thus contributes with a systematic evaluation of formal methods tools and it shows that despite the poor graphical interfaces, usability and maturity of the tools are not major problems, as claimed by contributions from the literature. Instead, support for process integration is the most relevant obstacle for the adoption of most of the tools. Our contribution can be useful to R&D engineers from railway signaling companies and infrastructure managers, but also to tool developers and academic researchers alike.",Achievement,Successful,The paper aims to guide companies in selecting the most appropriate tools for their work; which can lead to successful implementation and optimization of their systems. This aligns with the value item Successful in the Achievement value.,"The justification for aligning 'Paper X' with the value item Successful and its corresponding value Achievement from a ""Software User"" perspective is that the paper's main contribution lies in providing a clear assessment of the currently available formal methods tools, specifically in the context of the railway industry. By helping companies select the most appropriate tools for their work, the paper aims to support successful implementation and optimization of their systems. This aligns with the value item Successful, as it emphasizes the achievement of desired outcomes and the attainment of positive results through effective tool selection and usage.",Agreed-Justified,Agree,,Agree,,
Exp F,Exp J,2230,TSE,Software Engineering Practices,Systematic Evaluation and Usability Analysis of Formal Methods Tools for Railway Signaling System Design,"Formal methods and supporting tools have a long record of success in the development of safety-critical systems. However, no single tool has emerged as the dominant solution for system design. Each tool differs from the others in terms of the modeling language used, its verification capabilities and other complementary features, and each development context has peculiar needs that require different tools. This is particularly problematic for the railway industry, in which formal methods are highly recommended by the norms, but no actual guidance is provided for the selection of tools. To guide companies in the selection of the most appropriate formal methods tools to adopt in their contexts, a clear assessment of the features of the currently available tools is required. To address this goal, this paper considers a set of 13 formal methods tools that have been used for the early design of railway systems, and it presents a systematic evaluation of such tools and a preliminary usability analysis of a subset of 7 tools, involving railway practitioners. The results are discussed considering the most desired aspects by industry and earlier related studies. While the focus is on the railway signaling domain, the overall methodology can be applied to similar contexts. Our study thus contributes with a systematic evaluation of formal methods tools and it shows that despite the poor graphical interfaces, usability and maturity of the tools are not major problems, as claimed by contributions from the literature. Instead, support for process integration is the most relevant obstacle for the adoption of most of the tools. Our contribution can be useful to R&D engineers from railway signaling companies and infrastructure managers, but also to tool developers and academic researchers alike.",Security,Social Order,By providing a systematic evaluation of various tools for system design in the railway industry; the paper contributes to maintaining and enhancing the social order of the domain. This aligns with the value item Social Order in the Security value.,"The justification for aligning 'Paper X' with the value item Social Order and its corresponding value Security is that the paper's contribution of a systematic evaluation of tools for system design in the railway industry helps in maintaining and enhancing the overall social order within that domain. This is because the use of formal methods tools in the railway industry, as recommended by norms, is crucial for ensuring the safety and security of railway systems. By assisting companies in selecting appropriate tools, the paper indirectly supports the establishment of social order by enabling the development of reliable and safe railway systems.",Agreed-Clarified,Agree,,Agree,,
Exp G,Exp K,2230,TSE,Software Engineering Practices,Systematic Evaluation and Usability Analysis of Formal Methods Tools for Railway Signaling System Design,"Formal methods and supporting tools have a long record of success in the development of safety-critical systems. However, no single tool has emerged as the dominant solution for system design. Each tool differs from the others in terms of the modeling language used, its verification capabilities and other complementary features, and each development context has peculiar needs that require different tools. This is particularly problematic for the railway industry, in which formal methods are highly recommended by the norms, but no actual guidance is provided for the selection of tools. To guide companies in the selection of the most appropriate formal methods tools to adopt in their contexts, a clear assessment of the features of the currently available tools is required. To address this goal, this paper considers a set of 13 formal methods tools that have been used for the early design of railway systems, and it presents a systematic evaluation of such tools and a preliminary usability analysis of a subset of 7 tools, involving railway practitioners. The results are discussed considering the most desired aspects by industry and earlier related studies. While the focus is on the railway signaling domain, the overall methodology can be applied to similar contexts. Our study thus contributes with a systematic evaluation of formal methods tools and it shows that despite the poor graphical interfaces, usability and maturity of the tools are not major problems, as claimed by contributions from the literature. Instead, support for process integration is the most relevant obstacle for the adoption of most of the tools. Our contribution can be useful to R&D engineers from railway signaling companies and infrastructure managers, but also to tool developers and academic researchers alike.",Power,Authority,The paper offers guidance for seeking the most effective tools for a specific context; potentially empowering the users with better decision-making authority. This aligns with the value item Authority in the Power value.,"My justification for labeling 'Paper X' as aligning with the value item Authority and its corresponding value Power is based on the fact that the paper offers guidance for selecting the most appropriate tools in a specific context. By providing clear assessment and evaluation of formal methods tools, the paper empowers software users to make informed decisions and exercise authority in choosing the most effective tools for their needs. This aligns with the concept of Authority within the Power value, as it grants users the ability to exert control and make decisions that can have a significant impact on system design and development.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2236,TSE,Software Development Methodologies,A Family of Experiments to Compare Two Model-Driven Development Tools vs a Traditional Development Method,"Context: There are many papers which extol the benefits of Model-Driven Development (MDD) compared to traditional developments. However, the adoption of MDD to develop fully functional systems without coding is not frequent. Objective: This paper presents a family of experiments with 7 replications and 56 sample units to compare a traditional method versus MDD, analysing two MDD tools. Method: The factor in the experiment is the method with two treatments (traditional and MDD). We analyse together all replications thanks to two moderator variables in a multilevel hierarchy: Replication (from 1 to 7) and MDD Tool (INTEGRANOVA and WebRatio). Response variables are Functional Suitability, measured in terms of effectiveness in simple and complex problems; Effort, measured as time to develop simple problems; and Satisfaction, measured in terms of perceived ease of use, perceived usefulness and intention to use. Result: Functional Suitability in MDD yields significantly better results both in simple and complex problems, while Effort for simple problems is also significantly better for MDD. Differences for Functional Suitability in complex problems are greater than with simple ones. Considering the MDD tools, INTEGRANOVA yields significantly better Functional Suitability in complex problems. Regarding Satisfaction, replications with WebRatio have a better intention to use than replications with INTEGRANOVA. Conclusions: Even though MDD yields better Functional Suitability and Effort, the subjectsaEUR(tm) Satisfaction with MDD is not better than with a traditional method.",Self Direction,Independent,The paper focuses on enabling users to use software developed via MDD without the need to code; promoting the value item Independent under the value Self Direction.,"The justification for aligning 'Paper X' with the value item Independent and its corresponding value Self Direction from a ""Software User"" perspective is based on the fact that the paper highlights the adoption of Model-Driven Development (MDD) as a method to develop fully functional systems without coding. By enabling users to utilize software developed through MDD without the need for coding, the paper emphasizes the value of empowering users to independently navigate and interact with the software, which aligns with the value item Independent under the value Self Direction.",Agreed-Justified,Agree,,Agree,,
Exp B,Exp H,2236,TSE,Software Development Methodologies,A Family of Experiments to Compare Two Model-Driven Development Tools vs a Traditional Development Method,"Context: There are many papers which extol the benefits of Model-Driven Development (MDD) compared to traditional developments. However, the adoption of MDD to develop fully functional systems without coding is not frequent. Objective: This paper presents a family of experiments with 7 replications and 56 sample units to compare a traditional method versus MDD, analysing two MDD tools. Method: The factor in the experiment is the method with two treatments (traditional and MDD). We analyse together all replications thanks to two moderator variables in a multilevel hierarchy: Replication (from 1 to 7) and MDD Tool (INTEGRANOVA and WebRatio). Response variables are Functional Suitability, measured in terms of effectiveness in simple and complex problems; Effort, measured as time to develop simple problems; and Satisfaction, measured in terms of perceived ease of use, perceived usefulness and intention to use. Result: Functional Suitability in MDD yields significantly better results both in simple and complex problems, while Effort for simple problems is also significantly better for MDD. Differences for Functional Suitability in complex problems are greater than with simple ones. Considering the MDD tools, INTEGRANOVA yields significantly better Functional Suitability in complex problems. Regarding Satisfaction, replications with WebRatio have a better intention to use than replications with INTEGRANOVA. Conclusions: Even though MDD yields better Functional Suitability and Effort, the subjectsaEUR(tm) Satisfaction with MDD is not better than with a traditional method.",Achievement,Intelligent,The paper emphasizes the enhancement of system functional suitability; relating to Intelligent under Achievement.,"The justification for aligning 'Paper X' with the value item Intelligent and its corresponding value Achievement from a ""Software User"" perspective is based on the paper's focus on comparing the functional suitability and effort of traditional development methods versus Model-Driven Development (MDD). The term ""Functional Suitability"" implies the ability of a system to meet its intended purpose effectively and efficiently, which aligns with the value of Achievement. By emphasizing the improvement of functional suitability, the paper highlights the achievement of creating intelligent systems that are capable and successful in fulfilling their intended goals, thus aligning with the value item Intelligent under Achievement.",Agreed-Clarified,Agree,,Agree,,
Exp D,Exp K,2238,TSE,Software Development Methodologies,Pull Request Governance in Open Source Communities,"Pull requests facilitate inclusion and improvement of contributions in distributed software projects, especially in open source communities. An author makes a pull request to present a contribution as a candidate for inclusion in a code base. The request is inspected by maintainers and reviewers. The initiated process of review and collaborative improvement can be loaded with debates, opinions, and emotions. It heavily influences the atmosphere in the community. It can demotivate and detract contributors or it can fail to guard the code quality. Both problems put the existence of a community at risk. This mixed methods study aims to elucidate the mechanisms of evaluating pull requests in diverse open source software communities from the perspectives of developers and maintainers. We interviewed 30 participants from five different communities and conducted a survey with N=387 respondents. The data shows that acceptance of contributions in open source depends not only on technical criteria, but also significantly on social and strategic aspects. As a result, we identify three governance styles for pull requests: (1) protective, (2) equitable, and (3) lenient. While the protective style values trustworthiness and reliability of the contributor, the lenient style believes in creating a positive and welcoming environment where contributors are mentored to evolve contributions until the community standards are met. Each of the governance styles safeguards the quality of the project code in different ways. We hope that this material will help researchers and community managers to obtain a more nuanced view on the peculiarities of different communities and the strengths and weakness of their pull requests evaluation process.",Self Direction,Choosing Own Goals,"The paper aims to improve the process of evaluating pull requests in open source software communities and identifies three governance styles; each of which provides a different method for users to interact and improve their contributions. This offers software users the option of choosing their own goals on how their contributions would be evaluated and improved; aligning with the value item ""Choosing Own Goals"" under the value ""Self Direction.""","The paper's focus on improving the evaluation process of pull requests in open source software communities directly aligns with the value item ""Choosing Own Goals"" and its corresponding value ""Self Direction"" from a ""Software User"" perspective. By offering different governance styles for evaluating contributions, the paper provides software users with the opportunity to choose their own goals in terms of how their contributions will be evaluated and improved. This aligns with the value of self-direction, as users are empowered to have autonomy and control over their contributions within the community.",Agreed-Clarified,Agree,,Agree,,
Exp E,Exp J,2238,TSE,Software Development Methodologies,Pull Request Governance in Open Source Communities,"Pull requests facilitate inclusion and improvement of contributions in distributed software projects, especially in open source communities. An author makes a pull request to present a contribution as a candidate for inclusion in a code base. The request is inspected by maintainers and reviewers. The initiated process of review and collaborative improvement can be loaded with debates, opinions, and emotions. It heavily influences the atmosphere in the community. It can demotivate and detract contributors or it can fail to guard the code quality. Both problems put the existence of a community at risk. This mixed methods study aims to elucidate the mechanisms of evaluating pull requests in diverse open source software communities from the perspectives of developers and maintainers. We interviewed 30 participants from five different communities and conducted a survey with N=387 respondents. The data shows that acceptance of contributions in open source depends not only on technical criteria, but also significantly on social and strategic aspects. As a result, we identify three governance styles for pull requests: (1) protective, (2) equitable, and (3) lenient. While the protective style values trustworthiness and reliability of the contributor, the lenient style believes in creating a positive and welcoming environment where contributors are mentored to evolve contributions until the community standards are met. Each of the governance styles safeguards the quality of the project code in different ways. We hope that this material will help researchers and community managers to obtain a more nuanced view on the peculiarities of different communities and the strengths and weakness of their pull requests evaluation process.",Power,Social Recognition,"The study looks at pull request evaluations not only from a technical perspective but also from a social and strategic one. This considers the social recognition of the software users; as their contributions are evaluated and accepted based on their trustworthiness and reliability or the positive and welcoming environment they help create. It correlates with the value item ""Social Recognition"" under the value ""Power.""","The justification for aligning 'Paper X' with the value item Social Recognition and its corresponding value Power from a ""Software User"" perspective is based on the fact that the study explores how contributions in open source communities are evaluated not only based on technical criteria but also on social and strategic aspects. This recognition and evaluation of contributions directly impact the power and influence that software users have within the community, as their trustworthiness, reliability, and ability to create a positive environment contribute to the acceptance and inclusion of their contributions. This alignment highlights the importance of social recognition and power dynamics in the evaluation process, which directly affects the software user's experience within the software community.",Agreed-Clarified,Agree,,Agree,,
Exp F,Exp J,2238,TSE,Software Development Methodologies,Pull Request Governance in Open Source Communities,"Pull requests facilitate inclusion and improvement of contributions in distributed software projects, especially in open source communities. An author makes a pull request to present a contribution as a candidate for inclusion in a code base. The request is inspected by maintainers and reviewers. The initiated process of review and collaborative improvement can be loaded with debates, opinions, and emotions. It heavily influences the atmosphere in the community. It can demotivate and detract contributors or it can fail to guard the code quality. Both problems put the existence of a community at risk. This mixed methods study aims to elucidate the mechanisms of evaluating pull requests in diverse open source software communities from the perspectives of developers and maintainers. We interviewed 30 participants from five different communities and conducted a survey with N=387 respondents. The data shows that acceptance of contributions in open source depends not only on technical criteria, but also significantly on social and strategic aspects. As a result, we identify three governance styles for pull requests: (1) protective, (2) equitable, and (3) lenient. While the protective style values trustworthiness and reliability of the contributor, the lenient style believes in creating a positive and welcoming environment where contributors are mentored to evolve contributions until the community standards are met. Each of the governance styles safeguards the quality of the project code in different ways. We hope that this material will help researchers and community managers to obtain a more nuanced view on the peculiarities of different communities and the strengths and weakness of their pull requests evaluation process.",Security,Healthy,"By identifying the governance styles; the research ensures software users' contributions are thoroughly reviewed and improved; which protects the health of the project code. It also keeps the software running smoothly and effectively; securing the software's health and therefore aligning with the value item ""Healthy"" under the value ""Security.""","The identification of different governance styles for evaluating pull requests in open source software communities ensures that software users' contributions are carefully reviewed and improved, contributing to the security and health of the project code. This aligns with the value item ""Healthy"" under the value ""Security"" as it safeguards the smooth and effective functioning of the software, providing users with a secure and reliable software experience.",Agreed-Clarified,Agree,,Agree,,
Exp G,Exp K,2244,TSE,Software Engineering Practices,Motivation Under Gamification: An Empirical Study of DevelopersaEUR(tm) Motivations and Contributions in Stack Overflow,"To encourage developersaEUR(tm) volunteer contributions, modern programming question and answer (Q&A) sites like Stack Overflow (SO) employ gamified incentive mechanisms such as reputation and badges. Understanding developersaEUR(tm) motivations in the presence of gamification and the relationship between their motivations and behavioral outcomes is crucial for community building and designing good incentive mechanisms. Grounded on self-determination theory, we conducted a survey with 938 developers who participate in SO to understand their participation motivations and incentive perceptions. By connecting the survey responses with the SO data, we quantitatively analyzed how the developersaEUR(tm) motivations and satisfaction of needs relate to their effort and contribution quality. Our main findings are as follows: (1) despite the presence of gamified incentive mechanisms, developers are mainly motivated by intrinsic motivation to participate in SO; (2) developers who have strong motivations to gain gamification rewards are associated with higher intrinsic and integrated motivations, while developers with more development experiences are less motivated by the gamified incentives; (3) both extrinsic motivations (in terms of career prospects) and intrinsic motivations (regarding self-improvement and helping others) can motivate developers to make high-quantity and high-quality contributions; and (4) high-level satisfaction of needs for competency and autonomy has a positive effect on developers making high-quantity and high-quality contributions and addressing difficult problems. Based on these findings, we discuss implications for developer motivation and gamification in the crowdsourcing context and for the mechanism design of gamified crowdsourced platforms.",Achievement,Intelligent,The paper discusses the aspect of developer competency on crowdsourced platforms like Stack Overflow; emphasizing user competency as an important element. This aligns with the value item Intelligent and its corresponding value Achievement.,"My justification for labeling 'Paper X' as aligning with the value item Intelligent and its corresponding value Achievement from a ""Software User"" perspective is based on the paper's focus on analyzing the relationship between developers' motivations, satisfaction of needs, and their effort and contribution quality on Stack Overflow. By emphasizing the importance of a developer's competency in making high-quality contributions, the paper indirectly emphasizes the value of intelligence and the pursuit of achievement in the software development context.",Agreed-Clarified,Agree,,Agree,,
Exp A,Exp H,2244,TSE,Software Engineering Practices,Motivation Under Gamification: An Empirical Study of DevelopersaEUR(tm) Motivations and Contributions in Stack Overflow,"To encourage developersaEUR(tm) volunteer contributions, modern programming question and answer (Q&A) sites like Stack Overflow (SO) employ gamified incentive mechanisms such as reputation and badges. Understanding developersaEUR(tm) motivations in the presence of gamification and the relationship between their motivations and behavioral outcomes is crucial for community building and designing good incentive mechanisms. Grounded on self-determination theory, we conducted a survey with 938 developers who participate in SO to understand their participation motivations and incentive perceptions. By connecting the survey responses with the SO data, we quantitatively analyzed how the developersaEUR(tm) motivations and satisfaction of needs relate to their effort and contribution quality. Our main findings are as follows: (1) despite the presence of gamified incentive mechanisms, developers are mainly motivated by intrinsic motivation to participate in SO; (2) developers who have strong motivations to gain gamification rewards are associated with higher intrinsic and integrated motivations, while developers with more development experiences are less motivated by the gamified incentives; (3) both extrinsic motivations (in terms of career prospects) and intrinsic motivations (regarding self-improvement and helping others) can motivate developers to make high-quantity and high-quality contributions; and (4) high-level satisfaction of needs for competency and autonomy has a positive effect on developers making high-quantity and high-quality contributions and addressing difficult problems. Based on these findings, we discuss implications for developer motivation and gamification in the crowdsourcing context and for the mechanism design of gamified crowdsourced platforms.",Achievement,Capable,The paper mentions that high-level satisfaction of needs for competency leads to high-quantity and high-quality contributions. This suggests an alignment with the value item Capable and its corresponding value Achievement.,"In the paper abstract of 'Paper X', it is stated that high-level satisfaction of needs for competency positively impacts developers making high-quantity and high-quality contributions. This implies that developers who feel capable and competent in their skills and abilities are more likely to achieve success in their contributions. Therefore, the alignment with the value item Capable and its corresponding value Achievement is evident as the paper directly connects developers' competency satisfaction with their contributions' quality and quantity.",Agreed-Clarified,Agree,,Agree,,
Exp B,Exp H,2244,TSE,Software Engineering Practices,Motivation Under Gamification: An Empirical Study of DevelopersaEUR(tm) Motivations and Contributions in Stack Overflow,"To encourage developersaEUR(tm) volunteer contributions, modern programming question and answer (Q&A) sites like Stack Overflow (SO) employ gamified incentive mechanisms such as reputation and badges. Understanding developersaEUR(tm) motivations in the presence of gamification and the relationship between their motivations and behavioral outcomes is crucial for community building and designing good incentive mechanisms. Grounded on self-determination theory, we conducted a survey with 938 developers who participate in SO to understand their participation motivations and incentive perceptions. By connecting the survey responses with the SO data, we quantitatively analyzed how the developersaEUR(tm) motivations and satisfaction of needs relate to their effort and contribution quality. Our main findings are as follows: (1) despite the presence of gamified incentive mechanisms, developers are mainly motivated by intrinsic motivation to participate in SO; (2) developers who have strong motivations to gain gamification rewards are associated with higher intrinsic and integrated motivations, while developers with more development experiences are less motivated by the gamified incentives; (3) both extrinsic motivations (in terms of career prospects) and intrinsic motivations (regarding self-improvement and helping others) can motivate developers to make high-quantity and high-quality contributions; and (4) high-level satisfaction of needs for competency and autonomy has a positive effect on developers making high-quantity and high-quality contributions and addressing difficult problems. Based on these findings, we discuss implications for developer motivation and gamification in the crowdsourcing context and for the mechanism design of gamified crowdsourced platforms.",Benevolence,Helpful,The paper mentions intrinsic motivations whereby developers participate in Stack Overflow for self-improvement and helping others. This aspect of developers helping others aligns with the value item Helpful and its corresponding value Benevolence.,"In the paper, it is explicitly stated that developers participate in Stack Overflow for self-improvement and helping others. This aligns with the value item Helpful and the corresponding value Benevolence from Schwartz's Taxonomy. By actively participating in the Q&A site and providing assistance to other developers, software users can benefit from the helpful contributions of developers who are motivated by the intrinsic value of helping others. This alignment demonstrates the importance of benevolence and the positive impact it has on the software user community.",Agreed-Justified,Agree,,Agree,,
Exp C,Exp K,2251,TSE,AI & Machine Learning,BiasFinder: Metamorphic Test Generation to Uncover Bias for Sentiment Analysis Systems,"Artificial intelligence systems, such as Sentiment Analysis (SA) systems, typically learn from large amounts of data that may reflect human bias. Consequently, such systems may exhibit unintended demographic bias against specific characteristics (e.g., gender, occupation, country-of-origin, etc.). Such bias manifests in an SA system when it predicts different sentiments for similar texts that differ only in the characteristic of individuals described. To automatically uncover bias in SA systems, this paper presents BiasFinder, an approach that can discover biased predictions in SA systems via metamorphic testing. A key feature of BiasFinder is the automatic curation of suitable templates from any given text inputs, using various Natural Language Processing (NLP) techniques to identify words that describe demographic characteristics. Next, BiasFinder generates new texts from these templates by mutating words associated with a class of a characteristic (e.g., gender-specific words such as female names, aEURoesheaEUR, aEURoeheraEUR). These texts are then used to tease out bias in an SA system. BiasFinder identifies a bias-uncovering test case (BTC) when an SA system predicts different sentiments for texts that differ only in words associated with a different class (e.g., male vs. female) of a target characteristic (e.g., gender). We evaluate BiasFinder on 10 SA systems and 2 large scale datasets, and the results show that BiasFinder can create more BTCs than two popular baselines. We also conduct an annotation study and find that human annotators consistently think that test cases generated by BiasFinder are more fluent than the two baselines.",Stimulation,Variation in Life,The paper contributes an approach(BiasFinder) to uncover bias in Sentiment Analysis (SA) systems; introducing a sort of variation in SA systems; thus aligning with the value item 'Variation in Life' and its corresponding value 'Stimulation'.,"In the given abstract, 'Paper X' introduces BiasFinder, an approach that uncovers bias in Sentiment Analysis (SA) systems. By generating new texts with mutated words associated with different demographic characteristics, BiasFinder introduces a form of variation in SA systems, leading to the discovery of biased predictions. This aligns with the value item 'Variation in Life' as it introduces a diverse range of inputs and stimulates the exploration of different perspectives within SA systems. Overall, this alignment demonstrates how 'Paper X' directly contributes to the value item 'Variation in Life' and its corresponding value 'Stimulation' from a ""Software User"" perspective.",Agreed-Clarified,Agree,,Agree,,
Exp D,Exp K,2251,TSE,AI & Machine Learning,BiasFinder: Metamorphic Test Generation to Uncover Bias for Sentiment Analysis Systems,"Artificial intelligence systems, such as Sentiment Analysis (SA) systems, typically learn from large amounts of data that may reflect human bias. Consequently, such systems may exhibit unintended demographic bias against specific characteristics (e.g., gender, occupation, country-of-origin, etc.). Such bias manifests in an SA system when it predicts different sentiments for similar texts that differ only in the characteristic of individuals described. To automatically uncover bias in SA systems, this paper presents BiasFinder, an approach that can discover biased predictions in SA systems via metamorphic testing. A key feature of BiasFinder is the automatic curation of suitable templates from any given text inputs, using various Natural Language Processing (NLP) techniques to identify words that describe demographic characteristics. Next, BiasFinder generates new texts from these templates by mutating words associated with a class of a characteristic (e.g., gender-specific words such as female names, aEURoesheaEUR, aEURoeheraEUR). These texts are then used to tease out bias in an SA system. BiasFinder identifies a bias-uncovering test case (BTC) when an SA system predicts different sentiments for texts that differ only in words associated with a different class (e.g., male vs. female) of a target characteristic (e.g., gender). We evaluate BiasFinder on 10 SA systems and 2 large scale datasets, and the results show that BiasFinder can create more BTCs than two popular baselines. We also conduct an annotation study and find that human annotators consistently think that test cases generated by BiasFinder are more fluent than the two baselines.",Hedonism,Enjoying Life,The contribution of BiasFinder makes SA systems more accurately reflect the sentiments of different demographic groups; potentially enhancing the user's experience with these systems; thus aligning with 'Enjoying Life' and its corresponding value 'Hedonism'.,"The justification for aligning 'Paper X' with the value item Enjoying Life and its corresponding value Hedonism from a ""Software User"" perspective is based on the idea that the contribution of BiasFinder in making SA systems more accurately reflect the sentiments of different demographic groups can potentially enhance the user's experience with these systems. By reducing unintended demographic bias and predicting more consistent sentiments for similar texts, BiasFinder can contribute to a more enjoyable user experience where users can trust the system to accurately interpret their sentiments, leading to a greater sense of pleasure and satisfaction in using the software.",Agreed-Justified,Agree,,Agree,,
Exp E,Exp J,2251,TSE,AI & Machine Learning,BiasFinder: Metamorphic Test Generation to Uncover Bias for Sentiment Analysis Systems,"Artificial intelligence systems, such as Sentiment Analysis (SA) systems, typically learn from large amounts of data that may reflect human bias. Consequently, such systems may exhibit unintended demographic bias against specific characteristics (e.g., gender, occupation, country-of-origin, etc.). Such bias manifests in an SA system when it predicts different sentiments for similar texts that differ only in the characteristic of individuals described. To automatically uncover bias in SA systems, this paper presents BiasFinder, an approach that can discover biased predictions in SA systems via metamorphic testing. A key feature of BiasFinder is the automatic curation of suitable templates from any given text inputs, using various Natural Language Processing (NLP) techniques to identify words that describe demographic characteristics. Next, BiasFinder generates new texts from these templates by mutating words associated with a class of a characteristic (e.g., gender-specific words such as female names, aEURoesheaEUR, aEURoeheraEUR). These texts are then used to tease out bias in an SA system. BiasFinder identifies a bias-uncovering test case (BTC) when an SA system predicts different sentiments for texts that differ only in words associated with a different class (e.g., male vs. female) of a target characteristic (e.g., gender). We evaluate BiasFinder on 10 SA systems and 2 large scale datasets, and the results show that BiasFinder can create more BTCs than two popular baselines. We also conduct an annotation study and find that human annotators consistently think that test cases generated by BiasFinder are more fluent than the two baselines.",Universalism,Equality,The paper offers a unique approach to uncovering bias in SA systems. This approach reflects an emphasis on fairness and treating all demographic groups equally; aligning with the value item 'Equality' and its corresponding value 'Universalism'.,"The justification for aligning 'Paper X' with the value item Equality and its corresponding value Universalism is based on the fact that the paper's approach to uncovering bias in SA systems focuses on fairness and treating all demographic groups equally. By automatically generating test cases that vary based on demographic characteristics, the paper aims to identify and address biases that may disproportionately affect certain groups. This aligns with the value of Universalism, which emphasizes the importance of equality, broadmindedness, and social justice. Therefore, the main contributions of 'Paper X' directly align with the value item Equality and its corresponding value Universalism, as they prioritize fairness and equality within a software context for software users.",Agreed-Justified,Agree,,Agree,,
Exp F,Exp J,2255,TSE,Accessibility & User Experience,A Systematic Literature Review of Anti-Discrimination Design Strategies in the Digital Sharing Economy,"Applications of the Digital Sharing Economy (DSE), such as Uber, Airbnb, and TaskRabbit, have become a main facilitator of economic growth and shared prosperity in modern-day societies. However, recent research has revealed that the participation of minority groups in DSE activities is often hindered by different forms of bias and discrimination. Evidence of such behavior has been documented across almost all domains of DSE, including ridesharing, lodging, and freelancing. However, little is known about the underlying design decisions of DSE platforms which allow certain demographics of the market to gain unfair advantage over others. To bridge this knowledge gap, in this paper, we systematically synthesize evidence from 58 interdisciplinary studies to identify the pervasive discrimination concerns affecting DSE platforms along with their triggering features and mitigation strategies. Our objective is to consolidate such interdisciplinary evidence from a software design point of view. Our results show that existing evidence is mainly geared towards documenting and mitigating issues of racism and sexism affecting platforms of ridesharing, lodging, and freelancing. Our review further shows that discrimination concerns in the DSE market are commonly enabled by features of user profiles and commonly impact reputation systems.",Benevolence,A Spiritual Life,"The paper extensively discusses strategies to mitigate discrimination issues in DSE platforms. This directly aligns with the value item ""A Spiritual Life"" in the value ""Benevolence"" as it shows a focus on improving the state of well-being and fairness in the user community.","The justification for aligning 'Paper X' with the value item A Spiritual Life and its corresponding value Benevolence is based on the paper's focus on strategies to address discrimination issues in DSE platforms. By aiming to improve fairness and well-being in the user community, the paper aligns with the value of Benevolence, which is reflected in the value item A Spiritual Life. This alignment highlights the paper's emphasis on creating a just and equitable environment, which is an essential aspect of spiritual well-being.",Agreed-Clarified,Agree,,Agree,,
Exp G,Exp K,2255,TSE,Accessibility & User Experience,A Systematic Literature Review of Anti-Discrimination Design Strategies in the Digital Sharing Economy,"Applications of the Digital Sharing Economy (DSE), such as Uber, Airbnb, and TaskRabbit, have become a main facilitator of economic growth and shared prosperity in modern-day societies. However, recent research has revealed that the participation of minority groups in DSE activities is often hindered by different forms of bias and discrimination. Evidence of such behavior has been documented across almost all domains of DSE, including ridesharing, lodging, and freelancing. However, little is known about the underlying design decisions of DSE platforms which allow certain demographics of the market to gain unfair advantage over others. To bridge this knowledge gap, in this paper, we systematically synthesize evidence from 58 interdisciplinary studies to identify the pervasive discrimination concerns affecting DSE platforms along with their triggering features and mitigation strategies. Our objective is to consolidate such interdisciplinary evidence from a software design point of view. Our results show that existing evidence is mainly geared towards documenting and mitigating issues of racism and sexism affecting platforms of ridesharing, lodging, and freelancing. Our review further shows that discrimination concerns in the DSE market are commonly enabled by features of user profiles and commonly impact reputation systems.",Universalism,Equality,"The paper identifies the discrimination concerns affecting DSE platforms. This aligns with the value item ""Equality"" in the value ""Universalism"" as the main theme of the paper is addressing inequality and promoting equal treatment for all users irrespective of their demographics.","The main contribution of 'Paper X' aligns with the value item Equality and its corresponding value Universalism because the paper explicitly focuses on identifying and addressing discrimination concerns in DSE platforms, which directly relates to promoting equal treatment for all users regardless of their demographics. By aiming to mitigate bias and discrimination, the paper aligns with the value of Universalism, which emphasizes equal rights and opportunities for all individuals.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2255,TSE,Accessibility & User Experience,A Systematic Literature Review of Anti-Discrimination Design Strategies in the Digital Sharing Economy,"Applications of the Digital Sharing Economy (DSE), such as Uber, Airbnb, and TaskRabbit, have become a main facilitator of economic growth and shared prosperity in modern-day societies. However, recent research has revealed that the participation of minority groups in DSE activities is often hindered by different forms of bias and discrimination. Evidence of such behavior has been documented across almost all domains of DSE, including ridesharing, lodging, and freelancing. However, little is known about the underlying design decisions of DSE platforms which allow certain demographics of the market to gain unfair advantage over others. To bridge this knowledge gap, in this paper, we systematically synthesize evidence from 58 interdisciplinary studies to identify the pervasive discrimination concerns affecting DSE platforms along with their triggering features and mitigation strategies. Our objective is to consolidate such interdisciplinary evidence from a software design point of view. Our results show that existing evidence is mainly geared towards documenting and mitigating issues of racism and sexism affecting platforms of ridesharing, lodging, and freelancing. Our review further shows that discrimination concerns in the DSE market are commonly enabled by features of user profiles and commonly impact reputation systems.",Universalism,Social Justice,"The paper focuses on addressing issues of bias and discrimination in DSE platforms. This directly aligns with the value item ""Social Justice"" in the value ""Universalism"" as it exhibits a focus on rectifying social injustices by identifying and mitigating prevalent discrimination concerns.","In the given paper abstract, the authors explicitly state that their objective is to identify discrimination concerns affecting DSE platforms and provide mitigation strategies. This objective directly aligns with the value item of ""Social Justice"" in the value ""Universalism"" as it demonstrates a focus on rectifying inequalities and promoting fairness and justice in the software context. By addressing issues of bias and discrimination, the paper contributes to creating a more just and equitable digital sharing economy, catering to the values of social justice and universalism.",Agreed-Justified,Agree,,Agree,,
Exp B,Exp H,2256,TSE,Software Development Methodologies,How Can Human Values be Addressed in Agile Methods? A Case Study on SAFe,"Agile methods are predominantly focused on delivering business values. But can Agile methods be adapted to effectively address and deliver human values such as social justice, privacy, and sustainability in the software they produce? Human values are what an individual or a society considers important in life. Ignoring these human values in software can pose difficulties or risks for all stakeholders (e.g., user dissatisfaction, reputation damage, financial loss). To answer this question, we selected the Scaled Agile Framework (SAFe), one of the most commonly used Agile methods in the industry, and conducted a qualitative case study to identify possible intervention points within SAFe that are the most natural to address and integrate human values in software. We present five high-level empirically-justified sets of interventions in SAFe: artefacts, roles, ceremonies, practices, and culture. We elaborate how some current Agile artefacts (e.g., user story), roles (e.g., product owner), ceremonies (e.g., stand-up meeting), and practices (e.g., business-facing testing) in SAFe can be modified to support the inclusion of human values in software. Further, our study suggests new and exclusive values-based artefacts (e.g., legislative requirement), ceremonies (e.g., values conversation), roles (e.g., values champion), and cultural practices (e.g., induction and hiring) to be introduced in SAFe for this purpose. Guided by our findings, we argue that existing Agile methods can account for human values in software delivery with some evolutionary adaptations.",Tradition,Respect for Tradition,Paper X effectively addresses and comprehends the significance of human values in software; indicating respect for traditional values in life; which aligns with the value item Respect for Tradition and its corresponding value Tradition.,"In 'Paper X', the authors acknowledge the importance of human values in software and propose interventions within the Scaled Agile Framework (SAFe) to address and integrate these values. By considering the significance of traditional values in life, the paper aligns with the value item Respect for Tradition and its corresponding value Tradition. The inclusion of values-based artefacts, ceremonies, roles, and cultural practices, as suggested by the study, demonstrates a recognition of the importance of respecting and incorporating traditional values in software development, thereby directly aligning with the value item Respect for Tradition.",Agreed-Clarified,Agree,,Agree,,
Exp C,Exp K,2256,TSE,Software Development Methodologies,How Can Human Values be Addressed in Agile Methods? A Case Study on SAFe,"Agile methods are predominantly focused on delivering business values. But can Agile methods be adapted to effectively address and deliver human values such as social justice, privacy, and sustainability in the software they produce? Human values are what an individual or a society considers important in life. Ignoring these human values in software can pose difficulties or risks for all stakeholders (e.g., user dissatisfaction, reputation damage, financial loss). To answer this question, we selected the Scaled Agile Framework (SAFe), one of the most commonly used Agile methods in the industry, and conducted a qualitative case study to identify possible intervention points within SAFe that are the most natural to address and integrate human values in software. We present five high-level empirically-justified sets of interventions in SAFe: artefacts, roles, ceremonies, practices, and culture. We elaborate how some current Agile artefacts (e.g., user story), roles (e.g., product owner), ceremonies (e.g., stand-up meeting), and practices (e.g., business-facing testing) in SAFe can be modified to support the inclusion of human values in software. Further, our study suggests new and exclusive values-based artefacts (e.g., legislative requirement), ceremonies (e.g., values conversation), roles (e.g., values champion), and cultural practices (e.g., induction and hiring) to be introduced in SAFe for this purpose. Guided by our findings, we argue that existing Agile methods can account for human values in software delivery with some evolutionary adaptations.",Achievement,Intelligent,The paper's main contribution is its emphasis on Agile methods' ability to address human values; giving software users the potential to be intelligent and independent decision-makers. This aligns with the value item Intelligent and its corresponding value Achievement.,"In the context of the paper, the alignment with the value item ""Intelligent"" and its corresponding value ""Achievement"" can be inferred from the statement that Agile methods, when adapted to address human values, empower software users to be intelligent and independent decision-makers. This suggests that the focus on human values in software delivery allows users to achieve their own goals and make informed choices, reflecting the value of intelligence and a sense of accomplishment.",Agreed-Justified,Agree,,Agree,,
Exp D,Exp K,2256,TSE,Software Development Methodologies,How Can Human Values be Addressed in Agile Methods? A Case Study on SAFe,"Agile methods are predominantly focused on delivering business values. But can Agile methods be adapted to effectively address and deliver human values such as social justice, privacy, and sustainability in the software they produce? Human values are what an individual or a society considers important in life. Ignoring these human values in software can pose difficulties or risks for all stakeholders (e.g., user dissatisfaction, reputation damage, financial loss). To answer this question, we selected the Scaled Agile Framework (SAFe), one of the most commonly used Agile methods in the industry, and conducted a qualitative case study to identify possible intervention points within SAFe that are the most natural to address and integrate human values in software. We present five high-level empirically-justified sets of interventions in SAFe: artefacts, roles, ceremonies, practices, and culture. We elaborate how some current Agile artefacts (e.g., user story), roles (e.g., product owner), ceremonies (e.g., stand-up meeting), and practices (e.g., business-facing testing) in SAFe can be modified to support the inclusion of human values in software. Further, our study suggests new and exclusive values-based artefacts (e.g., legislative requirement), ceremonies (e.g., values conversation), roles (e.g., values champion), and cultural practices (e.g., induction and hiring) to be introduced in SAFe for this purpose. Guided by our findings, we argue that existing Agile methods can account for human values in software delivery with some evolutionary adaptations.",Universalism,Social Justice,By focusing on delivering human values such as social justice in the software; Paper X directly aligns with the value item Social Justice and its corresponding value Universalism.,"In 'Paper X', the authors explicitly state that they aim to address and integrate human values, including social justice, in the software produced using Agile methods. By acknowledging the importance of social justice and working towards its inclusion in the software, the paper aligns directly with the value item Social Justice and its corresponding value Universalism, as Universalism values emphasize the promotion of equality, social justice, and a world at peace. Therefore, the main contributions of 'Paper X' demonstrate a clear alignment with the value item Social Justice and its corresponding value Universalism from the perspective of a software user.",Agreed-Clarified,Agree,,Agree,,
Exp E,Exp J,2258,TSE,AI & Machine Learning,Astraea: Grammar-Based Fairness Testing,"Software often produces biased outputs. In particular, machine learning (ML) based software is known to produce erroneous predictions when processing discriminatory inputs. Such unfair program behavior can be caused by societal bias. In the last few years, Amazon, Microsoft and Google have provided software services that produce unfair outputs, mostly due to societal bias (e.g., gender or race). In such events, developers are saddled with the task of conducting fairness testing. Fairness testing is challenging; developers are tasked with generating discriminatory inputs that reveal and explain biases. We propose a grammar-based fairness testing approach (called Astraea) which leverages context-free grammars to generate discriminatory inputs that reveal fairness violations in software systems. Using probabilistic grammars, Astraea also provides fault diagnosis by isolating the cause of observed software bias. AstraeaaEUR(tm)s diagnoses facilitate the improvement of ML fairness. Astraea was evaluated on 18 software systems that provide three major natural language processing (NLP) services. In our evaluation, Astraea generated fairness violations at a rate of about 18%. Astraea generated over 573K discriminatory test cases and found over 102K fairness violations. Furthermore, Astraea improves software fairness by about 76% via model-retraining, on average.",Achievement,Successful,Paper X focuses on improving the fairness in software systems; which will contribute to the software users' success in achieving unbiased and fair results. This aligns with the value item Successful and its corresponding value Achievement.,"I apologize for any confusion. In my previous justification, I stated that the main contribution of 'Paper X' is aligned with the value item Successful and its corresponding value Achievement from a ""Software User"" perspective. This is because the paper aims to improve fairness in software systems, which directly contributes to the success of software users in achieving unbiased and fair results. By addressing societal bias and providing fault diagnosis for fairness violations, the paper ultimately helps software users achieve their desired outcomes without facing discriminatory outputs. This alignment with the value item Successful and its corresponding value Achievement highlights the importance of fairness and equal opportunities for software users in achieving their goals through the use of software systems.",Agreed-Justified,Agree,,Agree,,
Exp F,Exp J,2258,TSE,AI & Machine Learning,Astraea: Grammar-Based Fairness Testing,"Software often produces biased outputs. In particular, machine learning (ML) based software is known to produce erroneous predictions when processing discriminatory inputs. Such unfair program behavior can be caused by societal bias. In the last few years, Amazon, Microsoft and Google have provided software services that produce unfair outputs, mostly due to societal bias (e.g., gender or race). In such events, developers are saddled with the task of conducting fairness testing. Fairness testing is challenging; developers are tasked with generating discriminatory inputs that reveal and explain biases. We propose a grammar-based fairness testing approach (called Astraea) which leverages context-free grammars to generate discriminatory inputs that reveal fairness violations in software systems. Using probabilistic grammars, Astraea also provides fault diagnosis by isolating the cause of observed software bias. AstraeaaEUR(tm)s diagnoses facilitate the improvement of ML fairness. Astraea was evaluated on 18 software systems that provide three major natural language processing (NLP) services. In our evaluation, Astraea generated fairness violations at a rate of about 18%. Astraea generated over 573K discriminatory test cases and found over 102K fairness violations. Furthermore, Astraea improves software fairness by about 76% via model-retraining, on average.",Universalism,A World at Peace,With Astraea; Paper X is striving for fairness and eliminating bias in software; which aligns with the paper's contribution to A World at Peace; valuing fairness and harmony in software interactions with users. This aligns with the value item A World at Peace and its corresponding value Universalism.,"In the context of 'Paper X', the proposed Astraea approach aims to address biases and improve fairness in software systems. By striving for fairness and eliminating bias, 'Paper X' aligns with the value item A World at Peace and its corresponding value Universalism. Universalism, as a value, emphasizes broadmindedness, equality, and social justice. The pursuit of fairness and the reduction of bias in software align with these values by ensuring equal treatment and promoting harmony in software interactions with users. Therefore, the alignment with A World at Peace and Universalism is evident in the paper's contribution.",Agreed-Justified,Disagree,still relevant to peace ,Agree,,
Exp A,Exp H,2261,TSE,Security & Privacy,The Secret Life of Software Vulnerabilities: A Large-Scale Empirical Study,"Software vulnerabilities are weaknesses in source code that can be potentially exploited to cause loss or harm. While researchers have been devising a number of methods to deal with vulnerabilities, there is still a noticeable lack of knowledge on their software engineering life cycle, for example how vulnerabilities are introduced and removed by developers. This information can be exploited to design more effective methods for vulnerability prevention and detection, as well as to understand the granularity at which these methods should aim. To investigate the life cycle of known software vulnerabilities, we focus on how, when, and under which circumstances the contributions to the introduction of vulnerabilities in software projects are made, as well as how long, and how they are removed. We consider 3,663 vulnerabilities with public patches from the National Vulnerability DatabaseaEUR""pertaining to 1,096 open-source software projects on GitHubaEUR""and define an eight-step process involving both automated parts (e.g., using a procedure based on the SZZ algorithm to find the vulnerability-contributing commits) and manual analyses (e.g., how vulnerabilities were fixed). The investigated vulnerabilities can be classified in 144 categories, take on average at least 4 contributing commits before being introduced, and half of them remain unfixed for at least more than one year. Most of the contributions are done by developers with high workload, often when doing maintenance activities, and removed mostly with the addition of new source code aiming at implementing further checks on inputs. We conclude by distilling practical implications on how vulnerability detectors should work to assist developers in timely identifying these issues.",Security,Healthy,The paper investigates the life cycle of known software vulnerabilities with an aim to design more effective methods for vulnerability prevention and detection. This could directly enhance the safety and healthiness of software; thus align with the value item 'Healthy' and corresponding value 'Security'.,"The justification for aligning 'Paper X' with the value item Healthy and its corresponding value Security is based on the fact that vulnerabilities in software can potentially cause harm or loss. By investigating the life cycle of these vulnerabilities and designing more effective methods for prevention and detection, the paper aims to enhance the safety and healthiness of software. This aligns with the value item Healthy as it directly relates to the security and well-being of software users. Increased security measures can contribute to a healthier software environment by reducing the risk of vulnerabilities and potential harm to users.",Agreed-Justified,Disagree,reconcile with coder_2,Disagree,The justification doesn't align with the value item of the healthy,
Exp B,Exp H,2261,TSE,Security & Privacy,The Secret Life of Software Vulnerabilities: A Large-Scale Empirical Study,"Software vulnerabilities are weaknesses in source code that can be potentially exploited to cause loss or harm. While researchers have been devising a number of methods to deal with vulnerabilities, there is still a noticeable lack of knowledge on their software engineering life cycle, for example how vulnerabilities are introduced and removed by developers. This information can be exploited to design more effective methods for vulnerability prevention and detection, as well as to understand the granularity at which these methods should aim. To investigate the life cycle of known software vulnerabilities, we focus on how, when, and under which circumstances the contributions to the introduction of vulnerabilities in software projects are made, as well as how long, and how they are removed. We consider 3,663 vulnerabilities with public patches from the National Vulnerability DatabaseaEUR""pertaining to 1,096 open-source software projects on GitHubaEUR""and define an eight-step process involving both automated parts (e.g., using a procedure based on the SZZ algorithm to find the vulnerability-contributing commits) and manual analyses (e.g., how vulnerabilities were fixed). The investigated vulnerabilities can be classified in 144 categories, take on average at least 4 contributing commits before being introduced, and half of them remain unfixed for at least more than one year. Most of the contributions are done by developers with high workload, often when doing maintenance activities, and removed mostly with the addition of new source code aiming at implementing further checks on inputs. We conclude by distilling practical implications on how vulnerability detectors should work to assist developers in timely identifying these issues.",Security,Social Order,By defining an eight-step process involving both automated and manual analyses to find and fix vulnerabilities; the paper contributes to a systematic approach which indirectly fosters social order in the context of software use. Its research helps to understand how vulnerabilities could be introduced and eliminated; which corresponds to the 'Social Order' value item under 'Security'.,"In the context of a 'Software User', the alignment of 'Paper X' with the value item of Social Order and its corresponding value of Security is evident through the paper's contribution to a systematic approach in finding and fixing vulnerabilities. By investigating the life cycle of vulnerabilities and understanding how they are introduced and removed, the paper aids in enhancing the overall security and stability of software systems. This directly promotes social order within the realm of software usage as it ensures the smooth functioning and protection of users' data and systems from potential threats.",Agreed-Clarified,Agree,,Agree,,
Exp C,Exp K,2261,TSE,Security & Privacy,The Secret Life of Software Vulnerabilities: A Large-Scale Empirical Study,"Software vulnerabilities are weaknesses in source code that can be potentially exploited to cause loss or harm. While researchers have been devising a number of methods to deal with vulnerabilities, there is still a noticeable lack of knowledge on their software engineering life cycle, for example how vulnerabilities are introduced and removed by developers. This information can be exploited to design more effective methods for vulnerability prevention and detection, as well as to understand the granularity at which these methods should aim. To investigate the life cycle of known software vulnerabilities, we focus on how, when, and under which circumstances the contributions to the introduction of vulnerabilities in software projects are made, as well as how long, and how they are removed. We consider 3,663 vulnerabilities with public patches from the National Vulnerability DatabaseaEUR""pertaining to 1,096 open-source software projects on GitHubaEUR""and define an eight-step process involving both automated parts (e.g., using a procedure based on the SZZ algorithm to find the vulnerability-contributing commits) and manual analyses (e.g., how vulnerabilities were fixed). The investigated vulnerabilities can be classified in 144 categories, take on average at least 4 contributing commits before being introduced, and half of them remain unfixed for at least more than one year. Most of the contributions are done by developers with high workload, often when doing maintenance activities, and removed mostly with the addition of new source code aiming at implementing further checks on inputs. We conclude by distilling practical implications on how vulnerability detectors should work to assist developers in timely identifying these issues.",Security,National Security,The paper's contribution of focusing on vulnerability detection and prevention mechanisms aligns with the value item 'National Security'. It strengthens the resilience of software; which often serves as the infrastructure of electronic devices; including those used in nations' critical facilities; hence boosting 'National Security'.,"In the context of a ""Software User"", the alignment between 'Paper X' and the value item National Security can be seen through the paper's focus on vulnerability detection and prevention mechanisms. By addressing vulnerabilities in software, the paper indirectly strengthens the security of software systems, which are often the core infrastructure of electronic devices. This enhancement in the security of electronic devices used in critical facilities contributes to bolstering national security by minimizing the potential for security breaches and ensuring the integrity and reliability of these systems.",Agreed-Clarified,Agree,,Agree,,
Exp D,Exp K,2272,TSE,Accessibility & User Experience,Impact of Usability Mechanisms: A Family of Experiments on Efficiency Effectiveness and User Satisfaction,"Context: The usability software quality characteristic aims to improve system user performance. In a previous study, we found evidence of the impact of a set of usability features from the viewpoint of users in terms of efficiency, effectiveness and satisfaction. However, the impact level appears to depend on the usability feature and suggest priorities with respect to their implementation depending on how they promote user performance. Objectives: We use a family of three experiments to increase the precision and generalization of the results in the baseline experiment and provide findings regarding the impact on user performance of the Abort Operation, Progress Feedback and Preferences usability mechanisms. Method: We conduct two replications of the baseline experiment in academic settings. We analyse the data of 366 experimental subjects and apply aggregation (meta-analysis) procedures. Results: We find that the Abort Operation and Preferences usability mechanisms appear to improve system usability a great deal with respect to efficiency, effectiveness and user satisfaction. Conclusions: We find that the family of experiments further corroborates the results of the baseline experiment. Most of the results are statistically significant, and, because of the large number of experimental subjects, the evidence that we gathered in the replications is sufficient to outweigh other experiments.",Achievement,Capable,The paper highlights the usability mechanisms (Abort Operation and Preferences) that aim to improve user performance; measure efficiency; effectiveness and user satisfaction; aligning with the value item 'Capable' which pertains to the value 'Achievement'.,"In the abstract of 'Paper X', it is explicitly stated that the usability mechanisms examined (Abort Operation and Preferences) aim to improve system usability in terms of efficiency, effectiveness, and user satisfaction. These outcomes directly align with the value item 'Capable' from Schwartz's Taxonomy, as being capable involves being successful, achieving goals, and being competent. Therefore, the focus on improving user performance, measuring efficiency and effectiveness, and ensuring user satisfaction aligns with the value of achievement, which is associated with the value item 'Capable'.",Agreed-Justified,Agree,,Agree,,
Exp E,Exp J,2272,TSE,Accessibility & User Experience,Impact of Usability Mechanisms: A Family of Experiments on Efficiency Effectiveness and User Satisfaction,"Context: The usability software quality characteristic aims to improve system user performance. In a previous study, we found evidence of the impact of a set of usability features from the viewpoint of users in terms of efficiency, effectiveness and satisfaction. However, the impact level appears to depend on the usability feature and suggest priorities with respect to their implementation depending on how they promote user performance. Objectives: We use a family of three experiments to increase the precision and generalization of the results in the baseline experiment and provide findings regarding the impact on user performance of the Abort Operation, Progress Feedback and Preferences usability mechanisms. Method: We conduct two replications of the baseline experiment in academic settings. We analyse the data of 366 experimental subjects and apply aggregation (meta-analysis) procedures. Results: We find that the Abort Operation and Preferences usability mechanisms appear to improve system usability a great deal with respect to efficiency, effectiveness and user satisfaction. Conclusions: We find that the family of experiments further corroborates the results of the baseline experiment. Most of the results are statistically significant, and, because of the large number of experimental subjects, the evidence that we gathered in the replications is sufficient to outweigh other experiments.",Security,Healthy,The paper discusses usability features which lead to improved system usability - these features can enhance the productivity of the software user; therefore it aligns with the value item 'Healthy' within the value 'Security'.,"I apologize for any confusion. In my justification, I mentioned that the usability features discussed in the paper can enhance the productivity of the software user, which in turn aligns with the value item 'Healthy' within the value 'Security'. This is because when a software user is able to interact with a system efficiently and effectively, it contributes to a sense of security and stability, which are aspects of the 'Healthy' value. By prioritizing usability mechanisms that improve system usability, the paper indirectly promotes the well-being and satisfaction of software users, aligning with the 'Security' value.",Agreed-Clarified,Agree,,Agree,,
Exp F,Exp J,2272,TSE,Accessibility & User Experience,Impact of Usability Mechanisms: A Family of Experiments on Efficiency Effectiveness and User Satisfaction,"Context: The usability software quality characteristic aims to improve system user performance. In a previous study, we found evidence of the impact of a set of usability features from the viewpoint of users in terms of efficiency, effectiveness and satisfaction. However, the impact level appears to depend on the usability feature and suggest priorities with respect to their implementation depending on how they promote user performance. Objectives: We use a family of three experiments to increase the precision and generalization of the results in the baseline experiment and provide findings regarding the impact on user performance of the Abort Operation, Progress Feedback and Preferences usability mechanisms. Method: We conduct two replications of the baseline experiment in academic settings. We analyse the data of 366 experimental subjects and apply aggregation (meta-analysis) procedures. Results: We find that the Abort Operation and Preferences usability mechanisms appear to improve system usability a great deal with respect to efficiency, effectiveness and user satisfaction. Conclusions: We find that the family of experiments further corroborates the results of the baseline experiment. Most of the results are statistically significant, and, because of the large number of experimental subjects, the evidence that we gathered in the replications is sufficient to outweigh other experiments.",Benevolence,Helpful,Through improvements in the software's usability mechanisms; it indirectly encourages users to be helpful towards others in terms of sharing the efficient and effective use of the software; aligning with the value item 'Helpful' in the value of 'Benevolence'.,"The justification for aligning 'Paper X' with the value item Helpful and its corresponding value Benevolence from a ""Software User"" perspective is based on the idea that by improving the usability mechanisms of the software, users are more likely to share their knowledge and expertise with others, thus exhibiting helpful behavior. This aligns with the value of Benevolence as it emphasizes being helpful and considerate towards others.",Agreed-Justified,Agree,,Agree,,
Exp G,Exp K,2278,TSE,AI & Machine Learning,Using the SOCIO Chatbot for UML Modelling: A Family of Experiments,"Context: Recent developments in natural language processing have facilitated the adoption of chatbots in typically collaborative software engineering tasks (such as diagram modelling). Families of experiments can assess the performance of tools and processes and, at the same time, alleviate some of the typical shortcomings of individual experiments (e.g., inaccurate and potentially biased results due to a small number of participants). Objective: Compare the usability of a chatbot for collaborative modelling (i.e., SOCIO) and an online web tool (i.e., Creately). Method: We conducted a family of three experiments to evaluate the usability of SOCIO against the Creately online collaborative tool in academic settings. Results: The student participants were faster at building class diagrams using the chatbot than with the online collaborative tool and more satisfied with SOCIO. Besides, the class diagrams built using the chatbot tended to be more concise aEUR""albeit slightly less complete. Conclusion: Chatbots appear to be helpful for building class diagrams. In fact, our study has helped us to shed light on the future direction for experimentation in this field and lays the groundwork for researching the applicability of chatbots in diagramming.",Stimulation,Variation in Life,The paper explores the use of chatbots for diagram modelling; offering software users a variation in how they can approach and complete their tasks.,"In the context of 'Paper X', the use of chatbots for diagram modelling provides software users with a variation in their approach to completing tasks. Instead of solely relying on traditional online collaborative tools, users now have the option to interact with a chatbot for building class diagrams. This introduces a different and potentially more interactive experience, thereby stimulating software users and offering them a new way to engage with the software. Therefore, aligning with the value item Variation in Life and its corresponding value Stimulation, 'Paper X' directly addresses the need for novel and stimulating experiences in software usage.",Agreed-Justified,Agree,,Agree,,
Exp B,Exp H,2278,TSE,AI & Machine Learning,Using the SOCIO Chatbot for UML Modelling: A Family of Experiments,"Context: Recent developments in natural language processing have facilitated the adoption of chatbots in typically collaborative software engineering tasks (such as diagram modelling). Families of experiments can assess the performance of tools and processes and, at the same time, alleviate some of the typical shortcomings of individual experiments (e.g., inaccurate and potentially biased results due to a small number of participants). Objective: Compare the usability of a chatbot for collaborative modelling (i.e., SOCIO) and an online web tool (i.e., Creately). Method: We conducted a family of three experiments to evaluate the usability of SOCIO against the Creately online collaborative tool in academic settings. Results: The student participants were faster at building class diagrams using the chatbot than with the online collaborative tool and more satisfied with SOCIO. Besides, the class diagrams built using the chatbot tended to be more concise aEUR""albeit slightly less complete. Conclusion: Chatbots appear to be helpful for building class diagrams. In fact, our study has helped us to shed light on the future direction for experimentation in this field and lays the groundwork for researching the applicability of chatbots in diagramming.",Achievement,Intelligent,The paper's comparison of a chatbot and an online tool contributes to users being able to make intelligent choices about which software is most suitable for their needs.,"The main contribution of 'Paper X' aligning with the value item Intelligent and its corresponding value Achievement is that it provides users with information and insights that allow them to make informed and intelligent choices about which software is most suitable for their needs. By comparing the usability of a chatbot and an online tool, the paper helps users assess the performance of these tools and make decisions based on factors such as speed, satisfaction, and the quality of the class diagrams built. This aligns with the value of Achievement as it enables users to achieve their goals efficiently and effectively, showcasing their intelligence in selecting the most appropriate software for their specific requirements.",Agreed-Justified,Agree,,Agree,,
Exp C,Exp K,2279,TSE,AI & Machine Learning,Learning Configurations of Operating Environment of Autonomous Vehicles to Maximize their Collisions,"Autonomous vehicles must operate safely in their dynamic and continuously-changing environment. However, the operating environment of an autonomous vehicle is complicated and full of various types of uncertainties. Additionally, the operating environment has many configurations, including static and dynamic obstacles with which an autonomous vehicle must avoid collisions. Though various approaches targeting environment configuration for autonomous vehicles have shown promising results, their effectiveness in dealing with a continuous-changing environment is limited. Thus, it is essential to learn realistic environment configurations of continuously-changing environment, under which an autonomous vehicle should be tested regarding its ability to avoid collisions. Featured with agents dynamically interacting with the environment, Reinforcement Learning (RL) has shown great potential in dealing with complicated problems requiring adapting to the environment. To this end, we present an RL-based environment configuration learning approach, i.e., DeepCollision, which intelligently learns environment configurations that lead an autonomous vehicle to crash. DeepCollision employs Deep Q-Learning as the RL solution, and selects collision probability as the safety measure, to construct the reward function. We trained four DeepCollision models and conducted an experiment to compare them with two baselines, i.e., random and greedy. Results show that DeepCollision demonstrated significantly better effectiveness in generating collisions compared with the baselines. We also provide recommendations on configuring DeepCollision with the most suitable time interval based on different road structures.",Security,Healthy,The paper's main contribution is to develop a system that increases the safety of autonomous vehicles; which aligns with the value item Healthy and its corresponding value Security.,"The justification for aligning 'Paper X' with the value item Healthy and its corresponding value Security is based on the fact that the paper's main contribution revolves around developing an RL-based approach called DeepCollision that learns environment configurations to test the ability of autonomous vehicles to avoid collisions. By improving the safety of autonomous vehicles, the paper indirectly promotes the health and well-being of users and the general public, as accidents and collisions can result in physical harm or injury. Thus, the alignment with the value item Healthy and its corresponding value Security is evident.",Agreed-Justified,Agree,,Agree,,
Exp D,Exp K,2279,TSE,AI & Machine Learning,Learning Configurations of Operating Environment of Autonomous Vehicles to Maximize their Collisions,"Autonomous vehicles must operate safely in their dynamic and continuously-changing environment. However, the operating environment of an autonomous vehicle is complicated and full of various types of uncertainties. Additionally, the operating environment has many configurations, including static and dynamic obstacles with which an autonomous vehicle must avoid collisions. Though various approaches targeting environment configuration for autonomous vehicles have shown promising results, their effectiveness in dealing with a continuous-changing environment is limited. Thus, it is essential to learn realistic environment configurations of continuously-changing environment, under which an autonomous vehicle should be tested regarding its ability to avoid collisions. Featured with agents dynamically interacting with the environment, Reinforcement Learning (RL) has shown great potential in dealing with complicated problems requiring adapting to the environment. To this end, we present an RL-based environment configuration learning approach, i.e., DeepCollision, which intelligently learns environment configurations that lead an autonomous vehicle to crash. DeepCollision employs Deep Q-Learning as the RL solution, and selects collision probability as the safety measure, to construct the reward function. We trained four DeepCollision models and conducted an experiment to compare them with two baselines, i.e., random and greedy. Results show that DeepCollision demonstrated significantly better effectiveness in generating collisions compared with the baselines. We also provide recommendations on configuring DeepCollision with the most suitable time interval based on different road structures.",Security,National Security,The paper aims to improve autonomous vehicles' capability of avoiding collisions; effectively enhancing national security by reducing traffic accidents; this aligns with the value item National security and its corresponding value Security.,"My justification is based on the fact that the main contribution of 'Paper X' is the development of an RL-based approach, DeepCollision, which learns environment configurations that lead an autonomous vehicle to crash. By improving the ability of autonomous vehicles to avoid collisions, the paper indirectly contributes to national security by reducing traffic accidents and enhancing overall safety in the transportation system. This alignment corresponds to the value item National Security and its corresponding value Security from the perspective of a software user.",Agreed-Clarified,Agree,,Agree,,
Exp F,Exp J,2280,TSE,Accessibility & User Experience,Nighthawk: Fully Automated Localizing UI Display Issues via Visual Understanding,"Graphical User Interface (GUI) provides a visual bridge between a software application and end users, through which they can interact with each other. With the upgrading of mobile devices and the development of aesthetics, the visual effects of the GUI are more and more attracting, and users pay more attention to the accessibility and usability of applications. However, such GUI complexity posts a great challenge to the GUI implementation. According to our pilot study of crowdtesting bug reports, display issues such as text overlap, component occlusion, missing image always occur during GUI rendering on different devices due to the software or hardware compatibility. They negatively influence the app usability, resulting in poor user experience. To detect these issues, we propose a fully automated approach, Nighthawk, based on deep learning for modelling visual information of the GUI screenshot. Nighthawk can detect GUIs with display issues and also locate the detailed region of the issue in the given GUI for guiding developers to fix the bug. At the same time, training the model needs a large amount of labeled buggy screenshots, which requires considerable manual effort to prepare them. We therefore propose a heuristic-based training data auto-generation method to automatically generate the labeled training data. The evaluation demonstrates that our Nighthawk can achieve average 0.84 precision and 0.84 recall in detecting UI display issues, average 0.59 AP and 0.60 AR in localizing these issues. We also evaluate Nighthawk with popular Android apps on Google Play and F-Droid, and successfully uncover 151 previously-undetected UI display issues with 75 of them being confirmed or fixed so far.",Stimulation,Variation in Life,The paper is aimed at enhancing the user's interaction with software through GUI; which could contribute to 'Variation in Life' aspect of the 'Stimulation' value; as it can make their user experiences more diverse and interesting.,"The justification for aligning 'Paper X' with the value item Variation in Life and its corresponding value Stimulation from a ""Software User"" perspective is based on the fact that the paper aims to enhance the visual effects and usability of GUIs in software applications. By improving the GUIs, users are provided with a more visually appealing and engaging interface, which could lead to a more diverse and interesting user experience. This aligns with the idea of Variation in Life, as it introduces new and stimulating elements to the user's interaction with software, enhancing their overall experience.",Agreed-Clarified,Agree,,Agree,,
Exp G,Exp K,2280,TSE,Accessibility & User Experience,Nighthawk: Fully Automated Localizing UI Display Issues via Visual Understanding,"Graphical User Interface (GUI) provides a visual bridge between a software application and end users, through which they can interact with each other. With the upgrading of mobile devices and the development of aesthetics, the visual effects of the GUI are more and more attracting, and users pay more attention to the accessibility and usability of applications. However, such GUI complexity posts a great challenge to the GUI implementation. According to our pilot study of crowdtesting bug reports, display issues such as text overlap, component occlusion, missing image always occur during GUI rendering on different devices due to the software or hardware compatibility. They negatively influence the app usability, resulting in poor user experience. To detect these issues, we propose a fully automated approach, Nighthawk, based on deep learning for modelling visual information of the GUI screenshot. Nighthawk can detect GUIs with display issues and also locate the detailed region of the issue in the given GUI for guiding developers to fix the bug. At the same time, training the model needs a large amount of labeled buggy screenshots, which requires considerable manual effort to prepare them. We therefore propose a heuristic-based training data auto-generation method to automatically generate the labeled training data. The evaluation demonstrates that our Nighthawk can achieve average 0.84 precision and 0.84 recall in detecting UI display issues, average 0.59 AP and 0.60 AR in localizing these issues. We also evaluate Nighthawk with popular Android apps on Google Play and F-Droid, and successfully uncover 151 previously-undetected UI display issues with 75 of them being confirmed or fixed so far.",Stimulation,Excitement in Life,Since the paper proposes a method for detecting and addressing display issues; it can improve the 'Excitement in Life' aspect of 'Stimulation' value; by reducing discomfort and annoyance caused by GUI issues and making software usage smoother and more pleasant.,"Certainly! The justification is based on the idea that the main contribution of the paper, which is the proposed automated approach for detecting and addressing display issues in GUIs, can directly align with the value item of Excitement in Life and its corresponding value Stimulation from a ""Software User"" perspective. By addressing these display issues, the paper aims to improve the overall user experience by reducing discomfort and annoyance caused by GUI issues. This, in turn, can make software usage smoother and more pleasant, leading to an increased sense of excitement in the user's life. Therefore, the alignment between the paper's main contributions and the value item of Excitement in Life and its corresponding value of Stimulation can be evidenced by the potential positive impact on the user's software experience.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2280,TSE,Accessibility & User Experience,Nighthawk: Fully Automated Localizing UI Display Issues via Visual Understanding,"Graphical User Interface (GUI) provides a visual bridge between a software application and end users, through which they can interact with each other. With the upgrading of mobile devices and the development of aesthetics, the visual effects of the GUI are more and more attracting, and users pay more attention to the accessibility and usability of applications. However, such GUI complexity posts a great challenge to the GUI implementation. According to our pilot study of crowdtesting bug reports, display issues such as text overlap, component occlusion, missing image always occur during GUI rendering on different devices due to the software or hardware compatibility. They negatively influence the app usability, resulting in poor user experience. To detect these issues, we propose a fully automated approach, Nighthawk, based on deep learning for modelling visual information of the GUI screenshot. Nighthawk can detect GUIs with display issues and also locate the detailed region of the issue in the given GUI for guiding developers to fix the bug. At the same time, training the model needs a large amount of labeled buggy screenshots, which requires considerable manual effort to prepare them. We therefore propose a heuristic-based training data auto-generation method to automatically generate the labeled training data. The evaluation demonstrates that our Nighthawk can achieve average 0.84 precision and 0.84 recall in detecting UI display issues, average 0.59 AP and 0.60 AR in localizing these issues. We also evaluate Nighthawk with popular Android apps on Google Play and F-Droid, and successfully uncover 151 previously-undetected UI display issues with 75 of them being confirmed or fixed so far.",Security,Healthy,The main contribution of the paper is aimed at improving user experiences by resolving GUI issues. This aligns with 'Healthy'; which in this context could refer to a healthy way of interacting with the software; from the value 'Security'; as it reduces software malfunctions that could harm or disrupt user's productive activities.,"The alignment of 'Paper X' with the value item Healthy and its corresponding value Security is evident in the abstract where it is stated that GUI display issues negatively influence app usability, resulting in poor user experience. By addressing these issues, the paper aims to create a healthier software-user interaction by reducing software malfunctions that could harm or disrupt the user's productive activities. This alignment supports the value of Security by ensuring a secure and reliable software experience, thus promoting the overall healthiness of the user's interaction with the software.",Agreed-Clarified,Disagree,reconcile with coder_2,Disagree,The justification doesn't align with the value item of the healthy,
Exp B,Exp H,2283,TSE,Code Generation & Analysis,Generating Concise Patches for Newly Released Programming Assignments,"In programming courses, providing students with concise and constructive feedback on faulty submissions (programs) is highly desirable. However, providing feedback manually is often time-consuming and tedious. To release tutors from the manual construction of concise feedback, researchers have proposed approaches such as CLARA and Refactory to construct feedback automatically. The key to such approaches is to fix a faulty program by making it equivalent to one of its correct reference programs whose overall structure is identical to that of the faulty submission. However, for a newly released assignment, it is likely that there are no correct reference programs at all, let alone correct reference programs sharing identical structure with the faulty submission. Therefore, in this paper, we propose AssignmentMender generating concise patches for newly released assignments. The key insight of AssignmentMender is that a faulty submission can be repaired by reusing fine-grained code snippets from submissions (even when they are faulty) for the same assignment. It automatically locates suspicious code in the faulty program and leverages static analysis to retrieve reference code from existing submissions with a graph-based matching algorithm. Finally, it generates candidate patches by modifying the suspicious code based on the reference code. Different from existing approaches, AssignmentMender exploits faulty submissions in addition to bug-free submissions to generate patches. Another advantage of AssignmentMender is that it can leverage submissions whose overall structures are different from those of the to-be-fixed submission. Evaluation results on 128 faulty submissions from 10 assignments show that AssignmentMender improves the state-of-the-art in feedback generation for newly released assignments. A case study involving 40 students and 80 submissions further provides initial evidence showing that the proposed approach is useful in practice.",Achievement,Successful,The paper contributes the AssignmentMender approach which generates concise patches for newly released assignments; thereby helping users; in this case students; to be successful in their programming assignments. This aligns with the value item 'Successful' and its corresponding value 'Achievement'.,"The main contribution of 'Paper X' is the AssignmentMender approach, which generates concise patches for newly released assignments. These patches help students to fix their faulty program submissions, ultimately leading to successful outcomes in their programming assignments. This aligns with the value item 'Successful' and its corresponding value 'Achievement' because the paper's approach directly contributes to the achievement of successful outcomes for software users, in this case, students.",Agreed-Justified,Agree,,Agree,,
Exp C,Exp K,2283,TSE,Code Generation & Analysis,Generating Concise Patches for Newly Released Programming Assignments,"In programming courses, providing students with concise and constructive feedback on faulty submissions (programs) is highly desirable. However, providing feedback manually is often time-consuming and tedious. To release tutors from the manual construction of concise feedback, researchers have proposed approaches such as CLARA and Refactory to construct feedback automatically. The key to such approaches is to fix a faulty program by making it equivalent to one of its correct reference programs whose overall structure is identical to that of the faulty submission. However, for a newly released assignment, it is likely that there are no correct reference programs at all, let alone correct reference programs sharing identical structure with the faulty submission. Therefore, in this paper, we propose AssignmentMender generating concise patches for newly released assignments. The key insight of AssignmentMender is that a faulty submission can be repaired by reusing fine-grained code snippets from submissions (even when they are faulty) for the same assignment. It automatically locates suspicious code in the faulty program and leverages static analysis to retrieve reference code from existing submissions with a graph-based matching algorithm. Finally, it generates candidate patches by modifying the suspicious code based on the reference code. Different from existing approaches, AssignmentMender exploits faulty submissions in addition to bug-free submissions to generate patches. Another advantage of AssignmentMender is that it can leverage submissions whose overall structures are different from those of the to-be-fixed submission. Evaluation results on 128 faulty submissions from 10 assignments show that AssignmentMender improves the state-of-the-art in feedback generation for newly released assignments. A case study involving 40 students and 80 submissions further provides initial evidence showing that the proposed approach is useful in practice.",Stimulation,Variation in Life,By automating the feedback generation process; the paper ensures Variation in Life; a value item under Stimulation; as it eases the monotonous and tedious task of manually generating feedback; helping students to identify and fix their errors more efficiently.,"In the context of a software user, the value item ""Variation in Life"" and its corresponding value ""Stimulation"" can be aligned with the main contribution of Paper X, which is automating the feedback generation process. By automating this process, the paper reduces the monotony and tedium of manually generating feedback, providing a more stimulating and varied experience for the software users who are receiving feedback on their faulty submissions. This automation allows users to quickly identify and fix errors in their programs, promoting a sense of excitement and engagement in the learning process. Overall, the automation of feedback generation in Paper X aligns with the value item of ""Variation in Life"" and the corresponding value of ""Stimulation"" by bringing efficiency and stimulation to the software user's experience.",Agreed-Justified,Agree,,Agree,,
Exp D,Exp K,2283,TSE,Code Generation & Analysis,Generating Concise Patches for Newly Released Programming Assignments,"In programming courses, providing students with concise and constructive feedback on faulty submissions (programs) is highly desirable. However, providing feedback manually is often time-consuming and tedious. To release tutors from the manual construction of concise feedback, researchers have proposed approaches such as CLARA and Refactory to construct feedback automatically. The key to such approaches is to fix a faulty program by making it equivalent to one of its correct reference programs whose overall structure is identical to that of the faulty submission. However, for a newly released assignment, it is likely that there are no correct reference programs at all, let alone correct reference programs sharing identical structure with the faulty submission. Therefore, in this paper, we propose AssignmentMender generating concise patches for newly released assignments. The key insight of AssignmentMender is that a faulty submission can be repaired by reusing fine-grained code snippets from submissions (even when they are faulty) for the same assignment. It automatically locates suspicious code in the faulty program and leverages static analysis to retrieve reference code from existing submissions with a graph-based matching algorithm. Finally, it generates candidate patches by modifying the suspicious code based on the reference code. Different from existing approaches, AssignmentMender exploits faulty submissions in addition to bug-free submissions to generate patches. Another advantage of AssignmentMender is that it can leverage submissions whose overall structures are different from those of the to-be-fixed submission. Evaluation results on 128 faulty submissions from 10 assignments show that AssignmentMender improves the state-of-the-art in feedback generation for newly released assignments. A case study involving 40 students and 80 submissions further provides initial evidence showing that the proposed approach is useful in practice.",Self Direction,Privacy,The automated comparison in the AssignmentMender approach is likely to ensure better Privacy for the users' submissions which aligns with the value item 'Privacy' and the corresponding value 'Self Direction'.,"In the context of 'Paper X', the alignment with the value item Privacy and its corresponding value Self Direction is justified because the AssignmentMender approach aims to automatically fix faulty programming submissions by reusing code snippets from other submissions. This automated comparison process ensures better privacy for users' submissions as it avoids manual inspection of their code, allowing them to maintain their independence and control over their work.",Agreed-Justified,Agree,,Agree,,
Exp E,Exp J,2287,TSE,Software Deployment & Operations,"Web APIs: Features Issues and Expectations aEUR"" A Large-Scale Empirical Study of Web APIs From Two Publicly Accessible Registries Using Stack Overflow and a User Survey","With the increasing adoption of services-oriented computing and cloud computing technologies, web APIs have become the fundamental building blocks for constructing software applications. Web APIs are developed and published on the internet. The functionality of web APIs can be used to facilitate the development of software applications. There are numerous studies on retrieving and recommending candidate web APIs based on user requirements from a large set of web APIs. However, there are very limited studies on the features of web APIs that make them more likely to be used and the issues of using web APIs in practice. Moreover, usersaEUR(tm) expectations on the development and management of web APIs are rarely investigated. In this paper, we conduct a large-scale empirical study of 20,047 web APIs published at two popular and publicly accessible web API registries: ProgrammableWeb and APIs.guru. We first extract the questions posted in Stack Overflow (SO) that are relevant to the web APIs. We then manually analyze 1,885 randomly sampled SO questions and identify 24 web API issue types (e.g., authorization error) that are encountered by users. Afterwards, we conduct a user survey to investigate the features of web APIs that users often consider when shortlisting a web API for testing before they adopt it, validate the identified types of web API issues, and understand usersaEUR(tm) expectations on the development and management of web APIs. From the 191 received responses, we extract 14 important features for users to decide whether to use a web API (e.g., well-organized documentation). We also gain a better understanding of web API issue types and summarize 11 categories of user expectations on web APIs (e.g., documentation and SDK/library). As the result of our study, we provide guidelines for web API developers and registry managers to improve web APIs and promote the use of web APIs.",Achievement,Intelligent,The paper does a large-scale study on web APIs; extracting relevant features and understanding issues; which can improve the intelligence of a software user in selecting the right API.,"The paper's large-scale empirical study on web APIs, specifically extracting relevant features and understanding issues, directly aligns with the value item Intelligent and its corresponding value Achievement. By conducting this study, the paper helps software users make more informed and intelligent decisions when selecting web APIs, enhancing their capability to achieve their desired outcomes in software development.",Agreed-Clarified,Agree,,Agree,,
Exp F,Exp J,2289,TSE,Software Testing & QA,Using Metamorphic Testing to Improve the Quality of Tags in OpenStreetMap,"We present a metamorphic testing approach to validate the information included in OpenStreetMap, a collaborative effort to produce a free map of the world. We focus on the quality of the tags storing the information about the elements of the map. We identified metamorphic relations with the potential to detect different types of tagging errors. In particular, we carefully designed mechanisms to automatically generate follow-up inputs, a fundamental component in the successful application of a metamorphic testing approach. The intrinsic nature of automatically analysing tags implies that we will detect real errors but some false positives as well. In order to obtain a good trade-off between real errors and false positives, we introduce thresholds. Our MRs will raise an error associated with a certain value if, depending on the nature of the MR, we have a certain number of elements (not) fulfilling a given condition. In order to evaluate the goodness and versatility of our framework, we chose four cities in different continents with the goal of analysing very heterogeneous contributors adding information in different languages. The application of this framework to the analysis of the chosen cities revealed errors in all of them and in all the considered categories. In addition, around 66% of the errors found by our MRs in the analysed areas have not been previously reported by Osmose, the de facto standard OSM error checker.",Achievement,Intelligent,The paper's main contribution is an improved testing approach to validate information in OpenStreetMap. This aligns with the value item 'Intelligent' within the 'Achievement' value; as more intelligent verification methods are being used for the validation of map information.,"In the paper, the authors introduce a metamorphic testing approach to validate the accuracy of information in OpenStreetMap. This approach involves designing mechanisms to automatically generate follow-up inputs and identify tagging errors. By utilizing more intelligent verification methods, the authors aim to detect both real errors and false positives while maintaining a good trade-off. This aligns with the value item 'Intelligent' within the 'Achievement' value as it demonstrates the use of advanced techniques and strategies to improve the validation process and achieve more accurate results.",Agreed-Justified,Agree,,Agree,,
Exp G,Exp K,2289,TSE,Software Testing & QA,Using Metamorphic Testing to Improve the Quality of Tags in OpenStreetMap,"We present a metamorphic testing approach to validate the information included in OpenStreetMap, a collaborative effort to produce a free map of the world. We focus on the quality of the tags storing the information about the elements of the map. We identified metamorphic relations with the potential to detect different types of tagging errors. In particular, we carefully designed mechanisms to automatically generate follow-up inputs, a fundamental component in the successful application of a metamorphic testing approach. The intrinsic nature of automatically analysing tags implies that we will detect real errors but some false positives as well. In order to obtain a good trade-off between real errors and false positives, we introduce thresholds. Our MRs will raise an error associated with a certain value if, depending on the nature of the MR, we have a certain number of elements (not) fulfilling a given condition. In order to evaluate the goodness and versatility of our framework, we chose four cities in different continents with the goal of analysing very heterogeneous contributors adding information in different languages. The application of this framework to the analysis of the chosen cities revealed errors in all of them and in all the considered categories. In addition, around 66% of the errors found by our MRs in the analysed areas have not been previously reported by Osmose, the de facto standard OSM error checker.",Security,National Security,The automatic detection of errors in the OpenStreetMap system contributes to the value item 'National Security' under the value of 'Security'; as accurate mapping information is crucial for national infrastructure and planning.,"The accurate mapping information provided by the automatic detection of errors in OpenStreetMap contributes to the value item 'National Security' and its corresponding value 'Security' because ensuring the integrity and reliability of mapping data is essential for national infrastructure and planning, which are crucial for the safety and security of a country.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2289,TSE,Software Testing & QA,Using Metamorphic Testing to Improve the Quality of Tags in OpenStreetMap,"We present a metamorphic testing approach to validate the information included in OpenStreetMap, a collaborative effort to produce a free map of the world. We focus on the quality of the tags storing the information about the elements of the map. We identified metamorphic relations with the potential to detect different types of tagging errors. In particular, we carefully designed mechanisms to automatically generate follow-up inputs, a fundamental component in the successful application of a metamorphic testing approach. The intrinsic nature of automatically analysing tags implies that we will detect real errors but some false positives as well. In order to obtain a good trade-off between real errors and false positives, we introduce thresholds. Our MRs will raise an error associated with a certain value if, depending on the nature of the MR, we have a certain number of elements (not) fulfilling a given condition. In order to evaluate the goodness and versatility of our framework, we chose four cities in different continents with the goal of analysing very heterogeneous contributors adding information in different languages. The application of this framework to the analysis of the chosen cities revealed errors in all of them and in all the considered categories. In addition, around 66% of the errors found by our MRs in the analysed areas have not been previously reported by Osmose, the de facto standard OSM error checker.",Universalism,Protecting the Environment,By working to eliminate errors in OpenStreetMap; the paper contributes to the protection of the physical environment which indirectly aligns with the value item 'Protecting the Environment' from the 'Universalism' value. Accurate mapping information is critical to environmental planning and sustainable development.,"In 'Paper X', the contribution of eliminating errors in OpenStreetMap directly aligns with the value item 'Protecting the Environment' and its corresponding value 'Universalism' from a ""Software User"" perspective. Accurate mapping information is essential for environmental planning and sustainable development efforts, and by ensuring the quality of the map's tags and information, the paper indirectly contributes to protecting the physical environment. This alignment demonstrates a commitment to universal values, such as the preservation of nature and the protection of the environment, which are fundamental for a software user's perspective and their larger impact on society.",Agreed-Justified,Agree,,Agree,,
Exp B,Exp H,2290,TSE,AI & Machine Learning,SeqTrans: Automatic Vulnerability Fix Via Sequence to Sequence Learning,"Software vulnerabilities are now reported unprecedentedly due to the recent development of automated vulnerability hunting tools. However, fixing vulnerabilities still mainly depends on programmersaEUR(tm) manual efforts. Developers need to deeply understand the vulnerability and affect the systemaEUR(tm)s functions as little as possible. In this paper, with the advancement of Neural Machine Translation (NMT) techniques, we provide a novel approach called SeqTrans to exploit historical vulnerability fixes to provide suggestions and automatically fix the source code. To capture the contextual information around the vulnerable code, we propose to leverage data-flow dependencies to construct code sequences and feed them into the state-of-the-art transformer model. The fine-tuning strategy has been introduced to overcome the small sample size problem. We evaluate SeqTrans on a dataset containing 1,282 commits that fix 624 CVEs in 205 Java projects. Results show that the accuracy of SeqTrans outperforms the latest techniques and achieves 23.3% in statement-level fix and 25.3% in CVE-level fix. In the meantime, we look deep inside the result and observe that the NMT model performs very well in certain kinds of vulnerabilities like CWE-287 (Improper Authentication) and CWE-863 (Incorrect Authorization).",Achievement,Capable,The paper introduces SeqTrans; a novel approach that outperforms other techniques in fixing software vulnerabilities. This helps improve the user's capability to deal with vulnerabilities.,"The justification for aligning 'Paper X' with the value item Capable and its corresponding value Achievement from a ""Software User"" perspective is that the paper introduces a novel approach called SeqTrans which outperforms other techniques in fixing software vulnerabilities. By providing suggestions and automatically fixing source code based on historical vulnerability fixes, SeqTrans enhances the user's capability to address vulnerabilities effectively and showcases their achievement in ensuring the security and stability of the software they are using.",Agreed-Justified,Agree,,Disagree,The justification doesn't align with the value item of the Capable,
Exp C,Exp K,2290,TSE,AI & Machine Learning,SeqTrans: Automatic Vulnerability Fix Via Sequence to Sequence Learning,"Software vulnerabilities are now reported unprecedentedly due to the recent development of automated vulnerability hunting tools. However, fixing vulnerabilities still mainly depends on programmersaEUR(tm) manual efforts. Developers need to deeply understand the vulnerability and affect the systemaEUR(tm)s functions as little as possible. In this paper, with the advancement of Neural Machine Translation (NMT) techniques, we provide a novel approach called SeqTrans to exploit historical vulnerability fixes to provide suggestions and automatically fix the source code. To capture the contextual information around the vulnerable code, we propose to leverage data-flow dependencies to construct code sequences and feed them into the state-of-the-art transformer model. The fine-tuning strategy has been introduced to overcome the small sample size problem. We evaluate SeqTrans on a dataset containing 1,282 commits that fix 624 CVEs in 205 Java projects. Results show that the accuracy of SeqTrans outperforms the latest techniques and achieves 23.3% in statement-level fix and 25.3% in CVE-level fix. In the meantime, we look deep inside the result and observe that the NMT model performs very well in certain kinds of vulnerabilities like CWE-287 (Improper Authentication) and CWE-863 (Incorrect Authorization).",Security,Healthy,The paper presents an approach that can automatically fix the source code; potentially leading to a healthier software environment for the users.,"The justification for aligning 'Paper X' with the value item Healthy and its corresponding value Security from a ""Software User"" perspective is that the paper proposes an automated approach to fix vulnerabilities in software code. By automatically fixing vulnerabilities, the software environment becomes more secure, thereby promoting the health of the software system. This alignment is based on the assumption that secure software contributes to a healthier user experience by reducing the risk of security breaches and potential harm to the user's data or system.",Agreed-Clarified,Agree,"if a software works properly, users are not worried and feel relax when using it",Disagree,Software health is not necessarily Human health,
Exp D,Exp K,2290,TSE,AI & Machine Learning,SeqTrans: Automatic Vulnerability Fix Via Sequence to Sequence Learning,"Software vulnerabilities are now reported unprecedentedly due to the recent development of automated vulnerability hunting tools. However, fixing vulnerabilities still mainly depends on programmersaEUR(tm) manual efforts. Developers need to deeply understand the vulnerability and affect the systemaEUR(tm)s functions as little as possible. In this paper, with the advancement of Neural Machine Translation (NMT) techniques, we provide a novel approach called SeqTrans to exploit historical vulnerability fixes to provide suggestions and automatically fix the source code. To capture the contextual information around the vulnerable code, we propose to leverage data-flow dependencies to construct code sequences and feed them into the state-of-the-art transformer model. The fine-tuning strategy has been introduced to overcome the small sample size problem. We evaluate SeqTrans on a dataset containing 1,282 commits that fix 624 CVEs in 205 Java projects. Results show that the accuracy of SeqTrans outperforms the latest techniques and achieves 23.3% in statement-level fix and 25.3% in CVE-level fix. In the meantime, we look deep inside the result and observe that the NMT model performs very well in certain kinds of vulnerabilities like CWE-287 (Improper Authentication) and CWE-863 (Incorrect Authorization).",Security,National Security,The paper improves the security of the software by flagging and addressing vulnerabilities; potentially enhancing the national security by mitigating cyber threats.,"In the paper, 'Paper X', the authors propose a novel approach called SeqTrans to automatically fix software vulnerabilities. By addressing vulnerabilities and improving the security of the software, the paper indirectly contributes to national security by mitigating potential cyber threats. As software vulnerabilities can be exploited by malicious actors, resolving these vulnerabilities can help protect sensitive information and prevent potential breaches that could impact national security. Thus, the alignment with the value item National Security and its corresponding value Security is evidenced by the paper's focus on enhancing software security.",Agreed-Clarified,Agree,,Agree,,
Exp E,Exp J,2302,TSE,AI & Machine Learning,Increasing the Confidence of Deep Neural Networks by Coverage Analysis,"The great performance of machine learning algorithms and deep neural networks in several perception and control tasks is pushing the industry to adopt such technologies in safety-critical applications, as autonomous robots and self-driving vehicles. At present, however, several issues need to be solved to make deep learning methods more trustworthy, predictable, safe, and secure against adversarial attacks. Although several methods have been proposed to improve the trustworthiness of deep neural networks, most of them are tailored for specific classes of adversarial examples, hence failing to detect other corner cases or unsafe inputs that heavily deviate from the training samples. This paper presents a lightweight monitoring architecture based on coverage paradigms to enhance the model robustness against different unsafe inputs. In particular, four coverage analysis methods are proposed and tested in the architecture for evaluating multiple detection logic. Experimental results show that the proposed approach is effective in detecting both powerful adversarial examples and out-of-distribution inputs, introducing limited extra-execution time and memory requirements.",Security,Healthy,The paper contributes an approach for enhancing the safety and security of machine learning applications by ensuring the algorithms more trustworthy and predictable. Therefore; it aligns with the value item Healthy under the value Security; as it ensures users can safely use these applications.,"The paper aligns with the value item Healthy and its corresponding value Security because it introduces a lightweight monitoring architecture that enhances the model robustness against unsafe inputs, such as adversarial examples and out-of-distribution inputs. By addressing these issues, the paper aims to make deep learning methods more trustworthy, predictable, and safe in safety-critical applications. This alignment with the value item Healthy reflects a focus on ensuring the well-being and security of software users when utilizing machine learning algorithms in various contexts.",Agreed-Justified,Agree,,Agree,,
Exp F,Exp J,2302,TSE,AI & Machine Learning,Increasing the Confidence of Deep Neural Networks by Coverage Analysis,"The great performance of machine learning algorithms and deep neural networks in several perception and control tasks is pushing the industry to adopt such technologies in safety-critical applications, as autonomous robots and self-driving vehicles. At present, however, several issues need to be solved to make deep learning methods more trustworthy, predictable, safe, and secure against adversarial attacks. Although several methods have been proposed to improve the trustworthiness of deep neural networks, most of them are tailored for specific classes of adversarial examples, hence failing to detect other corner cases or unsafe inputs that heavily deviate from the training samples. This paper presents a lightweight monitoring architecture based on coverage paradigms to enhance the model robustness against different unsafe inputs. In particular, four coverage analysis methods are proposed and tested in the architecture for evaluating multiple detection logic. Experimental results show that the proposed approach is effective in detecting both powerful adversarial examples and out-of-distribution inputs, introducing limited extra-execution time and memory requirements.",Security,Social Order,The paper presents a lightweight monitoring architecture for enhancing the model robustness against different unsafe inputs; thereby promoting Social Order under the value Security as it directly contributes to ensuring the software functions properly within the expected order of operations.,"The paper's lightweight monitoring architecture, aimed at enhancing the model robustness against unsafe inputs, aligns with the value item Social Order and its corresponding value Security from a ""Software User"" perspective. By promoting the concept of Social Order, the paper ensures that the software functions properly within the expected order of operations, providing a secure and predictable environment for users. This directly contributes to the value of Security by mitigating potential risks and maintaining the integrity of the software system.",Agreed-Justified,Agree,,Agree,,
Exp G,Exp K,2302,TSE,AI & Machine Learning,Increasing the Confidence of Deep Neural Networks by Coverage Analysis,"The great performance of machine learning algorithms and deep neural networks in several perception and control tasks is pushing the industry to adopt such technologies in safety-critical applications, as autonomous robots and self-driving vehicles. At present, however, several issues need to be solved to make deep learning methods more trustworthy, predictable, safe, and secure against adversarial attacks. Although several methods have been proposed to improve the trustworthiness of deep neural networks, most of them are tailored for specific classes of adversarial examples, hence failing to detect other corner cases or unsafe inputs that heavily deviate from the training samples. This paper presents a lightweight monitoring architecture based on coverage paradigms to enhance the model robustness against different unsafe inputs. In particular, four coverage analysis methods are proposed and tested in the architecture for evaluating multiple detection logic. Experimental results show that the proposed approach is effective in detecting both powerful adversarial examples and out-of-distribution inputs, introducing limited extra-execution time and memory requirements.",Achievement,Successful,The paper contributes methods for making machine learning algorithms safer; more secure; and trustworthy. By offering an approach that enhances the algorithm's robustness and predictability; users of such software can be more Successful in their tasks ‚Äö√Ñ√∂‚àö√ë‚àö¬® aligning with the value item Successful under the value Achievement.,"The justification for aligning 'Paper X' with the value item Successful and its corresponding value Achievement is based on the fact that the paper presents methods that enhance the robustness and predictability of machine learning algorithms used in safety-critical applications. By making the algorithms more trustworthy and secure against adversarial attacks, users of the software can achieve their intended tasks successfully, leading to the alignment with the value item Successful under the value Achievement.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2311,TSE,Code Generation & Analysis,Program Synthesis for Cyber-Resilience,"Architectural tactics enable stakeholders to achieve cyber-resilience requirements. They permit systems to react, resist, detect, and recover from cyber incidents. This paper presents an approach to generate source code for architectural tactics typically used in safety and mission-critical systems. Our approach extensively relies on the use of the Event-B formal method and the EventB2Java code generation plugin of the Rodin platform. It leverages the modeling of architectural tactics in the Event-B formal language and uses a set of EventB2Java transformation rules to generate certified code implementations for the said tactics. Since resilience requirements are statements about a system over time, and because of the fact that the Event-B language does not provide (native) support for the writing of temporal specifications, we have implemented a novel Linear Temporal Logic (LTL) extension for Event-B. We support several architectural tactics for availability, performance, and security. The generated code is certified in the following sense: discharging proof obligations in Rodin - the platform we use for writing the Event-B models - attests to the soundness of the architectural tactics modelled in Event-B, and the soundness of the translation encoded by the EventB2Java tool attests to the code correctness. Finally, we demonstrate the usability of our resilience validation approach with the aid of an Autonomous Vehicle System. It further helped us increase our confidence in the soundness of our Event-B LTL extension.",Security,Healthy,The paper provides a system that enables cyber-resilience; which is essential in maintaining the health of the system being used by software users.,"The paper's approach to generating source code for architectural tactics in safety and mission-critical systems aligns with the value item Healthy and its corresponding value Security from a ""Software User"" perspective. By enabling systems to react, resist, detect, and recover from cyber incidents, the paper contributes to maintaining the health and security of the system being used by software users. This ensures that the software users can trust the system to function reliably and protect their data and interactions.",Agreed-Justified,Disagree,reconcile with coder_2,Disagree,"The justification is not supported by the evidence in the abstract, which focuses on 'Architectural tactics enable stakeholders to achieve cyber-resilience requirements.
",
Exp B,Exp H,2311,TSE,Code Generation & Analysis,Program Synthesis for Cyber-Resilience,"Architectural tactics enable stakeholders to achieve cyber-resilience requirements. They permit systems to react, resist, detect, and recover from cyber incidents. This paper presents an approach to generate source code for architectural tactics typically used in safety and mission-critical systems. Our approach extensively relies on the use of the Event-B formal method and the EventB2Java code generation plugin of the Rodin platform. It leverages the modeling of architectural tactics in the Event-B formal language and uses a set of EventB2Java transformation rules to generate certified code implementations for the said tactics. Since resilience requirements are statements about a system over time, and because of the fact that the Event-B language does not provide (native) support for the writing of temporal specifications, we have implemented a novel Linear Temporal Logic (LTL) extension for Event-B. We support several architectural tactics for availability, performance, and security. The generated code is certified in the following sense: discharging proof obligations in Rodin - the platform we use for writing the Event-B models - attests to the soundness of the architectural tactics modelled in Event-B, and the soundness of the translation encoded by the EventB2Java tool attests to the code correctness. Finally, we demonstrate the usability of our resilience validation approach with the aid of an Autonomous Vehicle System. It further helped us increase our confidence in the soundness of our Event-B LTL extension.",Achievement,Successful,The main focus of the paper is on the achievement of cyber-resilience in systems that are critical for the success of software user's tasks.,"I apologize for any confusion. My justification is based on the fact that the paper specifically mentions achieving cyber-resilience requirements in safety and mission-critical systems. This aligns with the value item ""Successful"" because ensuring the resilience of these systems would contribute to their successful operation, which is important for the software user in terms of achieving their tasks effectively and reliably.",Agreed-Clarified,Agree,,Agree,,
Exp D,Exp K,2313,TSE,Software Testing & QA,Finding Critical Scenarios for Automated Driving Systems: A Systematic Mapping Study,"Scenario-based approaches have been receiving a huge amount of attention in research and engineering of automated driving systems. Due to the complexity and uncertainty of the driving environment, and the complexity of the driving task itself, the number of possible driving scenarios that an Automated Driving System or Advanced Driving-Assistance System may encounter is virtually infinite. Therefore it is essential to be able to reason about the identification of scenarios and in particular critical ones that may impose unacceptable risk if not considered. Critical scenarios are particularly important to support design, verification and validation efforts, and as a basis for a safety case. In this paper, we present the results of a systematic mapping study in the context of autonomous driving. The main contributions are: (i) introducing a comprehensive taxonomy for critical scenario identification methods; (ii) giving an overview of the state-of-the-art research based on the taxonomy encompassing 86 papers between 2017 and 2020; and (iii) identifying open issues and directions for further research. The provided taxonomy comprises three main perspectives encompassing the problem definition (the why), the solution (the methods to derive scenarios), and the assessment of the established scenarios. In addition, we discuss open research issues considering the perspectives of coverage, practicability, and scenario space explosion.",Achievement,Capable,The presented taxonomy and research overview can contribute to making users of automated driving systems feel more capable and competent; thereby aligning with the value item Capable (v4.2) and its corresponding value Achievement (v4).,"The alignment of 'Paper X' with the value item Capable and its corresponding value Achievement is justified based on the main contributions stated in the paper abstract. The taxonomy introduced in the paper aims to identify critical scenarios in autonomous driving, which directly contributes to the design, verification, and validation efforts of automated driving systems. By providing a systematic mapping study and overview of the state-of-the-art research, the paper empowers software users by equipping them with the knowledge and tools to understand and assess critical scenarios. This understanding enhances their sense of capability and competence in using automated driving systems, aligning with the value item Capable and its corresponding value Achievement.",Agreed-Justified,Agree,,Agree,,
Exp E,Exp J,2313,TSE,Software Testing & QA,Finding Critical Scenarios for Automated Driving Systems: A Systematic Mapping Study,"Scenario-based approaches have been receiving a huge amount of attention in research and engineering of automated driving systems. Due to the complexity and uncertainty of the driving environment, and the complexity of the driving task itself, the number of possible driving scenarios that an Automated Driving System or Advanced Driving-Assistance System may encounter is virtually infinite. Therefore it is essential to be able to reason about the identification of scenarios and in particular critical ones that may impose unacceptable risk if not considered. Critical scenarios are particularly important to support design, verification and validation efforts, and as a basis for a safety case. In this paper, we present the results of a systematic mapping study in the context of autonomous driving. The main contributions are: (i) introducing a comprehensive taxonomy for critical scenario identification methods; (ii) giving an overview of the state-of-the-art research based on the taxonomy encompassing 86 papers between 2017 and 2020; and (iii) identifying open issues and directions for further research. The provided taxonomy comprises three main perspectives encompassing the problem definition (the why), the solution (the methods to derive scenarios), and the assessment of the established scenarios. In addition, we discuss open research issues considering the perspectives of coverage, practicability, and scenario space explosion.",Security,Healthy,By investigating critical scenarios for automated driving systems and providing a taxonomy for critical scenario identification; the paper contributes to the users' physical well-being and health; thus aligning with the value item Healthy (v6.1) and its corresponding value Security (v6).,"The justification for aligning 'Paper X' with the value item Healthy and its corresponding value Security is based on the fact that the paper focuses on investigating critical scenarios in automated driving systems. By identifying these scenarios and providing a taxonomy for their identification, the paper contributes to ensuring the safety and physical well-being of the software users who interact with the automated driving systems. This alignment is evident as the paper explicitly states that critical scenarios are important for supporting design, verification, and validation efforts, and as a basis for a safety case. The identification and understanding of these critical scenarios directly contribute to the users' sense of security and their physical health by enabling the development of safer and more reliable automated driving systems.",Agreed-Justified,Agree,,Agree,,
Exp F,Exp J,2313,TSE,Software Testing & QA,Finding Critical Scenarios for Automated Driving Systems: A Systematic Mapping Study,"Scenario-based approaches have been receiving a huge amount of attention in research and engineering of automated driving systems. Due to the complexity and uncertainty of the driving environment, and the complexity of the driving task itself, the number of possible driving scenarios that an Automated Driving System or Advanced Driving-Assistance System may encounter is virtually infinite. Therefore it is essential to be able to reason about the identification of scenarios and in particular critical ones that may impose unacceptable risk if not considered. Critical scenarios are particularly important to support design, verification and validation efforts, and as a basis for a safety case. In this paper, we present the results of a systematic mapping study in the context of autonomous driving. The main contributions are: (i) introducing a comprehensive taxonomy for critical scenario identification methods; (ii) giving an overview of the state-of-the-art research based on the taxonomy encompassing 86 papers between 2017 and 2020; and (iii) identifying open issues and directions for further research. The provided taxonomy comprises three main perspectives encompassing the problem definition (the why), the solution (the methods to derive scenarios), and the assessment of the established scenarios. In addition, we discuss open research issues considering the perspectives of coverage, practicability, and scenario space explosion.",Security,National Security,The systematic mapping study and the introduction of a comprehensive taxonomy for critical scenario identification can contribute to ensuring the safety and security of the users at a national level (given the potential widespread use of autonomous vehicles); therefore aligning with the value item National Security (v6.7) and its corresponding value Security (v6).,"The introduction of a comprehensive taxonomy for critical scenario identification and the systematic mapping study in 'Paper X' can directly align with the value item National Security and its corresponding value Security. By providing a clear framework for identifying critical scenarios and assessing risks in automated driving systems, the paper can contribute to enhancing the security and safety of users, which is essential at a national level, especially considering the potential widespread use of autonomous vehicles.",Agreed-Justified,Agree,,Agree,,
Exp G,Exp K,2315,TSE,Security & Privacy,Data Preparation for Software Vulnerability Prediction: A Systematic Literature Review,"Software Vulnerability Prediction (SVP) is a data-driven technique for software quality assurance that has recently gained considerable attention in the Software Engineering research community. However, the difficulties of preparing Software Vulnerability (SV) related data is considered as the main barrier to industrial adoption of SVP approaches. Given the increasing, but dispersed, literature on this topic, it is needed and timely to systematically select, review, and synthesize the relevant peer-reviewed papers reporting the existing SV data preparation techniques and challenges. We have carried out a Systematic Literature Review (SLR) of SVP research in order to develop a systematized body of knowledge of the data preparation challenges, solutions, and the needed research. Our review of the 61 relevant papers has enabled us to develop a taxonomy of data preparation for SVP related challenges. We have analyzed the identified challenges and available solutions using the proposed taxonomy. Our analysis of the state of the art has enabled us identify the opportunities for future research. This review also provides a set of recommendations for researchers and practitioners of SVP approaches.",Power,Social Recognition,The paper aims to gain social recognition by developing a taxonomy of data preparation for SVP related challenges; offering solutions to researchers and practitioners of SVP approaches. This aligns with the value item Social Recognition and its corresponding value Power.,"In the abstract of 'Paper X', it is stated that the paper aims to develop a taxonomy of data preparation for software vulnerability prediction (SVP) related challenges and provide solutions to researchers and practitioners. This can be seen as an achievement in the software engineering research community, as it addresses a significant challenge in the field. By offering solutions and contributing to the body of knowledge in SVP, the paper aligns with the value item Social Recognition and its corresponding value Power. Social recognition can be achieved by being influential and successful in one's field, and the paper's contributions demonstrate the authors' influence and success in addressing a relevant issue in software quality assurance.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2315,TSE,Security & Privacy,Data Preparation for Software Vulnerability Prediction: A Systematic Literature Review,"Software Vulnerability Prediction (SVP) is a data-driven technique for software quality assurance that has recently gained considerable attention in the Software Engineering research community. However, the difficulties of preparing Software Vulnerability (SV) related data is considered as the main barrier to industrial adoption of SVP approaches. Given the increasing, but dispersed, literature on this topic, it is needed and timely to systematically select, review, and synthesize the relevant peer-reviewed papers reporting the existing SV data preparation techniques and challenges. We have carried out a Systematic Literature Review (SLR) of SVP research in order to develop a systematized body of knowledge of the data preparation challenges, solutions, and the needed research. Our review of the 61 relevant papers has enabled us to develop a taxonomy of data preparation for SVP related challenges. We have analyzed the identified challenges and available solutions using the proposed taxonomy. Our analysis of the state of the art has enabled us identify the opportunities for future research. This review also provides a set of recommendations for researchers and practitioners of SVP approaches.",Achievement,Intelligent,The paper's main contribution is the intelligent analysis of the various challenges and available solutions regarding software vulnerability prediction; which corresponds to the value item Intelligent and the value Achievement.,"The paper's main contribution is aligned with the value item Intelligent and its corresponding value Achievement because it involves a systematic analysis and synthesis of existing literature on software vulnerability prediction, demonstrating a high level of expertise and knowledge in the field. By identifying challenges and available solutions in data preparation for software vulnerability prediction, the paper showcases an intelligent understanding of the domain, aiming to achieve a deeper understanding and improvement in the field. This aligns with the value of Achievement as the paper strives to make advancements and contribute to the body of knowledge in software vulnerability prediction through sophisticated analysis and insightful findings.",Agreed-Justified,Agree,,Agree,,
Exp B,Exp H,2315,TSE,Security & Privacy,Data Preparation for Software Vulnerability Prediction: A Systematic Literature Review,"Software Vulnerability Prediction (SVP) is a data-driven technique for software quality assurance that has recently gained considerable attention in the Software Engineering research community. However, the difficulties of preparing Software Vulnerability (SV) related data is considered as the main barrier to industrial adoption of SVP approaches. Given the increasing, but dispersed, literature on this topic, it is needed and timely to systematically select, review, and synthesize the relevant peer-reviewed papers reporting the existing SV data preparation techniques and challenges. We have carried out a Systematic Literature Review (SLR) of SVP research in order to develop a systematized body of knowledge of the data preparation challenges, solutions, and the needed research. Our review of the 61 relevant papers has enabled us to develop a taxonomy of data preparation for SVP related challenges. We have analyzed the identified challenges and available solutions using the proposed taxonomy. Our analysis of the state of the art has enabled us identify the opportunities for future research. This review also provides a set of recommendations for researchers and practitioners of SVP approaches.",Security,National Security,The paper's focus on Software Vulnerability Prediction aligns with the concept of National Security by enhancing the safety and security of tech infrastructure and systems; which corresponds to the Value Item National Security and its corresponding value Security.,"In the context of a ""Software User,"" the main contributions of 'Paper X' align with the value item National Security and its corresponding value Security. This is because the paper's focus on Software Vulnerability Prediction directly contributes to enhancing the safety and security of tech infrastructure and systems, which are crucial aspects of national security. By effectively identifying and addressing software vulnerabilities, the paper contributes to the protection of sensitive information, the prevention of unauthorized access or attacks, and overall safeguarding of critical systems and data. Therefore, it can be concluded that the alignment with National Security and Security values is evident through the focus on software vulnerability and its implications for the secure functioning of software systems.",Agreed-Justified,Disagree,"Related to Achievement: capable more, as it automatically identifies vulnerabilities",Disagree,The justification doesn't align with the value item of the national security,
Exp C,Exp K,2324,TSE,Software Testing & QA,Enhancing Mobile App Bug Reporting via Real-Time Understanding of Reproduction Steps,"One of the primary mechanisms by which developers receive feedback about in-field failures of software from users is through bug reports. Unfortunately, the quality of manually written bug reports can vary widely due to the effort required to include essential pieces of information, such as detailed reproduction steps (S2Rs). Despite the difficulty faced by reporters, few existing bug reporting systems attempt to offer automated assistance to users in crafting easily readable, and conveniently reproducible bug reports. To address the need for proactive bug reporting systems that actively aid the user in capturing crucial information, we introduce a novel bug reporting approach called EBug. EBug assists reporters in writing S2Rs for mobile applications by analyzing natural language information entered by reporters in real-time, and linking this data to information extracted via a combination of static and dynamic program analyses. As reporters write S2Rs, EBug is capable of automatically suggesting potential future steps using predictive models trained on realistic app usages. To evaluate EBug, we performed two user studies based on 20 failures from 11 real-world apps. The empirical studies involved ten participants that submitted ten bug reports each and ten developers that reproduced the submitted bug reports. In the studies, we found that reporters were able to construct bug reports 31% faster with EBug as compared to the state-of-the-art bug reporting system used as a baseline. EBug's reports were also more reproducible with respect to the ones generated with the baseline. Furthermore, we compared EBug's prediction models to other predictive modeling approaches and found that, overall, the predictive models of our approach outperformed the baseline approaches. Our results are promising and demonstrate the feasibility and potential benefits provided by proactively assistive bug reporting systems.",Achievement,Intelligent,"The paper introduces a novel bug reporting approach ""EBug"" that assists users in writing detailed bug reports; hence indirectly helping users feel more intelligent by understanding and communicating the issues they face more clearly.","In the paper abstract, it is stated that the EBug bug reporting approach assists users in writing detailed bug reports. By providing automated assistance in crafting easily readable and reproducible bug reports, users can effectively communicate the issues they face. This contributes to the value item Intelligent from Schwartz's Taxonomy, as users can demonstrate their intelligence by understanding and effectively reporting software failures. This aligns with the Achievement value, as users can achieve a sense of competence and mastery by accurately and comprehensively documenting software bugs.",Agreed-Justified,Agree,,Agree,,
Exp D,Exp K,2324,TSE,Software Testing & QA,Enhancing Mobile App Bug Reporting via Real-Time Understanding of Reproduction Steps,"One of the primary mechanisms by which developers receive feedback about in-field failures of software from users is through bug reports. Unfortunately, the quality of manually written bug reports can vary widely due to the effort required to include essential pieces of information, such as detailed reproduction steps (S2Rs). Despite the difficulty faced by reporters, few existing bug reporting systems attempt to offer automated assistance to users in crafting easily readable, and conveniently reproducible bug reports. To address the need for proactive bug reporting systems that actively aid the user in capturing crucial information, we introduce a novel bug reporting approach called EBug. EBug assists reporters in writing S2Rs for mobile applications by analyzing natural language information entered by reporters in real-time, and linking this data to information extracted via a combination of static and dynamic program analyses. As reporters write S2Rs, EBug is capable of automatically suggesting potential future steps using predictive models trained on realistic app usages. To evaluate EBug, we performed two user studies based on 20 failures from 11 real-world apps. The empirical studies involved ten participants that submitted ten bug reports each and ten developers that reproduced the submitted bug reports. In the studies, we found that reporters were able to construct bug reports 31% faster with EBug as compared to the state-of-the-art bug reporting system used as a baseline. EBug's reports were also more reproducible with respect to the ones generated with the baseline. Furthermore, we compared EBug's prediction models to other predictive modeling approaches and found that, overall, the predictive models of our approach outperformed the baseline approaches. Our results are promising and demonstrate the feasibility and potential benefits provided by proactively assistive bug reporting systems.",Security,Healthy,"The paper presents an improved bug reporting system that helps in the detailed capture of software issues which can contribute to maintaining software health and thus aligns with the value of Security; value item ""Healthy"".","The justification for aligning 'Paper X' with the value item Healthy and its corresponding value Security from a ""Software User"" perspective is based on the fact that the improved bug reporting system introduced in the paper enables the detailed capture of software issues. By capturing and addressing these issues, the system contributes to the overall health and reliability of the software, thereby enhancing the sense of security for the software user. The ability to report and address bugs efficiently and effectively promotes a secure software environment, which is aligned with the value of Security.",Agreed-Clarified,Disagree,,Disagree,Software health is not necessarily Human health,
Exp E,Exp J,2324,TSE,Software Testing & QA,Enhancing Mobile App Bug Reporting via Real-Time Understanding of Reproduction Steps,"One of the primary mechanisms by which developers receive feedback about in-field failures of software from users is through bug reports. Unfortunately, the quality of manually written bug reports can vary widely due to the effort required to include essential pieces of information, such as detailed reproduction steps (S2Rs). Despite the difficulty faced by reporters, few existing bug reporting systems attempt to offer automated assistance to users in crafting easily readable, and conveniently reproducible bug reports. To address the need for proactive bug reporting systems that actively aid the user in capturing crucial information, we introduce a novel bug reporting approach called EBug. EBug assists reporters in writing S2Rs for mobile applications by analyzing natural language information entered by reporters in real-time, and linking this data to information extracted via a combination of static and dynamic program analyses. As reporters write S2Rs, EBug is capable of automatically suggesting potential future steps using predictive models trained on realistic app usages. To evaluate EBug, we performed two user studies based on 20 failures from 11 real-world apps. The empirical studies involved ten participants that submitted ten bug reports each and ten developers that reproduced the submitted bug reports. In the studies, we found that reporters were able to construct bug reports 31% faster with EBug as compared to the state-of-the-art bug reporting system used as a baseline. EBug's reports were also more reproducible with respect to the ones generated with the baseline. Furthermore, we compared EBug's prediction models to other predictive modeling approaches and found that, overall, the predictive models of our approach outperformed the baseline approaches. Our results are promising and demonstrate the feasibility and potential benefits provided by proactively assistive bug reporting systems.",Benevolence,Helpful,"The paper provides assistance to users in writing bug reports; thus enabling them to contribute to improving the software which aligns with the value of Benevolence; value item ""Helpful"".","The paper's introduction explicitly states that the main contribution of 'Paper X' is to introduce a bug reporting approach called EBug that assists users in writing bug reports for mobile applications. This assistance is achieved through analyzing natural language information entered by reporters in real-time and suggesting potential future steps using predictive models trained on realistic app usages. By providing this assistance, 'Paper X' helps users in capturing crucial information and crafting easily readable and reproducible bug reports, ultimately contributing to the improvement of software. This aligns with the value item ""Helpful"" and its corresponding value of Benevolence, as the paper aims to support and benefit software users by enabling their active participation in bug reporting and software improvement processes.",Agreed-Justified,Agree,,Agree,,
Exp F,Exp J,2338,TSE,Security & Privacy,IoTCom: Dissecting Interaction Threats in IoT Systems,"Due to the growing presence of Internet of Things (IoT) apps and devices in smart homes and smart cities, there are more and more concerns about their security and privacy risks. IoT apps normally interact with each other and the physical world to offer utility to the users. In this paper, we investigate the safety and security risks brought by the interactive behaviors of IoT apps. Two major challenges ensue in identifying the interaction threats: i) how to discover the threats across both cyber and physical channels; and ii) how to ensure the scalability of the detection approach. To address these challenges, we first provide a taxonomy of interaction threats between IoT apps, which contains seven classes of coordination threats categorized based on their interaction behaviors. Then, we present IoTCom, a compositional threat detection system capable of automatically detecting and verifying unsafe interactions between IoT apps and devices. IoTCom applies static analysis to automatically infer relevant appsaEUR(tm) behaviors, and uses a novel strategy to trim the extracted app's behaviors prior to translating them into analyzable formal specifications, mitigating the state explosion associated with formal analysis. Our experiments with numerous bundles of real-world IoT apps have corroborated IoTCom's ability to effectively identify a broad spectrum of interaction threats triggered through cyber and physical channels, many of which were previously unknown. Finally, IoTCom uses an automatic verifier to validate the discovered threats. Our experimental results show that IoTCom significantly outperforms the existing techniques in terms of the computational time, and maintains the capability to perform its analysis across different IoT platforms.",Security,Healthy,"The paper investigates security risks in IoT apps and devices; which relates to keeping users' systems ""Healthy.""","In the context of a ""Software User,"" the value item ""Healthy"" and its corresponding value ""Security"" from Schwartz's Taxonomy align with the main contributions of 'Paper X' because the paper specifically addresses the safety and security risks associated with IoT apps and devices. By identifying and detecting unsafe interactions between these apps and devices, the paper aims to ensure the protection and well-being of users' systems, thereby promoting a sense of security and maintaining their overall ""healthy"" state.",Agreed-Justified,Agree,,Agree,,
Exp G,Exp K,2338,TSE,Security & Privacy,IoTCom: Dissecting Interaction Threats in IoT Systems,"Due to the growing presence of Internet of Things (IoT) apps and devices in smart homes and smart cities, there are more and more concerns about their security and privacy risks. IoT apps normally interact with each other and the physical world to offer utility to the users. In this paper, we investigate the safety and security risks brought by the interactive behaviors of IoT apps. Two major challenges ensue in identifying the interaction threats: i) how to discover the threats across both cyber and physical channels; and ii) how to ensure the scalability of the detection approach. To address these challenges, we first provide a taxonomy of interaction threats between IoT apps, which contains seven classes of coordination threats categorized based on their interaction behaviors. Then, we present IoTCom, a compositional threat detection system capable of automatically detecting and verifying unsafe interactions between IoT apps and devices. IoTCom applies static analysis to automatically infer relevant appsaEUR(tm) behaviors, and uses a novel strategy to trim the extracted app's behaviors prior to translating them into analyzable formal specifications, mitigating the state explosion associated with formal analysis. Our experiments with numerous bundles of real-world IoT apps have corroborated IoTCom's ability to effectively identify a broad spectrum of interaction threats triggered through cyber and physical channels, many of which were previously unknown. Finally, IoTCom uses an automatic verifier to validate the discovered threats. Our experimental results show that IoTCom significantly outperforms the existing techniques in terms of the computational time, and maintains the capability to perform its analysis across different IoT platforms.",Security,Social Order,"By addressing interaction threats in IoT systems; the paper contributes to maintaining ""Social Order;"" in a sense of ensuring the safe and orderly operation of interconnected systems.","In the context of the 'Paper X' abstract, the alignment with the value item Social Order and its corresponding value Security is justified because the paper specifically addresses the safety and security risks brought by the interactive behaviors of IoT apps. By developing a threat detection system, the paper aims to ensure the safe and orderly operation of interconnected systems, which is essential for maintaining social order. The focus on mitigating interaction threats contributes to the overall security and stability of the IoT ecosystem, thus aligning with the value of Security within the framework of Schwartz's Taxonomy.",Agreed-Clarified,Agree,,Agree,,
Exp A,Exp H,2338,TSE,Security & Privacy,IoTCom: Dissecting Interaction Threats in IoT Systems,"Due to the growing presence of Internet of Things (IoT) apps and devices in smart homes and smart cities, there are more and more concerns about their security and privacy risks. IoT apps normally interact with each other and the physical world to offer utility to the users. In this paper, we investigate the safety and security risks brought by the interactive behaviors of IoT apps. Two major challenges ensue in identifying the interaction threats: i) how to discover the threats across both cyber and physical channels; and ii) how to ensure the scalability of the detection approach. To address these challenges, we first provide a taxonomy of interaction threats between IoT apps, which contains seven classes of coordination threats categorized based on their interaction behaviors. Then, we present IoTCom, a compositional threat detection system capable of automatically detecting and verifying unsafe interactions between IoT apps and devices. IoTCom applies static analysis to automatically infer relevant appsaEUR(tm) behaviors, and uses a novel strategy to trim the extracted app's behaviors prior to translating them into analyzable formal specifications, mitigating the state explosion associated with formal analysis. Our experiments with numerous bundles of real-world IoT apps have corroborated IoTCom's ability to effectively identify a broad spectrum of interaction threats triggered through cyber and physical channels, many of which were previously unknown. Finally, IoTCom uses an automatic verifier to validate the discovered threats. Our experimental results show that IoTCom significantly outperforms the existing techniques in terms of the computational time, and maintains the capability to perform its analysis across different IoT platforms.",Security,National Security,"The abstract presents a contribution to detecting and verifying threats in home-based IoT applications; aligning with the value item of ""National Security;"" as the security of IoT forms an integral part of national cyber security infrastructure.","I apologize for any confusion caused by my previous justification. In the context of a ""Software User"" perspective, the alignment of 'Paper X' with the value item National Security and its corresponding value Security is based on the premise that the security risks associated with IoT apps and devices in smart homes and smart cities can have wide-ranging implications, including potential threats to national cyber security infrastructure. By providing a threat detection system like IoTCom that addresses these risks, 'Paper X' contributes to enhancing security measures that are crucial for maintaining national security.",Agreed-Justified,Disagree,"Related to Achievement: capable more, as it automatically identifies vulnerabilities",Disagree,The justification doesn't align with the value item of the national security,
Exp B,Exp H,2339,TSE,Security & Privacy,Open or Sneaky? Fast or Slow? Light or Heavy?: Investigating Security Releases of Open Source Packages,"Vulnerabilities in open source packages can be a security risk for the downstream client projects. When a new vulnerability is discovered, a package should quickly release a fix in a new version, referred to as a security release in this study. The security release should be well-documented and require minimal migration effort to facilitate fast adoption by the clients. However, to what extent the open source packages follow these recommendations is not known. In this paper, we study (1) the time lag between fix and release; (2) how security fixes are documented in the release notes; (3) code change characteristics (size and semantic versioning) of the release; and (4) the time lag between the release and an advisory publication for security releases over a dataset of 4,377 security advisories across seven package ecosystems. We find that the median security release becomes available within 4 days of the corresponding fix and contains 131 lines of code (LOC) change. However, one-fourth of the releases in our data set still came at least 20 days after the fix was made.Further, we find that 61.5% of the security releases come with a release note that documents the corresponding security fix. Still, Snyk and NVD, two popular databases, take a median of 17 days (from the release) to publish a security advisory, possibly resulting in delayed notifications to the client projects. We also find that security releases may contain breaking change(s) as 13.2% indicated backward incompatibility through semantic versioning, while 6.4% mentioned breaking change(s) in the release notes. Based on our findings, we point out areas for future work, such as private fork for security fixes and standardized practice for announcing security releases.",Security,Healthy,"The paper's focus on vulnerabilities in open source packages and their potential risk to client projects aligns with the value item ""Healthy"" as it indirectly looks at maintaining a healthy and secure digital environment for software users.","The paper's focus on vulnerabilities in open source packages aligns with the value item ""Healthy"" because addressing these vulnerabilities contributes to the overall health and security of the software ecosystem that software users rely on. By identifying the time lag between fix and release, documenting security fixes, and analyzing code change characteristics, the paper aims to facilitate fast adoption of security releases, thereby ensuring the health and security of software systems used by software users. Therefore, the alignment with the value item ""Healthy"" from a software user's perspective is evident in the paper's objective of addressing vulnerabilities to create a safer software environment.",Agreed-Justified,Disagree,reconcile with coder_2,Disagree,The justification doesn't align with the value item of the healthy,
Exp C,Exp K,2339,TSE,Security & Privacy,Open or Sneaky? Fast or Slow? Light or Heavy?: Investigating Security Releases of Open Source Packages,"Vulnerabilities in open source packages can be a security risk for the downstream client projects. When a new vulnerability is discovered, a package should quickly release a fix in a new version, referred to as a security release in this study. The security release should be well-documented and require minimal migration effort to facilitate fast adoption by the clients. However, to what extent the open source packages follow these recommendations is not known. In this paper, we study (1) the time lag between fix and release; (2) how security fixes are documented in the release notes; (3) code change characteristics (size and semantic versioning) of the release; and (4) the time lag between the release and an advisory publication for security releases over a dataset of 4,377 security advisories across seven package ecosystems. We find that the median security release becomes available within 4 days of the corresponding fix and contains 131 lines of code (LOC) change. However, one-fourth of the releases in our data set still came at least 20 days after the fix was made.Further, we find that 61.5% of the security releases come with a release note that documents the corresponding security fix. Still, Snyk and NVD, two popular databases, take a median of 17 days (from the release) to publish a security advisory, possibly resulting in delayed notifications to the client projects. We also find that security releases may contain breaking change(s) as 13.2% indicated backward incompatibility through semantic versioning, while 6.4% mentioned breaking change(s) in the release notes. Based on our findings, we point out areas for future work, such as private fork for security fixes and standardized practice for announcing security releases.",Security,Family Security,"In suggesting areas for future work; such as privacy fork for security fixes; it reflects a concern for ""Family Security"" as it emphasizes the need for better security protocols to protect user's data and privacy; ensuring software user's digital safety.","In suggesting areas for future work, such as a private fork for security fixes, 'Paper X' reflects a concern for ""Family Security"" as it emphasizes the need for better security protocols to protect user's data and privacy. This directly aligns with the value item of Family Security, which is a part of the broader value of Security in Schwartz's Taxonomy. By addressing the potential vulnerabilities in open source packages and advocating for improved security practices, 'Paper X' aims to ensure the digital safety of software users and their families.",Agreed-Clarified,Agree,,Agree,,
Exp D,Exp K,2339,TSE,Security & Privacy,Open or Sneaky? Fast or Slow? Light or Heavy?: Investigating Security Releases of Open Source Packages,"Vulnerabilities in open source packages can be a security risk for the downstream client projects. When a new vulnerability is discovered, a package should quickly release a fix in a new version, referred to as a security release in this study. The security release should be well-documented and require minimal migration effort to facilitate fast adoption by the clients. However, to what extent the open source packages follow these recommendations is not known. In this paper, we study (1) the time lag between fix and release; (2) how security fixes are documented in the release notes; (3) code change characteristics (size and semantic versioning) of the release; and (4) the time lag between the release and an advisory publication for security releases over a dataset of 4,377 security advisories across seven package ecosystems. We find that the median security release becomes available within 4 days of the corresponding fix and contains 131 lines of code (LOC) change. However, one-fourth of the releases in our data set still came at least 20 days after the fix was made.Further, we find that 61.5% of the security releases come with a release note that documents the corresponding security fix. Still, Snyk and NVD, two popular databases, take a median of 17 days (from the release) to publish a security advisory, possibly resulting in delayed notifications to the client projects. We also find that security releases may contain breaking change(s) as 13.2% indicated backward incompatibility through semantic versioning, while 6.4% mentioned breaking change(s) in the release notes. Based on our findings, we point out areas for future work, such as private fork for security fixes and standardized practice for announcing security releases.",Security,National Security,"The paper's analysis of the time lag between the release of a security fix and its advisory publication addresses the item ""National Security"" as it can be viewed as an attempt to improve public awareness and rapid response to national/cybersecurity issues.","In the paper abstract, it is mentioned that the time lag between the release of a security fix and its advisory publication is analyzed. This can be seen as a direct effort to improve the overall security of software systems, which aligns with the value item ""National Security."" By reducing the time it takes to publish security advisories, the paper aims to enhance public awareness and facilitate rapid response to cybersecurity issues, which is in line with the value of ""Security."" Therefore, the analysis in the paper directly addresses these specific value items from a ""Software User"" perspective.",Agreed-Clarified,Agree,,Agree,,
Exp E,Exp J,2355,TSE,AI & Machine Learning,Neural Network Guided Evolutionary Fuzzing for Finding Traffic Violations of Autonomous Vehicles,"Self-driving cars and trucks, autonomous vehicles (avs), should not be accepted by regulatory bodies and the public until they have much higher confidence in their safety and reliability aEUR"" which can most practically and convincingly be achieved by testing. But existing testing methods are inadequate for checking the end-to-end behaviors of av controllers against complex, real-world corner cases involving interactions with multiple independent agents such as pedestrians and human-driven vehicles. While test-driving avs on streets and highways fails to capture many rare events, existing simulation-based testing methods mainly focus on simple scenarios and do not scale well for complex driving situations that require sophisticated awareness of the surroundings. To address these limitations, we propose a new fuzz testing technique, called AutoFuzz, which can leverage widely-used av simulatorsaEUR(tm) API grammars to generate semantically and temporally valid complex driving scenarios (sequences of scenes). To efficiently search for traffic violations-inducing scenarios in a large search space, we propose a constrained neural network (NN) evolutionary search method to optimize AutoFuzz. Evaluation of our prototype on one state-of-the-art learning-based controller, two rule-based controllers, and one industrial-grade controller in five scenarios shows that AutoFuzz efficiently finds hundreds of traffic violationsin high-fidelity simulation environments. For each scenario, AutoFuzz can find on average 10-39% more unique traffic violationsthan the best-performing baseline method. Further, fine-tuning the learning-based controller with the traffic violationsfound by AutoFuzz successfully reduced the traffic violationsfound in the new version of the av controller software.",Stimulation,Excitement in Life,The paper proposes a new method to test self-driving cars in high-fidelity simulation environments that can expose the autonomous vehicles to complex driving situations. These simulated scenarios provide a simulation of an exciting real-world experience; aligning with the value item 'Excitement in Life' and its corresponding value 'Stimulation'.,"The justification for labeling 'Paper X' with the value item Excitement in Life and its corresponding value Stimulation is based on the fact that the paper proposes a new method, called AutoFuzz, which generates complex driving scenarios within high-fidelity simulation environments. These scenarios aim to simulate real-world experiences and interactions with multiple agents, such as pedestrians and human-driven vehicles. The ability to experience and interact with dynamic and complex driving situations can add an element of excitement and stimulation to the software user's experience, aligning with the value item Excitement in Life and its corresponding value Stimulation.",Agreed-Justified,Agree,,Agree,,
Exp F,Exp J,2355,TSE,AI & Machine Learning,Neural Network Guided Evolutionary Fuzzing for Finding Traffic Violations of Autonomous Vehicles,"Self-driving cars and trucks, autonomous vehicles (avs), should not be accepted by regulatory bodies and the public until they have much higher confidence in their safety and reliability aEUR"" which can most practically and convincingly be achieved by testing. But existing testing methods are inadequate for checking the end-to-end behaviors of av controllers against complex, real-world corner cases involving interactions with multiple independent agents such as pedestrians and human-driven vehicles. While test-driving avs on streets and highways fails to capture many rare events, existing simulation-based testing methods mainly focus on simple scenarios and do not scale well for complex driving situations that require sophisticated awareness of the surroundings. To address these limitations, we propose a new fuzz testing technique, called AutoFuzz, which can leverage widely-used av simulatorsaEUR(tm) API grammars to generate semantically and temporally valid complex driving scenarios (sequences of scenes). To efficiently search for traffic violations-inducing scenarios in a large search space, we propose a constrained neural network (NN) evolutionary search method to optimize AutoFuzz. Evaluation of our prototype on one state-of-the-art learning-based controller, two rule-based controllers, and one industrial-grade controller in five scenarios shows that AutoFuzz efficiently finds hundreds of traffic violationsin high-fidelity simulation environments. For each scenario, AutoFuzz can find on average 10-39% more unique traffic violationsthan the best-performing baseline method. Further, fine-tuning the learning-based controller with the traffic violationsfound by AutoFuzz successfully reduced the traffic violationsfound in the new version of the av controller software.",Security,Healthy,The paper focuses on fuzz testing for autonomous vehicles; which inherently contributes to the safety and reliability of these vehicles. As such; it aligns with the value item 'Healthy' as part of the 'Security' value considering that safer cars can contribute to fewer accidents and healthier lives for users of those cars.,"In 'Paper X', the focus on fuzz testing for autonomous vehicles directly aligns with the value item ""Healthy"" and its corresponding value ""Security"" from a ""Software User"" perspective. By conducting rigorous testing to enhance the safety and reliability of autonomous vehicles, the paper aims to contribute to a more secure environment for users. This alignment is evident as safer vehicles can lead to a reduction in accidents and ultimately promote healthier lives for the users of these vehicles.",Agreed-Clarified,Agree,,Agree,,
Exp G,Exp K,2355,TSE,AI & Machine Learning,Neural Network Guided Evolutionary Fuzzing for Finding Traffic Violations of Autonomous Vehicles,"Self-driving cars and trucks, autonomous vehicles (avs), should not be accepted by regulatory bodies and the public until they have much higher confidence in their safety and reliability aEUR"" which can most practically and convincingly be achieved by testing. But existing testing methods are inadequate for checking the end-to-end behaviors of av controllers against complex, real-world corner cases involving interactions with multiple independent agents such as pedestrians and human-driven vehicles. While test-driving avs on streets and highways fails to capture many rare events, existing simulation-based testing methods mainly focus on simple scenarios and do not scale well for complex driving situations that require sophisticated awareness of the surroundings. To address these limitations, we propose a new fuzz testing technique, called AutoFuzz, which can leverage widely-used av simulatorsaEUR(tm) API grammars to generate semantically and temporally valid complex driving scenarios (sequences of scenes). To efficiently search for traffic violations-inducing scenarios in a large search space, we propose a constrained neural network (NN) evolutionary search method to optimize AutoFuzz. Evaluation of our prototype on one state-of-the-art learning-based controller, two rule-based controllers, and one industrial-grade controller in five scenarios shows that AutoFuzz efficiently finds hundreds of traffic violationsin high-fidelity simulation environments. For each scenario, AutoFuzz can find on average 10-39% more unique traffic violationsthan the best-performing baseline method. Further, fine-tuning the learning-based controller with the traffic violationsfound by AutoFuzz successfully reduced the traffic violationsfound in the new version of the av controller software.",Achievement,Successful,Although the achievement aspect is indirect; the proposed method can contribute to its achievement by improving the safety and efficiency of self-driving cars; which are indicators of achievement for the end user. It aligns with the value item 'Successful' as part of the 'Achievement' value category. The success here is indirectly experienced by the user by utilizing an efficient and safer car.,"The proposed method in 'Paper X' aligns with the value item Successful and its corresponding value Achievement from a ""Software User"" perspective because it directly contributes to the improvement of safety and efficiency in self-driving cars. Users perceive achievement in the context of successful utilization of a software system, and in this case, a more efficient and safer self-driving car would be considered a successful achievement for the user. The method in the paper aims to address the limitations of existing testing methods for autonomous vehicles, which directly aligns with the user's desire to have a reliable and secure driving experience.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2359,TSE,Software Testing & QA,Mind the Gap! A Study on the Transferability of Virtual Versus Physical-World Testing of Autonomous Driving Systems,"Safe deployment of self-driving cars (SDC) necessitates thorough simulated and in-field testing. Most testing techniques consider virtualized SDCs within a simulation environment, whereas less effort has been directed towards assessing whether such techniques transfer to and are effective with a physical real-world vehicle. In this paper, we shed light on the problem of generalizing testing results obtained in a driving simulator to a physical platform and provide a characterization and quantification of the sim2real gap affecting SDC testing. In our empirical study, we compare SDC testing when deployed on a physical small-scale vehicle versus its digital twin. Due to the unavailability of driving quality indicators from the physical platform, we use neural rendering to estimate them through visual odometry, hence allowing full comparability with the digital twin. Then, we investigate the transferability of behavior and failure exposure between virtual and real-world environments, targeting both unintended abnormal test data and intended adversarial examples. Our study shows that, despite the usage of a faithful digital twin, there are still critical shortcomings that contribute to the reality gap between the virtual and physical world, threatening existing testing solutions that only consider virtual SDCs. On the positive side, our results present the test configurations for which physical testing can be avoided, either because their outcome does transfer between virtual and physical environments, or because the uncertainty profiles in the simulator can help predict their outcome in the real world.",Security,National Security,The paper aims to ensure safe deployment of self-driving cars through rigorous testing in real and virtual environments; which directly supports National Security under Security as it contributes to safer public transportation.,"By ensuring the safe deployment of self-driving cars, 'Paper X' directly aligns with the value item National Security and its corresponding value Security. This alignment is evident as the paper focuses on rigorous testing in both real and virtual environments, with the goal of improving public safety in transportation. By addressing potential gaps in testing techniques and considering the transferability of test results between virtual and physical platforms, the paper contributes to the overall security and protection of individuals using self-driving cars. This alignment is particularly relevant from a ""Software User"" perspective, as it emphasizes the importance of ensuring the safety and security of individuals who rely on such technologies.",Agreed-Justified,Agree,,Agree,,
Exp B,Exp H,2359,TSE,Software Testing & QA,Mind the Gap! A Study on the Transferability of Virtual Versus Physical-World Testing of Autonomous Driving Systems,"Safe deployment of self-driving cars (SDC) necessitates thorough simulated and in-field testing. Most testing techniques consider virtualized SDCs within a simulation environment, whereas less effort has been directed towards assessing whether such techniques transfer to and are effective with a physical real-world vehicle. In this paper, we shed light on the problem of generalizing testing results obtained in a driving simulator to a physical platform and provide a characterization and quantification of the sim2real gap affecting SDC testing. In our empirical study, we compare SDC testing when deployed on a physical small-scale vehicle versus its digital twin. Due to the unavailability of driving quality indicators from the physical platform, we use neural rendering to estimate them through visual odometry, hence allowing full comparability with the digital twin. Then, we investigate the transferability of behavior and failure exposure between virtual and real-world environments, targeting both unintended abnormal test data and intended adversarial examples. Our study shows that, despite the usage of a faithful digital twin, there are still critical shortcomings that contribute to the reality gap between the virtual and physical world, threatening existing testing solutions that only consider virtual SDCs. On the positive side, our results present the test configurations for which physical testing can be avoided, either because their outcome does transfer between virtual and physical environments, or because the uncertainty profiles in the simulator can help predict their outcome in the real world.",Stimulation,Variation in Life,The paper discusses the comparison and analysis of Self-Driving Cars in both physical and virtual environments leading to variety in the process; aligning with Variation in Life in the Stimulation value.,"The justification for aligning 'Paper X' with the value item Variation in Life and its corresponding value Stimulation from a ""Software User"" perspective is based on the fact that the paper discusses the comparison and analysis of Self-Driving Cars in both physical and virtual environments. This comparison and analysis process introduces a variety of scenarios and challenges, leading to a sense of stimulation and excitement for the users involved. By exploring different testing techniques and assessing the transferability of results between virtual and real-world environments, the paper contributes to the stimulation and variation in the life of software users by providing new insights and perspectives in the field of self-driving cars.",Agreed-Clarified,Agree,,Agree,,
Exp C,Exp K,2359,TSE,Software Testing & QA,Mind the Gap! A Study on the Transferability of Virtual Versus Physical-World Testing of Autonomous Driving Systems,"Safe deployment of self-driving cars (SDC) necessitates thorough simulated and in-field testing. Most testing techniques consider virtualized SDCs within a simulation environment, whereas less effort has been directed towards assessing whether such techniques transfer to and are effective with a physical real-world vehicle. In this paper, we shed light on the problem of generalizing testing results obtained in a driving simulator to a physical platform and provide a characterization and quantification of the sim2real gap affecting SDC testing. In our empirical study, we compare SDC testing when deployed on a physical small-scale vehicle versus its digital twin. Due to the unavailability of driving quality indicators from the physical platform, we use neural rendering to estimate them through visual odometry, hence allowing full comparability with the digital twin. Then, we investigate the transferability of behavior and failure exposure between virtual and real-world environments, targeting both unintended abnormal test data and intended adversarial examples. Our study shows that, despite the usage of a faithful digital twin, there are still critical shortcomings that contribute to the reality gap between the virtual and physical world, threatening existing testing solutions that only consider virtual SDCs. On the positive side, our results present the test configurations for which physical testing can be avoided, either because their outcome does transfer between virtual and physical environments, or because the uncertainty profiles in the simulator can help predict their outcome in the real world.",Security,Healthy,The paper indirectly contributes to the user's Health under Security by working towards deployment of thoroughly tested self-driving cars; which can potentially reduce traffic accidents.,"In order to align with the value item Healthy and its corresponding value Security from a ""Software User"" perspective, the paper addresses the safe deployment of self-driving cars through thorough testing. By ensuring that self-driving cars are extensively tested and reliable, there is a potential decrease in traffic accidents, which ultimately contributes to the user's health and well-being.",Agreed-Justified,Agree,,Agree,,
Exp D,Exp K,2361,TSE,AI & Machine Learning,A Declarative Metamorphic Testing Framework for Autonomous Driving,"Autonomous driving has gained much attention from both industry and academia. Currently, Deep Neural Networks (DNNs) are widely used for perception and control in autonomous driving. However, several fatal accidents caused by autonomous vehicles have raised serious safety concerns about autonomous driving models. Some recent studies have successfully used the metamorphic testing technique to detect thousands of potential issues in some popularly used autonomous driving models. However, prior study is limited to a small set of metamorphic relations, which do not reflect rich, real-world traffic scenarios and are also not customizable. This paper presents a novel declarative rule-based metamorphic testing framework called RMT. RMT provides a rule template with natural language syntax, allowing users to flexibly specify an enriched set of testing scenarios based on real-world traffic rules and domain knowledge. RMT automatically parses human-written rules to metamorphic relations using an NLP-based rule parser referring to an ontology list and generates test cases with a variety of image transformation engines. We evaluated RMT on three autonomous driving models. With an enriched set of metamorphic relations, RMT detected a significant number of abnormal model predictions that were not detected by prior work. Through a large-scale human study on Amazon Mechanical Turk, we further confirmed the authenticity of test cases generated by RMT and the validity of detected abnormal model predictions.",Security,Healthy,The main contribution of 'Paper X' is a novel rule-based testing framework for autonomous driving models to enhance safety and reduce potential risks or issues; therefore this aligns directly with 'Healthy' within the 'Security' value since healthier represents safety and absence of dangers.,"In 'Paper X', the main contribution is the development of a rule-based testing framework for autonomous driving models, which aims to enhance safety and reduce potential risks or issues. This aligns directly with the value item 'Healthy' within the value of 'Security' from a ""Software User"" perspective. The concept of ""Healthy"" in the context of security implies safety and the absence of dangers, which is exactly what the framework aims to achieve by detecting abnormal model predictions. By identifying and addressing potential safety concerns, this contribution directly aligns with the value of 'Security' and its value item 'Healthy' in Schwartz's Taxonomy.",Agreed-Justified,Agree,,Agree,,
Exp E,Exp J,2361,TSE,AI & Machine Learning,A Declarative Metamorphic Testing Framework for Autonomous Driving,"Autonomous driving has gained much attention from both industry and academia. Currently, Deep Neural Networks (DNNs) are widely used for perception and control in autonomous driving. However, several fatal accidents caused by autonomous vehicles have raised serious safety concerns about autonomous driving models. Some recent studies have successfully used the metamorphic testing technique to detect thousands of potential issues in some popularly used autonomous driving models. However, prior study is limited to a small set of metamorphic relations, which do not reflect rich, real-world traffic scenarios and are also not customizable. This paper presents a novel declarative rule-based metamorphic testing framework called RMT. RMT provides a rule template with natural language syntax, allowing users to flexibly specify an enriched set of testing scenarios based on real-world traffic rules and domain knowledge. RMT automatically parses human-written rules to metamorphic relations using an NLP-based rule parser referring to an ontology list and generates test cases with a variety of image transformation engines. We evaluated RMT on three autonomous driving models. With an enriched set of metamorphic relations, RMT detected a significant number of abnormal model predictions that were not detected by prior work. Through a large-scale human study on Amazon Mechanical Turk, we further confirmed the authenticity of test cases generated by RMT and the validity of detected abnormal model predictions.",Security,Social Order,Paper X' focuses on improving safety by applying real-world traffic rules and scenarios in their testing framework; which can contribute to maintaining 'Social Order' within the 'Security' value since a proper order in social traffic situation is crucial for the overall safety.,"In 'Paper X', the authors propose a novel declarative rule-based metamorphic testing framework that allows for the specification of testing scenarios based on real-world traffic rules and domain knowledge. By using this approach, the authors aim to detect abnormal model predictions in autonomous driving systems. This aligns with the value item of Social Order from Schwartz's Taxonomy, as it focuses on maintaining proper order in social traffic situations, which is crucial for overall safety. Ensuring a safe and orderly environment on the road directly contributes to the value of Security, as it helps to prevent accidents and maintain the well-being of individuals using the software in the context of autonomous driving.",Agreed-Justified,Agree,,Agree,,
Exp G,Exp K,2362,TSE,AI & Machine Learning,Open Science in Software Engineering: A Study on Deep Learning-Based Vulnerability Detection,"Open science is a practice that makes scientific research publicly accessible to anyone, hence is highly beneficial. Given the benefits, the software engineering (SE) community has been diligently advocating open science policies during peer reviews and publication processes. However, to this date, there has been few studies that look into the status and issues of open science in SE from a systematic perspective. In this paper, we set out to start filling this gap. Given the great breadth of SE in general, we constrained our scope to a particular topic area in SE as an example case. Recently, an increasing number of deep learning (DL) approaches have been explored in SE, including DL-based software vulnerability detection, a popular, fast-growing topic that addresses an important problem in software security. We exhaustively searched the literature in this area and identified 55 relevant works that propose a DL-based vulnerability detection approach. This was then followed by comprehensively investigating the four integral aspects of open science: availability, executability, reproducibility, and replicability. Among other findings, our study revealed that only a small percentage (25.5%) of the studied approaches provided publicly available tools. Some of these available tools did not provide sufficient documentation and complete implementation, making them not executable or not reproducible. The uses of balanced or artificially generated datasets caused significantly overrated performance of the respective techniques, making most of them not replicable. Based on our empirical results, we made actionable suggestions on improving the state of open science in each of the four aspects. We note that our results and recommendations on most of these aspects (availability, executability, reproducibility) are not tied to the nature of the chosen topic (DL-based vulnerability detection) hence are likely applicable to other SE topic areas. We also believe our results and recommendations on replicability to be applicable to other DL-based topics in SE as they are not tied to (the particular application of DL in) detecting software vulnerabilities.",Universalism,Social Justice,The paper contributes findings and suggestions on improving open science practices in software engineering; particularly on aspects of availability; executability; reproducibility; and replicability. Open science is a form of social justice as it advocates for equal access; transparency; and accountability in scientific research; Hence; this aligns with the value item Social Justice and its corresponding value Universalism.,"The paper's focus on improving open science practices, specifically on aspects of availability, executability, reproducibility, and replicability, aligns with the value item of Social Justice and its corresponding value of Universalism. By advocating for equal access and transparency in scientific research, open science promotes fairness and inclusivity, which are key principles of social justice. This directly aligns with the software user's perspective, as it ensures that the benefits of scientific research are accessible to all, regardless of their background or resources.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2362,TSE,AI & Machine Learning,Open Science in Software Engineering: A Study on Deep Learning-Based Vulnerability Detection,"Open science is a practice that makes scientific research publicly accessible to anyone, hence is highly beneficial. Given the benefits, the software engineering (SE) community has been diligently advocating open science policies during peer reviews and publication processes. However, to this date, there has been few studies that look into the status and issues of open science in SE from a systematic perspective. In this paper, we set out to start filling this gap. Given the great breadth of SE in general, we constrained our scope to a particular topic area in SE as an example case. Recently, an increasing number of deep learning (DL) approaches have been explored in SE, including DL-based software vulnerability detection, a popular, fast-growing topic that addresses an important problem in software security. We exhaustively searched the literature in this area and identified 55 relevant works that propose a DL-based vulnerability detection approach. This was then followed by comprehensively investigating the four integral aspects of open science: availability, executability, reproducibility, and replicability. Among other findings, our study revealed that only a small percentage (25.5%) of the studied approaches provided publicly available tools. Some of these available tools did not provide sufficient documentation and complete implementation, making them not executable or not reproducible. The uses of balanced or artificially generated datasets caused significantly overrated performance of the respective techniques, making most of them not replicable. Based on our empirical results, we made actionable suggestions on improving the state of open science in each of the four aspects. We note that our results and recommendations on most of these aspects (availability, executability, reproducibility) are not tied to the nature of the chosen topic (DL-based vulnerability detection) hence are likely applicable to other SE topic areas. We also believe our results and recommendations on replicability to be applicable to other DL-based topics in SE as they are not tied to (the particular application of DL in) detecting software vulnerabilities.",Security,Sense of Belonging,By proposing improvements to open science practices to ensure the availability; executability; reproducibility; and replicability of software; the paper contributes to creating a more trustworthy and reliable software landscape for users. Therefore; this aligns with the value item Sense of Belonging under the value Security as users can have a greater trust and confidence in using the software products from the open science community.,"The proposed improvements to open science practices suggested in 'Paper X' directly contribute to creating a more trustworthy and reliable software landscape for users. By ensuring the availability, executability, reproducibility, and replicability of software, the paper aligns with the value item Sense of Belonging under the value Security. Users can have a greater sense of trust and confidence in using software products from the open science community, knowing that these practices prioritize their security and contribute to a sense of belonging within a secure software ecosystem.",Agreed-Justified,Agree,,Agree,,
Exp B,Exp H,2362,TSE,AI & Machine Learning,Open Science in Software Engineering: A Study on Deep Learning-Based Vulnerability Detection,"Open science is a practice that makes scientific research publicly accessible to anyone, hence is highly beneficial. Given the benefits, the software engineering (SE) community has been diligently advocating open science policies during peer reviews and publication processes. However, to this date, there has been few studies that look into the status and issues of open science in SE from a systematic perspective. In this paper, we set out to start filling this gap. Given the great breadth of SE in general, we constrained our scope to a particular topic area in SE as an example case. Recently, an increasing number of deep learning (DL) approaches have been explored in SE, including DL-based software vulnerability detection, a popular, fast-growing topic that addresses an important problem in software security. We exhaustively searched the literature in this area and identified 55 relevant works that propose a DL-based vulnerability detection approach. This was then followed by comprehensively investigating the four integral aspects of open science: availability, executability, reproducibility, and replicability. Among other findings, our study revealed that only a small percentage (25.5%) of the studied approaches provided publicly available tools. Some of these available tools did not provide sufficient documentation and complete implementation, making them not executable or not reproducible. The uses of balanced or artificially generated datasets caused significantly overrated performance of the respective techniques, making most of them not replicable. Based on our empirical results, we made actionable suggestions on improving the state of open science in each of the four aspects. We note that our results and recommendations on most of these aspects (availability, executability, reproducibility) are not tied to the nature of the chosen topic (DL-based vulnerability detection) hence are likely applicable to other SE topic areas. We also believe our results and recommendations on replicability to be applicable to other DL-based topics in SE as they are not tied to (the particular application of DL in) detecting software vulnerabilities.",Self Direction,Self Respect,Through the paper's focus on open science practices in software engineering; it promotes the idea of self-respect among software users. This is evidenced through its call for improved practices in availability; executability; reproducibility; and replicability which can enhance the user's experience and epower their interaction with software. Therefore; this aligns with the value item Self Respect and the value Self Direction.,"The justification for aligning 'Paper X' with the value item Self Respect and its corresponding value Self Direction from a ""Software User"" perspective is based on the paper's emphasis on open science practices in software engineering. By advocating for improved practices in availability, executability, reproducibility, and replicability, the paper emphasizes the importance of empowering software users by enhancing their experience and enabling them to have greater control and direction in their interactions with software. This aligns with the value of Self Respect as it highlights the user's ability to make independent choices, exercise freedom, and have a sense of autonomy in their software usage.",Agreed-Clarified,Agree,,Agree,,
Exp C,Exp K,2369,TSE,Code Generation & Analysis,A Source-Level Instrumentation Framework for the Dynamic Analysis of Memory Safety,"Low-level control makes C unsafe, resulting in memory errors that can lead to data corruption, security vulnerabilities or program crashes. Dynamic analysis tools, which have been widely used for detecting memory errors at runtime, usually perform instrumentation at the IR or binary level. However, these non-source-level instrumentation frameworks and tools suffer from two inherent drawbacks: optimization sensitivity and platform dependence. Due to optimization sensitivity, the user of these tools must trade either performance for effectiveness by compiling the program at -O0 or effectiveness for performance by compiling the program at a higher optimization level, say, -O3. In this paper, we propose a new source-level instrumentation framework to overcome these two drawbacks, and implement it in a new dynamic analysis tool, called Movec, that adopts a pointer-based monitoring algorithm. We have evaluated Movec comprehensively by using the NIST's SARD benchmark suite (1152 programs), a set of 126 microbenchmarks (with ground truth), a set of 20 MiBench benchmarks and 5 pure-C SPEC CPU 2017 benchmarks. In terms of effectiveness, Movec outperforms three state-of-the-art dynamic analysis tools, AddressSanitizer, SoftBoundCETS and Valgrind, for all the standard optimization levels (from -O0 to -O3). In terms of performance, Movec outperforms SoftBoundCETS and Valgrind, and is slower than AddressSanitizer but consumes less memory.",Achievement,Capable,The main contribution of Paper X is the development of a new source-level instrumentation framework implemented in a tool called Movec that detects and prevents memory errors. This can be considered as improving the capability of software users to work with C language in a safer manner; aligning with the value item Capable and its corresponding value Achievement.,"The main contribution of 'Paper X' is the development of a new dynamic analysis tool, Movec, that detects and prevents memory errors in C language programs. By providing a source-level instrumentation framework, Movec improves the capability of software users to work with C language in a safer manner. This aligns with the value item Capable and its corresponding value Achievement as it enables software users to achieve their goals effectively and successfully by mitigating memory errors, which can lead to data corruption, security vulnerabilities, or program crashes.",Agreed-Justified,Agree,,Agree,,
Exp D,Exp K,2369,TSE,Code Generation & Analysis,A Source-Level Instrumentation Framework for the Dynamic Analysis of Memory Safety,"Low-level control makes C unsafe, resulting in memory errors that can lead to data corruption, security vulnerabilities or program crashes. Dynamic analysis tools, which have been widely used for detecting memory errors at runtime, usually perform instrumentation at the IR or binary level. However, these non-source-level instrumentation frameworks and tools suffer from two inherent drawbacks: optimization sensitivity and platform dependence. Due to optimization sensitivity, the user of these tools must trade either performance for effectiveness by compiling the program at -O0 or effectiveness for performance by compiling the program at a higher optimization level, say, -O3. In this paper, we propose a new source-level instrumentation framework to overcome these two drawbacks, and implement it in a new dynamic analysis tool, called Movec, that adopts a pointer-based monitoring algorithm. We have evaluated Movec comprehensively by using the NIST's SARD benchmark suite (1152 programs), a set of 126 microbenchmarks (with ground truth), a set of 20 MiBench benchmarks and 5 pure-C SPEC CPU 2017 benchmarks. In terms of effectiveness, Movec outperforms three state-of-the-art dynamic analysis tools, AddressSanitizer, SoftBoundCETS and Valgrind, for all the standard optimization levels (from -O0 to -O3). In terms of performance, Movec outperforms SoftBoundCETS and Valgrind, and is slower than AddressSanitizer but consumes less memory.",Achievement,Successful,The abstract of Paper X explicitly states that Movec outperforms three state-of-the-art dynamic analysis tools in terms of effectiveness and performance; demonstrating successful achievement in the realm of dynamic analysis tools. This can be aligned with the value item Successful and the corresponding value Achievement.,"In the abstract of 'Paper X', it is mentioned that Movec outperforms three state-of-the-art dynamic analysis tools in terms of both effectiveness and performance. This clear demonstration of superiority and achievement aligns with the value item Successful and its corresponding value Achievement from the perspective of a Software User. This indicates that the main contributions of 'Paper X' provide a successful achievement in the realm of dynamic analysis tools, fulfilling the user's desire for excellence and competence.",Agreed-Justified,Agree,,Agree,,
Exp E,Exp J,2369,TSE,Code Generation & Analysis,A Source-Level Instrumentation Framework for the Dynamic Analysis of Memory Safety,"Low-level control makes C unsafe, resulting in memory errors that can lead to data corruption, security vulnerabilities or program crashes. Dynamic analysis tools, which have been widely used for detecting memory errors at runtime, usually perform instrumentation at the IR or binary level. However, these non-source-level instrumentation frameworks and tools suffer from two inherent drawbacks: optimization sensitivity and platform dependence. Due to optimization sensitivity, the user of these tools must trade either performance for effectiveness by compiling the program at -O0 or effectiveness for performance by compiling the program at a higher optimization level, say, -O3. In this paper, we propose a new source-level instrumentation framework to overcome these two drawbacks, and implement it in a new dynamic analysis tool, called Movec, that adopts a pointer-based monitoring algorithm. We have evaluated Movec comprehensively by using the NIST's SARD benchmark suite (1152 programs), a set of 126 microbenchmarks (with ground truth), a set of 20 MiBench benchmarks and 5 pure-C SPEC CPU 2017 benchmarks. In terms of effectiveness, Movec outperforms three state-of-the-art dynamic analysis tools, AddressSanitizer, SoftBoundCETS and Valgrind, for all the standard optimization levels (from -O0 to -O3). In terms of performance, Movec outperforms SoftBoundCETS and Valgrind, and is slower than AddressSanitizer but consumes less memory.",Security,Healthy,The main aim of the proposed tool in Paper X is to improve memory safety in C programming. This aligns with the user being enabled to work in a healthier software environment. This aligns with the value item Healthy and its corresponding value Security.,"In Paper X, the main contribution is the introduction of a new source-level instrumentation framework called Movec, which aims to improve memory safety in C programming. The paper states that Movec outperforms three state-of-the-art dynamic analysis tools in terms of effectiveness for all standard optimization levels. This directly aligns with the value item Healthy and its corresponding value Security from a ""Software User"" perspective, as the proposed tool enables users to work in a healthier software environment by significantly enhancing memory safety, thereby reducing the likelihood of memory errors, data corruption, security vulnerabilities, or program crashes. This aligns with the user's desire for security and a secure software experience, which are essential values within a software context.",Agreed-Justified,Agree,,Agree,,
Exp F,Exp J,2370,TSE,Software Project Management,Improved Management of Issue Dependencies in Issue Trackers of Large Collaborative Projects,"Issue trackers, such as Jira, have become the prevalent collaborative tools in software engineering for managing issues, such as requirements, development tasks, and software bugs. However, issue trackers inherently focus on the lifecycle of single issues, although issues have and express dependencies on other issues that constitute issue dependency networks in large complex collaborative projects. The objective of this study is to develop supportive solutions for the improved management of dependent issues in an issue tracker. This study follows the Design Science methodology, consisting of eliciting drawbacks and constructing and evaluating a solution and system. The study was carried out in the context of The Qt Company's Jira, which exemplifies an actively used, almost two-decade-old issue tracker with over 100,000 issues. The drawbacks capture how users operate with issue trackers to handle issue information in large, collaborative, and long-lived projects. The basis of the solution is to keep issues and dependencies as separate objects and automatically construct an issue graph. Dependency detections complement the issue graph by proposing missing dependencies, while consistency checks and diagnoses identify conflicting issue priorities and release assignments. Jira's plugin and service-based system architecture realize the functional and quality concerns of the system implementation. We show how to adopt the intelligent supporting techniques of an issue tracker in a complex use context and a large data-set. The solution considers an integrated and holistic system view, practical applicability and utility, and the practical characteristics of issue data, such as inherent incompleteness.",Achievement,Capable,The study focuses on developing supportive solutions for the improved management of dependent issues in an issue tracker; contributing to the user's ability to manage tasks competently and thus aligns with the value item 'Capable' and its corresponding value 'Achievement'.,"In 'Paper X', the main contribution is the development of supportive solutions for managing dependent issues in an issue tracker. By addressing the challenge of dependencies and proposing the construction of an issue graph, the paper enables software users to effectively manage their tasks and ensure efficient progress in complex collaborative projects. This directly aligns with the value item 'Capable' from Schwartz's Taxonomy, as it empowers the users to competently handle and accomplish their software-related tasks. Furthermore, this aligns with the corresponding value of 'Achievement' since the solutions provided in the paper contribute to the users' ability to successfully complete their work and achieve their goals within the software context.",Agreed-Clarified,Agree,,Agree,,
Exp G,Exp K,2370,TSE,Software Project Management,Improved Management of Issue Dependencies in Issue Trackers of Large Collaborative Projects,"Issue trackers, such as Jira, have become the prevalent collaborative tools in software engineering for managing issues, such as requirements, development tasks, and software bugs. However, issue trackers inherently focus on the lifecycle of single issues, although issues have and express dependencies on other issues that constitute issue dependency networks in large complex collaborative projects. The objective of this study is to develop supportive solutions for the improved management of dependent issues in an issue tracker. This study follows the Design Science methodology, consisting of eliciting drawbacks and constructing and evaluating a solution and system. The study was carried out in the context of The Qt Company's Jira, which exemplifies an actively used, almost two-decade-old issue tracker with over 100,000 issues. The drawbacks capture how users operate with issue trackers to handle issue information in large, collaborative, and long-lived projects. The basis of the solution is to keep issues and dependencies as separate objects and automatically construct an issue graph. Dependency detections complement the issue graph by proposing missing dependencies, while consistency checks and diagnoses identify conflicting issue priorities and release assignments. Jira's plugin and service-based system architecture realize the functional and quality concerns of the system implementation. We show how to adopt the intelligent supporting techniques of an issue tracker in a complex use context and a large data-set. The solution considers an integrated and holistic system view, practical applicability and utility, and the practical characteristics of issue data, such as inherent incompleteness.",Security,Healthy,The paper aims to keep issues and dependencies as separate objects and automatically constructs an issue graph in order to ensure the efficient running of the issue tracker; which can be seen as contributing to the user's software 'Health'; aligning it with the value item 'Healthy' and corresponding value 'Security'.,"The justification for aligning 'Paper X' with the value item Healthy and its corresponding value Security is based on the idea that by keeping issues and dependencies as separate objects and automatically constructing an issue graph, the paper aims to ensure the efficient running of the issue tracker. This improved management of dependent issues can be seen as contributing to the user's overall software 'Health' by providing a more secure and reliable system for managing and resolving software issues.",Agreed-Justified,Disagree,Agree with coder_2,Disagree,Software health is not necessarily Human health,
Exp A,Exp H,2370,TSE,Software Project Management,Improved Management of Issue Dependencies in Issue Trackers of Large Collaborative Projects,"Issue trackers, such as Jira, have become the prevalent collaborative tools in software engineering for managing issues, such as requirements, development tasks, and software bugs. However, issue trackers inherently focus on the lifecycle of single issues, although issues have and express dependencies on other issues that constitute issue dependency networks in large complex collaborative projects. The objective of this study is to develop supportive solutions for the improved management of dependent issues in an issue tracker. This study follows the Design Science methodology, consisting of eliciting drawbacks and constructing and evaluating a solution and system. The study was carried out in the context of The Qt Company's Jira, which exemplifies an actively used, almost two-decade-old issue tracker with over 100,000 issues. The drawbacks capture how users operate with issue trackers to handle issue information in large, collaborative, and long-lived projects. The basis of the solution is to keep issues and dependencies as separate objects and automatically construct an issue graph. Dependency detections complement the issue graph by proposing missing dependencies, while consistency checks and diagnoses identify conflicting issue priorities and release assignments. Jira's plugin and service-based system architecture realize the functional and quality concerns of the system implementation. We show how to adopt the intelligent supporting techniques of an issue tracker in a complex use context and a large data-set. The solution considers an integrated and holistic system view, practical applicability and utility, and the practical characteristics of issue data, such as inherent incompleteness.",Universalism,Protecting the Environment,The support techniques proposed by the study inadvertently contribute to the protection of the software environment by ensuring a systematic approach to issue management and thus aligns with the value item 'Protecting the environment' and its corresponding value 'Universalism'.,"The justification provided aligning 'Paper X' with the value item Protecting the Environment and its corresponding value Universalism is based on the understanding that the proposed support techniques in the paper contribute to the overall environment of the software by improving issue management. By maintaining a systematic approach to issue tracking and ensuring the identification of dependencies and consistency checks, the paper indirectly helps in creating a more sustainable and harmonious software environment. This aligns with the value of Universalism, which emphasizes the importance of protecting and caring for the environment in a holistic and inclusive manner.",Agreed-Clarified,Agree,,Agree,,
Exp B,Exp H,2371,TSE,Accessibility & User Experience,Supporting Developers in Addressing Human-Centric Issues in Mobile Apps,"Failure to consider the characteristics, limitations, and abilities of diverse end-users during mobile app development may lead to problems for end-users, such as accessibility and usability issues. We refer to this class of problems as human-centric issues. Despite their importance, there is a limited understanding of the types of human-centric issues that are encountered by end-users and taken into account by the developers of mobile apps. In this paper, we examine what human-centric issues end-users report through Google App Store reviews, what human-centric issues are a topic of discussion for developers on GitHub, and whether end-users and developers discuss the same human-centric issues. We then investigate whether an automated tool might help detect such human-centric issues and whether developers would find such a tool useful. To do this, we conducted an empirical study by extracting and manually analysing a random sample of 1,200 app reviews and 1,200 issue comments from 12 diverse projects that exist on both Google App Store and GitHub. Our analysis led to a taxonomy of human-centric issues that characterises human-centric issues into three-high level categories: App Usage, Inclusiveness, and User Reaction. We then developed machine learning and deep learning models that are promising in automatically identifying and classifying human-centric issues from app reviews and developer discussions. A survey of mobile app developers shows that the automated detection of human-centric issues has practical applications. Guided by our findings, we highlight some implications and possible future work to further understand and better incorporate addressing human-centric issues into mobile app development.",Achievement,Intelligent,The paper develops automated tool to detect human-centric issues; demonstrating development of competence and intelligence in dealing with human-centric issues in mobile apps usage.,"The justification for aligning 'Paper X' with the value item Intelligent and its corresponding value Achievement is based on the development of an automated tool to detect human-centric issues in mobile apps. By creating this tool, the paper demonstrates the competence and intelligence of the developers in addressing and resolving these issues, which aligns with the value of Achievement. This automated tool shows their ability to understand and tackle complex problems, leading to the improvement of app usability and accessibility for end-users.",Agreed-Clarified,Agree,,Disagree,The justification doesn't align with the value item of the intelligent,
Exp D,Exp K,2371,TSE,Accessibility & User Experience,Supporting Developers in Addressing Human-Centric Issues in Mobile Apps,"Failure to consider the characteristics, limitations, and abilities of diverse end-users during mobile app development may lead to problems for end-users, such as accessibility and usability issues. We refer to this class of problems as human-centric issues. Despite their importance, there is a limited understanding of the types of human-centric issues that are encountered by end-users and taken into account by the developers of mobile apps. In this paper, we examine what human-centric issues end-users report through Google App Store reviews, what human-centric issues are a topic of discussion for developers on GitHub, and whether end-users and developers discuss the same human-centric issues. We then investigate whether an automated tool might help detect such human-centric issues and whether developers would find such a tool useful. To do this, we conducted an empirical study by extracting and manually analysing a random sample of 1,200 app reviews and 1,200 issue comments from 12 diverse projects that exist on both Google App Store and GitHub. Our analysis led to a taxonomy of human-centric issues that characterises human-centric issues into three-high level categories: App Usage, Inclusiveness, and User Reaction. We then developed machine learning and deep learning models that are promising in automatically identifying and classifying human-centric issues from app reviews and developer discussions. A survey of mobile app developers shows that the automated detection of human-centric issues has practical applications. Guided by our findings, we highlight some implications and possible future work to further understand and better incorporate addressing human-centric issues into mobile app development.",Conformity,Self-Discipline,The paper contributes to building self-discipline in the mobile app development process by pushing developers to consider and address the human-centric issues.,"In the paper abstract, it is stated that failure to consider the characteristics, limitations, and abilities of diverse end-users may lead to problems such as accessibility and usability issues. The paper aims to examine the human-centric issues reported by end-users and discussed by developers, and proposes the use of automated tools to detect and address these issues. By doing so, the paper emphasizes the importance of developers exercising self-discipline in considering and addressing the needs and issues of end-users. This aligns with the value item of Self-Discipline and its corresponding value of Conformity, as developers are encouraged to adhere to standards and guidelines in order to meet the expectations and requirements of software users.",Agreed-Justified,Agree,,Agree,,
Exp E,Exp J,2372,TSE,AI & Machine Learning,SynShine: Improved Fixing of Syntax Errors,"Novice programmers struggle with the complex syntax of modern programming languages like Java, and make lot of syntax errors. The diagnostic syntax error messages from compilers and IDEs are sometimes useful, but often the messages are cryptic and puzzling. Novices could be helped, and instructorsaEUR(tm) time saved, by automated repair suggestions when dealing with syntax errors. Large samples of novice errors and fixes are now available, offering the possibility of data-driven machine-learning approaches to help novices fix syntax errors. Current machine-learning approaches do a reasonable job fixing syntax errors in shorter programs, but don't work as well even for moderately longer programs. We introduce SynShine, a machine-learning based tool that substantially improves on the state-of-the-art, by learning to use compiler diagnostics, employing a very large neural model that leverages unsupervised pre-training, and relying on multi-label classification rather than autoregressive synthesis to generate the (repaired) output. We describe SynShine's architecture in detail, and provide a detailed evaluation. We have built SynShine into a free, open-source version of Visual Studio Code (VSCode); we make all our source code and models freely available.",Achievement,Intelligent,The paper introduces SynShine; a machine-learning based tool that helps improve the ability of novice programmers to fix syntax errors. This aligns with the value item Intelligent and its corresponding value Achievement as users of software could enhance their programming skills and intelligence by using this tool.,"Based on the information provided in the abstract, 'Paper X' introduces SynShine, a machine-learning tool that aids novice programmers in fixing syntax errors. This aligns with the value item Intelligent and its corresponding value Achievement because it enables software users to enhance their programming skills and intelligence by utilizing the tool. By providing automated repair suggestions for syntax errors, SynShine empowers novices to overcome complex programming challenges and improve their problem-solving abilities, ultimately leading to a sense of accomplishment and achievement in their programming endeavors.",Agreed-Justified,Agree,,Agree,,
Exp F,Exp J,2372,TSE,AI & Machine Learning,SynShine: Improved Fixing of Syntax Errors,"Novice programmers struggle with the complex syntax of modern programming languages like Java, and make lot of syntax errors. The diagnostic syntax error messages from compilers and IDEs are sometimes useful, but often the messages are cryptic and puzzling. Novices could be helped, and instructorsaEUR(tm) time saved, by automated repair suggestions when dealing with syntax errors. Large samples of novice errors and fixes are now available, offering the possibility of data-driven machine-learning approaches to help novices fix syntax errors. Current machine-learning approaches do a reasonable job fixing syntax errors in shorter programs, but don't work as well even for moderately longer programs. We introduce SynShine, a machine-learning based tool that substantially improves on the state-of-the-art, by learning to use compiler diagnostics, employing a very large neural model that leverages unsupervised pre-training, and relying on multi-label classification rather than autoregressive synthesis to generate the (repaired) output. We describe SynShine's architecture in detail, and provide a detailed evaluation. We have built SynShine into a free, open-source version of Visual Studio Code (VSCode); we make all our source code and models freely available.",Achievement,Successful,The tool SynShine significantly improves upon existing machine-learning approaches for fixing syntax errors. This contribution aligns with the value item Successful in the value Achievement since software users would be more successful in rectifying syntax errors when using this advanced tool.,"The contribution of the tool SynShine in improving upon existing machine-learning approaches for fixing syntax errors aligns with the value item Successful and its corresponding value Achievement from a ""Software User"" perspective. This alignment is based on the premise that the software users, who are not necessarily developers, would be more successful in rectifying syntax errors when utilizing this advanced tool. As novices struggle with syntax errors in programming languages, SynShine's ability to provide automated repair suggestions based on large samples of novice errors and fixes, leveraging machine learning techniques, would enhance the software user's success in effectively resolving syntax errors.",Agreed-Justified,Agree,,Agree,,
Exp B,Exp H,2373,TSE,Software Testing & QA,Data-Driven Mutation Analysis for Cyber-Physical Systems,"Cyber-physical systems (CPSs) typically consist of a wide set of integrated, heterogeneous components; consequently, most of their critical failures relate to the interoperability of such components. Unfortunately, most CPS test automation techniques are preliminary and industry still heavily relies on manual testing. With potentially incomplete, manually-generated test suites, it is of paramount importance to assess their quality. Though mutation analysis has demonstrated to be an effective means to assess test suite quality in some specific contexts, we lack approaches for CPSs. Indeed, existing approaches do not target interoperability problems and cannot be executed in the presence of black-box or simulated components, a typical situation with CPSs. In this article, we introduce data-driven mutation analysis, an approach that consists in assessing test suite quality by verifying if it detects interoperability faults simulated by mutating the data exchanged by software components. To this end, we describe a data-driven mutation analysis technique (DaMAT) that automatically alters the data exchanged through data buffers. Our technique is driven by fault models in tabular form where engineers specify how to mutate data items by selecting and configuring a set of mutation operators. We have evaluated DaMAT with CPSs in the space domain; specifically, the test suites for the software systems of a microsatellite and nanosatellites launched on orbit last year. Our results show that the approach effectively detects test suite shortcomings, is not affected by equivalent and redundant mutants, and entails acceptable costs.",Achievement,Successful,By ensuring that software systems are thoroughly and correctly tested; users can successfully accomplish tasks using the software products without critical failures. This guarantees a proficient accomplishment of user goals aligning the contribution with the value item Successful and the corresponding value Achievement.,"By focusing on test suite quality and the detection of interoperability faults in cyber-physical systems, 'Paper X' aims to ensure that software systems are thoroughly and correctly tested. This directly aligns with the value item Successful from Schwartz's Taxonomy, as it emphasizes the importance of achieving successful outcomes in the use of software products. By detecting and preventing critical failures through effective testing, users can successfully accomplish their tasks and achieve their goals when using these software products. This alignment with the value item Successful ultimately contributes to the value of Achievement, as users are able to proficiently achieve their desired outcomes with the software.",Agreed-Justified,Agree,,Agree,,
Exp C,Exp K,2373,TSE,Software Testing & QA,Data-Driven Mutation Analysis for Cyber-Physical Systems,"Cyber-physical systems (CPSs) typically consist of a wide set of integrated, heterogeneous components; consequently, most of their critical failures relate to the interoperability of such components. Unfortunately, most CPS test automation techniques are preliminary and industry still heavily relies on manual testing. With potentially incomplete, manually-generated test suites, it is of paramount importance to assess their quality. Though mutation analysis has demonstrated to be an effective means to assess test suite quality in some specific contexts, we lack approaches for CPSs. Indeed, existing approaches do not target interoperability problems and cannot be executed in the presence of black-box or simulated components, a typical situation with CPSs. In this article, we introduce data-driven mutation analysis, an approach that consists in assessing test suite quality by verifying if it detects interoperability faults simulated by mutating the data exchanged by software components. To this end, we describe a data-driven mutation analysis technique (DaMAT) that automatically alters the data exchanged through data buffers. Our technique is driven by fault models in tabular form where engineers specify how to mutate data items by selecting and configuring a set of mutation operators. We have evaluated DaMAT with CPSs in the space domain; specifically, the test suites for the software systems of a microsatellite and nanosatellites launched on orbit last year. Our results show that the approach effectively detects test suite shortcomings, is not affected by equivalent and redundant mutants, and entails acceptable costs.",Security,Healthy,The paper contributes a method that leads to testing the interoperability faults in cyber-physical systems effectively. This can contribute to user's well-being by providing a safe and error-free software environement. This aligns with the value item Healthy and its corresponding value Security.,"In the context of the paper's contributions, the alignment with the value item ""Healthy"" and its corresponding value ""Security"" is justified because the proposed method of data-driven mutation analysis allows for the effective detection of interoperability faults in cyber-physical systems. By improving the quality of test suites and identifying potential failures, this method aims to ensure a safe and error-free software environment. This directly aligns with the value of security, as it addresses the importance of protecting users from potential harm or risks associated with faulty software systems. Overall, the paper's focus on enhancing system reliability and security aligns with the value item ""Healthy"" and its corresponding value ""Security"" from the perspective of a software user.",Agreed-Clarified,Agree,,Agree,,
Exp D,Exp K,2381,TSE,Software Testing & QA,Leveraging Android Automated Testing to Assist Crowdsourced Testing,"Crowdsourced testing is an emerging trend in mobile application testing. The openness of crowdsourced testing provides a promising way to conduct large-scale and user-oriented testing scenarios on various mobile devices, while it also brings a problem, i.e., crowdworkers with different levels of testing experience severely threaten the quality of crowdsourced testing. Currently, many approaches have been proposed and studied to improve crowdsourced testing. However, these approaches do not fundamentally improve the ability of crowdworkers. In essence, the low-quality crowdsourced testing is caused by crowdworkers who are unfamiliar with the App Under Test (AUT) and do not know which part of the AUT should be tested. To address this problem, we propose a testing assistance approach, which leverages Android automated testing (i.e., dynamic and static analysis) to improve crowdsourced testing. Our approach constructs an Annotated Window Transition Graph (AWTG) model for the AUT by merging dynamic and static analysis results. Based on the AWTG model, our approach implements a testing assistance pipeline that provides the test task extraction, test task recommendation, and test task guidance to assist crowdworkers in testing the AUT. We experimentally evaluate our approach on real-world AUTs. The quantitative results demonstrate that our approach can effectively and efficiently assist crowdsourced testing. Besides, the qualitative results from a user study confirm the usefulness of our approach.",Achievement,Intelligent,The automated testing approach proposed in Paper X is designed to assist crowdworkers in testing the App Under Test (AUT). This helps users exhibit their intelligence and competence in testing; which aligns with the value item Intelligent and its corresponding value Achievement.,"In 'Paper X', the proposed testing assistance approach utilizes automated testing to improve crowdsourced testing. By providing crowdworkers with guidance and recommendations, the approach helps them demonstrate their intelligence and competence in testing the App Under Test (AUT). This aligns with the value item Intelligent and its corresponding value Achievement as it enables users to showcase their abilities and achieve successful testing outcomes.",Agreed-Justified,Agree,,Agree,,
Exp E,Exp J,2381,TSE,Software Testing & QA,Leveraging Android Automated Testing to Assist Crowdsourced Testing,"Crowdsourced testing is an emerging trend in mobile application testing. The openness of crowdsourced testing provides a promising way to conduct large-scale and user-oriented testing scenarios on various mobile devices, while it also brings a problem, i.e., crowdworkers with different levels of testing experience severely threaten the quality of crowdsourced testing. Currently, many approaches have been proposed and studied to improve crowdsourced testing. However, these approaches do not fundamentally improve the ability of crowdworkers. In essence, the low-quality crowdsourced testing is caused by crowdworkers who are unfamiliar with the App Under Test (AUT) and do not know which part of the AUT should be tested. To address this problem, we propose a testing assistance approach, which leverages Android automated testing (i.e., dynamic and static analysis) to improve crowdsourced testing. Our approach constructs an Annotated Window Transition Graph (AWTG) model for the AUT by merging dynamic and static analysis results. Based on the AWTG model, our approach implements a testing assistance pipeline that provides the test task extraction, test task recommendation, and test task guidance to assist crowdworkers in testing the AUT. We experimentally evaluate our approach on real-world AUTs. The quantitative results demonstrate that our approach can effectively and efficiently assist crowdsourced testing. Besides, the qualitative results from a user study confirm the usefulness of our approach.",Achievement,Capable,By assisting crowdworkers in testing the AUT effectively and efficiently; Paper X helps these users to become capable at their tasks; aligning with the value item Capable and its corresponding value Achievement.,"In 'Paper X', the proposed testing assistance approach aims to improve the quality of crowdsourced testing by assisting crowdworkers in testing the App Under Test effectively and efficiently. By providing test task extraction, recommendation, and guidance, the approach helps crowdworkers become capable and proficient in their testing tasks. This alignment with the value item Capable and its corresponding value Achievement is evident as it focuses on enhancing the abilities and skills of the software user in successfully conducting the testing process, ultimately leading to the attainment of their testing goals and achievements.",Agreed-Justified,Agree,,Agree,,
Exp F,Exp J,2381,TSE,Software Testing & QA,Leveraging Android Automated Testing to Assist Crowdsourced Testing,"Crowdsourced testing is an emerging trend in mobile application testing. The openness of crowdsourced testing provides a promising way to conduct large-scale and user-oriented testing scenarios on various mobile devices, while it also brings a problem, i.e., crowdworkers with different levels of testing experience severely threaten the quality of crowdsourced testing. Currently, many approaches have been proposed and studied to improve crowdsourced testing. However, these approaches do not fundamentally improve the ability of crowdworkers. In essence, the low-quality crowdsourced testing is caused by crowdworkers who are unfamiliar with the App Under Test (AUT) and do not know which part of the AUT should be tested. To address this problem, we propose a testing assistance approach, which leverages Android automated testing (i.e., dynamic and static analysis) to improve crowdsourced testing. Our approach constructs an Annotated Window Transition Graph (AWTG) model for the AUT by merging dynamic and static analysis results. Based on the AWTG model, our approach implements a testing assistance pipeline that provides the test task extraction, test task recommendation, and test task guidance to assist crowdworkers in testing the AUT. We experimentally evaluate our approach on real-world AUTs. The quantitative results demonstrate that our approach can effectively and efficiently assist crowdsourced testing. Besides, the qualitative results from a user study confirm the usefulness of our approach.",Achievement,Successful,The considerations for the crowdworkers' success in testing the AUT; as demonstrated by the testing assistance approach in Paper X; align with the value item Successful and its corresponding value Achievement.,"In 'Paper X', the proposed testing assistance approach aims to improve the effectiveness and efficiency of crowdsourced testing. By providing test task extraction, recommendation, and guidance to crowdworkers, the approach increases their ability to successfully test the App Under Test (AUT). This focus on improving the crowdworkers' capabilities aligns with the value item Successful from Schwartz's Taxonomy. By facilitating the crowdworkers' success in testing the AUT, the approach contributes to the value of Achievement, as it enables the crowdworkers to fulfill their testing tasks successfully and make valuable contributions to the overall testing process.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2385,TSE,Software Development Methodologies,A Software Requirements Ecosystem: Linking Forum Issue Tracker and FAQs for Requirements Management,"User feedback is an important resource in modern software development, often containing requirements that help address user concerns and desires for a software product. The feedback in online channels is a recent focus for software engineering researchers, with multiple studies proposing automatic analysis tools. In this work, we investigate the product forums of two large open source software projects. Through a quantitative analysis, we show that forum feedback is often manually linked to related issue tracker entries and product documentation. By linking feedback to their existing documentation, development teams enhance their understanding of known issues, and direct their users to known solutions. We discuss how the links between forum, issue tracker, and product documentation form a requirements ecosystem that has not been identified in the previous literature. We apply state-of-the-art deep-learning to automatically match forum posts with related issue tracker entries. Our approach identifies requirement matches with a mean average precision of 58.9% and hit ratio of 82.2%. Additionally, we apply deep-learning using an innovative clustering technique, achieving promising performance when matching forum posts to related product documentation. We discuss the possible applications of these automated techniques to support the flow of requirements between forum, issue tracker, and product documentation.",Security,Reciprocation of Favors,"The paper proposes automatic deep-learning methods to link forum posts with related issue tracker entries; essentially supporting Reciprocation of Favors in terms of acknowledging and responding to user concerns; corresponding to the value ""Security"".","In the 'Paper X' abstract, the authors discuss how the product forums of open-source software projects link user feedback to issue tracker entries and product documentation. This process enhances the understanding of known issues and directs users to known solutions. By applying deep-learning techniques, the paper aims to automatically match forum posts with related issue tracker entries and product documentation. This aligns with the value item Reciprocation of Favors and the corresponding value Security as it acknowledges and responds to user concerns, providing a sense of security to the software users by addressing their feedback and offering solutions to known issues.",Agreed-Clarified,Disagree,"Related to Achievement: capable more, as it link forum post with bug trackers",Disagree,The justification doesn't align with the value item of the Reciprocation of Favors,
Exp B,Exp H,2385,TSE,Software Development Methodologies,A Software Requirements Ecosystem: Linking Forum Issue Tracker and FAQs for Requirements Management,"User feedback is an important resource in modern software development, often containing requirements that help address user concerns and desires for a software product. The feedback in online channels is a recent focus for software engineering researchers, with multiple studies proposing automatic analysis tools. In this work, we investigate the product forums of two large open source software projects. Through a quantitative analysis, we show that forum feedback is often manually linked to related issue tracker entries and product documentation. By linking feedback to their existing documentation, development teams enhance their understanding of known issues, and direct their users to known solutions. We discuss how the links between forum, issue tracker, and product documentation form a requirements ecosystem that has not been identified in the previous literature. We apply state-of-the-art deep-learning to automatically match forum posts with related issue tracker entries. Our approach identifies requirement matches with a mean average precision of 58.9% and hit ratio of 82.2%. Additionally, we apply deep-learning using an innovative clustering technique, achieving promising performance when matching forum posts to related product documentation. We discuss the possible applications of these automated techniques to support the flow of requirements between forum, issue tracker, and product documentation.",Self Direction,Self Respect,"By incorporating user inputs into software development and acknowledging their needs; the software potentially contributes to enhancing the user's Self Respect; aligning with the value ""Self Direction"".","By incorporating user feedback and addressing their concerns and desires, the software development process becomes more user-centric and allows users to have a sense of control and independence in shaping the product to better serve their needs. This aligns with the value item of Self Direction, as users are given the opportunity to actively participate and have a say in the development of the software, thereby enhancing their self-respect.",Agreed-Clarified,Agree,,Agree,,
Exp C,Exp K,2388,TSE,AI & Machine Learning,FairMask: Better Fairness via Model-Based Rebalancing of Protected Attributes,"Context: Machine learning software can generate models that inappropriately discriminate against specific protected social groups (e.g., groups based on gender, ethnicity, etc.). Motivated by those results, software engineering researchers have proposed many methods for mitigating those discriminatory effects. While those methods are effective in mitigating bias, few of them can provide explanations on what is the root cause of bias. Objective: We aim to better detect and mitigate algorithmic discrimination in machine learning software problems. Method: Here we propose ${{\sf FairMask}}$FairMask, a model-based extrapolation method that is capable of both mitigating bias and explaining the cause. In our ${{\sf FairMask}}$FairMask approach, protected attributes are represented by models learned from the other independent variables (and these models offer extrapolations over the space between existing examples). We then use the extrapolation models to relabel protected attributes later seen in testing data or deployment time. Our approach aims to offset the biased predictions of the classification model by rebalancing the distribution of protected attributes. Results: The experiments of this paper show that, without compromising (original) model performance, ${{\sf FairMask}}$FairMask can achieve significantly better group and individual fairness (as measured in different metrics) than benchmark methods. Moreover, compared to another instance-based rebalancing method, our model-based approach shows faster runtime and thus better scalability. Conclusion: Algorithmic decision bias can be removed via extrapolation that corrects the misleading latent correlation between the protected attributes and other non-protected ones. As evidence for this, our proposed ${{\sf FairMask}}$FairMask is not only performance-wise better (measured by fairness and performance metrics) than two state-of-the-art fairness algorithms. Reproduction Package: In order to better support open science, all scripts and data used in this study are available online at https://github.com/anonymous12138/biasmitigation.",Benevolence,Meaning in Life,"The paper presents FairMask; an approach that mitigates bias in machine learning software; contributing to ""Meaning in Life"" by facilitating more fair and equitable outcomes in software. This aligns with the value Benevolence.","The paper's approach, FairMask, directly addresses the issue of algorithmic discrimination in machine learning software, which can have negative consequences for marginalized groups. By mitigating bias and promoting fair and equitable outcomes, the paper contributes to the value of ""Meaning in Life"" by prioritizing the well-being and inclusiveness of all software users. This aligns with the value of Benevolence, as it demonstrates a concern for the welfare of others and a commitment to social justice within the software context.",Agreed-Justified,Agree,,Agree,,
Exp D,Exp K,2388,TSE,AI & Machine Learning,FairMask: Better Fairness via Model-Based Rebalancing of Protected Attributes,"Context: Machine learning software can generate models that inappropriately discriminate against specific protected social groups (e.g., groups based on gender, ethnicity, etc.). Motivated by those results, software engineering researchers have proposed many methods for mitigating those discriminatory effects. While those methods are effective in mitigating bias, few of them can provide explanations on what is the root cause of bias. Objective: We aim to better detect and mitigate algorithmic discrimination in machine learning software problems. Method: Here we propose ${{\sf FairMask}}$FairMask, a model-based extrapolation method that is capable of both mitigating bias and explaining the cause. In our ${{\sf FairMask}}$FairMask approach, protected attributes are represented by models learned from the other independent variables (and these models offer extrapolations over the space between existing examples). We then use the extrapolation models to relabel protected attributes later seen in testing data or deployment time. Our approach aims to offset the biased predictions of the classification model by rebalancing the distribution of protected attributes. Results: The experiments of this paper show that, without compromising (original) model performance, ${{\sf FairMask}}$FairMask can achieve significantly better group and individual fairness (as measured in different metrics) than benchmark methods. Moreover, compared to another instance-based rebalancing method, our model-based approach shows faster runtime and thus better scalability. Conclusion: Algorithmic decision bias can be removed via extrapolation that corrects the misleading latent correlation between the protected attributes and other non-protected ones. As evidence for this, our proposed ${{\sf FairMask}}$FairMask is not only performance-wise better (measured by fairness and performance metrics) than two state-of-the-art fairness algorithms. Reproduction Package: In order to better support open science, all scripts and data used in this study are available online at https://github.com/anonymous12138/biasmitigation.",Universalism,Social Justice,"By reducing algorithmic discrimination; this paper contributes to the value item ""Social Justice"" in the Universalism category. The method proposed enforces fairness; ensuring equal treatment of all social groups.","In the context of 'Paper X', the alignment with the value item Social Justice and its corresponding value Universalism is evident through the paper's objective of mitigating algorithmic discrimination and achieving fairness. By addressing bias and promoting equal treatment of all social groups, the paper directly contributes to creating a more just and equitable society within the software domain.",Agreed-Clarified,Agree,,Agree,,
Exp E,Exp J,2388,TSE,AI & Machine Learning,FairMask: Better Fairness via Model-Based Rebalancing of Protected Attributes,"Context: Machine learning software can generate models that inappropriately discriminate against specific protected social groups (e.g., groups based on gender, ethnicity, etc.). Motivated by those results, software engineering researchers have proposed many methods for mitigating those discriminatory effects. While those methods are effective in mitigating bias, few of them can provide explanations on what is the root cause of bias. Objective: We aim to better detect and mitigate algorithmic discrimination in machine learning software problems. Method: Here we propose ${{\sf FairMask}}$FairMask, a model-based extrapolation method that is capable of both mitigating bias and explaining the cause. In our ${{\sf FairMask}}$FairMask approach, protected attributes are represented by models learned from the other independent variables (and these models offer extrapolations over the space between existing examples). We then use the extrapolation models to relabel protected attributes later seen in testing data or deployment time. Our approach aims to offset the biased predictions of the classification model by rebalancing the distribution of protected attributes. Results: The experiments of this paper show that, without compromising (original) model performance, ${{\sf FairMask}}$FairMask can achieve significantly better group and individual fairness (as measured in different metrics) than benchmark methods. Moreover, compared to another instance-based rebalancing method, our model-based approach shows faster runtime and thus better scalability. Conclusion: Algorithmic decision bias can be removed via extrapolation that corrects the misleading latent correlation between the protected attributes and other non-protected ones. As evidence for this, our proposed ${{\sf FairMask}}$FairMask is not only performance-wise better (measured by fairness and performance metrics) than two state-of-the-art fairness algorithms. Reproduction Package: In order to better support open science, all scripts and data used in this study are available online at https://github.com/anonymous12138/biasmitigation.",Security,Social Order,"The paper offers a method to provide social order by minimizing bias and discrimination; aligning with the value item ""Social Order"" and corresponding value ""Security"". This could potentially contribute to a more organized and harmonious social environment where software is used.","The main contribution of 'Paper X' is its proposal of a model-based extrapolation method, called FairMask, which mitigates bias and discrimination in machine learning software. By using extrapolation to relabel protected attributes, the approach aims to rebalance the distribution of protected attributes and offset biased predictions. This mitigation of algorithmic discrimination aligns with the value item of Social Order and its corresponding value of Security. By reducing bias and discrimination, FairMask contributes to a more fair and equitable social order within software systems, promoting a sense of security and harmony in their usage.",Agreed-Clarified,Agree,,Agree,,
Exp F,Exp J,2393,TSE,Software Testing & QA,New Reliability-Driven Bounds for Architecture-Based Multi-Objective Testing Resource Allocation,"The multi-objective testing resource allocation problem (MOTRAP) aims at seeking a good trade-off between system reliability, testing cost, and testing time, which is of significant importance to facilitate the testing planning. Yet most studies focus on the time constraint but rarely consider the practical reliability requirement. In this work, we address MOTRAP on an architecture-based model (ABM) with the personalized preference over reliability. More specifically, we first present a reliability-constrained MOTRAP model on the basis of ABM and illustrate how to use this model for real-world systems. Then, to leverage the problem's knowledge, we develop new lower and upper bounds on testing time invested in different components from both theoretical and algorithmic perspectives on the basis of the Lagrange multiplier and half-interval search. Importantly, these new derived bounds have strong implications due to the fact that they can be easily employed by optimizers as the limits of variables to prune the search space to the region of interests of the decision maker and locate feasible solutions with the expected reliability. Finally, we evaluate the proposed bounds in popular multi-objective optimizers for MOTRAP on application and empirical cases. Experimental results demonstrate that our new bounds practically improve the search performance of optimizers, and decision makers can easily combine these new bounds with off-the-shelf optimizers to find higher-quality solutions that they are interested in, which greatly soothes away stress on optimizer and solution selections of decision makers.",Achievement,Successful,The paper focuses on helping the software user find higher-quality solutions using the proposed new bounds and thus aligns with the value item Successful and its corresponding value Achievement.,"The main contribution of 'Paper X' is providing new bounds on testing time that can improve the search performance of optimizers and help software users find higher-quality solutions. By enabling users to achieve their desired reliability and optimize their testing process, the paper aligns with the value item Successful and its corresponding value Achievement.",Agreed-Clarified,Agree,,Agree,,
Exp G,Exp K,2393,TSE,Software Testing & QA,New Reliability-Driven Bounds for Architecture-Based Multi-Objective Testing Resource Allocation,"The multi-objective testing resource allocation problem (MOTRAP) aims at seeking a good trade-off between system reliability, testing cost, and testing time, which is of significant importance to facilitate the testing planning. Yet most studies focus on the time constraint but rarely consider the practical reliability requirement. In this work, we address MOTRAP on an architecture-based model (ABM) with the personalized preference over reliability. More specifically, we first present a reliability-constrained MOTRAP model on the basis of ABM and illustrate how to use this model for real-world systems. Then, to leverage the problem's knowledge, we develop new lower and upper bounds on testing time invested in different components from both theoretical and algorithmic perspectives on the basis of the Lagrange multiplier and half-interval search. Importantly, these new derived bounds have strong implications due to the fact that they can be easily employed by optimizers as the limits of variables to prune the search space to the region of interests of the decision maker and locate feasible solutions with the expected reliability. Finally, we evaluate the proposed bounds in popular multi-objective optimizers for MOTRAP on application and empirical cases. Experimental results demonstrate that our new bounds practically improve the search performance of optimizers, and decision makers can easily combine these new bounds with off-the-shelf optimizers to find higher-quality solutions that they are interested in, which greatly soothes away stress on optimizer and solution selections of decision makers.",Security,Healthy,The paper's approach aids in ensuring system reliability; which the user may identify as related to their security and health; aligning with the value item Healthy and its corresponding value Security.,"In the context of a software user, the alignment of 'Paper X' with the value item Healthy and its corresponding value Security stems from its focus on system reliability. A reliable software system can be perceived by users as secure and trustworthy, contributing to their peace of mind and overall well-being. Therefore, the emphasis on ensuring system reliability in 'Paper X' aligns with the value item Healthy and its corresponding value Security, as it addresses the user's concerns for a secure and stable software experience.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2393,TSE,Software Testing & QA,New Reliability-Driven Bounds for Architecture-Based Multi-Objective Testing Resource Allocation,"The multi-objective testing resource allocation problem (MOTRAP) aims at seeking a good trade-off between system reliability, testing cost, and testing time, which is of significant importance to facilitate the testing planning. Yet most studies focus on the time constraint but rarely consider the practical reliability requirement. In this work, we address MOTRAP on an architecture-based model (ABM) with the personalized preference over reliability. More specifically, we first present a reliability-constrained MOTRAP model on the basis of ABM and illustrate how to use this model for real-world systems. Then, to leverage the problem's knowledge, we develop new lower and upper bounds on testing time invested in different components from both theoretical and algorithmic perspectives on the basis of the Lagrange multiplier and half-interval search. Importantly, these new derived bounds have strong implications due to the fact that they can be easily employed by optimizers as the limits of variables to prune the search space to the region of interests of the decision maker and locate feasible solutions with the expected reliability. Finally, we evaluate the proposed bounds in popular multi-objective optimizers for MOTRAP on application and empirical cases. Experimental results demonstrate that our new bounds practically improve the search performance of optimizers, and decision makers can easily combine these new bounds with off-the-shelf optimizers to find higher-quality solutions that they are interested in, which greatly soothes away stress on optimizer and solution selections of decision makers.",Power,Social Recognition,By enabling the software user to find optimal solutions which can be of high importance in their work or tasks; the user might gain recognition among their peers or supervisors; which aligns with the value item Social Recognition and its corresponding value Power.,"In 'Paper X', the main contribution is focused on developing new bounds that improve the search performance of optimizers in the multi-objective testing resource allocation problem. By providing higher-quality solutions that are aligned with the software user's interests and needs, the user may achieve optimal results that can be recognized and valued by their peers and supervisors. This aligns with the value item Social Recognition and its corresponding value Power, as the software user can gain recognition and influence through their successful and efficient testing processes.",Agreed-Clarified,Agree,,Agree,,
Exp B,Exp H,2399,TSE,AI & Machine Learning,Sketch2Process: End-to-End BPMN Sketch Recognition Based on Neural Networks,"Process models play an important role in various software engineering contexts. Among others, they are used to capture business-related requirements and provide the basis for the development of process-oriented applications in low-code/no-code settings. To support modelers in creating, checking, and maintaining process models, dedicated tools are available. While these tools are generally considered as indispensable to capture process models for their later use, the initial version of a process model is often sketched on a whiteboard or a piece of paper. This has been found to have great advantages, especially with respect to communication and collaboration. It, however, also creates the need to subsequently transform the model sketch into a digital counterpart that can be further processed by modeling and analysis tools. Therefore, to automate this task, various so-called sketch recognition approaches have been defined in the past. Yet, these existing approaches are too limited for use in practice, since they, for instance, require sketches to be created on a digital device or do not address the recognition of edges or textual labels. Against this background, we use this paper to introduce Sketch2Process, the first end-to-end sketch recognition approach for process models captured using BPMN. Sketch2Process uses a neural network-based architecture to recognize the shapes, edges, and textual labels of highly expressive process models, covering 25 types of BPMN elements. To train and evaluate our approach, we created a dataset consisting of 704 hand-drawn and manually annotated BPMN models. Our experiments demonstrate that our approach is highly accurate and consistently outperforms the state of the art.",Self Direction,Freedom,The paper introduces Sketch2Process; an end-to-end sketch recognition approach that automates the transformation of hand-drawn process models into a digital format. This contributes to Freedom; allowing software users the choice and convenience to create initial process models on any medium they prefer (e.g.; sketches on whiteboards or papers) without requiring digital devices; as previous approaches do. This aligns with the value Self Direction.,"The justification for aligning 'Paper X' with the value item Freedom and its corresponding value Self Direction from a ""Software User"" perspective is that the Sketch2Process approach introduced in the paper allows software users the freedom to choose their preferred medium for creating initial process models, such as sketches on whiteboards or papers. This is in contrast to previous approaches that require the use of digital devices. By offering the flexibility to work with non-digital mediums, the paper enables software users to exercise their self-direction in the process modeling task, aligning with the value of Self Direction.",Agreed-Justified,Agree,,Agree,,
Exp C,Exp K,2399,TSE,AI & Machine Learning,Sketch2Process: End-to-End BPMN Sketch Recognition Based on Neural Networks,"Process models play an important role in various software engineering contexts. Among others, they are used to capture business-related requirements and provide the basis for the development of process-oriented applications in low-code/no-code settings. To support modelers in creating, checking, and maintaining process models, dedicated tools are available. While these tools are generally considered as indispensable to capture process models for their later use, the initial version of a process model is often sketched on a whiteboard or a piece of paper. This has been found to have great advantages, especially with respect to communication and collaboration. It, however, also creates the need to subsequently transform the model sketch into a digital counterpart that can be further processed by modeling and analysis tools. Therefore, to automate this task, various so-called sketch recognition approaches have been defined in the past. Yet, these existing approaches are too limited for use in practice, since they, for instance, require sketches to be created on a digital device or do not address the recognition of edges or textual labels. Against this background, we use this paper to introduce Sketch2Process, the first end-to-end sketch recognition approach for process models captured using BPMN. Sketch2Process uses a neural network-based architecture to recognize the shapes, edges, and textual labels of highly expressive process models, covering 25 types of BPMN elements. To train and evaluate our approach, we created a dataset consisting of 704 hand-drawn and manually annotated BPMN models. Our experiments demonstrate that our approach is highly accurate and consistently outperforms the state of the art.",Stimulation,Daring,Sketch2Process uses a novel neural network-based architecture for recognizing shapes; edges; and textual labels of process models. This introduces a daring and innovative approach to sketch recognition; which enables the capture of highly expressive process models covering more types of BPMN elements than previous methods. This aligns with the value Stimulation which invigorates the software user experience.,"The justification for aligning 'Paper X' with the value item Daring and its corresponding value Stimulation is based on the innovative approach introduced by Sketch2Process. By using a novel neural network-based architecture to recognize shapes, edges, and textual labels of process models, the paper presents a daring and novel solution that goes beyond previous methods. This brings stimulation to the software user experience by invigorating the process model capture process and allowing for the creation of highly expressive models covering a wider range of BPMN elements.",Agreed-Justified,Agree,,Agree,,
Exp D,Exp K,2399,TSE,AI & Machine Learning,Sketch2Process: End-to-End BPMN Sketch Recognition Based on Neural Networks,"Process models play an important role in various software engineering contexts. Among others, they are used to capture business-related requirements and provide the basis for the development of process-oriented applications in low-code/no-code settings. To support modelers in creating, checking, and maintaining process models, dedicated tools are available. While these tools are generally considered as indispensable to capture process models for their later use, the initial version of a process model is often sketched on a whiteboard or a piece of paper. This has been found to have great advantages, especially with respect to communication and collaboration. It, however, also creates the need to subsequently transform the model sketch into a digital counterpart that can be further processed by modeling and analysis tools. Therefore, to automate this task, various so-called sketch recognition approaches have been defined in the past. Yet, these existing approaches are too limited for use in practice, since they, for instance, require sketches to be created on a digital device or do not address the recognition of edges or textual labels. Against this background, we use this paper to introduce Sketch2Process, the first end-to-end sketch recognition approach for process models captured using BPMN. Sketch2Process uses a neural network-based architecture to recognize the shapes, edges, and textual labels of highly expressive process models, covering 25 types of BPMN elements. To train and evaluate our approach, we created a dataset consisting of 704 hand-drawn and manually annotated BPMN models. Our experiments demonstrate that our approach is highly accurate and consistently outperforms the state of the art.",Achievement,Capable,Our experiments demonstrate that our approach is highly accurate and consistently outperforms the state of the art; which directly empowers the software users by ensuring the reliability and effectiveness of process model creation and transformation; making them capable of performing their tasks more proficiently. This aligns with the value Achievement which represents the aspiration of software users to excel.,"The justification provided aligns with the value item Capable and its corresponding value Achievement from a ""Software User"" perspective because the experiments conducted in the paper demonstrate that the approach is highly accurate and consistently outperforms existing methods. This directly empowers software users by ensuring the reliability and effectiveness of process model creation and transformation, allowing them to excel and perform their tasks more proficiently.",Agreed-Justified,Agree,,Agree,,
Exp E,Exp J,2407,TSE,AI & Machine Learning,Learning Approximate Execution Semantics From Traces for Binary Function Similarity,"Detecting semantically similar binary functions aEUR"" a crucial capability with broad security usages including vulnerability detection, malware analysis, and forensics aEUR"" requires understanding function behaviors and intentions. This task is challenging as semantically similar functions can be compiled to run on different architectures and with diverse compiler optimizations or obfuscations. Most existing approaches match functions based on syntactic features without understanding the functionsaEUR(tm) execution semantics. We present Trex, a transfer-learning-based framework, to automate learning approximate execution semantics explicitly from functionsaEUR(tm) traces collected via forced-execution (i.e., by violating the control flow semantics) and transfer the learned knowledge to match semantically similar functions. While it is known that forced-execution traces are too imprecise to be directly used to detect semantic similarity, our key insight is that these traces can instead be used to teach an ML model approximate execution semantics of diverse instructions and their compositions. We thus design a pretraining task, which trains the model to learn approximate execution semantics from the two modalities (i.e., forced-executed code and traces) of the function. We then finetune the pretrained model to match semantically similar functions. We evaluate Trex on 1,472,066 functions from 13 popular software projects, compiled to run on 4 architectures (x86, x64, ARM, and MIPS), and with 4 optimizations (O0-O3) and 5 obfuscations. Trex outperforms the state-of-the-art solutions by 7.8%, 7.2%, and 14.3% in cross-architecture, optimization, and obfuscation function matching, respectively, while running 8'-- faster. Ablation studies suggest that the pretraining significantly boosts the function matching performance, underscoring the importance of learning execution semantics. Our case studies demonstrate the practical use-cases of Trex aEUR"" on 180 real-world firmware images, Trex uncovers 14 vulnerabilities not disclosed by previous studies. We release the code and dataset of Trex at https://github.com/CUMLSec/trex.",Achievement,Influential,The paper presents a framework (Trex) which significantly boosts the function matching performance; improving their effectiveness in tasks such as vulnerability detection; malware analysis; and forensics. This aligns with the value item Influential and its corresponding value Achievement as it enhances the capability of the user in these tasks.,"The justification for aligning 'Paper X' with the value item Influential and its corresponding value Achievement is based on the fact that the paper's framework, Trex, enhances the user's capability in tasks such as vulnerability detection, malware analysis, and forensics. By improving the function matching performance, Trex enables users to have a significant impact in their field of work, thereby aligning with the value of Achievement. The ability to influence and achieve results in these critical security tasks demonstrates the alignment with the value item Influential.",Agreed-Clarified,Agree,,Agree,,
Exp F,Exp J,2407,TSE,AI & Machine Learning,Learning Approximate Execution Semantics From Traces for Binary Function Similarity,"Detecting semantically similar binary functions aEUR"" a crucial capability with broad security usages including vulnerability detection, malware analysis, and forensics aEUR"" requires understanding function behaviors and intentions. This task is challenging as semantically similar functions can be compiled to run on different architectures and with diverse compiler optimizations or obfuscations. Most existing approaches match functions based on syntactic features without understanding the functionsaEUR(tm) execution semantics. We present Trex, a transfer-learning-based framework, to automate learning approximate execution semantics explicitly from functionsaEUR(tm) traces collected via forced-execution (i.e., by violating the control flow semantics) and transfer the learned knowledge to match semantically similar functions. While it is known that forced-execution traces are too imprecise to be directly used to detect semantic similarity, our key insight is that these traces can instead be used to teach an ML model approximate execution semantics of diverse instructions and their compositions. We thus design a pretraining task, which trains the model to learn approximate execution semantics from the two modalities (i.e., forced-executed code and traces) of the function. We then finetune the pretrained model to match semantically similar functions. We evaluate Trex on 1,472,066 functions from 13 popular software projects, compiled to run on 4 architectures (x86, x64, ARM, and MIPS), and with 4 optimizations (O0-O3) and 5 obfuscations. Trex outperforms the state-of-the-art solutions by 7.8%, 7.2%, and 14.3% in cross-architecture, optimization, and obfuscation function matching, respectively, while running 8'-- faster. Ablation studies suggest that the pretraining significantly boosts the function matching performance, underscoring the importance of learning execution semantics. Our case studies demonstrate the practical use-cases of Trex aEUR"" on 180 real-world firmware images, Trex uncovers 14 vulnerabilities not disclosed by previous studies. We release the code and dataset of Trex at https://github.com/CUMLSec/trex.",Security,Healthy,Trex uncovers 14 vulnerabilities not disclosed by previous studies. This directly contributes to the Security of users by presenting a potential to keep applications and systems Healthy; free from vulnerabilities.,"The identification and disclosure of vulnerabilities in software directly contributes to the security of users. By uncovering 14 vulnerabilities that were not previously known, Trex enhances the security of applications and systems, ensuring their healthy state by addressing and mitigating potential risks.",Agreed-Justified,Agree,,Agree,,
Exp G,Exp K,2407,TSE,AI & Machine Learning,Learning Approximate Execution Semantics From Traces for Binary Function Similarity,"Detecting semantically similar binary functions aEUR"" a crucial capability with broad security usages including vulnerability detection, malware analysis, and forensics aEUR"" requires understanding function behaviors and intentions. This task is challenging as semantically similar functions can be compiled to run on different architectures and with diverse compiler optimizations or obfuscations. Most existing approaches match functions based on syntactic features without understanding the functionsaEUR(tm) execution semantics. We present Trex, a transfer-learning-based framework, to automate learning approximate execution semantics explicitly from functionsaEUR(tm) traces collected via forced-execution (i.e., by violating the control flow semantics) and transfer the learned knowledge to match semantically similar functions. While it is known that forced-execution traces are too imprecise to be directly used to detect semantic similarity, our key insight is that these traces can instead be used to teach an ML model approximate execution semantics of diverse instructions and their compositions. We thus design a pretraining task, which trains the model to learn approximate execution semantics from the two modalities (i.e., forced-executed code and traces) of the function. We then finetune the pretrained model to match semantically similar functions. We evaluate Trex on 1,472,066 functions from 13 popular software projects, compiled to run on 4 architectures (x86, x64, ARM, and MIPS), and with 4 optimizations (O0-O3) and 5 obfuscations. Trex outperforms the state-of-the-art solutions by 7.8%, 7.2%, and 14.3% in cross-architecture, optimization, and obfuscation function matching, respectively, while running 8'-- faster. Ablation studies suggest that the pretraining significantly boosts the function matching performance, underscoring the importance of learning execution semantics. Our case studies demonstrate the practical use-cases of Trex aEUR"" on 180 real-world firmware images, Trex uncovers 14 vulnerabilities not disclosed by previous studies. We release the code and dataset of Trex at https://github.com/CUMLSec/trex.",Security,National Security,The use-cases of Trex include uncovering vulnerabilities in 180 real-world firmware images; this can contribute significantly to National Security; aligning this achievement with the value Security.,"The alignment of 'Paper X' with the value item National Security and its corresponding value Security is justified based on the practical use-cases of Trex in uncovering vulnerabilities in real-world firmware images. By improving vulnerability detection and analysis, 'Paper X' indirectly contributes to enhancing the security of computer systems and software, which in turn can have a positive impact on national security.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2413,TSE,Software Testing & QA,Automated Detection of Software Performance Antipatterns in Java-Based Applications,"The detection of performance issues in Java-based applications is not trivial since many factors concur to poor performance, and software engineers are not sufficiently supported for this task. The goal of this manuscript is the automated detection of performance problems in running systems to guarantee that no quality-based hinders prevent their successful usage. Starting from software performance antipatterns, i.e., bad practices (e.g., extensive interaction between software methods) expressing both the problem and the solution with the purpose of identifying shortcomings and promptly fixing them, we develop a framework that automatically detects seven software antipatterns capturing a variety of performance issues in Java-based applications. Our approach is applied to real-world case studies from different domains, and it captures four real-life performance issues of Hadoop and Cassandra that were not predicted by state-of-the-art approaches. As empirical evidence, we calculate the accuracy of the proposed detection rules, we show that code commits inducing and fixing real-life performance issues present interesting variations in the number of detected antipattern instances, and solving one of the detected antipatterns improves the system performance up to 50%.",Achievement,Successful,The paper discusses the detection of performance problems in applications to ensure their successful usage. This aligns with the value item Successful and its corresponding value Achievement.,"In the paper abstract, it is explicitly stated that the goal of the manuscript is to guarantee the successful usage of running systems by detecting performance problems and promptly fixing them. This aligns with the value item Successful and its corresponding value Achievement from the perspective of a software user. By ensuring the smooth and efficient performance of applications, the user can achieve their desired outcomes and goals, leading to a sense of accomplishment and success in using the software.",Agreed-Justified,Agree,,Agree,,
Exp B,Exp H,2413,TSE,Software Testing & QA,Automated Detection of Software Performance Antipatterns in Java-Based Applications,"The detection of performance issues in Java-based applications is not trivial since many factors concur to poor performance, and software engineers are not sufficiently supported for this task. The goal of this manuscript is the automated detection of performance problems in running systems to guarantee that no quality-based hinders prevent their successful usage. Starting from software performance antipatterns, i.e., bad practices (e.g., extensive interaction between software methods) expressing both the problem and the solution with the purpose of identifying shortcomings and promptly fixing them, we develop a framework that automatically detects seven software antipatterns capturing a variety of performance issues in Java-based applications. Our approach is applied to real-world case studies from different domains, and it captures four real-life performance issues of Hadoop and Cassandra that were not predicted by state-of-the-art approaches. As empirical evidence, we calculate the accuracy of the proposed detection rules, we show that code commits inducing and fixing real-life performance issues present interesting variations in the number of detected antipattern instances, and solving one of the detected antipatterns improves the system performance up to 50%.",Security,Healthy,The paper emphasizes on the health of software by detecting and fixing performance issues; aligning with the value item Healthy and its corresponding value Security.,"The justification for aligning 'Paper X' with the value item Healthy and its corresponding value Security is based on the fact that the paper focuses on the detection and fixing of performance issues in Java-based applications. By addressing and resolving these issues, the paper aims to ensure the smooth and secure usage of the software, ultimately contributing to the overall health and security of the system.",Agreed-Justified,Disagree,reconcile with coder_2,Disagree,The justification doesn't align with the value item of the healthy,
Exp D,Exp K,2414,TSE,Security & Privacy,DASP: A Framework for Driving the Adoption of Software Security Practices,"Implementing software security practices is a critical concern in modern software development. Industry practitioners, security tool providers, and researchers have provided standard security guidelines and sophisticated security development tools to ensure a secure software development pipeline. But despite these efforts, there continues to be an increase in the number of vulnerabilities that can be exploited by malicious hackers. There is thus an urgent need to understand why developers still introduce security vulnerabilities into their applications and to understand what can be done to motivate them to write more secure code. To understand and address this problem further, we propose DASP, a framework for diagnosing and driving the adoption of software security practices among developers. DASP was conceived by combining behavioral science theories to shape a cross-sectional interview study with 28 software practitioners. Our interviews lead to a framework that consists of a comprehensive set of 33 drivers grouped into 7 higher-level categories that represent what needs to happen or change so that the adoption of software security practices occurs. Using the DASP framework, organizations can design interventions suitable for developersaEUR(tm) specific development contexts that will motivate them to write more secure code.",Security,Healthy,The paper introduces a framework focusing on enhancing software security; which aligns with the value item 'Healthy' under the value 'Security'; as providing secure software fosters a healthy and safe environment for users.,"Certainly! In the context of a ""Software User,"" the value item 'Healthy' aligns with the main contribution of 'Paper X' because the paper proposes a framework for enhancing software security. By ensuring the development of secure software, the framework aims to create a healthy and safe environment for users, where they can confidently use the software without the fear of security vulnerabilities. This aligns with the value of security as it promotes the well-being and protection of users' personal information, data, and overall user experience. By addressing security vulnerabilities, the paper's contribution directly aligns with the value item 'Healthy' under the value 'Security' from the perspective of a software user.",Agreed-Clarified,Agree,,Agree,,
Exp E,Exp J,2414,TSE,Security & Privacy,DASP: A Framework for Driving the Adoption of Software Security Practices,"Implementing software security practices is a critical concern in modern software development. Industry practitioners, security tool providers, and researchers have provided standard security guidelines and sophisticated security development tools to ensure a secure software development pipeline. But despite these efforts, there continues to be an increase in the number of vulnerabilities that can be exploited by malicious hackers. There is thus an urgent need to understand why developers still introduce security vulnerabilities into their applications and to understand what can be done to motivate them to write more secure code. To understand and address this problem further, we propose DASP, a framework for diagnosing and driving the adoption of software security practices among developers. DASP was conceived by combining behavioral science theories to shape a cross-sectional interview study with 28 software practitioners. Our interviews lead to a framework that consists of a comprehensive set of 33 drivers grouped into 7 higher-level categories that represent what needs to happen or change so that the adoption of software security practices occurs. Using the DASP framework, organizations can design interventions suitable for developersaEUR(tm) specific development contexts that will motivate them to write more secure code.",Security,Social Order,The proposed framework in the paper supports the enforcement of secure development practices; aiming to reduce security vulnerabilities. This aligns with 'Social Order' under 'Security' by maintaining an orderly and risk-free digital environment that can prevent disruptions caused by security threats.,"The proposed framework in 'Paper X' aligns with the value item Social Order and its corresponding value Security from a ""Software User"" perspective because it aims to reduce security vulnerabilities and maintain an orderly and risk-free digital environment. By implementing secure development practices, the framework helps prevent disruptions caused by security threats, ensuring that software users can have a sense of stability and trust in the software they are using.",Agreed-Justified,Agree,,Agree,,
Exp F,Exp J,2414,TSE,Security & Privacy,DASP: A Framework for Driving the Adoption of Software Security Practices,"Implementing software security practices is a critical concern in modern software development. Industry practitioners, security tool providers, and researchers have provided standard security guidelines and sophisticated security development tools to ensure a secure software development pipeline. But despite these efforts, there continues to be an increase in the number of vulnerabilities that can be exploited by malicious hackers. There is thus an urgent need to understand why developers still introduce security vulnerabilities into their applications and to understand what can be done to motivate them to write more secure code. To understand and address this problem further, we propose DASP, a framework for diagnosing and driving the adoption of software security practices among developers. DASP was conceived by combining behavioral science theories to shape a cross-sectional interview study with 28 software practitioners. Our interviews lead to a framework that consists of a comprehensive set of 33 drivers grouped into 7 higher-level categories that represent what needs to happen or change so that the adoption of software security practices occurs. Using the DASP framework, organizations can design interventions suitable for developersaEUR(tm) specific development contexts that will motivate them to write more secure code.",Security,National Security,The paper's emphasis on reducing vulnerabilities and enhancing software security aligns with the value item 'National Security' under 'Security'; as secure software can contribute to the protection of critical national infrastructures and information systems from cyberattacks.,"In the context of a ""Software User,"" the alignment between 'Paper X' and the value item 'National Security' from the category 'Security' can be justified by recognizing that software vulnerabilities, if left unaddressed, can pose significant risks to critical national infrastructures and information systems. By focusing on reducing vulnerabilities and enhancing software security, 'Paper X' directly contributes to securing these systems, thereby aligning with the value of 'National Security' in safeguarding the country's critical assets and defending against potential cyberattacks.",Agreed-Justified,Agree,,Agree,,
Exp G,Exp K,2416,TSE,Mobile & IoT,Characterizing and Finding System Setting-Related Defects in Android Apps,"Android, the most popular mobile system, offers a number of user-configurable system settings (e.g., network, location, and permission) for controlling devices and apps. Even popular, well-tested apps may fail to properly adapt their behaviors to diverse setting changes, thus frustrating their users. However, there exists no effort to systematically investigate such defects. To this end, we conduct the first large-scale empirical study to understand and characterize these system setting-related defects (in short as aEURoesetting defectsaEUR), which reside in apps and are triggered by system setting changes. We devote substantial manual effort (over four person-months) to analyze 1,074 setting defects from 180 popular apps on GitHub. We investigate the impact, root causes, and consequences of these setting defects and their correlations. We find that (1) setting defects have a wide impact on appsaEUR(tm) correctness with diverse root causes, (2) the majority of these defects ($\approx$a%0^70.7%) cause non-crashing (logic) failures, and (3) some correlations exist between the setting categories, root causes, and consequences. Motivated and informed by these findings, we propose two bug-finding techniques that can synergistically detect setting defects from both the GUI and code levels. Specifically, at the GUI level, we design and introduce setting-wise metamorphic fuzzing, the first automated dynamic testing technique to detect setting defects (causing crash and non-crashing failures, respectively) for Android apps. We implement this technique as an end-to-end, automated GUI testing tool named SetDroid. At the code level, we distill two major fault patterns and implement a static analysis tool named SetChecker to identify potential setting defects. We evaluate SetDroid and SetChecker on 26 popular, open-source Android apps, and they find 48 unique, previously-unknown setting defects. To date, 35 have been confirmed and 21 have been fixed by app developers. We also apply SetDroid and SetChecker on five highly popular industrial apps, namely WeChat, QQMail, TikTok, CapCut, and AlipayHK, all of which each have billions of monthly active users. SetDroid successfully detects 17 previously unknown setting defects in these appsaEUR(tm) latest releases, and all defects have been confirmed and fixed by the app vendors. After that, we collaborate with ByteDance and deploy these two bug-finding techniques internally to stress-test TikTok, one of its major app products. Within a two-month testing campaign, SetDroid successfully finds 53 setting defects, and SetChecker finds 22 ones. So far, 59 have been confirmed and 31 have been fixed. All these defects escaped from prior developer testing. By now, SetDroid has been integrated into ByteDance's official app testing infrastructure named FastBot for daily testing. These results demonstrate the strong effectiveness and practicality of our proposed techniques.",Security,Healthy,"The paper presents system setting-related defects and two bug-finding techniques that identify these defects and therefore contribute to healthier; more robust software from a user's point of view. This directly aligns with the value item ""Healthy"" and its corresponding value ""Security"".","In the paper, the authors identify and investigate system setting-related defects in mobile apps. These defects can impact the correctness of the apps and cause non-crashing failures. By proposing bug-finding techniques that detect and help fix these defects, the paper contributes to improving the overall health and security of software from a user's perspective. Users value a sense of security, knowing that the software they use is reliable and free from unexpected failures. Therefore, the alignment with the value item ""Healthy"" and its corresponding value ""Security"" is evident in the paper's contributions.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2416,TSE,Mobile & IoT,Characterizing and Finding System Setting-Related Defects in Android Apps,"Android, the most popular mobile system, offers a number of user-configurable system settings (e.g., network, location, and permission) for controlling devices and apps. Even popular, well-tested apps may fail to properly adapt their behaviors to diverse setting changes, thus frustrating their users. However, there exists no effort to systematically investigate such defects. To this end, we conduct the first large-scale empirical study to understand and characterize these system setting-related defects (in short as aEURoesetting defectsaEUR), which reside in apps and are triggered by system setting changes. We devote substantial manual effort (over four person-months) to analyze 1,074 setting defects from 180 popular apps on GitHub. We investigate the impact, root causes, and consequences of these setting defects and their correlations. We find that (1) setting defects have a wide impact on appsaEUR(tm) correctness with diverse root causes, (2) the majority of these defects ($\approx$a%0^70.7%) cause non-crashing (logic) failures, and (3) some correlations exist between the setting categories, root causes, and consequences. Motivated and informed by these findings, we propose two bug-finding techniques that can synergistically detect setting defects from both the GUI and code levels. Specifically, at the GUI level, we design and introduce setting-wise metamorphic fuzzing, the first automated dynamic testing technique to detect setting defects (causing crash and non-crashing failures, respectively) for Android apps. We implement this technique as an end-to-end, automated GUI testing tool named SetDroid. At the code level, we distill two major fault patterns and implement a static analysis tool named SetChecker to identify potential setting defects. We evaluate SetDroid and SetChecker on 26 popular, open-source Android apps, and they find 48 unique, previously-unknown setting defects. To date, 35 have been confirmed and 21 have been fixed by app developers. We also apply SetDroid and SetChecker on five highly popular industrial apps, namely WeChat, QQMail, TikTok, CapCut, and AlipayHK, all of which each have billions of monthly active users. SetDroid successfully detects 17 previously unknown setting defects in these appsaEUR(tm) latest releases, and all defects have been confirmed and fixed by the app vendors. After that, we collaborate with ByteDance and deploy these two bug-finding techniques internally to stress-test TikTok, one of its major app products. Within a two-month testing campaign, SetDroid successfully finds 53 setting defects, and SetChecker finds 22 ones. So far, 59 have been confirmed and 31 have been fixed. All these defects escaped from prior developer testing. By now, SetDroid has been integrated into ByteDance's official app testing infrastructure named FastBot for daily testing. These results demonstrate the strong effectiveness and practicality of our proposed techniques.",Self Direction,Creativity,"By testing software for system setting-related defects and through the application of two bug-finding techniques these defects can be identified and fixed; therefore; fostering more creative use of software as it adapts properly to diverse setting changes and does not limit its users with technical hang-ups. This aligns with the value item ""Creativity"" and its corresponding value ""Self direction"".","Certainly! By detecting and fixing system setting-related defects in software, 'Paper X' enables users to have more freedom and independence in customizing their software experience. This aligns with the value item ""Creativity"" as it empowers users to explore different settings and adapt the software according to their specific needs and preferences. Users can exercise their self-direction by having control over network, location, and permission settings, allowing them to creatively enhance their overall software experience.",Agreed-Justified,Agree,,Agree,,
Exp B,Exp H,2416,TSE,Mobile & IoT,Characterizing and Finding System Setting-Related Defects in Android Apps,"Android, the most popular mobile system, offers a number of user-configurable system settings (e.g., network, location, and permission) for controlling devices and apps. Even popular, well-tested apps may fail to properly adapt their behaviors to diverse setting changes, thus frustrating their users. However, there exists no effort to systematically investigate such defects. To this end, we conduct the first large-scale empirical study to understand and characterize these system setting-related defects (in short as aEURoesetting defectsaEUR), which reside in apps and are triggered by system setting changes. We devote substantial manual effort (over four person-months) to analyze 1,074 setting defects from 180 popular apps on GitHub. We investigate the impact, root causes, and consequences of these setting defects and their correlations. We find that (1) setting defects have a wide impact on appsaEUR(tm) correctness with diverse root causes, (2) the majority of these defects ($\approx$a%0^70.7%) cause non-crashing (logic) failures, and (3) some correlations exist between the setting categories, root causes, and consequences. Motivated and informed by these findings, we propose two bug-finding techniques that can synergistically detect setting defects from both the GUI and code levels. Specifically, at the GUI level, we design and introduce setting-wise metamorphic fuzzing, the first automated dynamic testing technique to detect setting defects (causing crash and non-crashing failures, respectively) for Android apps. We implement this technique as an end-to-end, automated GUI testing tool named SetDroid. At the code level, we distill two major fault patterns and implement a static analysis tool named SetChecker to identify potential setting defects. We evaluate SetDroid and SetChecker on 26 popular, open-source Android apps, and they find 48 unique, previously-unknown setting defects. To date, 35 have been confirmed and 21 have been fixed by app developers. We also apply SetDroid and SetChecker on five highly popular industrial apps, namely WeChat, QQMail, TikTok, CapCut, and AlipayHK, all of which each have billions of monthly active users. SetDroid successfully detects 17 previously unknown setting defects in these appsaEUR(tm) latest releases, and all defects have been confirmed and fixed by the app vendors. After that, we collaborate with ByteDance and deploy these two bug-finding techniques internally to stress-test TikTok, one of its major app products. Within a two-month testing campaign, SetDroid successfully finds 53 setting defects, and SetChecker finds 22 ones. So far, 59 have been confirmed and 31 have been fixed. All these defects escaped from prior developer testing. By now, SetDroid has been integrated into ByteDance's official app testing infrastructure named FastBot for daily testing. These results demonstrate the strong effectiveness and practicality of our proposed techniques.",Achievement,Successful,"The paper contributes to the value item ""Successful"" under ""Achievement"" value by providing solutions to fix defects and improve the user experience. This is because a user would feel successful in using an application smoothly; achieving their intended outcome without any hiccup.","In the context of a software user, the paper's contributions align with the value item ""Successful"" and its corresponding value ""Achievement"" because it provides bug-finding techniques that detect and fix defects in Android apps. By addressing these defects and improving the user experience, users would feel successful in using the application smoothly and achieving their intended outcomes without any disruptions. This aligns with the value of achievement as users can successfully accomplish their tasks and goals within the software context.",Agreed-Justified,Agree,,Agree,,
Exp C,Exp K,2417,TSE,Mobile & IoT,Mixed Signals: Analyzing Software Attribution Challenges in the Android Ecosystem,"The ability to identify the author responsible for a given software object is critical for many research studies and for enhancing software transparency and accountability. However, as opposed to other application markets like Apple's iOS App Store, attribution in the Android ecosystem is known to be hard. Prior research has leveraged market metadata and signing certificates to identify software authors without questioning the validity and accuracy of these attribution signals. However, Android application (app) authors can, either intentionally or by mistake, hide their true identity due to: (1) the lack of policy enforcement by markets to ensure the accuracy and correctness of the information disclosed by developers in their market profiles during the app release process, and (2) the use of self-signed certificates for signing apps instead of certificates issued by trusted CAs. In this paper, we perform the first empirical analysis of the availability, volatility and overall aptness of publicly available market and app metadata for author attribution in Android markets. To that end, we analyze a dataset of over 2.5 million market entries and apps extracted from five Android markets for over two years. Our results show that widely used attribution signals are often missing from market profiles and that they change over time. We also invalidate the general belief about the validity of signing certificates for author attribution. For instance, we find that apps from different authors share signing certificates due to the proliferation of app building frameworks and software factories. Finally, we introduce the concept of an attribution graph and we apply it to evaluate the validity of existing attribution signals on the Google Play Store. Our results confirm that the lack of control over publicly available signals can confuse automatic attribution processes.",Security,Social Order,Paper X contributes to enhancing the transparency and accountability of software authors in the Android ecosystem. This transparency helps software users to maintain social order; as it ensures the software authors are responsible for their products.,"In the context of 'Paper X', the contribution of enhancing the transparency and accountability of software authors aligns with the value item of Social Order and its corresponding value of Security. By ensuring that software authors are responsible for their products, users can trust the software ecosystem and have confidence that the software they are using is reliable, secure, and in line with societal norms. This promotes social order within the software context by maintaining a sense of stability and trust among software users.",Agreed-Clarified,Agree,,Agree,,
Exp E,Exp J,2417,TSE,Mobile & IoT,Mixed Signals: Analyzing Software Attribution Challenges in the Android Ecosystem,"The ability to identify the author responsible for a given software object is critical for many research studies and for enhancing software transparency and accountability. However, as opposed to other application markets like Apple's iOS App Store, attribution in the Android ecosystem is known to be hard. Prior research has leveraged market metadata and signing certificates to identify software authors without questioning the validity and accuracy of these attribution signals. However, Android application (app) authors can, either intentionally or by mistake, hide their true identity due to: (1) the lack of policy enforcement by markets to ensure the accuracy and correctness of the information disclosed by developers in their market profiles during the app release process, and (2) the use of self-signed certificates for signing apps instead of certificates issued by trusted CAs. In this paper, we perform the first empirical analysis of the availability, volatility and overall aptness of publicly available market and app metadata for author attribution in Android markets. To that end, we analyze a dataset of over 2.5 million market entries and apps extracted from five Android markets for over two years. Our results show that widely used attribution signals are often missing from market profiles and that they change over time. We also invalidate the general belief about the validity of signing certificates for author attribution. For instance, we find that apps from different authors share signing certificates due to the proliferation of app building frameworks and software factories. Finally, we introduce the concept of an attribution graph and we apply it to evaluate the validity of existing attribution signals on the Google Play Store. Our results confirm that the lack of control over publicly available signals can confuse automatic attribution processes.",Achievement,Successful,By introducing the concept of an attribution graph; the paper potentially improves users' ability to make successful and informed decisions about the software they use through a more accurate system of author attribution.,"The introduction of an attribution graph by 'Paper X' allows for a more accurate system of author attribution, thereby improving the software user's ability to make successful and informed decisions about the software they use. With the enhanced understanding of software authors, users can assess their credibility and track their past performance, aligning with the value item Successful and its corresponding value Achievement from a user perspective.",Agreed-Clarified,Agree,,Agree,,
Exp F,Exp J,2423,TSE,Software Deployment & Operations,TrinityRCL: Multi-Granular and Code-Level Root Cause Localization Using Multiple Types of Telemetry Data in Microservice Systems,"The microservice architecture has been commonly adopted by large scale software systems exemplified by a wide range of online services. Service monitoring through anomaly detection and root cause analysis (RCA) is crucial for these microservice systems to provide stable and continued services. However, compared with monolithic systems, software systems based on the layered microservice architecture are inherently complex and commonly involve entities at different levels of granularity. Therefore, for effective service monitoring, these systems have a special requirement of multi-granular RCA. Furthermore, as a large proportion of anomalies in microservice systems pertain to problematic code, to timely troubleshoot these anomalies, these systems have another special requirement of RCA at the finest code-level. Microservice systems rely on telemetry data to perform service monitoring and RCA of service anomalies. The majority of existing RCA approaches are only based on a single type of telemetry data and as a result can only support uni-granular RCA at either application-level or service-level. Although there are attempts to combine metric and tracing data in RCA, their objective is to improve RCA's efficiency or accuracy rather than to support multi-granular RCA. In this article, we propose a new RCA solution TrinityRCL that is able to localize the root causes of anomalies at multiple levels of granularity including application-level, service-level, host-level, and metric-level, with the unique capability of code-level localization by harnessing all three types of telemetry data to construct a causal graph representing the intricate, dynamic, and nondeterministic relationships among the various entities related to the anomalies. By implementing and deploying TrinityRCL in a real production environment, we evaluate TrinityRCL against two baseline methods and the results show that TrinityRCL has a significant performance advantage in terms of accuracy at the same level of granularity with comparable efficiency and is particularly effective to support large-scale systems with massive telemetry data.",Achievement,Intelligent,The paper presents TrinityRCL; a new method for root cause analysis in microservices. This aligns with the value item 'Intelligence' and the corresponding value 'Achievement'; as the tool offers intelligent identification and localization of anomalies across multiple levels of system architecture.,"The justification for aligning 'Paper X' with the value item Intelligent and its corresponding value Achievement is based on the fact that the TrinityRCL method, proposed in the paper, demonstrates intelligent capabilities by effectively identifying and localizing anomalies in microservices at different levels of system architecture. The ability to perform root cause analysis and provide accurate results aligns with the value of Achievement, as it enables software users to achieve stable and continued services by troubleshooting and resolving problematic code issues.",Agreed-Clarified,Agree,,Agree,,
Exp G,Exp K,2423,TSE,Software Deployment & Operations,TrinityRCL: Multi-Granular and Code-Level Root Cause Localization Using Multiple Types of Telemetry Data in Microservice Systems,"The microservice architecture has been commonly adopted by large scale software systems exemplified by a wide range of online services. Service monitoring through anomaly detection and root cause analysis (RCA) is crucial for these microservice systems to provide stable and continued services. However, compared with monolithic systems, software systems based on the layered microservice architecture are inherently complex and commonly involve entities at different levels of granularity. Therefore, for effective service monitoring, these systems have a special requirement of multi-granular RCA. Furthermore, as a large proportion of anomalies in microservice systems pertain to problematic code, to timely troubleshoot these anomalies, these systems have another special requirement of RCA at the finest code-level. Microservice systems rely on telemetry data to perform service monitoring and RCA of service anomalies. The majority of existing RCA approaches are only based on a single type of telemetry data and as a result can only support uni-granular RCA at either application-level or service-level. Although there are attempts to combine metric and tracing data in RCA, their objective is to improve RCA's efficiency or accuracy rather than to support multi-granular RCA. In this article, we propose a new RCA solution TrinityRCL that is able to localize the root causes of anomalies at multiple levels of granularity including application-level, service-level, host-level, and metric-level, with the unique capability of code-level localization by harnessing all three types of telemetry data to construct a causal graph representing the intricate, dynamic, and nondeterministic relationships among the various entities related to the anomalies. By implementing and deploying TrinityRCL in a real production environment, we evaluate TrinityRCL against two baseline methods and the results show that TrinityRCL has a significant performance advantage in terms of accuracy at the same level of granularity with comparable efficiency and is particularly effective to support large-scale systems with massive telemetry data.",Security,Healthy,By providing effective monitoring and anomaly detection for microservice systems; the proposed TrinityRCL contributes to a stable and continued software service; which aligns with the value item 'Healthy' and its corresponding value 'Security'.,"The justification for aligning 'Paper X' with the value item Healthy and its corresponding value Security from a ""Software User"" perspective is that the proposed TrinityRCL solution aims to provide effective monitoring and anomaly detection for microservice systems. This directly contributes to ensuring the stability and continuity of software services, which is crucial for a software user's sense of security and well-being. With reliable monitoring and root cause analysis, potential issues can be detected and resolved promptly, minimizing disruptions and maintaining the overall health and security of the software system.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2423,TSE,Software Deployment & Operations,TrinityRCL: Multi-Granular and Code-Level Root Cause Localization Using Multiple Types of Telemetry Data in Microservice Systems,"The microservice architecture has been commonly adopted by large scale software systems exemplified by a wide range of online services. Service monitoring through anomaly detection and root cause analysis (RCA) is crucial for these microservice systems to provide stable and continued services. However, compared with monolithic systems, software systems based on the layered microservice architecture are inherently complex and commonly involve entities at different levels of granularity. Therefore, for effective service monitoring, these systems have a special requirement of multi-granular RCA. Furthermore, as a large proportion of anomalies in microservice systems pertain to problematic code, to timely troubleshoot these anomalies, these systems have another special requirement of RCA at the finest code-level. Microservice systems rely on telemetry data to perform service monitoring and RCA of service anomalies. The majority of existing RCA approaches are only based on a single type of telemetry data and as a result can only support uni-granular RCA at either application-level or service-level. Although there are attempts to combine metric and tracing data in RCA, their objective is to improve RCA's efficiency or accuracy rather than to support multi-granular RCA. In this article, we propose a new RCA solution TrinityRCL that is able to localize the root causes of anomalies at multiple levels of granularity including application-level, service-level, host-level, and metric-level, with the unique capability of code-level localization by harnessing all three types of telemetry data to construct a causal graph representing the intricate, dynamic, and nondeterministic relationships among the various entities related to the anomalies. By implementing and deploying TrinityRCL in a real production environment, we evaluate TrinityRCL against two baseline methods and the results show that TrinityRCL has a significant performance advantage in terms of accuracy at the same level of granularity with comparable efficiency and is particularly effective to support large-scale systems with massive telemetry data.",Universalism,Social Justice,The development and deployment of TrinityRCL aim at improving service stability and performance; which can lead to the promotion of social justice in the context of software services provision; thus aligning with the value item 'Social Justice' and its corresponding value 'Universalism'.,"My justification is that by improving service stability and performance, TrinityRCL contributes to the overall fairness and equality of software service provision, aligning with the value item of Social Justice. This directly links to the value of Universalism, as it emphasizes the promotion of equal rights and opportunities for all users within a software context.",Agreed-Justified,Agree,,Agree,,
Exp B,Exp H,2425,TSE,Software Project Management,Understanding MentorsaEUR(tm) Engagement in OSS Communities via Google Summer of Code,"A constant influx of newcomers is essential for the sustainability and success of open source software (OSS) projects. However, successful onboarding is always challenging because newcomers face various initial contributing barriers. To support newcomer onboarding, OSS communities widely adopt the mentoring approach. Despite its significance, previous mentoring studies tend to focus on the newcomer's perspective, leaving the mentor's perspective relatively under-studied. To better support mentoring, we study the popular Google Summer of Code (GSoC). It is a well-established global program that offers stipends and mentors to students aiming to bring more student developers into OSS development. We combine online data analysis, an email survey, and semi-structured interviews with the GSoC mentors to understand their motivations, challenges, strategies, and gains. We propose a taxonomy of GSoC mentorsaEUR(tm) engagement with four themes, ten categories, 34 sub-categories, and 118 codes, as well as the mentorsaEUR(tm) attitudes toward the codes. In particular, we find that mentors participating in GSoC are primarily intrinsically motivated, and some new motivators emerge adapting to the contemporary challenges, e.g., sustainability and advertisement of projects. Forty-one challenges and 52 strategies associated with the program timeline are identified, most of which are first time revealed. Although almost all the challenges are agreed upon by specific mentors, some mentors believe that several challenges are reasonable and even have a positive effect. For example, the cognitive differences between mentors and mentees can stimulate new perspectives. Most of the mentors agreed that they had adopted these strategies during the mentoring process, but a few strategies recommended by the GSoC administration were not agreed upon. Self-satisfaction, different skills, and peer recognition are the main gains of mentors to participate in GSoC. Eventually, we discuss practical implications for mentors, students, OSS communities, GSoC programs, and researchers.",Self Direction,Curiosity,The paper is about enabling newcomers to contribute to open-source software projects in a more effective manners and this requires curiosity to learn and understand new programs; which aligns with the value of Self-Direction.,"The paper's main contribution focuses on supporting the onboarding process for newcomers in open-source software projects, specifically through a mentoring approach. The alignment with the value item Curiosity and its corresponding value Self Direction is evident because fostering curiosity is essential for newcomers to learn and understand new programs, which is emphasized in the paper. By encouraging curiosity, the paper aligns with the value of Self-Direction as newcomers actively choose to explore and direct their learning in the open-source software context.",Agreed-Justified,Agree,,Agree,,
Exp C,Exp K,2425,TSE,Software Project Management,Understanding MentorsaEUR(tm) Engagement in OSS Communities via Google Summer of Code,"A constant influx of newcomers is essential for the sustainability and success of open source software (OSS) projects. However, successful onboarding is always challenging because newcomers face various initial contributing barriers. To support newcomer onboarding, OSS communities widely adopt the mentoring approach. Despite its significance, previous mentoring studies tend to focus on the newcomer's perspective, leaving the mentor's perspective relatively under-studied. To better support mentoring, we study the popular Google Summer of Code (GSoC). It is a well-established global program that offers stipends and mentors to students aiming to bring more student developers into OSS development. We combine online data analysis, an email survey, and semi-structured interviews with the GSoC mentors to understand their motivations, challenges, strategies, and gains. We propose a taxonomy of GSoC mentorsaEUR(tm) engagement with four themes, ten categories, 34 sub-categories, and 118 codes, as well as the mentorsaEUR(tm) attitudes toward the codes. In particular, we find that mentors participating in GSoC are primarily intrinsically motivated, and some new motivators emerge adapting to the contemporary challenges, e.g., sustainability and advertisement of projects. Forty-one challenges and 52 strategies associated with the program timeline are identified, most of which are first time revealed. Although almost all the challenges are agreed upon by specific mentors, some mentors believe that several challenges are reasonable and even have a positive effect. For example, the cognitive differences between mentors and mentees can stimulate new perspectives. Most of the mentors agreed that they had adopted these strategies during the mentoring process, but a few strategies recommended by the GSoC administration were not agreed upon. Self-satisfaction, different skills, and peer recognition are the main gains of mentors to participate in GSoC. Eventually, we discuss practical implications for mentors, students, OSS communities, GSoC programs, and researchers.",Stimulation,Variation in Life,The paper emphasizes the influx of newcomers to bring in new perspectives and variation; aligning with the value item Variation in Life under the Stimulation value.,"In the paper abstract, it is explicitly stated that the success of open source software projects relies on a constant influx of newcomers. To support newcomer onboarding, the paper focuses on the mentoring approach, specifically studying the popular Google Summer of Code program. The program aims to bring more student developers into OSS development, providing them with mentors and stipends. This aligns with the value item Variation in Life, as the influx of newcomers brings in new perspectives and diversity, leading to stimulation in the software development context.",Agreed-Justified,Agree,,Agree,,
Exp D,Exp K,2425,TSE,Software Project Management,Understanding MentorsaEUR(tm) Engagement in OSS Communities via Google Summer of Code,"A constant influx of newcomers is essential for the sustainability and success of open source software (OSS) projects. However, successful onboarding is always challenging because newcomers face various initial contributing barriers. To support newcomer onboarding, OSS communities widely adopt the mentoring approach. Despite its significance, previous mentoring studies tend to focus on the newcomer's perspective, leaving the mentor's perspective relatively under-studied. To better support mentoring, we study the popular Google Summer of Code (GSoC). It is a well-established global program that offers stipends and mentors to students aiming to bring more student developers into OSS development. We combine online data analysis, an email survey, and semi-structured interviews with the GSoC mentors to understand their motivations, challenges, strategies, and gains. We propose a taxonomy of GSoC mentorsaEUR(tm) engagement with four themes, ten categories, 34 sub-categories, and 118 codes, as well as the mentorsaEUR(tm) attitudes toward the codes. In particular, we find that mentors participating in GSoC are primarily intrinsically motivated, and some new motivators emerge adapting to the contemporary challenges, e.g., sustainability and advertisement of projects. Forty-one challenges and 52 strategies associated with the program timeline are identified, most of which are first time revealed. Although almost all the challenges are agreed upon by specific mentors, some mentors believe that several challenges are reasonable and even have a positive effect. For example, the cognitive differences between mentors and mentees can stimulate new perspectives. Most of the mentors agreed that they had adopted these strategies during the mentoring process, but a few strategies recommended by the GSoC administration were not agreed upon. Self-satisfaction, different skills, and peer recognition are the main gains of mentors to participate in GSoC. Eventually, we discuss practical implications for mentors, students, OSS communities, GSoC programs, and researchers.",Benevolence,Helpful,The paper studies a mentoring program that supports newcomers in open-source software projects; which aligns with the value item Helpful under the Benevolence value.,"The main contribution of 'Paper X' is the study of the mentoring approach in the context of a specific program that supports newcomers in open-source software projects. This aligns with the value item Helpful and its corresponding value Benevolence from a ""Software User"" perspective because the mentoring approach is aimed at providing assistance and support to these newcomers, helping them overcome initial contributing barriers and successfully integrate into the OSS community. The focus on supporting and guiding newcomers in their software development journey reflects a helpful and benevolent intention of the mentors involved, as they contribute their time, knowledge, and expertise to assist others in their software learning and growth.",Agreed-Justified,Agree,,Agree,,
Exp E,Exp J,2427,TSE,Security & Privacy,Plumber: Boosting the Propagation of Vulnerability Fixes in the npm Ecosystem,"Vulnerabilities are known reported security threats that affect a large amount of packages in the npm ecosystem. To mitigate these security threats, the open-source community strongly suggests vulnerable packages to timely publish vulnerability fixes and recommends affected packages to update their dependencies. However, there are still serious lags in the propagation of vulnerability fixes in the ecosystem. In our preliminary study on the latest versions of 356,283 active npm packages, we found that 20.0% of them can still introduce vulnerabilities via direct or transitive dependencies although the involved vulnerable packages have already published fix versions for over a year. Prior study by (Chinthanet et al. 2021) lays the groundwork for research on how to mitigate propagation lags of vulnerability fixes in an ecosystem. They conducted an empirical investigation to identify lags that might occur between the vulnerable package release and its fixing release. They found that factors such as the branch upon which a fix landed and the severity of the vulnerability had a small effect on its propagation trajectory throughout the ecosystem. To ensure quick adoption and propagation of a release that contains the fix, they gave several actionable advice to developers and researchers. However, it is still an open question how to design an effective technique to accelerate the propagation of vulnerability fixes. Motivated by this problem, in this paper, we conducted an empirical study to learn the scale of packages that block the propagation of vulnerability fixes in the ecosystem and investigate their evolution characteristics. Furthermore, we distilled the remediation strategies that have better effects on mitigating the fix propagation lags. Leveraging our empirical findings, we propose an ecosystem-level technique, Plumber, for deriving feasible remediation strategies to boost the propagation of vulnerability fixes. To precisely diagnose the causes of fix propagation blocking, Plumber models the vulnerability metadata, and npm dependency metadata and continuously monitors their evolution. By analyzing a full-picture of the ecosystem-level dependency graph and the corresponding fix propagation statuses, it derives remediation schemes for pivotal packages. In the schemes, Plumber provides customized remediation suggestions with vulnerability impact analysis to arouse package developersaEUR(tm) awareness. We applied Plumber to generating 268 remediation reports for the identified pivotal packages, to evaluate its remediation effectiveness based on developersaEUR(tm) feedback. Encouragingly, 47.4% our remediation reports received positive feedback from many well-known npm projects, such as Tensorflow/tfjs, Ethers.js, and GoogleChrome/workbox. Our reports have boosted the propagation of vulnerability fixes into 16,403 root packages through 92,469 dependency paths. On average, each remediated package version is receiving 72,678 downloads per week by the time of this work.",Achievement,Intelligent,"The paper contributes to user's intelligence by proposing ""Plumber""; an effective technique to diagnose the causes of fix propagation blocking; enhancing user's understanding and awareness of their software's security challenges.","Apologies for the confusion. To clarify my justification, the main contribution of 'Paper X' aligns with the value item Intelligent and its corresponding value Achievement from a ""Software User"" perspective because the proposed technique, Plumber, enhances the user's intelligence by providing them with insight into the causes of fix propagation blocking. This helps users gain a better understanding of their software's security challenges and enables them to take proactive measures to mitigate vulnerabilities and ensure the safety and stability of their software systems. By empowering users with this knowledge, 'Paper X' focuses on enabling users to achieve a higher level of intelligence and achievement in effectively managing and securing their software.",Agreed-Clarified,Agree,,Agree,,
Exp F,Exp J,2427,TSE,Security & Privacy,Plumber: Boosting the Propagation of Vulnerability Fixes in the npm Ecosystem,"Vulnerabilities are known reported security threats that affect a large amount of packages in the npm ecosystem. To mitigate these security threats, the open-source community strongly suggests vulnerable packages to timely publish vulnerability fixes and recommends affected packages to update their dependencies. However, there are still serious lags in the propagation of vulnerability fixes in the ecosystem. In our preliminary study on the latest versions of 356,283 active npm packages, we found that 20.0% of them can still introduce vulnerabilities via direct or transitive dependencies although the involved vulnerable packages have already published fix versions for over a year. Prior study by (Chinthanet et al. 2021) lays the groundwork for research on how to mitigate propagation lags of vulnerability fixes in an ecosystem. They conducted an empirical investigation to identify lags that might occur between the vulnerable package release and its fixing release. They found that factors such as the branch upon which a fix landed and the severity of the vulnerability had a small effect on its propagation trajectory throughout the ecosystem. To ensure quick adoption and propagation of a release that contains the fix, they gave several actionable advice to developers and researchers. However, it is still an open question how to design an effective technique to accelerate the propagation of vulnerability fixes. Motivated by this problem, in this paper, we conducted an empirical study to learn the scale of packages that block the propagation of vulnerability fixes in the ecosystem and investigate their evolution characteristics. Furthermore, we distilled the remediation strategies that have better effects on mitigating the fix propagation lags. Leveraging our empirical findings, we propose an ecosystem-level technique, Plumber, for deriving feasible remediation strategies to boost the propagation of vulnerability fixes. To precisely diagnose the causes of fix propagation blocking, Plumber models the vulnerability metadata, and npm dependency metadata and continuously monitors their evolution. By analyzing a full-picture of the ecosystem-level dependency graph and the corresponding fix propagation statuses, it derives remediation schemes for pivotal packages. In the schemes, Plumber provides customized remediation suggestions with vulnerability impact analysis to arouse package developersaEUR(tm) awareness. We applied Plumber to generating 268 remediation reports for the identified pivotal packages, to evaluate its remediation effectiveness based on developersaEUR(tm) feedback. Encouragingly, 47.4% our remediation reports received positive feedback from many well-known npm projects, such as Tensorflow/tfjs, Ethers.js, and GoogleChrome/workbox. Our reports have boosted the propagation of vulnerability fixes into 16,403 root packages through 92,469 dependency paths. On average, each remediated package version is receiving 72,678 downloads per week by the time of this work.",Security,Healthy,The paper directly addresses user's health (in terms of software health) by investigating the scale of packages that block the propagation of vulnerability fixes in the ecosystem; aiming to improve software integrity and overall security.,"In the abstract, the paper explicitly states that it investigates the scale of packages that block the propagation of vulnerability fixes in the ecosystem, with the aim of improving software integrity and overall security. By addressing this issue, the paper is directly aligned with the value item ""Healthy"" and its corresponding value ""Security"" from a ""Software User"" perspective. Ensuring that software is secure and free from vulnerabilities contributes to the user's sense of security and well-being, making the alignment with the value item ""Healthy"" evident.",Agreed-Justified,Agree,,Agree,,
Exp G,Exp K,2427,TSE,Security & Privacy,Plumber: Boosting the Propagation of Vulnerability Fixes in the npm Ecosystem,"Vulnerabilities are known reported security threats that affect a large amount of packages in the npm ecosystem. To mitigate these security threats, the open-source community strongly suggests vulnerable packages to timely publish vulnerability fixes and recommends affected packages to update their dependencies. However, there are still serious lags in the propagation of vulnerability fixes in the ecosystem. In our preliminary study on the latest versions of 356,283 active npm packages, we found that 20.0% of them can still introduce vulnerabilities via direct or transitive dependencies although the involved vulnerable packages have already published fix versions for over a year. Prior study by (Chinthanet et al. 2021) lays the groundwork for research on how to mitigate propagation lags of vulnerability fixes in an ecosystem. They conducted an empirical investigation to identify lags that might occur between the vulnerable package release and its fixing release. They found that factors such as the branch upon which a fix landed and the severity of the vulnerability had a small effect on its propagation trajectory throughout the ecosystem. To ensure quick adoption and propagation of a release that contains the fix, they gave several actionable advice to developers and researchers. However, it is still an open question how to design an effective technique to accelerate the propagation of vulnerability fixes. Motivated by this problem, in this paper, we conducted an empirical study to learn the scale of packages that block the propagation of vulnerability fixes in the ecosystem and investigate their evolution characteristics. Furthermore, we distilled the remediation strategies that have better effects on mitigating the fix propagation lags. Leveraging our empirical findings, we propose an ecosystem-level technique, Plumber, for deriving feasible remediation strategies to boost the propagation of vulnerability fixes. To precisely diagnose the causes of fix propagation blocking, Plumber models the vulnerability metadata, and npm dependency metadata and continuously monitors their evolution. By analyzing a full-picture of the ecosystem-level dependency graph and the corresponding fix propagation statuses, it derives remediation schemes for pivotal packages. In the schemes, Plumber provides customized remediation suggestions with vulnerability impact analysis to arouse package developersaEUR(tm) awareness. We applied Plumber to generating 268 remediation reports for the identified pivotal packages, to evaluate its remediation effectiveness based on developersaEUR(tm) feedback. Encouragingly, 47.4% our remediation reports received positive feedback from many well-known npm projects, such as Tensorflow/tfjs, Ethers.js, and GoogleChrome/workbox. Our reports have boosted the propagation of vulnerability fixes into 16,403 root packages through 92,469 dependency paths. On average, each remediated package version is receiving 72,678 downloads per week by the time of this work.",Security,Social Order,"The paper contributes to maintaining social order in the software ecosystem. Through ""Plumber""; it aims to boost the propagation of vulnerability fixes; and enforce the principles of good practices of software development throughout the community.","The justification provided aligns with the value item of Social Order and its corresponding value of Security from a ""Software User"" perspective. The paper's contribution of ""Plumber"" addresses the propagation of vulnerability fixes, which in turn helps maintain social order in the software ecosystem. By enforcing good practices and ensuring timely adoption of fixed releases, it promotes security and stability in software usage, thus aligning with the value of Security and the value item of Social Order.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2438,TSE,Security & Privacy,LeakageVerif: Efficient and Scalable Formal Verification of Leakage in Symbolic Expressions,"Side-channel attacks are a powerful class of attacks targeting cryptographic devices. Masking is a popular protection technique to thwart such attacks as it can be theoretically proven secure. However, correctly implementing masking schemes is a non-trivial task and error-prone. If several techniques have been proposed to formally verify masked implementations, they all come with limitations regarding expressiveness, scalability or accuracy. In this work, we propose a symbolic approach, based on a variant of the classical substitution method, for formally verifying arithmetic and boolean masked programs. This approach is more accurate and scalable than existing approaches thanks to a careful design and implementation of key heuristics, algorithms and data structures involved in the verification process. We present all the details of this approach and the open-source tool called LeakageVerif which implements it as a python library, and which offers constructions for symbolic expressions and functions for their verification. We compare LeakageVerif to three existing state-of-the-art tools on a set of 46 masked programs, and we show that it has very good scalability and accuracy results while providing all the necessary constructs for describing algorithmic to assembly masking schemes. Finally, we also provide the set of 46 benchmarks, named MaskedVerifBenchs and written for comparing the different verification tools, in the hope that they will be useful to the community for future comparisons.",Security,Healthy,The paper contributes to improving the software's resilience to security attacks; which enhances the users' assurance of a healthier; safer software environment (Value: Security; Value Item: Healthy).,"The justification for aligning ""Paper X"" with the value item Healthy and its corresponding value Security from a ""Software User"" perspective is based on the fact that the paper presents a protective technique (masking) to defend against side-channel attacks, which are security threats to cryptographic devices. By implementing this technique, the software becomes more resilient to such attacks, thereby creating a healthier and safer software environment for users. This aligns with the value of Security, as users can have increased trust in the software's ability to protect their sensitive data and ensure their overall well-being in terms of security.",Agreed-Justified,Agree,,Agree,,
Exp B,Exp H,2438,TSE,Security & Privacy,LeakageVerif: Efficient and Scalable Formal Verification of Leakage in Symbolic Expressions,"Side-channel attacks are a powerful class of attacks targeting cryptographic devices. Masking is a popular protection technique to thwart such attacks as it can be theoretically proven secure. However, correctly implementing masking schemes is a non-trivial task and error-prone. If several techniques have been proposed to formally verify masked implementations, they all come with limitations regarding expressiveness, scalability or accuracy. In this work, we propose a symbolic approach, based on a variant of the classical substitution method, for formally verifying arithmetic and boolean masked programs. This approach is more accurate and scalable than existing approaches thanks to a careful design and implementation of key heuristics, algorithms and data structures involved in the verification process. We present all the details of this approach and the open-source tool called LeakageVerif which implements it as a python library, and which offers constructions for symbolic expressions and functions for their verification. We compare LeakageVerif to three existing state-of-the-art tools on a set of 46 masked programs, and we show that it has very good scalability and accuracy results while providing all the necessary constructs for describing algorithmic to assembly masking schemes. Finally, we also provide the set of 46 benchmarks, named MaskedVerifBenchs and written for comparing the different verification tools, in the hope that they will be useful to the community for future comparisons.",Security,National Security,The paper contributes to thwarting side-channel attacks; fortifying the national security aspect of software (Value: Security; Value Item: National Security).,"In the abstract, 'Paper X' explicitly states that side-channel attacks targeting cryptographic devices are a concern, and it proposes a protection technique called masking to mitigate these attacks. By addressing the security vulnerabilities and providing a more accurate and scalable approach for verifying masked programs, the paper contributes to enhancing the overall security of software. National security is an important aspect of security, and by extension, the contributions of 'Paper X' can be aligned with the value item National Security from a ""Software User"" perspective.",Agreed-Clarified,Agree,,Agree,,
Exp C,Exp K,2438,TSE,Security & Privacy,LeakageVerif: Efficient and Scalable Formal Verification of Leakage in Symbolic Expressions,"Side-channel attacks are a powerful class of attacks targeting cryptographic devices. Masking is a popular protection technique to thwart such attacks as it can be theoretically proven secure. However, correctly implementing masking schemes is a non-trivial task and error-prone. If several techniques have been proposed to formally verify masked implementations, they all come with limitations regarding expressiveness, scalability or accuracy. In this work, we propose a symbolic approach, based on a variant of the classical substitution method, for formally verifying arithmetic and boolean masked programs. This approach is more accurate and scalable than existing approaches thanks to a careful design and implementation of key heuristics, algorithms and data structures involved in the verification process. We present all the details of this approach and the open-source tool called LeakageVerif which implements it as a python library, and which offers constructions for symbolic expressions and functions for their verification. We compare LeakageVerif to three existing state-of-the-art tools on a set of 46 masked programs, and we show that it has very good scalability and accuracy results while providing all the necessary constructs for describing algorithmic to assembly masking schemes. Finally, we also provide the set of 46 benchmarks, named MaskedVerifBenchs and written for comparing the different verification tools, in the hope that they will be useful to the community for future comparisons.",Power,Social Power,The paper contributes to impeding unauthorized access to cryptographic devices; strengthening the user's social power in the software context (Value: Power; Value Item: Social Power).,"The paper's contribution in impeding unauthorized access to cryptographic devices aligns with the value item Social Power and its corresponding value Power from a ""Software User"" perspective because by strengthening the security of cryptographic devices, it enhances the user's control and influence over their own data and information, thereby empowering them in the software context. This aligns with the idea of social power, which is about having control and influence over others or one's own environment.",Agreed-Clarified,Agree,,Agree,,
Exp D,Exp K,2440,TSE,Software Testing & QA,Specification-Based Autonomous Driving System Testing,"Autonomous vehicle (AV) systems must be comprehensively tested and evaluated before they can be deployed. High-fidelity simulators such as CARLA or LGSVL allow this to be done safely in very realistic and highly customizable environments. Existing testing approaches, however, fail to test simulated AVs systematically, as they focus on specific scenarios and oracles (e.g., lane following scenario with the aEURoeno collisionaEUR requirement) and lack any coverage criteria measures. In this paper, we propose $\mathtt {AVUnit}$AVUnit, a framework for systematically testing AV systems against customizable correctness specifications. Designed modularly to support different simulators, $\mathtt {AVUnit}$AVUnit consists of two new languages for specifying dynamic properties of scenes (e.g., changing pedestrian behaviour after waypoints) and fine-grained assertions about the AV's journey. $\mathtt {AVUnit}$AVUnit further supports multiple fuzzing algorithms that automatically search for test cases that violate these assertions, using robustness and coverage measures as fitness metrics. We evaluated the implementation of $\mathtt {AVUnit}$AVUnit for the LGSVL+Apollo simulation environment, finding 19 kinds of issues in Apollo, which indicate that the open-source Apollo does not perform well in complex intersections and lane-changing related scenarios.",Security,Healthy,The paper contributes to the development of a framework for systematically testing Autonomous Vehicle systems; ensuring they are safe and efficient before deployment; which aligns with the value item Health and its corresponding value Security.,"I apologize for any confusion. In my justification, I labeled 'Paper X' as aligning with the value item Healthy and its corresponding value Security because the paper proposes a framework for systematically testing Autonomous Vehicle systems, which ultimately contributes to ensuring the safety and efficiency of these systems before deployment. By promoting the thorough evaluation and testing of AV systems, the paper indirectly addresses the value of Health by aiming to prevent accidents and promote security in the context of software users utilizing these systems.",Agreed-Justified,Agree,,Agree,,
Exp E,Exp J,2440,TSE,Software Testing & QA,Specification-Based Autonomous Driving System Testing,"Autonomous vehicle (AV) systems must be comprehensively tested and evaluated before they can be deployed. High-fidelity simulators such as CARLA or LGSVL allow this to be done safely in very realistic and highly customizable environments. Existing testing approaches, however, fail to test simulated AVs systematically, as they focus on specific scenarios and oracles (e.g., lane following scenario with the aEURoeno collisionaEUR requirement) and lack any coverage criteria measures. In this paper, we propose $\mathtt {AVUnit}$AVUnit, a framework for systematically testing AV systems against customizable correctness specifications. Designed modularly to support different simulators, $\mathtt {AVUnit}$AVUnit consists of two new languages for specifying dynamic properties of scenes (e.g., changing pedestrian behaviour after waypoints) and fine-grained assertions about the AV's journey. $\mathtt {AVUnit}$AVUnit further supports multiple fuzzing algorithms that automatically search for test cases that violate these assertions, using robustness and coverage measures as fitness metrics. We evaluated the implementation of $\mathtt {AVUnit}$AVUnit for the LGSVL+Apollo simulation environment, finding 19 kinds of issues in Apollo, which indicate that the open-source Apollo does not perform well in complex intersections and lane-changing related scenarios.",Security,Social Order,The testing framework proposed in 'Paper X' abstract is aimed at maintainaing the social order during AV driven rides; which aligns with the value item Social Order in Security category of Schwartz's taxonomy.,"I apologize for any confusion caused. My justification for aligning 'Paper X' with the value item Social Order and its corresponding value Security is that the proposed testing framework in the paper aims to ensure the safe and secure operation of autonomous vehicles. By systematically testing AV systems against customizable correctness specifications, the framework helps maintain social order by reducing the risk of accidents and ensuring the security of both the passengers and the surrounding environment. This alignment with Social Order and Security reflects the importance of ensuring a safe and orderly software user experience in the context of autonomous vehicles.",Agreed-Clarified,Agree,,Agree,,
Exp F,Exp J,2440,TSE,Software Testing & QA,Specification-Based Autonomous Driving System Testing,"Autonomous vehicle (AV) systems must be comprehensively tested and evaluated before they can be deployed. High-fidelity simulators such as CARLA or LGSVL allow this to be done safely in very realistic and highly customizable environments. Existing testing approaches, however, fail to test simulated AVs systematically, as they focus on specific scenarios and oracles (e.g., lane following scenario with the aEURoeno collisionaEUR requirement) and lack any coverage criteria measures. In this paper, we propose $\mathtt {AVUnit}$AVUnit, a framework for systematically testing AV systems against customizable correctness specifications. Designed modularly to support different simulators, $\mathtt {AVUnit}$AVUnit consists of two new languages for specifying dynamic properties of scenes (e.g., changing pedestrian behaviour after waypoints) and fine-grained assertions about the AV's journey. $\mathtt {AVUnit}$AVUnit further supports multiple fuzzing algorithms that automatically search for test cases that violate these assertions, using robustness and coverage measures as fitness metrics. We evaluated the implementation of $\mathtt {AVUnit}$AVUnit for the LGSVL+Apollo simulation environment, finding 19 kinds of issues in Apollo, which indicate that the open-source Apollo does not perform well in complex intersections and lane-changing related scenarios.",Achievement,Successful,Through the systematic testing of AV systems; 'Paper X' abstract indirectly contributes towards making the AV user successful in their journey; which aligns with the value item Successful under Achievement.,"The systematic testing of AV systems proposed in 'Paper X' aims to ensure the correctness and reliability of autonomous vehicles. By providing customizable correctness specifications and fine-grained assertions about the AV's journey, the paper contributes to making the AV user successful in their journey by ensuring that the AV performs as intended and safely navigates complex intersections and lane-changing scenarios. This aligns with the value item Successful under Achievement, as it emphasizes the importance of the AV system's performance and the user's ability to achieve their desired outcome of a successful and safe journey.",Agreed-Clarified,Agree,,Agree,,
Exp G,Exp K,2442,TSE,Security & Privacy,Metamorphic Testing for Web System Security,"Security testing aims at verifying that the software meets its security properties. In modern Web systems, however, this often entails the verification of the outputs generated when exercising the system with a very large set of inputs. Full automation is thus required to lower costs and increase the effectiveness of security testing. Unfortunately, to achieve such automation, in addition to strategies for automatically deriving test inputs, we need to address the oracle problem, which refers to the challenge, given an input for a system, of distinguishing correct from incorrect behavior (e.g., the response to be received after a specific HTTP GET request). In this paper, we propose Metamorphic Security Testing for Web-interactions (MST-wi), a metamorphic testing approach that integrates test input generation strategies inspired by mutational fuzzing and alleviates the oracle problem in security testing. It enables engineers to specify metamorphic relations (MRs) that capture many security properties of Web systems. To facilitate the specification of such MRs, we provide a domain-specific language accompanied by an Eclipse editor. MST-wi automatically collects the input data and transforms the MRs into executable Java code to automatically perform security testing. It automatically tests Web systems to detect vulnerabilities based on the relations and collected data. We provide a catalog of 76 system-agnostic MRs to automate security testing in Web systems. It covers 39% of the OWASP security testing activities not automated by state-of-the-art techniques; further, our MRs can automatically discover 102 different types of vulnerabilities, which correspond to 45% of the vulnerabilities due to violations of security design principles according to the MITRE CWE database. We also define guidelines that enable test engineers to improve the testability of the system under test with respect to our approach. We evaluated MST-wi effectiveness and scalability with two well-known Web systems (i.e., Jenkins and Joomla). It automatically detected 85% of their vulnerabilities and showed a high specificity (99.81% of the generated inputs do not lead to a false positive); our findings include a new security vulnerability detected in Jenkins. Finally, our results demonstrate that the approach scale, thus enabling automated security testing overnight.",Security,Healthy,The paper contributes to automation of security testing where system vulnerabilities are detected and thus users in web systems remain healthy avoiding risky scenarios.,"In the abstract of 'Paper X', it is clearly stated that the proposed approach, Metamorphic Security Testing for Web-interactions (MST-wi), automates security testing to detect vulnerabilities in web systems. By automating security testing and detecting vulnerabilities, the paper aims to ensure the security of web systems, which directly aligns with the value item Healthy and its corresponding value Security. This alignment indicates that the main contribution of 'Paper X' is focused on providing a secure software environment for users, where they can avoid potential risks and ensure their well-being while using web systems.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2442,TSE,Security & Privacy,Metamorphic Testing for Web System Security,"Security testing aims at verifying that the software meets its security properties. In modern Web systems, however, this often entails the verification of the outputs generated when exercising the system with a very large set of inputs. Full automation is thus required to lower costs and increase the effectiveness of security testing. Unfortunately, to achieve such automation, in addition to strategies for automatically deriving test inputs, we need to address the oracle problem, which refers to the challenge, given an input for a system, of distinguishing correct from incorrect behavior (e.g., the response to be received after a specific HTTP GET request). In this paper, we propose Metamorphic Security Testing for Web-interactions (MST-wi), a metamorphic testing approach that integrates test input generation strategies inspired by mutational fuzzing and alleviates the oracle problem in security testing. It enables engineers to specify metamorphic relations (MRs) that capture many security properties of Web systems. To facilitate the specification of such MRs, we provide a domain-specific language accompanied by an Eclipse editor. MST-wi automatically collects the input data and transforms the MRs into executable Java code to automatically perform security testing. It automatically tests Web systems to detect vulnerabilities based on the relations and collected data. We provide a catalog of 76 system-agnostic MRs to automate security testing in Web systems. It covers 39% of the OWASP security testing activities not automated by state-of-the-art techniques; further, our MRs can automatically discover 102 different types of vulnerabilities, which correspond to 45% of the vulnerabilities due to violations of security design principles according to the MITRE CWE database. We also define guidelines that enable test engineers to improve the testability of the system under test with respect to our approach. We evaluated MST-wi effectiveness and scalability with two well-known Web systems (i.e., Jenkins and Joomla). It automatically detected 85% of their vulnerabilities and showed a high specificity (99.81% of the generated inputs do not lead to a false positive); our findings include a new security vulnerability detected in Jenkins. Finally, our results demonstrate that the approach scale, thus enabling automated security testing overnight.",Achievement,Successful,Through the automation of security testing; the success rate of identifying vulnerabilities increased significantly; thereby improving the performance of users interacting with these systems reflecting the Achievement value.,"The justification for aligning 'Paper X' with the value item Successful and its corresponding value Achievement from a ""Software User"" perspective is based on the fact that the paper proposes a metamorphic testing approach (MST-wi) that automates security testing in web systems. By automatically detecting vulnerabilities and improving the performance of users interacting with these systems, the proposed approach increases the success rate of identifying vulnerabilities. This directly aligns with the Achievement value, as it aims to achieve successful outcomes in terms of security testing and overall system performance.",Agreed-Justified,Agree,,Agree,,
Exp B,Exp H,2442,TSE,Security & Privacy,Metamorphic Testing for Web System Security,"Security testing aims at verifying that the software meets its security properties. In modern Web systems, however, this often entails the verification of the outputs generated when exercising the system with a very large set of inputs. Full automation is thus required to lower costs and increase the effectiveness of security testing. Unfortunately, to achieve such automation, in addition to strategies for automatically deriving test inputs, we need to address the oracle problem, which refers to the challenge, given an input for a system, of distinguishing correct from incorrect behavior (e.g., the response to be received after a specific HTTP GET request). In this paper, we propose Metamorphic Security Testing for Web-interactions (MST-wi), a metamorphic testing approach that integrates test input generation strategies inspired by mutational fuzzing and alleviates the oracle problem in security testing. It enables engineers to specify metamorphic relations (MRs) that capture many security properties of Web systems. To facilitate the specification of such MRs, we provide a domain-specific language accompanied by an Eclipse editor. MST-wi automatically collects the input data and transforms the MRs into executable Java code to automatically perform security testing. It automatically tests Web systems to detect vulnerabilities based on the relations and collected data. We provide a catalog of 76 system-agnostic MRs to automate security testing in Web systems. It covers 39% of the OWASP security testing activities not automated by state-of-the-art techniques; further, our MRs can automatically discover 102 different types of vulnerabilities, which correspond to 45% of the vulnerabilities due to violations of security design principles according to the MITRE CWE database. We also define guidelines that enable test engineers to improve the testability of the system under test with respect to our approach. We evaluated MST-wi effectiveness and scalability with two well-known Web systems (i.e., Jenkins and Joomla). It automatically detected 85% of their vulnerabilities and showed a high specificity (99.81% of the generated inputs do not lead to a false positive); our findings include a new security vulnerability detected in Jenkins. Finally, our results demonstrate that the approach scale, thus enabling automated security testing overnight.",Achievement,Intelligent,With the help of MST-wi; users are now interacting with an intelligent system that is capable to test its own vulnerabilities and preventing any risks; demonstrating alignment with the value item of Intelligence under the Achievement value.,"In 'Paper X', the authors propose a metamorphic testing approach called MST-wi that allows users to interact with an intelligent system capable of automatically testing and detecting vulnerabilities in web systems. This alignment with the value item Intelligent and its corresponding value Achievement becomes evident as the system's capability to autonomously test and prevent risks demonstrates intelligence and achievement, providing users with a secure and reliable software experience.",Agreed-Justified,Agree,,Agree,,
Exp D,Exp K,2447,TSE,Security & Privacy,Detecting and Characterizing Propagation of Security Weaknesses in Puppet-Based Infrastructure Management,"Despite being beneficial for managing computing infrastructure automatically, Puppet manifests are susceptible to security weaknesses, e.g., hard-coded secrets and use of weak cryptography algorithms. Adequate mitigation of security weaknesses in Puppet manifests is thus necessary to secure computing infrastructure that are managed with Puppet manifests. A characterization of how security weaknesses propagate and affect Puppet-based infrastructure management, can inform practitioners on the relevance of the detected security weaknesses, as well as help them take necessary actions for mitigation. We conduct an empirical study with 17,629 Puppet manifests with Taint Tracker for Puppet Manifests (TaintPup). We observe 2.4 times more precision, and 1.8 times more F-measure for TaintPup, compared to that of a state-of-the-art security static analysis tool. From our empirical study, we observe security weaknesses to propagate into 4,457 resources, i.e, Puppet-specific code elements used to manage infrastructure. A single instance of a security weakness can propagate into as many as 35 distinct resources. We observe security weaknesses to propagate into 7 categories of resources, which include resources used to manage continuous integration servers and network controllers. According to our survey with 24 practitioners, propagation of security weaknesses into data storage-related resources is rated to have the most severe impact for Puppet-based infrastructure management.",Security,National Security,By addressing security weaknesses in computing infrastructure management; the paper aligns with the value of National Security; ensuring safe use of computing resources.,"In the paper abstract, it is explicitly mentioned that the aim of 'Paper X' is to mitigate security weaknesses in Puppet manifests and secure computing infrastructure managed with Puppet manifests. By addressing these security weaknesses, the paper aligns with the value of National Security, as it focuses on ensuring the safe use of computing resources and protecting the integrity and confidentiality of infrastructure managed with Puppet. This directly contributes to the value of Security, which is a fundamental aspect of National Security. Therefore, the alignment of 'Paper X' with the value item National Security and the corresponding value Security is evident based on its stated contributions in the abstract.",Agreed-Clarified,Agree,,Agree,,
Exp E,Exp J,2447,TSE,Security & Privacy,Detecting and Characterizing Propagation of Security Weaknesses in Puppet-Based Infrastructure Management,"Despite being beneficial for managing computing infrastructure automatically, Puppet manifests are susceptible to security weaknesses, e.g., hard-coded secrets and use of weak cryptography algorithms. Adequate mitigation of security weaknesses in Puppet manifests is thus necessary to secure computing infrastructure that are managed with Puppet manifests. A characterization of how security weaknesses propagate and affect Puppet-based infrastructure management, can inform practitioners on the relevance of the detected security weaknesses, as well as help them take necessary actions for mitigation. We conduct an empirical study with 17,629 Puppet manifests with Taint Tracker for Puppet Manifests (TaintPup). We observe 2.4 times more precision, and 1.8 times more F-measure for TaintPup, compared to that of a state-of-the-art security static analysis tool. From our empirical study, we observe security weaknesses to propagate into 4,457 resources, i.e, Puppet-specific code elements used to manage infrastructure. A single instance of a security weakness can propagate into as many as 35 distinct resources. We observe security weaknesses to propagate into 7 categories of resources, which include resources used to manage continuous integration servers and network controllers. According to our survey with 24 practitioners, propagation of security weaknesses into data storage-related resources is rated to have the most severe impact for Puppet-based infrastructure management.",Power,Authority,The paper identifies how security weaknesses propagate and their impact on infrastructure management. This knowledge gives software users greater authority and control over their computing infrastructure.,"In the paper, it is stated that the knowledge of how security weaknesses propagate in Puppet manifests gives software users greater authority and control over their computing infrastructure. This aligns with the value item Authority and its corresponding value Power because having the ability to understand and mitigate security weaknesses gives software users the power to make informed decisions and take necessary actions to secure their infrastructure. By having this authority and control, software users can ensure the reliability and security of their computing environment, which is a valuable aspect from their perspective.",Agreed-Clarified,Agree,,Agree,,
Exp F,Exp J,2450,TSE,Software Testing & QA,Taming Android Fragmentation Through Lightweight Crowdsourced Testing,"Android fragmentation refers to the overwhelming diversity of Android devices and OS versions. These lead to the impossibility of testing an app on every supported device, leaving a number of compatibility bugs scattered in the community and thereby resulting in poor user experiences. To mitigate this, our fellow researchers have designed various works to automatically detect such compatibility issues. However, the current state-of-the-art tools can only be used to detect specific kinds of compatibility issues (i.e., compatibility issues caused by API signature evolution), i.e., many other essential types of compatibility issues are still unrevealed. For example, customized OS versions on real devices and semantic changes of OS could lead to serious compatibility issues, which are non-trivial to be detected statically. To this end, we propose a novel, lightweight, crowdsourced testing approach, LazyCow, to fill this research gap and enable the possibility of taming Android fragmentation through crowdsourced efforts. Specifically, crowdsourced testing is an emerging alternative to conventional mobile testing mechanisms that allow developers to test their products on real devices to pinpoint platform-specific issues. Experimental results on thousands of test cases on real-world Android devices show that LazyCow is effective in automatically identifying and verifying API-induced compatibility issues. Also, after investigating the user experience through qualitative metrics, usersaEUR(tm) satisfaction provides strong evidence that LazyCow is useful and welcome in practice.",Achievement,Intelligent,The paper introduces a model; LazyCow; that has been shown to effectively identify and verify compatibility issues in Android devices. This could be seen as contributing to the Intelligence of the end-users; since they can use this tool to better understand and address API-induced compatibility issues; which aligns with the value of Achievement.,"In the context of a software user, the main contribution of 'Paper X' is the development of LazyCow, a crowdsourced testing approach that automatically identifies and verifies compatibility issues in Android devices. By providing users with a tool to address API-induced compatibility issues, LazyCow enables them to better understand and troubleshoot these issues, thereby enhancing their intelligence in navigating and optimizing their software experience. This alignment with the value of Achievement stems from the empowerment and ability it offers to software users in effectively addressing and resolving compatibility problems, ultimately resulting in a more successful and fulfilling software usage experience.",Agreed-Clarified,Agree,,Agree,,
Exp G,Exp K,2450,TSE,Software Testing & QA,Taming Android Fragmentation Through Lightweight Crowdsourced Testing,"Android fragmentation refers to the overwhelming diversity of Android devices and OS versions. These lead to the impossibility of testing an app on every supported device, leaving a number of compatibility bugs scattered in the community and thereby resulting in poor user experiences. To mitigate this, our fellow researchers have designed various works to automatically detect such compatibility issues. However, the current state-of-the-art tools can only be used to detect specific kinds of compatibility issues (i.e., compatibility issues caused by API signature evolution), i.e., many other essential types of compatibility issues are still unrevealed. For example, customized OS versions on real devices and semantic changes of OS could lead to serious compatibility issues, which are non-trivial to be detected statically. To this end, we propose a novel, lightweight, crowdsourced testing approach, LazyCow, to fill this research gap and enable the possibility of taming Android fragmentation through crowdsourced efforts. Specifically, crowdsourced testing is an emerging alternative to conventional mobile testing mechanisms that allow developers to test their products on real devices to pinpoint platform-specific issues. Experimental results on thousands of test cases on real-world Android devices show that LazyCow is effective in automatically identifying and verifying API-induced compatibility issues. Also, after investigating the user experience through qualitative metrics, usersaEUR(tm) satisfaction provides strong evidence that LazyCow is useful and welcome in practice.",Security,Healthy,LazyCow is proven to be an effective method of crowdsourced testing; which can improve the health/functionality of users' software environment through identifying potential issues. This aligns with the value of Security and its corresponding value item Healthy.,"LazyCow aligns with the value item Healthy and its corresponding value Security from a ""Software User"" perspective because it provides a method of crowdsourced testing that can effectively identify and verify compatibility issues in real-world Android devices. By detecting and addressing these issues, LazyCow contributes to improving the overall health and functionality of users' software environment, ensuring a more secure and reliable user experience.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2450,TSE,Software Testing & QA,Taming Android Fragmentation Through Lightweight Crowdsourced Testing,"Android fragmentation refers to the overwhelming diversity of Android devices and OS versions. These lead to the impossibility of testing an app on every supported device, leaving a number of compatibility bugs scattered in the community and thereby resulting in poor user experiences. To mitigate this, our fellow researchers have designed various works to automatically detect such compatibility issues. However, the current state-of-the-art tools can only be used to detect specific kinds of compatibility issues (i.e., compatibility issues caused by API signature evolution), i.e., many other essential types of compatibility issues are still unrevealed. For example, customized OS versions on real devices and semantic changes of OS could lead to serious compatibility issues, which are non-trivial to be detected statically. To this end, we propose a novel, lightweight, crowdsourced testing approach, LazyCow, to fill this research gap and enable the possibility of taming Android fragmentation through crowdsourced efforts. Specifically, crowdsourced testing is an emerging alternative to conventional mobile testing mechanisms that allow developers to test their products on real devices to pinpoint platform-specific issues. Experimental results on thousands of test cases on real-world Android devices show that LazyCow is effective in automatically identifying and verifying API-induced compatibility issues. Also, after investigating the user experience through qualitative metrics, usersaEUR(tm) satisfaction provides strong evidence that LazyCow is useful and welcome in practice.",Benevolence,Helpful,"LazyCow uses crowdsourced efforts for testing applications. This collective effort where one's testing can help others find out potential compatibility issues in their devices could be seen as contributing to being ""Helpful"" towards others; aligning with the value of Benevolence.","LazyCow's use of crowdsourced testing aligns with the value item Helpful and its corresponding value Benevolence from a ""Software User"" perspective because the collaborative nature of the approach allows users to contribute to the detection of compatibility issues in Android devices. By sharing their test results and experiences, users are helping others in the community to identify potential problems and improve their user experiences. This act of providing assistance and support to fellow users reflects the value of being Helpful and demonstrates a sense of benevolence towards others in the software context.",Agreed-Justified,Agree,,Agree,,
Exp B,Exp H,2452,TSE,Security & Privacy,Combatting Front-Running in Smart Contracts: Attack Mining Benchmark Construction and Vulnerability Detector Evaluation,"Front-running attacks have been a major concern on the blockchain. Attackers launch front-running attacks by inserting additional transactions before upcoming victim transactions to manipulate victim transaction executions and make profits. Recent studies have shown that front-running attacks are prevalent on the Ethereum blockchain and have caused millions of US dollars loss. It is the vulnerabilities in smart contracts, which are blockchain programs invoked by transactions, that enable the front-running attack opportunities. Although techniques to detect front-running vulnerabilities have been proposed, their performance on real-world vulnerable contracts is unclear. There is no large-scale benchmark based on real attacks to evaluate their capabilities. We make four contributions in this paper. First, we design an effective algorithm to mine real-world attacks in the blockchain history. The evaluation shows that our mining algorithm is more effective and comprehensive, achieving higher recall in finding real attacks than the previous study. Second, we propose an automated and scalable vulnerability localization approach to localize code snippets in smart contracts that enable front-running attacks. The evaluation also shows that our localization approaches are effective in achieving higher precision in pinpointing vulnerabilities compared to the baseline technique. Third, we build a benchmark consisting of 513 real-world attacks with vulnerable code labeled in 235 distinct smart contracts, which is useful to help understand the nature of front-running attacks, vulnerabilities in smart contracts, and evaluate vulnerability detection techniques. Last but not least, we conduct an empirical evaluation of seven state-of-the-art vulnerability detection techniques on our benchmark. The evaluation experiment reveals the inadequacy of existing techniques in detecting front-running vulnerabilities, with a low recall of $\leq$a%0$? 6.04%. Our further analysis identifies four common limitations in existing techniques: lack of support for inter-contract analysis, inefficient constraint solving for cryptographic operations, improper vulnerability patterns, and lack of token support.",Security,Healthy,"The paper is focused on addressing vulnerabilities in smart contracts. This contributes to the value item ""Healthy"" under the value ""Security"" in a software context where a healthy system is a secure system free from vulnerabilities.","In the context of a software user, the value item ""Healthy"" aligns with the main contribution of 'Paper X' because it addresses vulnerabilities in smart contracts. By identifying and localizing code snippets that enable front-running attacks, the paper aims to make the system more secure and free from vulnerabilities. A healthy software system is one that is protected against potential attacks and ensures the integrity and safety of user transactions. Thus, the alignment between 'Paper X' and the value item ""Healthy"" reflects the emphasis on a secure and trustworthy software environment for the user.",Agreed-Justified,Disagree,reconcile with coder_2,Disagree,"The justification doesn't align with the value item of the healthy
",
Exp C,Exp K,2452,TSE,Security & Privacy,Combatting Front-Running in Smart Contracts: Attack Mining Benchmark Construction and Vulnerability Detector Evaluation,"Front-running attacks have been a major concern on the blockchain. Attackers launch front-running attacks by inserting additional transactions before upcoming victim transactions to manipulate victim transaction executions and make profits. Recent studies have shown that front-running attacks are prevalent on the Ethereum blockchain and have caused millions of US dollars loss. It is the vulnerabilities in smart contracts, which are blockchain programs invoked by transactions, that enable the front-running attack opportunities. Although techniques to detect front-running vulnerabilities have been proposed, their performance on real-world vulnerable contracts is unclear. There is no large-scale benchmark based on real attacks to evaluate their capabilities. We make four contributions in this paper. First, we design an effective algorithm to mine real-world attacks in the blockchain history. The evaluation shows that our mining algorithm is more effective and comprehensive, achieving higher recall in finding real attacks than the previous study. Second, we propose an automated and scalable vulnerability localization approach to localize code snippets in smart contracts that enable front-running attacks. The evaluation also shows that our localization approaches are effective in achieving higher precision in pinpointing vulnerabilities compared to the baseline technique. Third, we build a benchmark consisting of 513 real-world attacks with vulnerable code labeled in 235 distinct smart contracts, which is useful to help understand the nature of front-running attacks, vulnerabilities in smart contracts, and evaluate vulnerability detection techniques. Last but not least, we conduct an empirical evaluation of seven state-of-the-art vulnerability detection techniques on our benchmark. The evaluation experiment reveals the inadequacy of existing techniques in detecting front-running vulnerabilities, with a low recall of $\leq$a%0$? 6.04%. Our further analysis identifies four common limitations in existing techniques: lack of support for inter-contract analysis, inefficient constraint solving for cryptographic operations, improper vulnerability patterns, and lack of token support.",Security,Sense of Belonging,"The main contributions of the paper revolve around the identification and localization of security vulnerabilities; creating a sense of ""Sense of Belonging"" in the software users by ensuring the software they use is secure and reliable.","In the context of 'Paper X', the contributions directly align with the value item Sense of Belonging and its corresponding value Security from a ""Software User"" perspective by addressing the concerns of users regarding the security of the software they use. By designing an effective algorithm to mine real-world attacks, proposing a vulnerability localization approach, and building a benchmark, the paper aims to provide software users with a sense of belonging and assurance that their software is secure, thus fulfilling their need for security and fostering a sense of trust and belonging in the software ecosystem.",Agreed-Clarified,Agree,,Agree,,
Exp D,Exp K,2452,TSE,Security & Privacy,Combatting Front-Running in Smart Contracts: Attack Mining Benchmark Construction and Vulnerability Detector Evaluation,"Front-running attacks have been a major concern on the blockchain. Attackers launch front-running attacks by inserting additional transactions before upcoming victim transactions to manipulate victim transaction executions and make profits. Recent studies have shown that front-running attacks are prevalent on the Ethereum blockchain and have caused millions of US dollars loss. It is the vulnerabilities in smart contracts, which are blockchain programs invoked by transactions, that enable the front-running attack opportunities. Although techniques to detect front-running vulnerabilities have been proposed, their performance on real-world vulnerable contracts is unclear. There is no large-scale benchmark based on real attacks to evaluate their capabilities. We make four contributions in this paper. First, we design an effective algorithm to mine real-world attacks in the blockchain history. The evaluation shows that our mining algorithm is more effective and comprehensive, achieving higher recall in finding real attacks than the previous study. Second, we propose an automated and scalable vulnerability localization approach to localize code snippets in smart contracts that enable front-running attacks. The evaluation also shows that our localization approaches are effective in achieving higher precision in pinpointing vulnerabilities compared to the baseline technique. Third, we build a benchmark consisting of 513 real-world attacks with vulnerable code labeled in 235 distinct smart contracts, which is useful to help understand the nature of front-running attacks, vulnerabilities in smart contracts, and evaluate vulnerability detection techniques. Last but not least, we conduct an empirical evaluation of seven state-of-the-art vulnerability detection techniques on our benchmark. The evaluation experiment reveals the inadequacy of existing techniques in detecting front-running vulnerabilities, with a low recall of $\leq$a%0$? 6.04%. Our further analysis identifies four common limitations in existing techniques: lack of support for inter-contract analysis, inefficient constraint solving for cryptographic operations, improper vulnerability patterns, and lack of token support.",Benevolence,Responsibility,"By designing an effective algorithm to mine real-world attacks; the Paper X is promoting responsibility of effectively detecting and handling security vulnerabilities; directly aligning with the value item ""Responsibility"" under the value ""Benevolence"".","The main contribution of 'Paper X' is the design of an effective algorithm to mine real-world attacks and propose an automated vulnerability detection approach. By doing so, the paper promotes the responsibility of detecting and handling security vulnerabilities in smart contracts, which directly aligns with the value item ""Responsibility"" under the value ""Benevolence"" from a ""Software User"" perspective. This alignment demonstrates the paper's focus on ensuring the well-being and protection of users within the software context.",Agreed-Clarified,Agree,,Agree,,
Exp E,Exp J,2457,TSE,AI & Machine Learning,A Search-Based Testing Approach for Deep Reinforcement Learning Agents,"Deep Reinforcement Learning (DRL) algorithms have been increasingly employed during the last decade to solve various decision-making problems such as autonomous driving, trading decisions, and robotics. However, these algorithms have faced great challenges when deployed in safety-critical environments since they often exhibit erroneous behaviors that can lead to potentially critical errors. One of the ways to assess the safety of DRL agents is to test them to detect possible faults leading to critical failures during their execution. This raises the question of how we can efficiently test DRL policies to ensure their correctness and adherence to safety requirements. Most existing works on testing DRL agents use adversarial attacks that perturb states or actions of the agent. However, such attacks often lead to unrealistic states of the environment. Furthermore, their main goal is to test the robustness of DRL agents rather than testing the compliance of the agentsaEUR(tm) policies with respect to requirements. Due to the huge state space of DRL environments, the high cost of test execution, and the black-box nature of DRL algorithms, exhaustive testing of DRL agents is impossible. In this paper, we propose a Search-based Testing Approach of Reinforcement Learning Agents (STARLA) to test the policy of a DRL agent by effectively searching for failing executions of the agent within a limited testing budget. We rely on machine learning models and a dedicated genetic algorithm to narrow the search toward faulty episodes (i.e., sequences of states and actions produced by the DRL agent). We apply STARLA on Deep-Q-Learning agents trained on two different RL problems widely used as benchmarks and show that STARLA significantly outperforms Random Testing by detecting more faults related to the agent's policy. We also investigate how to extract rules that characterize faulty episodes of the DRL agent using our search results. Such rules can be used to understand the conditions under which the agent fails and thus assess the risks of deploying it.",Achievement,Successful,The paper is about a Search-based Testing Approach of Reinforcement Learning Agents (STARLA) which aims to ensure the correctness and safety requirements of Deep Reinforcement Learning algorithms used in various decision-making problems. This aligns with the value item Successful and its corresponding value Achievement; considering the user's success in achieving desired outcomes and avoiding potential errors or risks when using the software.,"The justification for aligning 'Paper X' with the value item Successful and its corresponding value Achievement from a ""Software User"" perspective is based on the goal of the paper to ensure correctness and safety requirements of Deep Reinforcement Learning algorithms. By providing a Search-based Testing Approach, the paper aims to enhance the success of users in achieving desired outcomes and avoiding potential errors or risks when using the software. This aligns with the user's need for successful outcomes and the value placed on achieving goals and being capable in the context of software usage.",Agreed-Justified,Agree,,Agree,,
Exp F,Exp J,2457,TSE,AI & Machine Learning,A Search-Based Testing Approach for Deep Reinforcement Learning Agents,"Deep Reinforcement Learning (DRL) algorithms have been increasingly employed during the last decade to solve various decision-making problems such as autonomous driving, trading decisions, and robotics. However, these algorithms have faced great challenges when deployed in safety-critical environments since they often exhibit erroneous behaviors that can lead to potentially critical errors. One of the ways to assess the safety of DRL agents is to test them to detect possible faults leading to critical failures during their execution. This raises the question of how we can efficiently test DRL policies to ensure their correctness and adherence to safety requirements. Most existing works on testing DRL agents use adversarial attacks that perturb states or actions of the agent. However, such attacks often lead to unrealistic states of the environment. Furthermore, their main goal is to test the robustness of DRL agents rather than testing the compliance of the agentsaEUR(tm) policies with respect to requirements. Due to the huge state space of DRL environments, the high cost of test execution, and the black-box nature of DRL algorithms, exhaustive testing of DRL agents is impossible. In this paper, we propose a Search-based Testing Approach of Reinforcement Learning Agents (STARLA) to test the policy of a DRL agent by effectively searching for failing executions of the agent within a limited testing budget. We rely on machine learning models and a dedicated genetic algorithm to narrow the search toward faulty episodes (i.e., sequences of states and actions produced by the DRL agent). We apply STARLA on Deep-Q-Learning agents trained on two different RL problems widely used as benchmarks and show that STARLA significantly outperforms Random Testing by detecting more faults related to the agent's policy. We also investigate how to extract rules that characterize faulty episodes of the DRL agent using our search results. Such rules can be used to understand the conditions under which the agent fails and thus assess the risks of deploying it.",Security,Healthy,The paper highlights the need to test and assess the safety of DRL agents to avoid critical errors. This aligns with the value item Healthy and its corresponding value Security; as users can benefit from the software's focus on maintaining a safe; error-free environment.,"The justification for labeling 'Paper X' as aligning with the value item Healthy and its corresponding value Security from a ""Software User"" perspective is that the paper explicitly discusses the challenges of deploying DRL agents in safety-critical environments and the need to test them for correctness and adherence to safety requirements. This emphasis on safety and avoiding critical errors directly aligns with the value of Security, as it prioritizes the well-being and protection of users. By ensuring the correctness and safety of DRL agents, users can trust and rely on the software to maintain a secure and error-free environment, which aligns with the value of Healthy.",Agreed-Justified,Agree,,Agree,,
Exp G,Exp K,2457,TSE,AI & Machine Learning,A Search-Based Testing Approach for Deep Reinforcement Learning Agents,"Deep Reinforcement Learning (DRL) algorithms have been increasingly employed during the last decade to solve various decision-making problems such as autonomous driving, trading decisions, and robotics. However, these algorithms have faced great challenges when deployed in safety-critical environments since they often exhibit erroneous behaviors that can lead to potentially critical errors. One of the ways to assess the safety of DRL agents is to test them to detect possible faults leading to critical failures during their execution. This raises the question of how we can efficiently test DRL policies to ensure their correctness and adherence to safety requirements. Most existing works on testing DRL agents use adversarial attacks that perturb states or actions of the agent. However, such attacks often lead to unrealistic states of the environment. Furthermore, their main goal is to test the robustness of DRL agents rather than testing the compliance of the agentsaEUR(tm) policies with respect to requirements. Due to the huge state space of DRL environments, the high cost of test execution, and the black-box nature of DRL algorithms, exhaustive testing of DRL agents is impossible. In this paper, we propose a Search-based Testing Approach of Reinforcement Learning Agents (STARLA) to test the policy of a DRL agent by effectively searching for failing executions of the agent within a limited testing budget. We rely on machine learning models and a dedicated genetic algorithm to narrow the search toward faulty episodes (i.e., sequences of states and actions produced by the DRL agent). We apply STARLA on Deep-Q-Learning agents trained on two different RL problems widely used as benchmarks and show that STARLA significantly outperforms Random Testing by detecting more faults related to the agent's policy. We also investigate how to extract rules that characterize faulty episodes of the DRL agent using our search results. Such rules can be used to understand the conditions under which the agent fails and thus assess the risks of deploying it.",Security,National Security,The proposed testing approach would lead to understanding the conditions under which the DRL agent fails and assess the risks of deploying it. This alignment with the value item National Security under the value of Security suggests an association between the paper's main contributions and considerations of national-level consequences; such as the application of autonomous driving mentioned in the abstract.,"The justification for aligning 'Paper X' with the value item National Security and the corresponding value Security from a ""Software User"" perspective is based on the fact that the proposed testing approach aims to understand the conditions under which the DRL agent fails and assess the risks of deploying it. By ensuring the correctness and adherence to safety requirements of DRL agents, particularly in safety-critical environments like autonomous driving, the paper directly addresses concerns related to national-level consequences and the security of individuals utilizing software systems.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2459,TSE,Mobile & IoT,Driving the Technology Value Stream by Analyzing App Reviews,"An emerging feature of mobile application software is the need to quickly produce new versions to solve problems that emerged in previous versions. This helps adapt to changing user needs and preferences. In a continuous software development process, the user reviews collected by the apps themselves can play a crucial role to detect which components need to be reworked. This paper proposes a novel framework that enables software companies to drive their technology value stream based on the feedback (or reviews) provided by the end-users of an application. The proposed end-to-end framework exploits different Natural Language Processing (NLP) tasks to best understand the needs and goals of the end users. We also provide a thorough and in-depth analysis of the framework, the performance of each of the modules, and the overall contribution in driving the technology value stream. An analysis of reviews with sixteen popular Android Play Store applications from various genres over a long period of time provides encouraging evidence of the effectiveness of the proposed approach.",Stimulation,Variation in Life,Paper X emphasizes on the development of new versions of mobile application software for adapting to changing user needs and this directly aligns with 'Variation in Life' as it involves changes and improvements over time.,"In 'Paper X', the main contribution is focused on the development of new versions of mobile application software to adapt to changing user needs. This aligns with the value item of 'Variation in Life', as it involves introducing changes and improvements over time. Specifically, by continuously producing new versions, the software company can provide users with varied and updated features, enhancing their experience and offering stimulation through the discovery of new functionalities. Therefore, the alignment between the main contribution of 'Paper X' and the value item of 'Variation in Life' from a ""Software User"" perspective is evident.",Agreed-Clarified,Agree,,Agree,,
Exp B,Exp H,2459,TSE,Mobile & IoT,Driving the Technology Value Stream by Analyzing App Reviews,"An emerging feature of mobile application software is the need to quickly produce new versions to solve problems that emerged in previous versions. This helps adapt to changing user needs and preferences. In a continuous software development process, the user reviews collected by the apps themselves can play a crucial role to detect which components need to be reworked. This paper proposes a novel framework that enables software companies to drive their technology value stream based on the feedback (or reviews) provided by the end-users of an application. The proposed end-to-end framework exploits different Natural Language Processing (NLP) tasks to best understand the needs and goals of the end users. We also provide a thorough and in-depth analysis of the framework, the performance of each of the modules, and the overall contribution in driving the technology value stream. An analysis of reviews with sixteen popular Android Play Store applications from various genres over a long period of time provides encouraging evidence of the effectiveness of the proposed approach.",Stimulation,Excitement in Life,The proposal of a novel framework for software companies to drive their technology value stream based on user feedback directly explains 'Excitement in Life' as this creates a sense of excitement for the user in the process of engagement with the application.,"The justification for aligning 'Paper X' with the value item Excitement in Life and its corresponding value Stimulation from a ""Software User"" perspective is that the proposed framework for software companies to drive their technology value stream based on user feedback creates a sense of excitement for the user in the process of engaging with the application. This excitement stems from the anticipation of new versions that solve problems and adapt to changing needs and preferences. By continuously improving the application based on user reviews, the framework enhances the user experience and brings a sense of excitement and stimulation to the user's life.",Agreed-Justified,Agree,,Agree,,
Exp D,Exp K,2461,TSE,Security & Privacy,Demystifying Random Number in Ethereum Smart Contract: Taxonomy Vulnerability Identification and Attack Detection,"Recent years have witnessed explosive growth in blockchain smart contract applications. As smart contracts become increasingly popular and carry trillion dollars worth of digital assets, they become more of an appealing target for attackers, who have exploited vulnerabilities in smart contracts to cause catastrophic economic losses. Notwithstanding a proliferation of work that has been developed to detect an impressive list of vulnerabilities, the bad randomness vulnerability is overlooked by many existing tools. In this article, we make the first attempt to provide a systematic analysis of random numbers in Ethereum smart contracts, by investigating the principles behind pseudo-random number generation and organizing them into a taxonomy. We also lucubrate various attacks against bad random numbers and group them into four categories. Furthermore, we present RNVulDet aEUR"" a tool that incorporates taint analysis techniques to automatically identify bad randomness vulnerabilities and detect corresponding attack transactions. To extensively verify the effectiveness of RNVulDet, we construct three new datasets: i) 34 well-known contracts that are reported to possess bad randomness vulnerabilities, ii) 214 popular contracts that have been rigorously audited before launch and are regarded as free of bad randomness vulnerabilities, and iii) a dataset consisting of 47,668 smart contracts and 49,951 suspicious transactions. We compare RNVulDet with three state-of-the-art smart contract vulnerability detectors, and our tool significantly outperforms them. Meanwhile, RNVulDet spends 2.98 s per contract on average, in most cases orders-of-magnitude faster than other tools. RNVulDet successfully reveals 44,264 attack transactions. Our implementation and datasets are released, hoping to inspire others.",Achievement,Intelligent,The tool RNVulDet developed in the paper detects vulnerabilities in smart contracts and protects users' assets; valuing user's Intelligent management of their assets,"The use of the RNVulDet tool in 'Paper X' aligns with the value item Intelligent and its corresponding value Achievement from a ""Software User"" perspective because it facilitates the intelligent management of user assets by detecting vulnerabilities in smart contracts. By identifying and protecting against potential attacks on users' assets, the tool enables users to make intelligent decisions and take actions that contribute to achieving their desired outcomes in the software context.",Agreed-Justified,Agree,,Agree,,
Exp E,Exp J,2464,TSE,Mobile & IoT,Detecting Android API Compatibility Issues With API Differences,"Android application programming interface (API) enables app developers to harness the functionalities of Android devices by interfacing with services and hardware using a Software Development Kit (SDK). However, API frequently evolves together with its associated SDK, and compatibility issues may arise when the API level supported by the underlying device differs from the API level targeted by app developers. These issues can lead to unexpected behaviors, resulting in a bad user experience. This article presents ACID, a novel approach to detecting Android API compatibility issues induced by API evolution. It detects both API invocation compatibility issues and API callback compatibility issues using API differences and static analysis of the app code. Experiments with 20 benchmark apps show that ACID is more accurate and faster than the state-of-the-art techniques in detecting API compatibility issues. The application of ACID on 2965 real-world apps further demonstrates its practical applicability. To eliminate the false positives reported by ACID, this article also presents a simple yet effective method to quickly verify the compatibility issues by selecting and executing the relevant tests from app's test suite, and experimental results demonstrate the verification method can eliminate most false positives when app's test suite has good coverage of the API usages.",Self Direction,Independent,Paper X's ACID approach aims to provide software users with a more independent experience by mitigating API compatibility issues; thus resulting in less dependency on developers and a smoother operation.,"In 'Paper X', the ACID approach directly aligns with the value item Independent and its corresponding value Self Direction from a ""Software User"" perspective. By detecting and mitigating API compatibility issues, ACID empowers software users to have a more independent experience with Android applications. This means that users can rely less on developers for resolving compatibility problems and have a smoother operation of the apps they use, enabling them to exercise their freedom of choice and self-direction in selecting and using software.",Agreed-Justified,Agree,,Agree,,
Exp F,Exp J,2464,TSE,Mobile & IoT,Detecting Android API Compatibility Issues With API Differences,"Android application programming interface (API) enables app developers to harness the functionalities of Android devices by interfacing with services and hardware using a Software Development Kit (SDK). However, API frequently evolves together with its associated SDK, and compatibility issues may arise when the API level supported by the underlying device differs from the API level targeted by app developers. These issues can lead to unexpected behaviors, resulting in a bad user experience. This article presents ACID, a novel approach to detecting Android API compatibility issues induced by API evolution. It detects both API invocation compatibility issues and API callback compatibility issues using API differences and static analysis of the app code. Experiments with 20 benchmark apps show that ACID is more accurate and faster than the state-of-the-art techniques in detecting API compatibility issues. The application of ACID on 2965 real-world apps further demonstrates its practical applicability. To eliminate the false positives reported by ACID, this article also presents a simple yet effective method to quickly verify the compatibility issues by selecting and executing the relevant tests from app's test suite, and experimental results demonstrate the verification method can eliminate most false positives when app's test suite has good coverage of the API usages.",Security,Healthy,By detecting API compatibility issues; ACID strives to maintain the healthiness of the user's software environment; supporting the value item of being Healthy under Schwartz's value of Security.,"In the provided justification, the alignment of ""Paper X"" with the value item Healthy and its corresponding value Security is based on the fact that by detecting API compatibility issues, ACID aims to maintain the healthiness of the user's software environment. This aligns with the value item of being Healthy under Schwartz's value of Security, as a secure and stable software environment contributes to the overall well-being and safety of the user. By addressing compatibility issues, the paper ensures that users can confidently use and rely on their software without facing unexpected behaviors or negative user experiences, thus promoting a sense of security and maintaining the healthiness of the software system.",Agreed-Clarified,Agree,,Agree,,
Exp G,Exp K,2464,TSE,Mobile & IoT,Detecting Android API Compatibility Issues With API Differences,"Android application programming interface (API) enables app developers to harness the functionalities of Android devices by interfacing with services and hardware using a Software Development Kit (SDK). However, API frequently evolves together with its associated SDK, and compatibility issues may arise when the API level supported by the underlying device differs from the API level targeted by app developers. These issues can lead to unexpected behaviors, resulting in a bad user experience. This article presents ACID, a novel approach to detecting Android API compatibility issues induced by API evolution. It detects both API invocation compatibility issues and API callback compatibility issues using API differences and static analysis of the app code. Experiments with 20 benchmark apps show that ACID is more accurate and faster than the state-of-the-art techniques in detecting API compatibility issues. The application of ACID on 2965 real-world apps further demonstrates its practical applicability. To eliminate the false positives reported by ACID, this article also presents a simple yet effective method to quickly verify the compatibility issues by selecting and executing the relevant tests from app's test suite, and experimental results demonstrate the verification method can eliminate most false positives when app's test suite has good coverage of the API usages.",Achievement,Successful,The effective verification method proposed in Paper X facilitates successful application operation; aligning with the value item of Successful; thereby promoting the user's sense of Achievement.,"The effective verification method proposed in Paper X ensures that compatibility issues are quickly identified and resolved, which leads to a successful operation of the application. This aligns with the value item of Successful from a user perspective because it promotes a positive user experience and the achievement of their goals when using the software.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2466,TSE,Security & Privacy,Can We Trust the Phone Vendors? Comprehensive Security Measurements on the Android Firmware Ecosystem,"Android is the most popular smartphone platform with over 85% market share. Its success is built on openness, and phone vendors can utilize the Android source code to make customized products with unique software/hardware features. On the other hand, the fragmentation and customization of Android also bring many security risks that have attracted the attention of researchers. Many efforts were put in to investigate the security of customized Android firmware. However, most of the previous works focus on designing efficient analysis tools or analyzing particular aspects of the firmware. There still lacks a panoramic view of Android firmware ecosystem security and the corresponding understandings based on large-scale firmware datasets. In this work, we made a large-scale comprehensive measurement of the Android firmware ecosystem security. Our study is based on 8,325 firmware images from 153 vendors and 813 Android-related CVEs, which is the largest Android firmware dataset ever used for security measurements. In particular, our study followed a series of research questions, covering vulnerabilities, patches, security updates, and pre-installed apps. To automate the analysis process, we designed a framework, AndScanner+, to complete firmware crawling, firmware parsing, patch analysis, and app analysis. Through massive data analysis and case explorations, several interesting findings are obtained. For example, the patch delay and missing issues are widespread in Android firmware images, say 31.4% and 5.6% of all images, respectively. The latest images of several phones still contain vulnerable pre-installed apps, and even the corresponding vulnerabilities have been publicly disclosed. In addition to data measurements, we also explore the causes behind these security threats through case studies and demonstrate that the discovered security threats can be converted into exploitable vulnerabilities. There are 46 new vulnerabilities found by AndScanner+, 36 of which have been assigned CVE/CNVD IDs. This study provides much new knowledge of the Android firmware ecosystem with a deep understanding of software engineering security practices.",Security,Healthy,The paper focuses on investigating the security of customized Android firmware; which can contribute to maintaining healthy environments for software users; aligning with the value item Healthy and the corresponding value Security.,"The paper directly aligns with the value item Healthy and its corresponding value Security from a ""Software User"" perspective because it investigates the security of Android firmware, which directly impacts the safety and well-being of software users. By identifying vulnerabilities and patching issues in firmware images and pre-installed apps, the paper contributes to creating a more secure environment for software users, ensuring their devices and personal data are protected against potential threats and breaches. This aligns with the value of Security, which is essential for software users to have a healthy and worry-free experience.",Agreed-Justified,Disagree,reconcile with coder_2,Disagree,The justification doesn't align with the value item of the healthy,
Exp B,Exp H,2466,TSE,Security & Privacy,Can We Trust the Phone Vendors? Comprehensive Security Measurements on the Android Firmware Ecosystem,"Android is the most popular smartphone platform with over 85% market share. Its success is built on openness, and phone vendors can utilize the Android source code to make customized products with unique software/hardware features. On the other hand, the fragmentation and customization of Android also bring many security risks that have attracted the attention of researchers. Many efforts were put in to investigate the security of customized Android firmware. However, most of the previous works focus on designing efficient analysis tools or analyzing particular aspects of the firmware. There still lacks a panoramic view of Android firmware ecosystem security and the corresponding understandings based on large-scale firmware datasets. In this work, we made a large-scale comprehensive measurement of the Android firmware ecosystem security. Our study is based on 8,325 firmware images from 153 vendors and 813 Android-related CVEs, which is the largest Android firmware dataset ever used for security measurements. In particular, our study followed a series of research questions, covering vulnerabilities, patches, security updates, and pre-installed apps. To automate the analysis process, we designed a framework, AndScanner+, to complete firmware crawling, firmware parsing, patch analysis, and app analysis. Through massive data analysis and case explorations, several interesting findings are obtained. For example, the patch delay and missing issues are widespread in Android firmware images, say 31.4% and 5.6% of all images, respectively. The latest images of several phones still contain vulnerable pre-installed apps, and even the corresponding vulnerabilities have been publicly disclosed. In addition to data measurements, we also explore the causes behind these security threats through case studies and demonstrate that the discovered security threats can be converted into exploitable vulnerabilities. There are 46 new vulnerabilities found by AndScanner+, 36 of which have been assigned CVE/CNVD IDs. This study provides much new knowledge of the Android firmware ecosystem with a deep understanding of software engineering security practices.",Security,National Security,The paper highlights several security threats in Android firmware images. By identifying these threats and vulnerabilities; the study contributes to safeguarding against security breaches; thereby aligning with the value item National Security and the corresponding value Security.,"In the paper, the authors conducted a comprehensive measurement of the security of the Android firmware ecosystem and identified widespread patch delays, missing patches, and vulnerable pre-installed apps. By providing insights into these security threats and vulnerabilities, the study contributes to enhancing the security of the Android platform, which ultimately aligns with the value item National Security and the corresponding value Security.",Agreed-Justified,Agree,,Agree,,
Exp C,Exp K,2466,TSE,Security & Privacy,Can We Trust the Phone Vendors? Comprehensive Security Measurements on the Android Firmware Ecosystem,"Android is the most popular smartphone platform with over 85% market share. Its success is built on openness, and phone vendors can utilize the Android source code to make customized products with unique software/hardware features. On the other hand, the fragmentation and customization of Android also bring many security risks that have attracted the attention of researchers. Many efforts were put in to investigate the security of customized Android firmware. However, most of the previous works focus on designing efficient analysis tools or analyzing particular aspects of the firmware. There still lacks a panoramic view of Android firmware ecosystem security and the corresponding understandings based on large-scale firmware datasets. In this work, we made a large-scale comprehensive measurement of the Android firmware ecosystem security. Our study is based on 8,325 firmware images from 153 vendors and 813 Android-related CVEs, which is the largest Android firmware dataset ever used for security measurements. In particular, our study followed a series of research questions, covering vulnerabilities, patches, security updates, and pre-installed apps. To automate the analysis process, we designed a framework, AndScanner+, to complete firmware crawling, firmware parsing, patch analysis, and app analysis. Through massive data analysis and case explorations, several interesting findings are obtained. For example, the patch delay and missing issues are widespread in Android firmware images, say 31.4% and 5.6% of all images, respectively. The latest images of several phones still contain vulnerable pre-installed apps, and even the corresponding vulnerabilities have been publicly disclosed. In addition to data measurements, we also explore the causes behind these security threats through case studies and demonstrate that the discovered security threats can be converted into exploitable vulnerabilities. There are 46 new vulnerabilities found by AndScanner+, 36 of which have been assigned CVE/CNVD IDs. This study provides much new knowledge of the Android firmware ecosystem with a deep understanding of software engineering security practices.",Benevolence,Honesty,By contributing accurate information about the security status of Android firmware; the paper enhances transparency; aligning with the value item Honesty and the corresponding value Benevolence.,"The main contribution of 'Paper X' is providing a comprehensive measurement of the security of the Android firmware ecosystem based on a large-scale dataset. This accurate information enhances transparency by highlighting vulnerabilities, patch delays, missing issues, and vulnerable pre-installed apps, providing software users with a deeper understanding of the security practices employed in Android firmware. By doing so, the paper aligns with the value item Honesty and its corresponding value Benevolence, as it promotes honesty through the accurate disclosure of security threats and benevolence by aiming to enhance the overall security of the Android ecosystem for the benefit of software users.",Agreed-Justified,Agree,,Agree,,
Exp D,Exp K,2470,TSE,Code Generation & Analysis,Graph-of-Code: Semantic Clone Detection Using Graph Fingerprints,"The code clone detection issue has been researched using a number of explicit factors based on the tokens and contents and found effective results. However, exposing code contents may be an impractical option because of privacy and security factors. Moreover, the lack of scalability of past methods is an important challenge. The code flow states can be inferred by code structure and implicitly represented using empirical graphs. The assumption is that modelling of the code clone detection problem can be achieved without the content of the codes being revealed. Here, a Graph-of-Code concept for the code clone detection problem is introduced, which represents codes into graphs. While Graph-of-Code provides structural properties and quantification of its characteristics, it can exclude code contents or tokens to identify the clone type. The aim is to evaluate the impact of graph-of-code structural properties on the performance of code clone detection. This work employs a feature extraction-based approach for unlabelled graphs. The approach generates a aEURoeGraph FingerprintaEUR which represents different topological feature levels. The results of code clone detection indicate that code structure has a significant role in detecting clone types. We found different GoC-models outperform others. The models achieve between 96% to 99% in detecting code clones based on recall, precision, and F1-Score. The GoC approach is capable in detecting code clones with scalable dataset and with preserving codes privacy.",Self Direction,Privacy,"The paper introduces the concept of a ""Graph-of-Code;"" a method that allows detecting code clones without revealing code contents; thus contributing to the preservation of software user data privacy. This aligns with the value item 'Privacy' under the value 'Self Direction'.","The justification is that the ""Paper X"" aligns with the value item Privacy and its corresponding value Self Direction from a ""Software User"" perspective because it introduces the concept of a ""Graph-of-Code"" which allows detecting code clones without revealing code contents. By preserving the privacy of code contents, the paper contributes to the software user's ability to independently choose how their data is shared or accessed, aligning with the value of Self Direction.",Agreed-Justified,Agree,,Agree,,
Exp E,Exp J,2470,TSE,Code Generation & Analysis,Graph-of-Code: Semantic Clone Detection Using Graph Fingerprints,"The code clone detection issue has been researched using a number of explicit factors based on the tokens and contents and found effective results. However, exposing code contents may be an impractical option because of privacy and security factors. Moreover, the lack of scalability of past methods is an important challenge. The code flow states can be inferred by code structure and implicitly represented using empirical graphs. The assumption is that modelling of the code clone detection problem can be achieved without the content of the codes being revealed. Here, a Graph-of-Code concept for the code clone detection problem is introduced, which represents codes into graphs. While Graph-of-Code provides structural properties and quantification of its characteristics, it can exclude code contents or tokens to identify the clone type. The aim is to evaluate the impact of graph-of-code structural properties on the performance of code clone detection. This work employs a feature extraction-based approach for unlabelled graphs. The approach generates a aEURoeGraph FingerprintaEUR which represents different topological feature levels. The results of code clone detection indicate that code structure has a significant role in detecting clone types. We found different GoC-models outperform others. The models achieve between 96% to 99% in detecting code clones based on recall, precision, and F1-Score. The GoC approach is capable in detecting code clones with scalable dataset and with preserving codes privacy.",Security,Social Order,"The ""Graph-of-Code"" concept also contributes to software's 'Security'. The method ensures that code clones are detected while keeping code contents private; thus maintaining the software's security. This directly aligns with the 'Social Order' value item under the 'Security' value.","The justification for aligning 'Paper X' with the value item Social Order and its corresponding value Security from a ""Software User"" perspective is that the ""Graph-of-Code"" concept contributes to maintaining the security of software by detecting code clones while keeping code contents private. This directly aligns with the value of Social Order under the broader value of Security, as ensuring privacy and protecting against unauthorized access aligns with the principles of social order and maintaining a secure environment.",Agreed-Justified,Agree,,Agree,,
Exp G,Exp K,2475,TSE,AI & Machine Learning,$\mathtt {SIEGE}$SIEGE: A Semantics-Guided Safety Enhancement Framework for AI-Enabled Cyber-Physical Systems,"Cyber-Physical Systems (CPSs) have been widely adopted in various industry domains to support many important tasks that impact our daily lives, such as automotive vehicles, robotics manufacturing, and energy systems. As Artificial Intelligence (AI) has demonstrated its promising abilities in diverse tasks like decision-making, prediction, and optimization, a growing number of CPSs adopt AI components in the loop to further extend their efficiency and performance. However, these modern AI-enabled CPSs have to tackle pivotal problems that the AI-enabled control systems might need to compensate the balance across multiple operation requirements and avoid possible defections in advance to safeguard human lives and properties. Modular redundancy and ensemble method are two widely adopted solutions in the traditional CPSs and AI communities to enhance the functionality and flexibility of a system. Nevertheless, there is a lack of deep understanding of the effectiveness of such ensemble design on AI-CPSs across diverse industrial applications. Considering the complexity of AI-CPSs, existing ensemble methods fall short of handling such huge state space and sophisticated system dynamics. Furthermore, an ideal control solution should consider the multiple system specifications in real-time and avoid erroneous behaviors beforehand. Such that, a new specification-oriented ensemble control system is of urgent need for AI-CPSs. In this paper, we propose $\mathtt {SIEGE}$SIEGE, a semantics-guided ensemble control framework to initiate an early exploratory study of ensemble methods on AI-CPSs and aim to construct an efficient, robust, and reliable control solution for multi-tasks AI-CPSs. We first utilize a semantic-based abstraction to decompose the large state space, capture the ongoing system status and predict future conditions in terms of the satisfaction of specifications. We propose a series of new semantics-aware ensemble strategies and an end-to-end Deep Reinforcement Learning (DRL) hierarchical ensemble method to improve the flexibility and reliability of the control systems. Our large-scale, comprehensive evaluations over five subject CPSs show that 1) the semantics abstraction can efficiently narrow the large state space and predict the semantics of incoming states, 2) our semantics-guided methods outperform state-of-the-art individual controllers and traditional ensemble methods, and 3) the DRL hierarchical ensemble approach shows promising capabilities to deliver a more robust, efficient, and safety-assured control system. To enable further research along this direction to build better AI-enabled CPS, we made all of the code and experimental results data publicly. (https://sites.google.com/view/ai-cps-siege/home).",Security,Healthy,The paper introduces a new framework; SIEGE; for AI-enabled Cyber-Physical Systems (CPSs); aiming to construct an efficient; robust; and reliable control solution for these systems. This can enhance the safety and health status of users who engage with such systems in various industry domains like automotive vehicles; robotics manufacturing; and energy systems.,"The main contributions of 'Paper X' align with the value item Healthy and its corresponding value Security from a ""Software User"" perspective because the proposed framework, SIEGE, aims to enhance the efficiency, robustness, and reliability of AI-enabled Cyber-Physical Systems (CPSs). By improving the control solutions for these systems, the safety and health status of users who engage with them in various industry domains like automotive vehicles, robotics manufacturing, and energy systems can be safeguarded. This directly aligns with the value item Healthy, as it promotes the well-being and physical safety of users, and the value item Security, as it ensures the protection and stability of the systems they interact with.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2475,TSE,AI & Machine Learning,$\mathtt {SIEGE}$SIEGE: A Semantics-Guided Safety Enhancement Framework for AI-Enabled Cyber-Physical Systems,"Cyber-Physical Systems (CPSs) have been widely adopted in various industry domains to support many important tasks that impact our daily lives, such as automotive vehicles, robotics manufacturing, and energy systems. As Artificial Intelligence (AI) has demonstrated its promising abilities in diverse tasks like decision-making, prediction, and optimization, a growing number of CPSs adopt AI components in the loop to further extend their efficiency and performance. However, these modern AI-enabled CPSs have to tackle pivotal problems that the AI-enabled control systems might need to compensate the balance across multiple operation requirements and avoid possible defections in advance to safeguard human lives and properties. Modular redundancy and ensemble method are two widely adopted solutions in the traditional CPSs and AI communities to enhance the functionality and flexibility of a system. Nevertheless, there is a lack of deep understanding of the effectiveness of such ensemble design on AI-CPSs across diverse industrial applications. Considering the complexity of AI-CPSs, existing ensemble methods fall short of handling such huge state space and sophisticated system dynamics. Furthermore, an ideal control solution should consider the multiple system specifications in real-time and avoid erroneous behaviors beforehand. Such that, a new specification-oriented ensemble control system is of urgent need for AI-CPSs. In this paper, we propose $\mathtt {SIEGE}$SIEGE, a semantics-guided ensemble control framework to initiate an early exploratory study of ensemble methods on AI-CPSs and aim to construct an efficient, robust, and reliable control solution for multi-tasks AI-CPSs. We first utilize a semantic-based abstraction to decompose the large state space, capture the ongoing system status and predict future conditions in terms of the satisfaction of specifications. We propose a series of new semantics-aware ensemble strategies and an end-to-end Deep Reinforcement Learning (DRL) hierarchical ensemble method to improve the flexibility and reliability of the control systems. Our large-scale, comprehensive evaluations over five subject CPSs show that 1) the semantics abstraction can efficiently narrow the large state space and predict the semantics of incoming states, 2) our semantics-guided methods outperform state-of-the-art individual controllers and traditional ensemble methods, and 3) the DRL hierarchical ensemble approach shows promising capabilities to deliver a more robust, efficient, and safety-assured control system. To enable further research along this direction to build better AI-enabled CPS, we made all of the code and experimental results data publicly. (https://sites.google.com/view/ai-cps-siege/home).",Power,Social Recognition,By making the code and experimental results data publicly available; the paper helps users receive social recognition or validation among peers for predicting future conditions and meeting multi-task operation needs using the proposed ensemble control framework. It aligns with the value item of Social Recognition and its corresponding value of Power.,"In the paper, the authors state that by making the code and experimental results data publicly available, they aim to enable further research and allow users to benefit from their proposed ensemble control framework. This act of sharing their work can potentially lead to social recognition or validation among peers for the users who utilize the framework to make advancements in the field of AI-CPSs. By aligning with the value item of Social Recognition and its corresponding value of Power, the paper acknowledges the importance of recognition and influence that users can gain by implementing the proposed framework.",Agreed-Justified,Agree,,Agree,,
Exp B,Exp H,2475,TSE,AI & Machine Learning,$\mathtt {SIEGE}$SIEGE: A Semantics-Guided Safety Enhancement Framework for AI-Enabled Cyber-Physical Systems,"Cyber-Physical Systems (CPSs) have been widely adopted in various industry domains to support many important tasks that impact our daily lives, such as automotive vehicles, robotics manufacturing, and energy systems. As Artificial Intelligence (AI) has demonstrated its promising abilities in diverse tasks like decision-making, prediction, and optimization, a growing number of CPSs adopt AI components in the loop to further extend their efficiency and performance. However, these modern AI-enabled CPSs have to tackle pivotal problems that the AI-enabled control systems might need to compensate the balance across multiple operation requirements and avoid possible defections in advance to safeguard human lives and properties. Modular redundancy and ensemble method are two widely adopted solutions in the traditional CPSs and AI communities to enhance the functionality and flexibility of a system. Nevertheless, there is a lack of deep understanding of the effectiveness of such ensemble design on AI-CPSs across diverse industrial applications. Considering the complexity of AI-CPSs, existing ensemble methods fall short of handling such huge state space and sophisticated system dynamics. Furthermore, an ideal control solution should consider the multiple system specifications in real-time and avoid erroneous behaviors beforehand. Such that, a new specification-oriented ensemble control system is of urgent need for AI-CPSs. In this paper, we propose $\mathtt {SIEGE}$SIEGE, a semantics-guided ensemble control framework to initiate an early exploratory study of ensemble methods on AI-CPSs and aim to construct an efficient, robust, and reliable control solution for multi-tasks AI-CPSs. We first utilize a semantic-based abstraction to decompose the large state space, capture the ongoing system status and predict future conditions in terms of the satisfaction of specifications. We propose a series of new semantics-aware ensemble strategies and an end-to-end Deep Reinforcement Learning (DRL) hierarchical ensemble method to improve the flexibility and reliability of the control systems. Our large-scale, comprehensive evaluations over five subject CPSs show that 1) the semantics abstraction can efficiently narrow the large state space and predict the semantics of incoming states, 2) our semantics-guided methods outperform state-of-the-art individual controllers and traditional ensemble methods, and 3) the DRL hierarchical ensemble approach shows promising capabilities to deliver a more robust, efficient, and safety-assured control system. To enable further research along this direction to build better AI-enabled CPS, we made all of the code and experimental results data publicly. (https://sites.google.com/view/ai-cps-siege/home).",Achievement,Successful,The detailed evaluations demonstrating that their semantics-guided methods outperform traditional ensemble methods and state-of-the-art individual controllers align with the value item of Successful; where users achieve desirable outcomes or meet objectives by using their improved control system. It corresponds to the value of Achievement.,"The justification for aligning 'Paper X' with the value item Successful and its corresponding value Achievement from a ""Software User"" perspective is based on the evidence provided in the abstract. The abstract states that the proposed semantics-guided methods outperform traditional ensemble methods and state-of-the-art individual controllers in comprehensive evaluations. This indicates that users of the improved control system can achieve desirable outcomes or meet their objectives more successfully, which aligns with the value of Achievement.",Agreed-Justified,Agree,,Agree,,
Exp C,Exp K,2478,TSE,Software Testing & QA,Mobile App Crowdsourced Test Report Consistency Detection via Deep Image-and-Text Fusion Understanding,"Crowdsourced testing, as a distinct testing paradigm, has attracted much attention in software testing, especially in mobile application (app) testing field. Compared with in-house testing, crowdsourced testing shows superiority with the diverse testing environments when faced with the mobile testing fragmentation problem. However, crowdsourced testing also encounters the low-quality test report problem caused by unprofessional crowdworkers involved with different expertise. In order to handle the submitted reports of uneven quality, app developers have to distinguish high-quality reports from low-quality ones to help the bug inspection. One kind of typical low-quality test report is inconsistent test reports, which means the textual descriptions are not focusing on the attached bug-occurring screenshots. According to our empirical survey, only 18.07% crowdsourced test reports are consistent. Inconsistent reports cause waste on mobile app testing. To solve the inconsistency problem, we propose ReCoDe to detect the consistency of crowdsourced test reports via deep image-and-text fusion understanding. ReCoDe is a two-stage approach that first classifies the reports based on textual descriptions into different categories according to the bug feature. In the second stage, ReCoDe has a deep understanding of the GUI image features of the app screenshots and then applies different strategies to handle different types of bugs to detect the consistency of the crowdsourced test reports. We conduct an experiment on a dataset with over 22 k test reports to evaluate ReCoDe, and the results show the effectiveness of ReCoDe in detecting the consistency of crowdsourced test reports. Besides, a user study is conducted to prove the practical value of ReCoDe in effectively helping app developers improve the efficiency of reviewing the crowdsourced test reports.",Stimulation,Excitement in Life,The paper outlines the creation of ReCoDe; a tool for crowdsourced testing; aiming to provide an exciting; novel solution for mobile application testing; which aligns with the value item Excitement in Life and its corresponding value Stimulation.,"In the abstract of 'Paper X', it is mentioned that ReCoDe, the tool proposed in the paper, aims to handle the low-quality test reports in crowdsourced testing for mobile applications. By providing a novel solution for improving the efficiency of reviewing crowdsourced test reports, ReCoDe contributes to creating an exciting and stimulating experience for software users. This aligns with the value item Excitement in Life and its corresponding value Stimulation, as it enhances the overall testing process and provides a more engaging and dynamic user experience while using software applications.",Agreed-Justified,Agree,,Agree,,
Exp D,Exp K,2478,TSE,Software Testing & QA,Mobile App Crowdsourced Test Report Consistency Detection via Deep Image-and-Text Fusion Understanding,"Crowdsourced testing, as a distinct testing paradigm, has attracted much attention in software testing, especially in mobile application (app) testing field. Compared with in-house testing, crowdsourced testing shows superiority with the diverse testing environments when faced with the mobile testing fragmentation problem. However, crowdsourced testing also encounters the low-quality test report problem caused by unprofessional crowdworkers involved with different expertise. In order to handle the submitted reports of uneven quality, app developers have to distinguish high-quality reports from low-quality ones to help the bug inspection. One kind of typical low-quality test report is inconsistent test reports, which means the textual descriptions are not focusing on the attached bug-occurring screenshots. According to our empirical survey, only 18.07% crowdsourced test reports are consistent. Inconsistent reports cause waste on mobile app testing. To solve the inconsistency problem, we propose ReCoDe to detect the consistency of crowdsourced test reports via deep image-and-text fusion understanding. ReCoDe is a two-stage approach that first classifies the reports based on textual descriptions into different categories according to the bug feature. In the second stage, ReCoDe has a deep understanding of the GUI image features of the app screenshots and then applies different strategies to handle different types of bugs to detect the consistency of the crowdsourced test reports. We conduct an experiment on a dataset with over 22 k test reports to evaluate ReCoDe, and the results show the effectiveness of ReCoDe in detecting the consistency of crowdsourced test reports. Besides, a user study is conducted to prove the practical value of ReCoDe in effectively helping app developers improve the efficiency of reviewing the crowdsourced test reports.",Hedonism,Enjoying Life,The end result of ReCoDe's application would be to provide a more efficient; enjoyable experience for end users of mobile applications; as it helps detect inconsistencies in test reports; thus aligns with the value item Enjoying Life and its corresponding value Hedonism.,"The main contribution of 'Paper X' is the development of ReCoDe, which aims to detect the consistency of crowdsourced test reports in mobile applications. By improving the efficiency and accuracy of bug inspection through the identification of high-quality reports, ReCoDe ultimately enhances the overall user experience of mobile applications. This aligns with the value item Enjoying Life and its corresponding value Hedonism, as it directly contributes to the enjoyment and pleasure that software users derive from using mobile applications without the frustration and inconvenience caused by inconsistent test reports.",Agreed-Justified,Agree,,Agree,,
Exp E,Exp J,2478,TSE,Software Testing & QA,Mobile App Crowdsourced Test Report Consistency Detection via Deep Image-and-Text Fusion Understanding,"Crowdsourced testing, as a distinct testing paradigm, has attracted much attention in software testing, especially in mobile application (app) testing field. Compared with in-house testing, crowdsourced testing shows superiority with the diverse testing environments when faced with the mobile testing fragmentation problem. However, crowdsourced testing also encounters the low-quality test report problem caused by unprofessional crowdworkers involved with different expertise. In order to handle the submitted reports of uneven quality, app developers have to distinguish high-quality reports from low-quality ones to help the bug inspection. One kind of typical low-quality test report is inconsistent test reports, which means the textual descriptions are not focusing on the attached bug-occurring screenshots. According to our empirical survey, only 18.07% crowdsourced test reports are consistent. Inconsistent reports cause waste on mobile app testing. To solve the inconsistency problem, we propose ReCoDe to detect the consistency of crowdsourced test reports via deep image-and-text fusion understanding. ReCoDe is a two-stage approach that first classifies the reports based on textual descriptions into different categories according to the bug feature. In the second stage, ReCoDe has a deep understanding of the GUI image features of the app screenshots and then applies different strategies to handle different types of bugs to detect the consistency of the crowdsourced test reports. We conduct an experiment on a dataset with over 22 k test reports to evaluate ReCoDe, and the results show the effectiveness of ReCoDe in detecting the consistency of crowdsourced test reports. Besides, a user study is conducted to prove the practical value of ReCoDe in effectively helping app developers improve the efficiency of reviewing the crowdsourced test reports.",Security,Healthy,The main goal of ReCoDe is to enhance the quality of software for end-users by improving the overall consistency of test reports; which aligns with the value item Health and its corresponding value Security.,"The justification for aligning 'Paper X' with the value item Healthy and its corresponding value Security is based on the fact that by improving the consistency of test reports, ReCoDe helps in identifying and resolving bugs more effectively, thus enhancing the overall quality and reliability of the software. This directly contributes to the sense of security and well-being for software users as they can rely on the software to function as expected without encountering major issues or vulnerabilities that may compromise their data or system integrity. In turn, this promotes a healthier and more secure software experience for users.",Agreed-Justified,Agree,,Agree,,
Exp F,Exp J,2480,TSE,Security & Privacy,CPVD: Cross Project Vulnerability Detection Based on Graph Attention Network and Domain Adaptation,"Code vulnerability detection is critical for software security prevention. Vulnerability annotation in large-scale software code is quite tedious and challenging, which requires domain experts to spend a lot of time annotating. This work offers CPVD, a cross-domain vulnerability detection approach based on the challenge of aEURlearning to predict the vulnerability labels of another item quickly using one item with rich vulnerability labels.aEUR CPVD uses the code property graph to represent the code and uses the Graph Attention Network and Convolution Pooling Network to extract the graph feature vector. It reduces the distribution between the source domain and target domain data in the Domain Adaptation Representation Learning stage for cross-domain vulnerability detection. In this paper, we test each other on different real-world project codes. Compared with methods without domain adaptation and domain adaptation methods based on natural language processing, CPVD is more general and performs better in cross-domain vulnerability detection tasks. Specifically, for the four datasets of chr_deb, qemu, libav, and sard, they achieved the best results of 70.2%, 81.1%, 59.7%, and 78.1% respectively on the F1-Score, and 88.4%,86.3%, 85.2%, and 88.6% on the AUC.",Security,Healthy,The work provides a cross-domain vulnerability detection approach in software; which contributes to healthier (hence safer) software for users.,"The cross-domain vulnerability detection approach proposed by 'Paper X' contributes to a healthier and safer software environment for users. By effectively detecting vulnerabilities in software code, the approach helps to prevent security breaches and potential harm to users. This alignment with the value item of Healthy and its corresponding value of Security is evident in the main contributions of 'Paper X', as it directly addresses the need for software security and prevention.",Agreed-Justified,Agree,,Agree,,
Exp G,Exp K,2480,TSE,Security & Privacy,CPVD: Cross Project Vulnerability Detection Based on Graph Attention Network and Domain Adaptation,"Code vulnerability detection is critical for software security prevention. Vulnerability annotation in large-scale software code is quite tedious and challenging, which requires domain experts to spend a lot of time annotating. This work offers CPVD, a cross-domain vulnerability detection approach based on the challenge of aEURlearning to predict the vulnerability labels of another item quickly using one item with rich vulnerability labels.aEUR CPVD uses the code property graph to represent the code and uses the Graph Attention Network and Convolution Pooling Network to extract the graph feature vector. It reduces the distribution between the source domain and target domain data in the Domain Adaptation Representation Learning stage for cross-domain vulnerability detection. In this paper, we test each other on different real-world project codes. Compared with methods without domain adaptation and domain adaptation methods based on natural language processing, CPVD is more general and performs better in cross-domain vulnerability detection tasks. Specifically, for the four datasets of chr_deb, qemu, libav, and sard, they achieved the best results of 70.2%, 81.1%, 59.7%, and 78.1% respectively on the F1-Score, and 88.4%,86.3%, 85.2%, and 88.6% on the AUC.",Security,Social Order,The approach aims to detect vulnerabilities in software coding; which essentially helps in maintaining a safer and more orderly software environment improving the sense of social order.,"In 'Paper X', the main contribution is the development of a cross-domain vulnerability detection approach. This approach aims to enhance software security by detecting vulnerabilities in coding. By doing so, it helps in maintaining a more secure and organized software environment, which aligns with the value item of Social Order and its corresponding value of Security. As a software user, the ability to identify and address vulnerabilities in software code contributes to a sense of security and orderliness in using the software.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2480,TSE,Security & Privacy,CPVD: Cross Project Vulnerability Detection Based on Graph Attention Network and Domain Adaptation,"Code vulnerability detection is critical for software security prevention. Vulnerability annotation in large-scale software code is quite tedious and challenging, which requires domain experts to spend a lot of time annotating. This work offers CPVD, a cross-domain vulnerability detection approach based on the challenge of aEURlearning to predict the vulnerability labels of another item quickly using one item with rich vulnerability labels.aEUR CPVD uses the code property graph to represent the code and uses the Graph Attention Network and Convolution Pooling Network to extract the graph feature vector. It reduces the distribution between the source domain and target domain data in the Domain Adaptation Representation Learning stage for cross-domain vulnerability detection. In this paper, we test each other on different real-world project codes. Compared with methods without domain adaptation and domain adaptation methods based on natural language processing, CPVD is more general and performs better in cross-domain vulnerability detection tasks. Specifically, for the four datasets of chr_deb, qemu, libav, and sard, they achieved the best results of 70.2%, 81.1%, 59.7%, and 78.1% respectively on the F1-Score, and 88.4%,86.3%, 85.2%, and 88.6% on the AUC.",Security,National Security,By detecting vulnerabilities in software code; the approach contributes to ensuring the security of software that could potentially be used at a national level; aligning with the aspect of national security.,"The justification for the alignment of 'Paper X' with the value item National Security and its corresponding value Security is that by detecting vulnerabilities in software code, the approach contributes to ensuring the security of software that could potentially be used at a national level. This is important in protecting sensitive information and systems, which aligns with the aspect of national security.",Agreed-Justified,Disagree,reconcile with coder_2,Disagree,The justification claims that the paper aligns with the value 'Security' because of 'By detecting vulnerabilities in software code; but is not supported value item of the National Security by the evidence in the abstract.,
Exp B,Exp H,2486,TSE,AI & Machine Learning,NLP-Based Automated Compliance Checking of Data Processing Agreements Against GDPR,"When the entity processing personal data (the processor) differs from the one collecting personal data (the controller), processing personal data is regulated in Europe by the General Data Protection Regulation (GDPR) through data processing agreements (DPAs). Checking the compliance of DPAs contributes to the compliance verification of software systems as DPAs are an important source of requirements for software development involving the processing of personal data. However, manually checking whether a given DPA complies with GDPR is challenging as it requires significant time and effort for understanding and identifying DPA-relevant compliance requirements in GDPR and then verifying these requirements in the DPA. Legal texts introduce additional complexity due to convoluted language and inherent ambiguity leading to potential misunderstandings. In this paper, we propose an automated solution to check the compliance of a given DPA against GDPR. In close interaction with legal experts, we first built two artifacts: (i) the aEURoeshallaEUR requirements extracted from the GDPR provisions relevant to DPA compliance and (ii) a glossary table defining the legal concepts in the requirements. Then, we developed an automated solution that leverages natural language processing (NLP) technologies to check the compliance of a given DPA against these aEURoeshallaEUR requirements. Specifically, our approach automatically generates phrasal-level representations for the textual content of the DPA and compares them against predefined representations of the aEURoeshallaEUR requirements. By comparing these two representations, the approach not only assesses whether the DPA is GDPR compliant but it further provides recommendations about missing information in the DPA. Over a dataset of 30 actual DPAs, the approach correctly finds 618 out of 750 genuine violations while raising 76 false violations, and further correctly identifies 524 satisfied requirements. The approach has thus an average precision of 89.1%, a recall of 82.4%, and an accuracy of 84.6%. Compared to a baseline that relies on off-the-shelf NLP tools, our approach provides an average accuracy gain of $\approx$a%0^20 percentage points. The accuracy of our approach can be improved to $\approx$a%0^94% with limited manual verification effort.",Security,Social Order,The paper contributes an automated solution for the verification of data processing agreements against GDPR requirements; thereby ensuring compliance and promoting social order.,"In the paper, the authors propose an automated solution to check the compliance of data processing agreements (DPAs) with GDPR requirements. By ensuring compliance with GDPR, the paper contributes to establishing social order within the software context. Compliance with GDPR regulations promotes security and protects individuals' personal data, which contributes to maintaining a sense of social order and trust in software systems.",Agreed-Clarified,Disagree,reconcile with coder_2,Disagree,"The justification doesn't align with the value item of the social order
",
Exp C,Exp K,2486,TSE,AI & Machine Learning,NLP-Based Automated Compliance Checking of Data Processing Agreements Against GDPR,"When the entity processing personal data (the processor) differs from the one collecting personal data (the controller), processing personal data is regulated in Europe by the General Data Protection Regulation (GDPR) through data processing agreements (DPAs). Checking the compliance of DPAs contributes to the compliance verification of software systems as DPAs are an important source of requirements for software development involving the processing of personal data. However, manually checking whether a given DPA complies with GDPR is challenging as it requires significant time and effort for understanding and identifying DPA-relevant compliance requirements in GDPR and then verifying these requirements in the DPA. Legal texts introduce additional complexity due to convoluted language and inherent ambiguity leading to potential misunderstandings. In this paper, we propose an automated solution to check the compliance of a given DPA against GDPR. In close interaction with legal experts, we first built two artifacts: (i) the aEURoeshallaEUR requirements extracted from the GDPR provisions relevant to DPA compliance and (ii) a glossary table defining the legal concepts in the requirements. Then, we developed an automated solution that leverages natural language processing (NLP) technologies to check the compliance of a given DPA against these aEURoeshallaEUR requirements. Specifically, our approach automatically generates phrasal-level representations for the textual content of the DPA and compares them against predefined representations of the aEURoeshallaEUR requirements. By comparing these two representations, the approach not only assesses whether the DPA is GDPR compliant but it further provides recommendations about missing information in the DPA. Over a dataset of 30 actual DPAs, the approach correctly finds 618 out of 750 genuine violations while raising 76 false violations, and further correctly identifies 524 satisfied requirements. The approach has thus an average precision of 89.1%, a recall of 82.4%, and an accuracy of 84.6%. Compared to a baseline that relies on off-the-shelf NLP tools, our approach provides an average accuracy gain of $\approx$a%0^20 percentage points. The accuracy of our approach can be improved to $\approx$a%0^94% with limited manual verification effort.",Universalism,Social Justice,The proposed solution in this paper contributes to the enforcement of GDPR; a regulation that promotes social justice and equality.,"The proposed solution in 'Paper X' contributes to the alignment with the value item Social Justice and its corresponding value Universalism from a ""Software User"" perspective by addressing the compliance verification of data processing agreements (DPAs) with the General Data Protection Regulation (GDPR). The GDPR promotes social justice and equality by ensuring the protection of individuals' personal data and their rights. By automating the process of checking the compliance of DPAs against GDPR requirements, the solution helps to uphold social justice principles by enhancing transparency, accountability, and fairness in the processing of personal data. It empowers software users to have control over their personal information and promotes equal treatment and protection for all individuals in the digital realm.",Agreed-Clarified,Agree,,Agree,,
Exp D,Exp K,2486,TSE,AI & Machine Learning,NLP-Based Automated Compliance Checking of Data Processing Agreements Against GDPR,"When the entity processing personal data (the processor) differs from the one collecting personal data (the controller), processing personal data is regulated in Europe by the General Data Protection Regulation (GDPR) through data processing agreements (DPAs). Checking the compliance of DPAs contributes to the compliance verification of software systems as DPAs are an important source of requirements for software development involving the processing of personal data. However, manually checking whether a given DPA complies with GDPR is challenging as it requires significant time and effort for understanding and identifying DPA-relevant compliance requirements in GDPR and then verifying these requirements in the DPA. Legal texts introduce additional complexity due to convoluted language and inherent ambiguity leading to potential misunderstandings. In this paper, we propose an automated solution to check the compliance of a given DPA against GDPR. In close interaction with legal experts, we first built two artifacts: (i) the aEURoeshallaEUR requirements extracted from the GDPR provisions relevant to DPA compliance and (ii) a glossary table defining the legal concepts in the requirements. Then, we developed an automated solution that leverages natural language processing (NLP) technologies to check the compliance of a given DPA against these aEURoeshallaEUR requirements. Specifically, our approach automatically generates phrasal-level representations for the textual content of the DPA and compares them against predefined representations of the aEURoeshallaEUR requirements. By comparing these two representations, the approach not only assesses whether the DPA is GDPR compliant but it further provides recommendations about missing information in the DPA. Over a dataset of 30 actual DPAs, the approach correctly finds 618 out of 750 genuine violations while raising 76 false violations, and further correctly identifies 524 satisfied requirements. The approach has thus an average precision of 89.1%, a recall of 82.4%, and an accuracy of 84.6%. Compared to a baseline that relies on off-the-shelf NLP tools, our approach provides an average accuracy gain of $\approx$a%0^20 percentage points. The accuracy of our approach can be improved to $\approx$a%0^94% with limited manual verification effort.",Benevolence,Responsibility,The paper introduces a system that holds individuals or organizations accountable for the processing of personal data; aligning with the value of 'Responsibility' within the value 'Benevolence'.,"The paper's automated solution for checking the compliance of data processing agreements (DPAs) against the General Data Protection Regulation (GDPR) emphasizes the responsibility of individuals or organizations in ensuring the proper processing of personal data. This aligns with the value item 'Responsibility' within the value 'Benevolence' from the perspective of a software user. By holding individuals or organizations accountable for their data processing practices, the paper's main contribution promotes responsible behavior and benevolence towards the protection of users' personal data within a software context.",Agreed-Clarified,Agree,,Agree,,
Exp G,Exp K,2488,TSE,Security & Privacy,Privacy Engineering in the Wild: Understanding the PractitionersaEUR(tm) Mindset Organizational Aspects and Current Practices,"Privacy engineering, as an emerging field of research and practice, comprises the technical capabilities and management processes needed to implement, deploy, and operate privacy features and controls in working systems. For that, software practitioners and other stakeholders in software companies need to work cooperatively toward building privacy-preserving businesses and engineering solutions. Significant research has been done to understand the software practitionersaEUR(tm) perceptions of information privacy, but more emphasis should be given to the uptake of concrete privacy engineering components. This research delves into the software practitionersaEUR(tm) perspectives and mindset, organizational aspects, and current practices on privacy and its engineering processes. A total of 30 practitioners from nine countries and backgrounds were interviewed, sharing their experiences and voicing their opinions on a broad range of privacy topics. The thematic analysis methodology was adopted to code the interview data qualitatively and construct a rich and nuanced thematic framework. As a result, we identified three critical interconnected themes that compose our thematic framework for privacy engineering aEURoein the wildaEUR: (1) personal privacy mindset and stance, categorised into practitionersaEUR(tm) privacy knowledge, attitudes and behaviours; (2) organizational privacy aspects, such as decision-power and positive and negative examples of privacy climate; and, (3) privacy engineering practices, such as procedures and controls concretely used in the industry. Among the main findings, this study provides many insights about the state-of-the-practice of privacy engineering, pointing to a positive influence of privacy laws (e.g., EU General Data Protection Regulation) on practitionersaEUR(tm) behaviours and organizationsaEUR(tm) cultures. Aspects such as organizational privacy culture and climate were also confirmed to have a powerful influence on the practitionersaEUR(tm) privacy behaviours. A conducive environment for privacy engineering needs to be created, aligning the privacy values of practitioners and their organizations, with particular attention to the leaders and top management's commitment to privacy. Organizations can also facilitate education and awareness training for software practitioners on existing privacy engineering theories, methods and tools that have already been proven effective.",Security,Healthy,The paper's focus on enhancing privacy engineering practices and controls contributes to 'Health' (the security and well-being) of users in 'Security' value.,"The justification for aligning 'Paper X' with the value item Healthy and its corresponding value Security from a ""Software User"" perspective is based on the fact that privacy engineering practices and controls directly contribute to the security and well-being of users. By emphasizing the importance of implementing privacy features and controls in working systems, the paper aims to ensure that users' personal information is protected and their online experiences are secure. This aligns with the value of Security, as users' health in the software context is dependent on the security measures implemented to safeguard their personal data.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2489,TSE,Data Management & Processing,Dealing With Data Challenges When Delivering Data-Intensive Software Solutions,"The predicted increase in demand for data-intensive solution development is driving the need for software, data, and domain experts to effectively collaborate in multi-disciplinary data-intensive software teams (MDSTs). We conducted a socio-technical grounded theory study through interviews with 24 practitioners in MDSTs to better understand the challenges these teams face when delivering data-intensive software solutions. The interviews provided perspectives across different types of roles including domain, data and software experts, and covered different organisational levels from team members, team managers to executive leaders. We found that the key concern for these teams is dealing with data-related challenges. In this article, we present a theory of dealing with data challenges that explains the challenges faced by MDSTs including gaining access to data, aligning data, understanding data, and resolving data quality issues; the context in and condition under which these challenges occur, the causes that lead to the challenges, and the related consequences such as having to conduct remediation activities, inability to achieve expected outcomes and lack of trust in the delivered solutions. We also identified contingencies or strategies applied to address the challenges including high-level strategic approaches such as implementing data governance, implementing new tools and techniques such as data quality visualisation and monitoring tools, as well as building stronger teams by focusing on people dynamics, communication skill development and cross-skilling. Our findings have direct implications for practitioners and researchers to better understand the landscape of data challenges and how to deal with them.",Security,Sense of Belonging,The paper emphasizes facilitating effective collaboration in multidisciplinary teams; contributing to a sense of belonging among software users in this context. This aligns with the value item Sense of Belonging under the value Security.,"The justification for the alignment of 'Paper X' with the value item Sense of Belonging and its corresponding value Security from a ""Software User"" perspective is based on the emphasis of the paper on facilitating effective collaboration in multidisciplinary teams. This collaboration creates a sense of belonging for software users as they are part of a cohesive team that works together towards common goals. This sense of belonging contributes to their overall security and well-being within the software context.",Agreed-Clarified,Disagree,"Related to Achievement: capable more, as it automatically identifies vulnerabilities",Disagree,The justification doesn't align with the value item of Sense of Belonging,
Exp B,Exp H,2494,TSE,Software Testing & QA,Optimizing Highly-Parallel Simulation-Based Verification of Cyber-Physical Systems,"Cyber-Physical Systems (CPSs), comprising both software and physical components, arise in many industry-relevant domains and are often mission- or safety-critical. System-Level Verification (SLV) of CPSs aims at certifying that given (e.g., safety or liveness) specifications are met, or at estimating the value of some Key Performance Indicators, when the system runs in its operational environment, that is in presence of inputs and/or of additional, uncontrolled disturbances. To enable SLV of complex systems from the early design phases, the currently most adopted approach envisions the simulation of a system model under the (time bounded) operational scenarios deemed of interest. Unfortunately, simulation-based SLV can be computationally prohibitive (years of sequential simulation), since system model simulation is computationally intensive and the set of scenarios of interest can be extremely large. In this article, we present a technique that, given a collection of scenarios of interest (extracted from databases or from symbolic structures), computes parallel shortest simulation campaigns, which drive a possibly large number of system model simulators running in parallel in a HPC infrastructure through all (and only) those scenarios in the user-defined (possibly random) order, by wisely avoiding multiple simulations of repeated trajectories, thus minimising completion time. Our experiments on SLV of Modelica/FMU and Simulink models with up to almost 200 million scenarios show that our optimisation yields speedups as high as 8$\boldsymbol{\times}$'--. This, together with the enabled massive parallelisation, makes practically viable (a few weeks in a HPC infrastructure) verification tasks (both statistical and exhaustive) which would otherwise take inconceivably long time.",Achievement,Successful,The paper outlines a new technique that increases the speed and viability of system-level verification tasks for CPSs; which corresponds to the user's successful utilization of the software; aligning with the value item Successful and its corresponding value Achievement.,"The paper's contribution of a technique that improves the speed and viability of system-level verification tasks directly aligns with the value item Successful and its corresponding value Achievement from a ""Software User"" perspective. By enabling efficient verification of complex software and physical systems, users can achieve successful utilization of the software, fulfilling their goals and objectives in a timely manner.",Agreed-Justified,Agree,,Agree,,
Exp C,Exp K,2494,TSE,Software Testing & QA,Optimizing Highly-Parallel Simulation-Based Verification of Cyber-Physical Systems,"Cyber-Physical Systems (CPSs), comprising both software and physical components, arise in many industry-relevant domains and are often mission- or safety-critical. System-Level Verification (SLV) of CPSs aims at certifying that given (e.g., safety or liveness) specifications are met, or at estimating the value of some Key Performance Indicators, when the system runs in its operational environment, that is in presence of inputs and/or of additional, uncontrolled disturbances. To enable SLV of complex systems from the early design phases, the currently most adopted approach envisions the simulation of a system model under the (time bounded) operational scenarios deemed of interest. Unfortunately, simulation-based SLV can be computationally prohibitive (years of sequential simulation), since system model simulation is computationally intensive and the set of scenarios of interest can be extremely large. In this article, we present a technique that, given a collection of scenarios of interest (extracted from databases or from symbolic structures), computes parallel shortest simulation campaigns, which drive a possibly large number of system model simulators running in parallel in a HPC infrastructure through all (and only) those scenarios in the user-defined (possibly random) order, by wisely avoiding multiple simulations of repeated trajectories, thus minimising completion time. Our experiments on SLV of Modelica/FMU and Simulink models with up to almost 200 million scenarios show that our optimisation yields speedups as high as 8$\boldsymbol{\times}$'--. This, together with the enabled massive parallelisation, makes practically viable (a few weeks in a HPC infrastructure) verification tasks (both statistical and exhaustive) which would otherwise take inconceivably long time.",Security,Healthy,Through system-level verification of cyber-physical systems; the proposed technique ensures the reliability and safety of the system's operation; contributing to the user's Health and aligns with the value Security.,"The justification for labeling 'Paper X' as aligning with the value item Healthy and its corresponding value Security from a ""Software User"" perspective is based on the idea that system-level verification of cyber-physical systems contributes to the reliability and safety of the system's operation. By ensuring the system operates correctly and minimizing potential failures or malfunctions, the user's health and well-being are protected. This alignment with the value of security indicates a concern for the user's safety and the importance of maintaining a secure and dependable system, which directly relates to the value of health.",Agreed-Clarified,Agree,,Agree,,
Exp D,Exp K,2494,TSE,Software Testing & QA,Optimizing Highly-Parallel Simulation-Based Verification of Cyber-Physical Systems,"Cyber-Physical Systems (CPSs), comprising both software and physical components, arise in many industry-relevant domains and are often mission- or safety-critical. System-Level Verification (SLV) of CPSs aims at certifying that given (e.g., safety or liveness) specifications are met, or at estimating the value of some Key Performance Indicators, when the system runs in its operational environment, that is in presence of inputs and/or of additional, uncontrolled disturbances. To enable SLV of complex systems from the early design phases, the currently most adopted approach envisions the simulation of a system model under the (time bounded) operational scenarios deemed of interest. Unfortunately, simulation-based SLV can be computationally prohibitive (years of sequential simulation), since system model simulation is computationally intensive and the set of scenarios of interest can be extremely large. In this article, we present a technique that, given a collection of scenarios of interest (extracted from databases or from symbolic structures), computes parallel shortest simulation campaigns, which drive a possibly large number of system model simulators running in parallel in a HPC infrastructure through all (and only) those scenarios in the user-defined (possibly random) order, by wisely avoiding multiple simulations of repeated trajectories, thus minimising completion time. Our experiments on SLV of Modelica/FMU and Simulink models with up to almost 200 million scenarios show that our optimisation yields speedups as high as 8$\boldsymbol{\times}$'--. This, together with the enabled massive parallelisation, makes practically viable (a few weeks in a HPC infrastructure) verification tasks (both statistical and exhaustive) which would otherwise take inconceivably long time.",Benevolence,Responsibility,The proposed technique in the paper adds responsibility in ensuring the functionality and safety of cyber-physical systems through system-level verification; aligning with the value item Responsibility and its corresponding value Benevolence.,"The proposed technique in 'Paper X' aligns with the value item Responsibility and its corresponding value Benevolence because it addresses the need for certifying that given specifications are met and estimating key performance indicators in complex cyber-physical systems. This reflects a responsible approach towards ensuring the functionality and safety of these systems, which is in line with the value of Benevolence as it seeks to contribute positively to the well-being of users and society as a whole.",Agreed-Clarified,Agree,,Agree,,
Exp E,Exp J,2495,TSE,Security & Privacy,BiAn: Smart Contract Source Code Obfuscation,"With the rising prominence of smart contracts, security attacks targeting them have increased, posing severe threats to their security and intellectual property rights. Existing simplistic datasets hinder effective vulnerability detection, raising security concerns. To address these challenges, we propose BiAn, a source code level smart contract obfuscation method that generates complex vulnerability test datasets. BiAn protects contracts by obfuscating data flows, control flows, and code layouts, increasing complexity and making it harder for attackers to discover vulnerabilities. Our experiments with buggy contracts showed an average complexity enhancement of approximately 174% after obfuscation. Decompilers Vandal and Gigahorse had total failure rate increments of 38.8% and 40.5% respectively. Obfuscated contracts also decreased vulnerability detection rates in more than 50% of cases for ten widely-used static analysis detection tools.",Security,Healthy,The main contribution of 'Paper X' is to enhance the security of smart contracts; which aligns with value item 'Healthy' under the value 'Security' as it is ensuring the health of the software user's digital assets.,"I apologize for any confusion. The justification for aligning 'Paper X' with the value item Healthy and its corresponding value Security is based on the fact that the main contribution of the paper is to enhance the security of smart contracts. By obfuscating data flows, control flows, and code layouts, the paper aims to make it harder for attackers to discover vulnerabilities, thereby protecting the digital assets of software users. This aligns with the value of Security as it ensures the health and safety of the software user's digital assets.",Agreed-Clarified,Agree,,Agree,,
Exp F,Exp J,2495,TSE,Security & Privacy,BiAn: Smart Contract Source Code Obfuscation,"With the rising prominence of smart contracts, security attacks targeting them have increased, posing severe threats to their security and intellectual property rights. Existing simplistic datasets hinder effective vulnerability detection, raising security concerns. To address these challenges, we propose BiAn, a source code level smart contract obfuscation method that generates complex vulnerability test datasets. BiAn protects contracts by obfuscating data flows, control flows, and code layouts, increasing complexity and making it harder for attackers to discover vulnerabilities. Our experiments with buggy contracts showed an average complexity enhancement of approximately 174% after obfuscation. Decompilers Vandal and Gigahorse had total failure rate increments of 38.8% and 40.5% respectively. Obfuscated contracts also decreased vulnerability detection rates in more than 50% of cases for ten widely-used static analysis detection tools.",Achievement,Successful,By obfuscating the data flows; control flows; and code layouts of smart contracts; 'Paper X' allows software users to be more successful in protecting their smart contracts against security attacks. This aligns with the value item 'Successful' under the value 'Achievement'.,"The obfuscation technique proposed in 'Paper X' aligns with the value item Successful and the corresponding value Achievement from a ""Software User"" perspective because it increases the complexity of smart contracts, making it harder for attackers to discover vulnerabilities. By implementing this obfuscation method, software users can achieve the successful protection of their smart contracts against security attacks, ultimately leading to a sense of achievement in maintaining the security and integrity of their software assets.",Agreed-Clarified,Agree,,Agree,,
Exp G,Exp K,2495,TSE,Security & Privacy,BiAn: Smart Contract Source Code Obfuscation,"With the rising prominence of smart contracts, security attacks targeting them have increased, posing severe threats to their security and intellectual property rights. Existing simplistic datasets hinder effective vulnerability detection, raising security concerns. To address these challenges, we propose BiAn, a source code level smart contract obfuscation method that generates complex vulnerability test datasets. BiAn protects contracts by obfuscating data flows, control flows, and code layouts, increasing complexity and making it harder for attackers to discover vulnerabilities. Our experiments with buggy contracts showed an average complexity enhancement of approximately 174% after obfuscation. Decompilers Vandal and Gigahorse had total failure rate increments of 38.8% and 40.5% respectively. Obfuscated contracts also decreased vulnerability detection rates in more than 50% of cases for ten widely-used static analysis detection tools.",Power,Wealth,The protection of smart contracts enhanced by 'Paper X' contributes to the software user's ability to preserve and increase their digital wealth; aligning with the value item 'Wealth' under the value 'Power'.,"In 'Paper X', the proposal of the BiAn method for obfuscating smart contract code directly contributes to the software user's ability to preserve and protect their digital wealth. By increasing the complexity and making it harder for attackers to discover vulnerabilities, 'Paper X' enables software users to safeguard their smart contracts, which may contain valuable digital assets. This alignment with the value item 'Wealth' under the value 'Power' is evident as it empowers software users to maintain control over their financial resources in the software context.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2503,TSE,Security & Privacy,An Empirical Study on the Effectiveness of Privacy Indicators,"The increasing diffusion of mobile devices and their integration with sophisticated hardware and software components has promoted the development of numerous applications in which developers find new ingenious ways to exploit the possibilities offered by the access to resources such as cameras, biometric sensors, and GPS receivers. As a result, we are increasingly used to seeing applications that make extensive use of sensitive resources, potentially dangerous for our privacy. To address this problem, the latest approach to support user awareness in terms of privacy is represented by the Privacy Indicators (PI), a software solution implemented by the operating system to provide a visual stimulus to inform users whenever a dangerous resource is exploited by the app. However, the effectiveness of this approach has not been assessed yet. In this article, we present the result of a study on the effectiveness of using the PI to inform the user every time an app accesses the mobile device camera or microphone. We have chosen these two resources as the PI are currently implemented only for a very limited number of permissions. The controlled experiment involved 122 Android users who were asked to complete a series of tasks on their smartphone through prototypes using the involved resources in an explicit and latent way. Although the PI mechanism is very similar between Android and iOS, we have decided to focus on the former due to its greater diffusion. The results show no significant correlation between the use of PI and the detection of the resource being used by the app, suggesting that the effectiveness of PI in improving sensitive-related resources usage awareness, as currently implemented, is still unsatisfactory. In order to understand if the problem was due to the specific implementation of the PI, we implemented an enhanced version and compared it with the standard one. The results confirmed that an implementation that makes the indicators more visible and that is clearer in highlighting the fact that the app is accessing a resource improves resources usage awareness.",Self Direction,Privacy,"The paper addresses the issue of potential threats to user privacy caused by mobile applications accessing sensitive resources like camera; microphone etc. and discusses an approach (Privacy Indicators) to enhance user awareness about privacy risks. This aligns with the value item ""Privacy"" and its corresponding value ""Self Direction"".","The paper's focus on addressing privacy risks and enhancing user awareness aligns with the value item ""Privacy"" and its corresponding value ""Self Direction"" because it empowers users to have control and make independent choices about their privacy when using mobile applications. By providing visual indicators, the paper aims to enable users to make informed decisions about granting access to sensitive resources, ultimately allowing them to have more autonomy and control over their own privacy.",Agreed-Justified,Agree,,Agree,,
Exp B,Exp H,2503,TSE,Security & Privacy,An Empirical Study on the Effectiveness of Privacy Indicators,"The increasing diffusion of mobile devices and their integration with sophisticated hardware and software components has promoted the development of numerous applications in which developers find new ingenious ways to exploit the possibilities offered by the access to resources such as cameras, biometric sensors, and GPS receivers. As a result, we are increasingly used to seeing applications that make extensive use of sensitive resources, potentially dangerous for our privacy. To address this problem, the latest approach to support user awareness in terms of privacy is represented by the Privacy Indicators (PI), a software solution implemented by the operating system to provide a visual stimulus to inform users whenever a dangerous resource is exploited by the app. However, the effectiveness of this approach has not been assessed yet. In this article, we present the result of a study on the effectiveness of using the PI to inform the user every time an app accesses the mobile device camera or microphone. We have chosen these two resources as the PI are currently implemented only for a very limited number of permissions. The controlled experiment involved 122 Android users who were asked to complete a series of tasks on their smartphone through prototypes using the involved resources in an explicit and latent way. Although the PI mechanism is very similar between Android and iOS, we have decided to focus on the former due to its greater diffusion. The results show no significant correlation between the use of PI and the detection of the resource being used by the app, suggesting that the effectiveness of PI in improving sensitive-related resources usage awareness, as currently implemented, is still unsatisfactory. In order to understand if the problem was due to the specific implementation of the PI, we implemented an enhanced version and compared it with the standard one. The results confirmed that an implementation that makes the indicators more visible and that is clearer in highlighting the fact that the app is accessing a resource improves resources usage awareness.",Security,Social Order,"The paper discusses the use of Privacy Indicators to inform and warn the user every time an application accesses sensitive resources in their mobile device; an awareness approach that can contribute to maintaining social order by preventing unauthorized and unnoticed access to personal resources of users. This aligns with the value item ""Social Order"" and its corresponding value ""Security"".","The notion of social order implies maintaining a structured and regulated society where individuals' rights and security are protected. In the context of 'Paper X,' the use of Privacy Indicators serves as a mechanism to inform and warn users about the access to their personal resources by mobile applications. By enhancing users' awareness and providing them with control over their sensitive resources, this approach contributes to maintaining social order by preventing unauthorized and unnoticed access, thereby ensuring the security of individuals' personal information and promoting a sense of trust and order within the software environment.",Agreed-Justified,Agree,,Agree,,
Exp D,Exp K,2506,TSE,Software Testing & QA,scenoRITA: Generating Diverse Fully Mutable Test Scenarios for Autonomous Vehicle Planning,"Autonomous Vehicles (AVs) leverage advanced sensing and networking technologies (e.g., camera, LiDAR, RADAR, GPS, DSRC, 5G, etc.) to enable safe and efficient driving without human drivers. Although still in its infancy, AV technology is becoming increasingly common and could radically transform our transportation system and by extension, our economy and society. As a result, there is tremendous global enthusiasm for research, development, and deployment of AVs, e.g., self-driving taxis and trucks from Waymo and Baidu. The current practice for testing AVs uses virtual testsaEUR""where AVs are tested in software simulationsaEUR""since they offer a more efficient and safer alternative compared to field operational tests. Specifically, search-based approaches are used to find particularly critical situations. These approaches provide an opportunity to automatically generate tests; however, systematically creating valid and effective tests for AV software remains a major challenge. To address this challenge, we introduce scenoRITA, a test generation approach for AVs that uses an evolutionary algorithm with (1) a novel gene representation that allows obstacles to be fully mutable, hence, resulting in more reported violations and more diverse scenarios, (2) 5 test oracles to determine both safety and motion sickness-inducing violations and (3) a novel technique to identify and eliminate duplicate tests. Our extensive evaluation shows that scenoRITA can produce test scenarios that are more effective in revealing ADS bugs and more diverse in covering different parts of the map compared to other state-of-the-art test generation approaches.",Security,Healthy,The main contribution of 'Paper X' is the development of a new method for testing driverless vehicles; specifically aiming to improve the safety of autonomous vehicle software. This strongly aligns with the value item 'Healthy' under the value 'Security'; as the testing aims to ensure that the systems are safe and reliable; preventing potential accidents and harm to the user.,"In 'Paper X', the main contribution of the development of a test generation approach for autonomous vehicles directly aligns with the value item 'Healthy' and its corresponding value 'Security' from a ""Software User"" perspective. This alignment is evident as the testing method aims to improve the safety and reliability of autonomous vehicle software, which in turn ensures the well-being and protection of users by preventing potential accidents and harm. By identifying and addressing possible software vulnerabilities, this contribution directly supports the value of 'Security' by emphasizing the importance of maintaining a safe and secure environment for the users of autonomous vehicles.",Agreed-Justified,Agree,,Agree,,
Exp E,Exp J,2506,TSE,Software Testing & QA,scenoRITA: Generating Diverse Fully Mutable Test Scenarios for Autonomous Vehicle Planning,"Autonomous Vehicles (AVs) leverage advanced sensing and networking technologies (e.g., camera, LiDAR, RADAR, GPS, DSRC, 5G, etc.) to enable safe and efficient driving without human drivers. Although still in its infancy, AV technology is becoming increasingly common and could radically transform our transportation system and by extension, our economy and society. As a result, there is tremendous global enthusiasm for research, development, and deployment of AVs, e.g., self-driving taxis and trucks from Waymo and Baidu. The current practice for testing AVs uses virtual testsaEUR""where AVs are tested in software simulationsaEUR""since they offer a more efficient and safer alternative compared to field operational tests. Specifically, search-based approaches are used to find particularly critical situations. These approaches provide an opportunity to automatically generate tests; however, systematically creating valid and effective tests for AV software remains a major challenge. To address this challenge, we introduce scenoRITA, a test generation approach for AVs that uses an evolutionary algorithm with (1) a novel gene representation that allows obstacles to be fully mutable, hence, resulting in more reported violations and more diverse scenarios, (2) 5 test oracles to determine both safety and motion sickness-inducing violations and (3) a novel technique to identify and eliminate duplicate tests. Our extensive evaluation shows that scenoRITA can produce test scenarios that are more effective in revealing ADS bugs and more diverse in covering different parts of the map compared to other state-of-the-art test generation approaches.",Stimulation,Variation in Life,The introduction of this new test generation approach called 'scenoRITA' contribute to creating diverse test scenarios. This aligns with the value item 'Variation in Life' and its corresponding value 'Stimulation'. With the diverse test scenarios; the autonomous vehicle software can cater to different driving situations; contributing to varying user experiences.,"The alignment between 'Paper X' and the value item Variation in Life and its corresponding value Stimulation is evident through the introduction of the test generation approach 'scenoRITA', which creates diverse test scenarios for autonomous vehicle software. This diversity in test scenarios allows for different driving situations to be catered to, thereby contributing to varied and stimulating user experiences. By enabling autonomous vehicles to handle a wide range of scenarios, the software enhances the stimulation and variety that users can experience in their daily lives, aligning with the value of Stimulation and its corresponding value item Variation in Life.",Agreed-Justified,Agree,,Agree,,
Exp F,Exp J,2506,TSE,Software Testing & QA,scenoRITA: Generating Diverse Fully Mutable Test Scenarios for Autonomous Vehicle Planning,"Autonomous Vehicles (AVs) leverage advanced sensing and networking technologies (e.g., camera, LiDAR, RADAR, GPS, DSRC, 5G, etc.) to enable safe and efficient driving without human drivers. Although still in its infancy, AV technology is becoming increasingly common and could radically transform our transportation system and by extension, our economy and society. As a result, there is tremendous global enthusiasm for research, development, and deployment of AVs, e.g., self-driving taxis and trucks from Waymo and Baidu. The current practice for testing AVs uses virtual testsaEUR""where AVs are tested in software simulationsaEUR""since they offer a more efficient and safer alternative compared to field operational tests. Specifically, search-based approaches are used to find particularly critical situations. These approaches provide an opportunity to automatically generate tests; however, systematically creating valid and effective tests for AV software remains a major challenge. To address this challenge, we introduce scenoRITA, a test generation approach for AVs that uses an evolutionary algorithm with (1) a novel gene representation that allows obstacles to be fully mutable, hence, resulting in more reported violations and more diverse scenarios, (2) 5 test oracles to determine both safety and motion sickness-inducing violations and (3) a novel technique to identify and eliminate duplicate tests. Our extensive evaluation shows that scenoRITA can produce test scenarios that are more effective in revealing ADS bugs and more diverse in covering different parts of the map compared to other state-of-the-art test generation approaches.",Achievement,Capable,The development and application of this novel testing method could potentially demonstrate the advanced capability and competency of the autonomous vehicle software; that it can handle different critical situations; providing a sense of achievement in terms of software user experience. This aligns with the value item 'Capable' and its corresponding value 'Achievement'.,"In 'Paper X', the main contribution is the development of a test generation approach for autonomous vehicles (AVs), specifically aimed at identifying critical situations and generating effective tests for the AV software. By demonstrating the capability of handling different critical situations, the AV software can provide a sense of achievement in terms of ensuring a safe and efficient driving experience for the software user. This aligns with the value item 'Capable' and its corresponding value 'Achievement' from a software user perspective.",Agreed-Clarified,Agree,,Agree,,
Exp G,Exp K,2510,TSE,Software Project Management,A Grounded Theory of Cross-Community SECOs: Feedback Diversity Versus Synchronization,"Despite their proliferation, growing sustainable software ecosystems (SECOs) remains a substantial challenge. One approach to mitigate this challenge is by collecting and integrating feedback from distributors (distros) and end-users of the SECO releases into future SECO releases, tools, or policies. This paper performs a socio-technical analysis of cross-community collaboration in the OpenStack SECO, which consists of the upstream OpenStack project and 21 distribution (distro) communities. First, we followed Masood et al.'s adaptation of Strauss-Corbinian grounded theory methodology for socio-technical contexts on data from an open-ended unstructured interview, a survey, focus groups, and 384 mailing list threads to investigate how SECOs manage to sustain cross-community collaboration. Our theory has 15 constructs divided into four categories: diverse feedback types and mechanisms (2), characteristics of feedback (2), challenges (7), and the benefits (4) of cross-community collaboration. We then empirically study the salient aspects of the theory, i.e., diversity and synchronization, among 21 OpenStack distros. We empirically mined feedback that distros contribute to upstream, i.e., 140,261 mailing list threads, 142,914 bugs reported, 65,179 bugs resolved, and 4,349 new features. Then, we use influence maximization social network analysis to model the synchronization of feedback in the OpenStack SECO. Our results suggest that distros contribute substantially towards the sustainability of the SECO in the form of 25.6% of new features, 30.7% of emails, 44.3% of bug reports, and 30.7% of bug fixes. Finally, we found evidence of distros playing different roles in a SECO, with nine distros contributing all four types of feedback in equal proportions, while 12 distros specialize in one type of feedback. Distros that are influential in propagating a given type of feedback to the SECO community are not necessarily specialized in that feedback type.",Tradition,Respect for Tradition,The paper emphasizes the respect for traditional means of feedback and collaboration in sustainable software ecosystems and utilizes these traditions to improve the software. This aligns with the value item 'Respect for Tradition' in the value 'Tradition'.,"In the paper, 'Paper X', there is a clear emphasis on the utilization of traditional means of feedback and collaboration in sustainable software ecosystems. The authors highlight the importance of collecting and integrating feedback from distributors and end-users of software releases into future releases, tools, or policies. By recognizing and valuing these established traditions, the paper aligns with the value item 'Respect for Tradition' and its corresponding value 'Tradition' from Schwartz's Taxonomy. This alignment demonstrates a recognition of the value of traditional practices and their role in improving and sustaining software ecosystems from the perspective of a software user.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2510,TSE,Software Project Management,A Grounded Theory of Cross-Community SECOs: Feedback Diversity Versus Synchronization,"Despite their proliferation, growing sustainable software ecosystems (SECOs) remains a substantial challenge. One approach to mitigate this challenge is by collecting and integrating feedback from distributors (distros) and end-users of the SECO releases into future SECO releases, tools, or policies. This paper performs a socio-technical analysis of cross-community collaboration in the OpenStack SECO, which consists of the upstream OpenStack project and 21 distribution (distro) communities. First, we followed Masood et al.'s adaptation of Strauss-Corbinian grounded theory methodology for socio-technical contexts on data from an open-ended unstructured interview, a survey, focus groups, and 384 mailing list threads to investigate how SECOs manage to sustain cross-community collaboration. Our theory has 15 constructs divided into four categories: diverse feedback types and mechanisms (2), characteristics of feedback (2), challenges (7), and the benefits (4) of cross-community collaboration. We then empirically study the salient aspects of the theory, i.e., diversity and synchronization, among 21 OpenStack distros. We empirically mined feedback that distros contribute to upstream, i.e., 140,261 mailing list threads, 142,914 bugs reported, 65,179 bugs resolved, and 4,349 new features. Then, we use influence maximization social network analysis to model the synchronization of feedback in the OpenStack SECO. Our results suggest that distros contribute substantially towards the sustainability of the SECO in the form of 25.6% of new features, 30.7% of emails, 44.3% of bug reports, and 30.7% of bug fixes. Finally, we found evidence of distros playing different roles in a SECO, with nine distros contributing all four types of feedback in equal proportions, while 12 distros specialize in one type of feedback. Distros that are influential in propagating a given type of feedback to the SECO community are not necessarily specialized in that feedback type.",Universalism,Equality,The paper promotes the equal contribution of different distributors towards the software ecosystem; reflecting the idea of 'Equality' under the value 'Universalism'.,"In 'Paper X', the authors emphasize the significant contribution of different distributors towards the sustainability of the software ecosystem. This aligns with the value item Equality under the value Universalism because it demonstrates that all distributors are given equal opportunities and recognition for their feedback and contributions, promoting a sense of fairness and equal participation within the software community.",Agreed-Clarified,Agree,,Agree,,
Exp B,Exp H,2510,TSE,Software Project Management,A Grounded Theory of Cross-Community SECOs: Feedback Diversity Versus Synchronization,"Despite their proliferation, growing sustainable software ecosystems (SECOs) remains a substantial challenge. One approach to mitigate this challenge is by collecting and integrating feedback from distributors (distros) and end-users of the SECO releases into future SECO releases, tools, or policies. This paper performs a socio-technical analysis of cross-community collaboration in the OpenStack SECO, which consists of the upstream OpenStack project and 21 distribution (distro) communities. First, we followed Masood et al.'s adaptation of Strauss-Corbinian grounded theory methodology for socio-technical contexts on data from an open-ended unstructured interview, a survey, focus groups, and 384 mailing list threads to investigate how SECOs manage to sustain cross-community collaboration. Our theory has 15 constructs divided into four categories: diverse feedback types and mechanisms (2), characteristics of feedback (2), challenges (7), and the benefits (4) of cross-community collaboration. We then empirically study the salient aspects of the theory, i.e., diversity and synchronization, among 21 OpenStack distros. We empirically mined feedback that distros contribute to upstream, i.e., 140,261 mailing list threads, 142,914 bugs reported, 65,179 bugs resolved, and 4,349 new features. Then, we use influence maximization social network analysis to model the synchronization of feedback in the OpenStack SECO. Our results suggest that distros contribute substantially towards the sustainability of the SECO in the form of 25.6% of new features, 30.7% of emails, 44.3% of bug reports, and 30.7% of bug fixes. Finally, we found evidence of distros playing different roles in a SECO, with nine distros contributing all four types of feedback in equal proportions, while 12 distros specialize in one type of feedback. Distros that are influential in propagating a given type of feedback to the SECO community are not necessarily specialized in that feedback type.",Security,Social Order,The maintained order and structure in the sustainable software ecosystem due to collaboration and feedback mechanisms aligns with the value item 'Social Order' in the value 'Security'.,"In the context of a sustainable software ecosystem, the value items 'Social Order' and 'Security' align with the main contributions of 'Paper X' in the following way: The collaboration and feedback mechanisms discussed in 'Paper X' enable the maintenance of order and structure within the software ecosystem, ensuring that it functions smoothly and securely. By collecting and integrating feedback from distributors and end-users, the SECO releases can be improved, tools can be enhanced, and policies can be formulated, all contributing to the overall security and stability of the software system. This alignment with the value items of 'Social Order' and 'Security' is evident in the abstract as the paper addresses the challenges and benefits of cross-community collaboration in sustaining the software ecosystem, highlighting the importance of maintaining order and security in the process.",Agreed-Justified,Agree,,Agree,,
Exp C,Exp K,2513,TSE,Code Generation & Analysis,K-ST: A Formal Executable Semantics of the Structured Text Language for PLCs,"Programmable Logic Controllers (PLCs) are responsible for automating process control in many industrial systems (e.g. in manufacturing and public infrastructure), and thus it is critical to ensure that they operate correctly and safely. The majority of PLCs are programmed in languages such as Structured Text (ST). However, a lack of formal semantics makes it difficult to ascertain the correctness of their translators and compilers, which vary from vendor-to-vendor. In this work, we develop K-ST, a formal executable semantics for ST in the $\boldsymbol{\mathbb{K}}$K framework. Defined with respect to the IEC 61131-3 standard and PLC vendor manuals, K-ST is a high-level reference semantics that can be used to evaluate the correctness and consistency of different ST implementations. We validate K-ST by executing 567 ST programs extracted from GitHub and comparing the results against existing commercial compilers (i.e., CODESYS, CX-Programmer, and GX Works2). We then apply K-ST to validate the implementation of the open source OpenPLC platform, comparing the executions of several test programs to uncover five bugs and nine functional defects in the compiler.",Achievement,Intelligent,The development of K-ST in 'Paper X' allows for the accurate evaluation and validation of Structured Text programs; thereby directly contributing to the value of 'Intelligent'; since it enables software users to ascertain the correctness of their PLCs; aligning with the broader value of 'Achievement'.,"The justification for aligning 'Paper X' with the value item Intelligent and its corresponding value Achievement is based on the fact that the development of K-ST provides a formal and executable semantics for Structured Text programs. This enables software users to accurately evaluate and validate their PLCs, ensuring their correct operation. By empowering software users to independently ascertain the correctness of their programs, 'Paper X' ultimately contributes to the value of Intelligent. Furthermore, by enabling software users to achieve the goal of correctly and safely automating process control in industrial systems, 'Paper X' aligns with the broader value of Achievement.",Agreed-Justified,Agree,,Agree,,
Exp D,Exp K,2513,TSE,Code Generation & Analysis,K-ST: A Formal Executable Semantics of the Structured Text Language for PLCs,"Programmable Logic Controllers (PLCs) are responsible for automating process control in many industrial systems (e.g. in manufacturing and public infrastructure), and thus it is critical to ensure that they operate correctly and safely. The majority of PLCs are programmed in languages such as Structured Text (ST). However, a lack of formal semantics makes it difficult to ascertain the correctness of their translators and compilers, which vary from vendor-to-vendor. In this work, we develop K-ST, a formal executable semantics for ST in the $\boldsymbol{\mathbb{K}}$K framework. Defined with respect to the IEC 61131-3 standard and PLC vendor manuals, K-ST is a high-level reference semantics that can be used to evaluate the correctness and consistency of different ST implementations. We validate K-ST by executing 567 ST programs extracted from GitHub and comparing the results against existing commercial compilers (i.e., CODESYS, CX-Programmer, and GX Works2). We then apply K-ST to validate the implementation of the open source OpenPLC platform, comparing the executions of several test programs to uncover five bugs and nine functional defects in the compiler.",Security,Healthy,The work carried out in 'Paper X' highlights an improved ability to uncover bugs and functional defects in the open source OpenPLC platform. This contributes to better operational safety of PLCs; aligning with the value item of 'Healthy' as it retains system wellbeing; thus aligning with the broader value of 'Security'.,"In 'Paper X', the development of a formal executable semantics for Structured Text (ST) programming language provides a means to evaluate the correctness and consistency of different implementations. By applying this formal semantics to validate the implementation of the OpenPLC platform, the paper uncovers bugs and functional defects, which enhances the operational safety of Programmable Logic Controllers (PLCs). This aligns with the value item of 'Healthy' because it contributes to improving the overall system wellbeing and safety, thus promoting the broader value of 'Security' in the software context.",Agreed-Clarified,Agree,,Agree,,
Exp F,Exp J,2515,TSE,Security & Privacy,Constructing Cyber-Physical System Testing Suites Using Active Sensor Fuzzing,"Cyber-physical systems (CPSs) automating critical public infrastructure face a pervasive threat of attack, motivating research into different types of countermeasures. Assessing the effectiveness of these countermeasures is challenging, however, as benchmarks are difficult to construct manually, existing automated testing solutions often make unrealistic assumptions, and blindly fuzzing is ineffective at finding attacks due to the enormous search spaces and resource requirements. In this work, we propose active sensor fuzzing, a fully automated approach for building test suites without requiring any a prior knowledge about a CPS. Our approach employs active learning techniques. Applied to a real-world water treatment system, our approach manages to find attacks that drive the system into 15 different unsafe states involving water flow, pressure, and tank levels, including nine that were not covered by an established attack benchmark. Furthermore, we successfully generate targeted multi-point attacks which have been long suspected to be possible. We reveal that active sensor fuzzing successfully extends the attack benchmarks generated by our previous work, an ML-guided fuzzing tool, with two more kinds of attacks. Finally, we investigate the impact of active learning on models and the reason that the model trained with active learning is able to discover more attacks.",Security,Social Order,Paper X contributes to improved Cyber-physical system security which is imperative for ensuring social order; hence aligning with the value item Social Order and its corresponding value Security.,"I apologize for any confusion caused. My justification is that the main contribution of 'Paper X' in improving the security of cyber-physical systems directly aligns with the value item of Social Order and its corresponding value of Security from a ""Software User"" perspective. Cyber-physical systems play a crucial role in critical public infrastructure, and ensuring their security is vital to maintain social order. By proposing an automated approach for building test suites and identifying attacks in these systems, 'Paper X' directly contributes to enhancing security and thereby helps in maintaining social order.",Agreed-Justified,Agree,,Agree,,
Exp G,Exp K,2515,TSE,Security & Privacy,Constructing Cyber-Physical System Testing Suites Using Active Sensor Fuzzing,"Cyber-physical systems (CPSs) automating critical public infrastructure face a pervasive threat of attack, motivating research into different types of countermeasures. Assessing the effectiveness of these countermeasures is challenging, however, as benchmarks are difficult to construct manually, existing automated testing solutions often make unrealistic assumptions, and blindly fuzzing is ineffective at finding attacks due to the enormous search spaces and resource requirements. In this work, we propose active sensor fuzzing, a fully automated approach for building test suites without requiring any a prior knowledge about a CPS. Our approach employs active learning techniques. Applied to a real-world water treatment system, our approach manages to find attacks that drive the system into 15 different unsafe states involving water flow, pressure, and tank levels, including nine that were not covered by an established attack benchmark. Furthermore, we successfully generate targeted multi-point attacks which have been long suspected to be possible. We reveal that active sensor fuzzing successfully extends the attack benchmarks generated by our previous work, an ML-guided fuzzing tool, with two more kinds of attacks. Finally, we investigate the impact of active learning on models and the reason that the model trained with active learning is able to discover more attacks.",Security,Healthy,By proposing a method for identifying and preventing potential attacks; 'Paper X' helps ensure the health of the public infrastructure systems for software users aligning with the value item Health under the value Security.,"In 'Paper X', the main contribution of proposing active sensor fuzzing to identify and prevent attacks aligns with the value item Healthy and its corresponding value Security from a ""Software User"" perspective. By actively testing and uncovering potential attacks, the paper aims to ensure the security and integrity of the public infrastructure systems that software users rely on. This directly aligns with the value of Health as it seeks to protect and maintain the well-being and safety of the users by safeguarding the critical systems they interact with. Furthermore, by addressing the challenges in assessing the effectiveness of countermeasures, the paper contributes to the value of Security by offering a more robust and automated approach for enhancing system resilience against cyber threats.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2515,TSE,Security & Privacy,Constructing Cyber-Physical System Testing Suites Using Active Sensor Fuzzing,"Cyber-physical systems (CPSs) automating critical public infrastructure face a pervasive threat of attack, motivating research into different types of countermeasures. Assessing the effectiveness of these countermeasures is challenging, however, as benchmarks are difficult to construct manually, existing automated testing solutions often make unrealistic assumptions, and blindly fuzzing is ineffective at finding attacks due to the enormous search spaces and resource requirements. In this work, we propose active sensor fuzzing, a fully automated approach for building test suites without requiring any a prior knowledge about a CPS. Our approach employs active learning techniques. Applied to a real-world water treatment system, our approach manages to find attacks that drive the system into 15 different unsafe states involving water flow, pressure, and tank levels, including nine that were not covered by an established attack benchmark. Furthermore, we successfully generate targeted multi-point attacks which have been long suspected to be possible. We reveal that active sensor fuzzing successfully extends the attack benchmarks generated by our previous work, an ML-guided fuzzing tool, with two more kinds of attacks. Finally, we investigate the impact of active learning on models and the reason that the model trained with active learning is able to discover more attacks.",Security,National Security,The paper's contributions towards protecting public infrastructure systems align with the value item National Security and its corresponding value Security.,"The main contributions of 'Paper X' in protecting critical public infrastructure systems directly align with the value item National Security and its corresponding value Security. By proposing active sensor fuzzing as a fully automated approach for building test suites, the paper aims to assess the effectiveness of countermeasures against cyber-physical attacks on public infrastructure. This aligns with the value of Security as it addresses the pervasive threat of attack on these systems, contributing to the overall goal of protecting national security.",Agreed-Justified,Agree,,Agree,,
Exp B,Exp H,2518,TSE,Security & Privacy,VulHunter: Hunting Vulnerable Smart Contracts at EVM Bytecode-Level via Multiple Instance Learning,"With the economic development of Ethereum, the frequent security incidents involving smart contracts running on this platform have caused billions of dollars in losses. Consequently, there is a pressing need to identify the vulnerabilities in contracts, while the state-of-the-art (SOTA) detection methods have been limited in this regard as they cannot overcome three challenges at the same time. (i) Meet the requirements of detecting the source code, bytecode, and opcode of contracts simultaneously; (ii) reduce the reliance on manual pre-defined rules/patterns and expert involvement; (iii) assist contract developers in completing the contract lifecycle more safely, e.g., vulnerability repair and abnormal monitoring. With the development of machine learning (ML), using it to detect the contract runtime execution sequences (called instances) has made it possible to address these challenges. However, the lack of datasets with fine-grained sequence labels poses a significant obstacle, given the unreadability of bytecode/opcode. To this end, we propose a method named VulHunter that extracts the instances by traversing the Control Flow Graph built from contract opcodes. Based on the hybrid attention and multi-instance learning mechanisms, VulHunter reasons the instance labels and designs an optional classifier to automatically capture the subtle features of both normal and defective contracts, thereby identifying the vulnerable instances. Then, it combines the symbolic execution to construct and solve symbolic constraints to validate their feasibility. Finally, we implement a prototype of VulHunter with 15K lines of code and compare it with 9 SOTA methods on five open source datasets including 52,042 source codes and 184,289 bytecodes. The results indicate that VulHunter can detect contract vulnerabilities more accurately (90.04% accuracy and 85.60% F1 score), efficiently (only 4.4 seconds per contract), and robustly (0% analysis failure rate) than SOTA methods. Also, it can focus on specific metrics such as precision and recall by employing different baseline models and hyperparameters to meet the various user requirements, e.g., vulnerability discovery and misreport mitigation. More importantly, compared with the previous ML-based arts, it can not only provide classification results, defective contract source code statements, key opcode fragments, and vulnerable execution paths, but also eliminate misreports and facilitate more operations such as vulnerability repair and attack simulation during the contract lifecycle.",Achievement,Influential,The paper presents VulHunter; which notably enhances users' ability to detect vulnerable instances in Ethereum contracts; outperforming nine state-of-the-art methods in terms of accuracy; efficiency; and robustness. Therefore; it contributes to the value item Influential (demonstrative ability) and its corresponding value Achievement.,"Sure! In the paper abstract, it is explicitly mentioned that VulHunter, the method proposed in 'Paper X', outperforms nine state-of-the-art methods in terms of accuracy, efficiency, and robustness. By providing users with a tool that allows them to accurately detect vulnerable instances in Ethereum contracts, VulHunter enables users to have a demonstrative ability to make a significant impact in the software context. This aligns with the value item Influential and its corresponding value Achievement because users can achieve a higher level of influence and accomplishment through the successful identification and prevention of vulnerabilities in their software systems.",Agreed-Clarified,Agree,,Agree,,
Exp C,Exp K,2518,TSE,Security & Privacy,VulHunter: Hunting Vulnerable Smart Contracts at EVM Bytecode-Level via Multiple Instance Learning,"With the economic development of Ethereum, the frequent security incidents involving smart contracts running on this platform have caused billions of dollars in losses. Consequently, there is a pressing need to identify the vulnerabilities in contracts, while the state-of-the-art (SOTA) detection methods have been limited in this regard as they cannot overcome three challenges at the same time. (i) Meet the requirements of detecting the source code, bytecode, and opcode of contracts simultaneously; (ii) reduce the reliance on manual pre-defined rules/patterns and expert involvement; (iii) assist contract developers in completing the contract lifecycle more safely, e.g., vulnerability repair and abnormal monitoring. With the development of machine learning (ML), using it to detect the contract runtime execution sequences (called instances) has made it possible to address these challenges. However, the lack of datasets with fine-grained sequence labels poses a significant obstacle, given the unreadability of bytecode/opcode. To this end, we propose a method named VulHunter that extracts the instances by traversing the Control Flow Graph built from contract opcodes. Based on the hybrid attention and multi-instance learning mechanisms, VulHunter reasons the instance labels and designs an optional classifier to automatically capture the subtle features of both normal and defective contracts, thereby identifying the vulnerable instances. Then, it combines the symbolic execution to construct and solve symbolic constraints to validate their feasibility. Finally, we implement a prototype of VulHunter with 15K lines of code and compare it with 9 SOTA methods on five open source datasets including 52,042 source codes and 184,289 bytecodes. The results indicate that VulHunter can detect contract vulnerabilities more accurately (90.04% accuracy and 85.60% F1 score), efficiently (only 4.4 seconds per contract), and robustly (0% analysis failure rate) than SOTA methods. Also, it can focus on specific metrics such as precision and recall by employing different baseline models and hyperparameters to meet the various user requirements, e.g., vulnerability discovery and misreport mitigation. More importantly, compared with the previous ML-based arts, it can not only provide classification results, defective contract source code statements, key opcode fragments, and vulnerable execution paths, but also eliminate misreports and facilitate more operations such as vulnerability repair and attack simulation during the contract lifecycle.",Security,Healthy,By detecting and managing vulnerabilities in Ethereum contracts; VulHunter contributes to a safer contract lifecycle; potentially leading to healthier functioning of the software. This directly aligns with the value item Healthy and its corresponding value Security.,"VulHunter aligns with the value item Healthy and its corresponding value Security because it addresses the pressing need to identify vulnerabilities in Ethereum contracts. By detecting and managing these vulnerabilities, it contributes to a safer contract lifecycle. This enhanced security ultimately leads to a healthier functioning of the software, ensuring that users' data, transactions, and investments are protected from potential threats and vulnerabilities.",Agreed-Clarified,Agree,,Agree,,
Exp D,Exp K,2518,TSE,Security & Privacy,VulHunter: Hunting Vulnerable Smart Contracts at EVM Bytecode-Level via Multiple Instance Learning,"With the economic development of Ethereum, the frequent security incidents involving smart contracts running on this platform have caused billions of dollars in losses. Consequently, there is a pressing need to identify the vulnerabilities in contracts, while the state-of-the-art (SOTA) detection methods have been limited in this regard as they cannot overcome three challenges at the same time. (i) Meet the requirements of detecting the source code, bytecode, and opcode of contracts simultaneously; (ii) reduce the reliance on manual pre-defined rules/patterns and expert involvement; (iii) assist contract developers in completing the contract lifecycle more safely, e.g., vulnerability repair and abnormal monitoring. With the development of machine learning (ML), using it to detect the contract runtime execution sequences (called instances) has made it possible to address these challenges. However, the lack of datasets with fine-grained sequence labels poses a significant obstacle, given the unreadability of bytecode/opcode. To this end, we propose a method named VulHunter that extracts the instances by traversing the Control Flow Graph built from contract opcodes. Based on the hybrid attention and multi-instance learning mechanisms, VulHunter reasons the instance labels and designs an optional classifier to automatically capture the subtle features of both normal and defective contracts, thereby identifying the vulnerable instances. Then, it combines the symbolic execution to construct and solve symbolic constraints to validate their feasibility. Finally, we implement a prototype of VulHunter with 15K lines of code and compare it with 9 SOTA methods on five open source datasets including 52,042 source codes and 184,289 bytecodes. The results indicate that VulHunter can detect contract vulnerabilities more accurately (90.04% accuracy and 85.60% F1 score), efficiently (only 4.4 seconds per contract), and robustly (0% analysis failure rate) than SOTA methods. Also, it can focus on specific metrics such as precision and recall by employing different baseline models and hyperparameters to meet the various user requirements, e.g., vulnerability discovery and misreport mitigation. More importantly, compared with the previous ML-based arts, it can not only provide classification results, defective contract source code statements, key opcode fragments, and vulnerable execution paths, but also eliminate misreports and facilitate more operations such as vulnerability repair and attack simulation during the contract lifecycle.",Stimulation,Excitement in Life,The implementation of VulHunter introduces excitement into users' experience by reducing reliance on manual pre-defined rules/patterns and expert involvement. Therefore; it aligns with the value item Excitement in Life and corresponding value Stimulation.,"In the context of software usage, the value item Excitement in Life and its corresponding value Stimulation is aligned with the main contribution of ""Paper X"" because the implementation of VulHunter introduces a sense of novelty and excitement for software users. By reducing reliance on manual pre-defined rules/patterns and expert involvement, VulHunter offers a new and innovative approach to detecting vulnerabilities in smart contracts. This new method and its potential to accurately and efficiently identify vulnerable instances add excitement and stimulation to the software user's experience, providing a fresh perspective on contract security and potentially avoiding significant financial losses.",Agreed-Justified,Agree,,Agree,,
Exp E,Exp J,2520,TSE,Security & Privacy,Are Your Dependencies Code Reviewed?: Measuring Code Review Coverage in Dependency Updates,"As modern software extensively uses free open source packages as dependencies, developers have to regularly pull in new third-party code through frequent updates. However, without a proper review of every incoming change, vulnerable and malicious code can sneak into the codebase through these dependencies. The goal of this study is to aid developers in securely accepting dependency updates by measuring if the code changes in an update have passed through a code review process. We implement Depdive, an update audit tool for packages in Crates.io, npm, PyPI, and RubyGems registry. Depdive first (i) identifies the files and the code changes in an update that cannot be traced back to the package's source repository, i.e., phantom artifacts; and then (ii) measures what portion of changes in the update, excluding the phantom artifacts, has passed through a code review process, i.e., code review coverage. Using Depdive, we present an empirical study across the latest ten updates of the most downloaded 1000 packages in each of the four registries. We further evaluated our results through a maintainer agreement survey. We find that phantom artifacts are not uncommon in the updates (20.1% of the analyzed updates had at least one phantom file). The phantoms can appear either due to legitimate reasons, such as in the case of programmatically generated files, or from accidental inclusion, such as in the case of files that are ignored in the repository. Regarding code review coverage (CRC), we find the updates are typically only partially code-reviewed (52.5% of the time). Further, only 9.0% of the packages had all their updates in our data set fully code-reviewed, indicating that even the most used packages can introduce non-reviewed code in the software supply chain. We also observe that updates either tend to have high CRC or low CRC, suggesting that packages at the opposite end of the spectrum may require a separate set of treatments.",Security,Healthy,"The main goal of the research presented in 'Paper X' is to create a tool that systematically checks and evaluates newly integrated third-party code in widely-used software packages. By ensuring that code updates pass through the code review process and minimizing the introduction of vulnerable and malicious code via dependencies; the paper directly aligns with the value item ""Healthy"" and the corresponding value; ""Security""; by contributing to the overall health and integrity of software from a user perspective.","In 'Paper X', the main contributions of creating an update audit tool for packages in various registries and measuring the code review coverage align with the value item ""Healthy"" and the corresponding value ""Security"" from a software user perspective. By addressing the potential risks of vulnerable and malicious code through thorough code review processes, the paper contributes to the overall health and integrity of software, ensuring a sense of security for the users.",Agreed-Justified,Agree,,Agree,,
Exp F,Exp J,2520,TSE,Security & Privacy,Are Your Dependencies Code Reviewed?: Measuring Code Review Coverage in Dependency Updates,"As modern software extensively uses free open source packages as dependencies, developers have to regularly pull in new third-party code through frequent updates. However, without a proper review of every incoming change, vulnerable and malicious code can sneak into the codebase through these dependencies. The goal of this study is to aid developers in securely accepting dependency updates by measuring if the code changes in an update have passed through a code review process. We implement Depdive, an update audit tool for packages in Crates.io, npm, PyPI, and RubyGems registry. Depdive first (i) identifies the files and the code changes in an update that cannot be traced back to the package's source repository, i.e., phantom artifacts; and then (ii) measures what portion of changes in the update, excluding the phantom artifacts, has passed through a code review process, i.e., code review coverage. Using Depdive, we present an empirical study across the latest ten updates of the most downloaded 1000 packages in each of the four registries. We further evaluated our results through a maintainer agreement survey. We find that phantom artifacts are not uncommon in the updates (20.1% of the analyzed updates had at least one phantom file). The phantoms can appear either due to legitimate reasons, such as in the case of programmatically generated files, or from accidental inclusion, such as in the case of files that are ignored in the repository. Regarding code review coverage (CRC), we find the updates are typically only partially code-reviewed (52.5% of the time). Further, only 9.0% of the packages had all their updates in our data set fully code-reviewed, indicating that even the most used packages can introduce non-reviewed code in the software supply chain. We also observe that updates either tend to have high CRC or low CRC, suggesting that packages at the opposite end of the spectrum may require a separate set of treatments.",Achievement,Intelligent,"The paper presents an update audit tool (Depdive) that identifies phantom artifacts and measures the code review coverage; aiming to ensure that software user doesn't come across errors or issues resulting from unchecked updates. Hence; it aligns with the value item ""Intelligent"" (v4.1) and its corresponding value ""Achievement"" by contributing to the intelligent operation of software.","The main contribution of 'Paper X' is the implementation of Depdive, an update audit tool that helps developers securely accept dependency updates by identifying phantom artifacts and measuring code review coverage. By ensuring that software users do not encounter errors or issues resulting from unchecked updates, the paper aligns with the value item Intelligent (v4.1) and its corresponding value Achievement. This alignment is because the intelligent operation of software, achieved through thorough code review and error detection, contributes to the successful and impactful use of technology by software users.",Agreed-Clarified,Agree,,Agree,,
Exp G,Exp K,2520,TSE,Security & Privacy,Are Your Dependencies Code Reviewed?: Measuring Code Review Coverage in Dependency Updates,"As modern software extensively uses free open source packages as dependencies, developers have to regularly pull in new third-party code through frequent updates. However, without a proper review of every incoming change, vulnerable and malicious code can sneak into the codebase through these dependencies. The goal of this study is to aid developers in securely accepting dependency updates by measuring if the code changes in an update have passed through a code review process. We implement Depdive, an update audit tool for packages in Crates.io, npm, PyPI, and RubyGems registry. Depdive first (i) identifies the files and the code changes in an update that cannot be traced back to the package's source repository, i.e., phantom artifacts; and then (ii) measures what portion of changes in the update, excluding the phantom artifacts, has passed through a code review process, i.e., code review coverage. Using Depdive, we present an empirical study across the latest ten updates of the most downloaded 1000 packages in each of the four registries. We further evaluated our results through a maintainer agreement survey. We find that phantom artifacts are not uncommon in the updates (20.1% of the analyzed updates had at least one phantom file). The phantoms can appear either due to legitimate reasons, such as in the case of programmatically generated files, or from accidental inclusion, such as in the case of files that are ignored in the repository. Regarding code review coverage (CRC), we find the updates are typically only partially code-reviewed (52.5% of the time). Further, only 9.0% of the packages had all their updates in our data set fully code-reviewed, indicating that even the most used packages can introduce non-reviewed code in the software supply chain. We also observe that updates either tend to have high CRC or low CRC, suggesting that packages at the opposite end of the spectrum may require a separate set of treatments.",Universalism,Social Justice,"The study aims to secure the software supply chain by facilitating received updates to undergo the code review process and not introducing non-reviewed code into the system. This approach aligns with the value item ""Social Justice"" (v10.8) under the value ""Universalim"" as it seeks for fair and just distribution; access and utilization of technology and resources.","In the context of software development and the goal of securing the software supply chain, aligning with the value item of Social Justice (v10.8) and its corresponding value of Universalism is justified. By ensuring that updates undergo a proper code review process and not introducing non-reviewed code, the study promotes fairness and justice in the distribution, access, and utilization of technology and resources. This aligns with the idea of social justice, which advocates for equal opportunities and fair treatment for all individuals within a society or system. By prioritizing code review coverage and minimizing the introduction of vulnerabilities and malicious code, the study contributes to a more equitable and just software ecosystem, benefiting software users in their interactions with technology.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2531,TSE,Software Testing & QA,Prevent: An Unsupervised Approach to Predict Software Failures in Production,"This paper presents Prevent, a fully unsupervised approach to predict and localize failures in distributed enterprise applications. Software failures in production are unavoidable. Predicting failures and locating failing components online are the first steps to proactively manage faults in production. Many techniques predict failures from anomalous combinations of system metrics with supervised, weakly supervised, and semi-supervised learning models. Supervised approaches require large sets of labelled data not commonly available in large enterprise applications, and address failure types that can be either captured with predefined rules or observed while training supervised models. Prevent integrates the core ingredients of unsupervised approaches into a novel fully unsupervised approach to predict failures and localize failing resources. The results of experimenting with Prevent on a commercially-compliant distributed cloud system indicate that Prevent provides more stable, reliable and timely predictions than supervised learning approaches, without requiring the often impractical training with labeled data.",Security,Healthy,"The paper presents an approach ""Prevent"" that predicts and localizes failures in distributed enterprise applications; contributing towards the maintenance of a healthy software environment for users.","The paper's approach of predicting and localizing failures in distributed enterprise applications aligns with the value item ""Healthy"" and its corresponding value ""Security"" from a ""Software User"" perspective because it contributes to maintaining a secure and stable software environment for users, ensuring that their applications run smoothly without unexpected failures or disruptions.",Agreed-Justified,Disagree,reconcile with coder_2,Disagree,The justification is not related to value item of healthy,
Exp B,Exp H,2531,TSE,Software Testing & QA,Prevent: An Unsupervised Approach to Predict Software Failures in Production,"This paper presents Prevent, a fully unsupervised approach to predict and localize failures in distributed enterprise applications. Software failures in production are unavoidable. Predicting failures and locating failing components online are the first steps to proactively manage faults in production. Many techniques predict failures from anomalous combinations of system metrics with supervised, weakly supervised, and semi-supervised learning models. Supervised approaches require large sets of labelled data not commonly available in large enterprise applications, and address failure types that can be either captured with predefined rules or observed while training supervised models. Prevent integrates the core ingredients of unsupervised approaches into a novel fully unsupervised approach to predict failures and localize failing resources. The results of experimenting with Prevent on a commercially-compliant distributed cloud system indicate that Prevent provides more stable, reliable and timely predictions than supervised learning approaches, without requiring the often impractical training with labeled data.",Achievement,Intelligent,"By implementing a fully unsupervised method; the ""Prevent"" approach harnesses the intelligence of an automated system and helps users successfully manage software faults.","Intelligent is aligned with the main contributions of 'Paper X' because the ""Prevent"" approach utilizes an automated system that is capable of predicting and localizing failures in distributed enterprise applications without requiring large sets of labeled data. This demonstrates the intelligence of the system in proactively managing faults in production. The achievement value is also aligned as the approach provides more stable, reliable, and timely predictions compared to supervised learning approaches, highlighting the successful management of software faults. From a ""Software User"" perspective, this alignment signifies the accomplishment of effectively addressing and mitigating software failures through intelligent and achievement-oriented methods.",Agreed-Justified,Agree,,Agree,,
Exp C,Exp K,2531,TSE,Software Testing & QA,Prevent: An Unsupervised Approach to Predict Software Failures in Production,"This paper presents Prevent, a fully unsupervised approach to predict and localize failures in distributed enterprise applications. Software failures in production are unavoidable. Predicting failures and locating failing components online are the first steps to proactively manage faults in production. Many techniques predict failures from anomalous combinations of system metrics with supervised, weakly supervised, and semi-supervised learning models. Supervised approaches require large sets of labelled data not commonly available in large enterprise applications, and address failure types that can be either captured with predefined rules or observed while training supervised models. Prevent integrates the core ingredients of unsupervised approaches into a novel fully unsupervised approach to predict failures and localize failing resources. The results of experimenting with Prevent on a commercially-compliant distributed cloud system indicate that Prevent provides more stable, reliable and timely predictions than supervised learning approaches, without requiring the often impractical training with labeled data.",Security,Social Order,The paper's approach helps to maintain the social order within the software environment; supporting the localized identification of failing components; which contributes to operational structure and system stability.,"The alignment of 'Paper X' with the value item Social Order and its corresponding value Security is justified based on the paper's contribution to maintaining a stable and reliable software environment through the localized identification of failing components. This allows for proactive fault management, which in turn contributes to the overall operational structure and system stability. By ensuring the smooth functioning of the software system, the paper indirectly supports the establishment and maintenance of social order within the software context, thereby aligning with the value item Social Order and the value of Security.",Agreed-Clarified,Agree,,Agree,,
Exp D,Exp K,2533,TSE,AI & Machine Learning,Studying the Influence and Distribution of the Human Effort in a Hybrid Fitness Function for Search-Based Model-Driven Engineering,"Search-Based Software Engineering (SBSE) offers solutions that efficiently explore large complex problem spaces. To obtain more favorable solutions, human participation in the search process is needed. However, humans cannot handle the same number of solutions as an algorithm. We propose the first hybrid fitness function that combines human effort with human simulations. Human effort refers to human participation for providing evaluations of candidate solutions during the search process, whereas human simulations refer to recreations of a scenario in a specific situation for automatically obtaining the evaluation of candidate solutions. We also propose three variants for the hybrid fitness function that vary in the distribution of human effort in order to study whether the variants influence the performance in terms of solution quality. Specifically, we leverage our hybrid fitness function to locate bugs in software models for the video games of game software engineering. Video games are a fertile domain for these hybrid functions because simulated players are naturally developed as part of the video games (e.g., bots in First-Person Shooters). Our evaluation is at the scale of industrial settings with a commercial video game (Play Station 4 and Steam) and 29 professional video game developers. Hybridizing the fitness function outperforms the results of the best baseline by 33.46% in F-measure. A focus group confirms the acceptance of the hybrid fitness function. Hybridizing the fitness function significantly improves the bug localization process by reducing the amount of tedious manual work and by minimizing the number of bugs that go unnoticed. Furthermore, the variant that obtains the best results is a counter-intuitive result that was under the radar of the interactive SBSE community. These results can help not only video game developers to locate bugs, but they can also inspire SBSE researchers to bring hybrid fitness functions to other software engineering tasks.",Achievement,Successful,The paper proposes a hybrid fitness function and three variants of it that significantly improve the bug localization process in video games; effectively contributing to the Software User's successful use of the video game by reducing the number of unnoticed bugs.,"The justification for labeling 'Paper X' as aligning with the value item Successful and its corresponding value Achievement from a ""Software User"" perspective is based on the fact that the paper introduces a hybrid fitness function that improves the bug localization process in video games. This enhancement directly benefits the Software User by reducing the number of unnoticed bugs, thereby ensuring a more successful and enjoyable experience while using the video game.",Agreed-Justified,Agree,,Agree,,
Exp E,Exp J,2533,TSE,AI & Machine Learning,Studying the Influence and Distribution of the Human Effort in a Hybrid Fitness Function for Search-Based Model-Driven Engineering,"Search-Based Software Engineering (SBSE) offers solutions that efficiently explore large complex problem spaces. To obtain more favorable solutions, human participation in the search process is needed. However, humans cannot handle the same number of solutions as an algorithm. We propose the first hybrid fitness function that combines human effort with human simulations. Human effort refers to human participation for providing evaluations of candidate solutions during the search process, whereas human simulations refer to recreations of a scenario in a specific situation for automatically obtaining the evaluation of candidate solutions. We also propose three variants for the hybrid fitness function that vary in the distribution of human effort in order to study whether the variants influence the performance in terms of solution quality. Specifically, we leverage our hybrid fitness function to locate bugs in software models for the video games of game software engineering. Video games are a fertile domain for these hybrid functions because simulated players are naturally developed as part of the video games (e.g., bots in First-Person Shooters). Our evaluation is at the scale of industrial settings with a commercial video game (Play Station 4 and Steam) and 29 professional video game developers. Hybridizing the fitness function outperforms the results of the best baseline by 33.46% in F-measure. A focus group confirms the acceptance of the hybrid fitness function. Hybridizing the fitness function significantly improves the bug localization process by reducing the amount of tedious manual work and by minimizing the number of bugs that go unnoticed. Furthermore, the variant that obtains the best results is a counter-intuitive result that was under the radar of the interactive SBSE community. These results can help not only video game developers to locate bugs, but they can also inspire SBSE researchers to bring hybrid fitness functions to other software engineering tasks.",Power,Social Recognition,The paper involves the use of simulated players or bots in video games that might contribute to the Software User's social recognition; especially in multi-player video games where displaying competence is often publicly acknowledged.,"In the paper, it is mentioned that video games, particularly those with simulated players or bots, are a fertile domain for the hybrid fitness function proposed. These simulated players can be seen as a form of social recognition for the software user within the context of multi-player video games. In such games, the display of competence through interactions with these simulated players could lead to the Software User's social recognition, as competence is often publicly acknowledged in these environments. Therefore, the alignment with the value item Social Recognition and its corresponding value Power is supported by the direct mention of the use of simulated players in video games.",Agreed-Justified,Agree,,Agree,,
Exp F,Exp J,2533,TSE,AI & Machine Learning,Studying the Influence and Distribution of the Human Effort in a Hybrid Fitness Function for Search-Based Model-Driven Engineering,"Search-Based Software Engineering (SBSE) offers solutions that efficiently explore large complex problem spaces. To obtain more favorable solutions, human participation in the search process is needed. However, humans cannot handle the same number of solutions as an algorithm. We propose the first hybrid fitness function that combines human effort with human simulations. Human effort refers to human participation for providing evaluations of candidate solutions during the search process, whereas human simulations refer to recreations of a scenario in a specific situation for automatically obtaining the evaluation of candidate solutions. We also propose three variants for the hybrid fitness function that vary in the distribution of human effort in order to study whether the variants influence the performance in terms of solution quality. Specifically, we leverage our hybrid fitness function to locate bugs in software models for the video games of game software engineering. Video games are a fertile domain for these hybrid functions because simulated players are naturally developed as part of the video games (e.g., bots in First-Person Shooters). Our evaluation is at the scale of industrial settings with a commercial video game (Play Station 4 and Steam) and 29 professional video game developers. Hybridizing the fitness function outperforms the results of the best baseline by 33.46% in F-measure. A focus group confirms the acceptance of the hybrid fitness function. Hybridizing the fitness function significantly improves the bug localization process by reducing the amount of tedious manual work and by minimizing the number of bugs that go unnoticed. Furthermore, the variant that obtains the best results is a counter-intuitive result that was under the radar of the interactive SBSE community. These results can help not only video game developers to locate bugs, but they can also inspire SBSE researchers to bring hybrid fitness functions to other software engineering tasks.",Security,Healthy,The paper's contribution toward reducing bugs in video games can support the Software User's health in a sense that it may minimize potential frustrations or stress due to software glitches; thus indirectly contribute to their mental well-being.,"The paper's focus on bug localization in video games aligns with the value item Healthy and its corresponding value Security from a ""Software User"" perspective because by reducing bugs, it improves the overall experience of using the software. This improvement can contribute to the user's mental well-being by minimizing frustrations, stress, and potential negative emotions that may arise from encountering software glitches. Therefore, the paper's contribution indirectly promotes a sense of security and mental health for the software users.",Agreed-Clarified,Agree,,Agree,,
Exp G,Exp K,2537,TSE,Security & Privacy,PatchDiscovery: Patch Presence Test for Identifying Binary Vulnerabilities Based on Key Basic Blocks,"Software vulnerabilities are easily propagated through code reuses, which pose dire threats to software system security. Automatic patch presence test offers an effective way to detect whether vulnerabilities have been patched, which is significant for large-scale software system maintenance. However, most existing approaches cannot handle binary codes. They suffer from low accuracy and poor efficiency. None of them are resilient to version gap, function size, and patch size. To tackle the above problems, we propose PatchDiscovery, a patch presence test approach to identify binary vulnerabilities by extracting key basic blocks of patch and vulnerability as their signatures for patch discovery. We propose an efficient and accurate basic block matching method over the normalized and simplified control flow graphs (CFGs) of a vulnerable function (VF) and its patched function (PF) to precisely locate a vulnerability and a patch. Then, we conduct fine-grained patch-level analysis on the patch and the vulnerability to gain their key basic blocks as the signatures of PF and VF for patch presence test. Concretely, the key basic blocks of PF and VF are separately searched in a target function (TF) to identify whether the TF is more similar to PF or VF, i.e., patched or not. Extensive experiments based on two real-world binary datasets that contain 524 common vulnerabilities and exposures (CVEs) with 11607 target functions reveal that PatchDiscovery is very effective and efficient. It achieves $92.2\%$92.2% F-measure and takes only 0.091s on average to test a target function. It is also resilient to version gap, patch size, and function size to a good extent. Moreover, it is outperforming the state-of-the-art works and has a much faster testing speed for large-scale patch detection. Moreover, PatchDiscovery achieves good performance in firmware vulnerability discovery scenario.",Achievement,Intelligent,The paper creates an intelligent system to automatically test if vulnerabilities have been patched; aligning with the value item 'Intelligent' from the value of 'Achievement'.,"I apologize for the lack of clarity in my justification. The paper aligns with the value item 'Intelligent' from the value of 'Achievement' because it proposes an intelligent system, PatchDiscovery, that utilizes efficient and accurate methods to detect patched vulnerabilities in binary code. By leveraging key basic blocks as signatures for patch discovery and conducting fine-grained patch-level analysis, the system achieves a high level of intelligence in identifying patched or vulnerable target functions. This contribution demonstrates achievement in developing an intelligent solution for software system security maintenance.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2537,TSE,Security & Privacy,PatchDiscovery: Patch Presence Test for Identifying Binary Vulnerabilities Based on Key Basic Blocks,"Software vulnerabilities are easily propagated through code reuses, which pose dire threats to software system security. Automatic patch presence test offers an effective way to detect whether vulnerabilities have been patched, which is significant for large-scale software system maintenance. However, most existing approaches cannot handle binary codes. They suffer from low accuracy and poor efficiency. None of them are resilient to version gap, function size, and patch size. To tackle the above problems, we propose PatchDiscovery, a patch presence test approach to identify binary vulnerabilities by extracting key basic blocks of patch and vulnerability as their signatures for patch discovery. We propose an efficient and accurate basic block matching method over the normalized and simplified control flow graphs (CFGs) of a vulnerable function (VF) and its patched function (PF) to precisely locate a vulnerability and a patch. Then, we conduct fine-grained patch-level analysis on the patch and the vulnerability to gain their key basic blocks as the signatures of PF and VF for patch presence test. Concretely, the key basic blocks of PF and VF are separately searched in a target function (TF) to identify whether the TF is more similar to PF or VF, i.e., patched or not. Extensive experiments based on two real-world binary datasets that contain 524 common vulnerabilities and exposures (CVEs) with 11607 target functions reveal that PatchDiscovery is very effective and efficient. It achieves $92.2\%$92.2% F-measure and takes only 0.091s on average to test a target function. It is also resilient to version gap, patch size, and function size to a good extent. Moreover, it is outperforming the state-of-the-art works and has a much faster testing speed for large-scale patch detection. Moreover, PatchDiscovery achieves good performance in firmware vulnerability discovery scenario.",Security,Healthy,The paper contributes to the well-being of the software by reducing vulnerabilities; indirectly maintaining user health by minimizing harmful cyber attacks; aligning with the value item 'Healthy' from the value of 'Security'.,"The paper's contribution to reducing software vulnerabilities aligns with the value item of ""Healthy"" in Schwartz's Taxonomy because it indirectly helps maintain the well-being of the software system. By minimizing harmful cyber attacks through the patch presence test approach, the paper aims to enhance the security of the software system, ensuring its stability and reliability. This aligns with the value of ""Security,"" which is an essential aspect for software users to feel confident in the system's ability to protect their data and privacy.",Agreed-Justified,Disagree,reconcile with coder_2,Disagree,The justification is not related to value item of healthy,
Exp B,Exp H,2537,TSE,Security & Privacy,PatchDiscovery: Patch Presence Test for Identifying Binary Vulnerabilities Based on Key Basic Blocks,"Software vulnerabilities are easily propagated through code reuses, which pose dire threats to software system security. Automatic patch presence test offers an effective way to detect whether vulnerabilities have been patched, which is significant for large-scale software system maintenance. However, most existing approaches cannot handle binary codes. They suffer from low accuracy and poor efficiency. None of them are resilient to version gap, function size, and patch size. To tackle the above problems, we propose PatchDiscovery, a patch presence test approach to identify binary vulnerabilities by extracting key basic blocks of patch and vulnerability as their signatures for patch discovery. We propose an efficient and accurate basic block matching method over the normalized and simplified control flow graphs (CFGs) of a vulnerable function (VF) and its patched function (PF) to precisely locate a vulnerability and a patch. Then, we conduct fine-grained patch-level analysis on the patch and the vulnerability to gain their key basic blocks as the signatures of PF and VF for patch presence test. Concretely, the key basic blocks of PF and VF are separately searched in a target function (TF) to identify whether the TF is more similar to PF or VF, i.e., patched or not. Extensive experiments based on two real-world binary datasets that contain 524 common vulnerabilities and exposures (CVEs) with 11607 target functions reveal that PatchDiscovery is very effective and efficient. It achieves $92.2\%$92.2% F-measure and takes only 0.091s on average to test a target function. It is also resilient to version gap, patch size, and function size to a good extent. Moreover, it is outperforming the state-of-the-art works and has a much faster testing speed for large-scale patch detection. Moreover, PatchDiscovery achieves good performance in firmware vulnerability discovery scenario.",Security,National Security,The tool developed in the paper can detect vulnerabilities in large-scale software systems; which could be part of national infrastructure; aligning with the value item 'National Security' from the value 'Security'.,"I apologize for any confusion. The justification for aligning 'Paper X' with the value item National Security and its corresponding value Security is based on the fact that the paper presents a method to detect vulnerabilities in large-scale software systems. These vulnerabilities, if left unpatched, can pose security risks not only to individual software systems but also to national infrastructure. By addressing the issue of software vulnerabilities, the paper contributes to enhancing the security of software systems, which can indirectly contribute to national security as well.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2627,ICSE-SEIP,Code Generation & Analysis,Analyzing the CMake Build System,"CMake is one of the most widely used build automation tools in the industry. Facebook engineers often rely on examining large and complex CMake build files for various program analyses tasks. In this paper, we report on some of the unique challenges when analyzing CMake files at Facebook.",Achievement,Capable,The paper discusses the challenges of analyzing large and complex CMake build files. By addressing these challenges; software users can demonstrate their competence and intelligence in effectively utilizing CMake as a build automation tool. This aligns with the value item Capable and its corresponding value Achievement.,"In the paper abstract, it is stated that Facebook engineers rely on examining large and complex CMake build files for program analysis tasks. By addressing the challenges in analyzing these files, software users can demonstrate their competence and intelligence in effectively utilizing CMake. This alignment reflects the value item v4.2 ""Capable"" and its corresponding value v4 ""Achievement"" as software users are able to achieve successful outcomes and showcase their skills in utilizing CMake for software development tasks.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2631,ICSE-SEIP,Software Engineering Practices,An Industrial Experience Report on Retro-Inspection,"To reinforce the quality of code delivery, especially to improve future coding quality, one global Information and Communication Technology (ICT) enterprise has institutionalized a retrospective style inspection (namely retro-inspection), which is similar to Fagan inspection but differs in terms of stage, participants, etc. This paper reports an industrial case study that aims to investigate the experiences and lessons from this software practice. To this end, we collected and analyzed various empirical evidence for data triangulation. The results reflect that retro-inspection distinguishes itself from peer code review by identifying more complicated and underlying defects, providing more indicative and suggestive comments. Many experienced inspectors indicate defects together with their rationale behind and offer suggestions for correction and prevention. As a result, retro-inspection can benefit not only quality assurance (like Fagan inspection), but also internal audit, interdivision communication, and competence promotion. On the other side, we identify several lessons of retro-inspection at this stage, e.g., developersaEUR(tm) acceptance and organizersaEUR(tm) predicament, for next-step improvement of this practice. To be specific, some recommendations are discussed for retro-inspection, e.g., more adequate preparation and more careful publicity. This study concludes that most of the expected benefits of retro-inspection can be empirically confirmed in this enterprise and its value on the progress to continuous maturity can be recognized organization-wide. The experiences on executing this altered practice in a large enterprise provide reference value on code quality assurance to other software organizations.",Achievement,Successful,The paper reports on an industrial case study that aims to investigate the experiences and lessons from the retro-inspection software practice. This aligns with the value item Successful and its corresponding value Achievement.,"The alignment of 'Paper X' with the value item v4.3 (Successful) from a ""Software User"" perspective is justified based on the fact that the industrial case study reported in the paper aims to investigate the experiences and lessons from the retro-inspection software practice. By conducting this study and analyzing empirical evidence, the paper contributes to the improvement of code delivery quality, which is directly related to the recognition of successful outcomes in software development. Therefore, the main contribution of 'Paper X' aligns with the value of Achievement, as it strives to improve the success and effectiveness of software practices.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2631,ICSE-SEIP,Software Engineering Practices,An Industrial Experience Report on Retro-Inspection,"To reinforce the quality of code delivery, especially to improve future coding quality, one global Information and Communication Technology (ICT) enterprise has institutionalized a retrospective style inspection (namely retro-inspection), which is similar to Fagan inspection but differs in terms of stage, participants, etc. This paper reports an industrial case study that aims to investigate the experiences and lessons from this software practice. To this end, we collected and analyzed various empirical evidence for data triangulation. The results reflect that retro-inspection distinguishes itself from peer code review by identifying more complicated and underlying defects, providing more indicative and suggestive comments. Many experienced inspectors indicate defects together with their rationale behind and offer suggestions for correction and prevention. As a result, retro-inspection can benefit not only quality assurance (like Fagan inspection), but also internal audit, interdivision communication, and competence promotion. On the other side, we identify several lessons of retro-inspection at this stage, e.g., developersaEUR(tm) acceptance and organizersaEUR(tm) predicament, for next-step improvement of this practice. To be specific, some recommendations are discussed for retro-inspection, e.g., more adequate preparation and more careful publicity. This study concludes that most of the expected benefits of retro-inspection can be empirically confirmed in this enterprise and its value on the progress to continuous maturity can be recognized organization-wide. The experiences on executing this altered practice in a large enterprise provide reference value on code quality assurance to other software organizations.",Benevolence,Helpful,The paper reveals that retro-inspection can benefit not only quality assurance; but also internal audit; interdivision communication; and competence promotion. This aligns with the value item Helpful and its corresponding value Benevolence.,"The alignment of the paper's contributions with the value item v9.3 (Helpful) and its corresponding value v9 (Benevolence) is justified because retro-inspection, as described in the paper, contributes to the improvement of code quality and overall software development process. By identifying and addressing more complicated and underlying defects, retro-inspection provides helpful and suggestive comments that assist in the correction and prevention of these issues. This aligns with the value of being helpful, as it pertains to promoting a positive and supportive environment within a software organization, which ultimately benefits software users by ensuring higher quality and reliable software products.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2631,ICSE-SEIP,Software Engineering Practices,An Industrial Experience Report on Retro-Inspection,"To reinforce the quality of code delivery, especially to improve future coding quality, one global Information and Communication Technology (ICT) enterprise has institutionalized a retrospective style inspection (namely retro-inspection), which is similar to Fagan inspection but differs in terms of stage, participants, etc. This paper reports an industrial case study that aims to investigate the experiences and lessons from this software practice. To this end, we collected and analyzed various empirical evidence for data triangulation. The results reflect that retro-inspection distinguishes itself from peer code review by identifying more complicated and underlying defects, providing more indicative and suggestive comments. Many experienced inspectors indicate defects together with their rationale behind and offer suggestions for correction and prevention. As a result, retro-inspection can benefit not only quality assurance (like Fagan inspection), but also internal audit, interdivision communication, and competence promotion. On the other side, we identify several lessons of retro-inspection at this stage, e.g., developersaEUR(tm) acceptance and organizersaEUR(tm) predicament, for next-step improvement of this practice. To be specific, some recommendations are discussed for retro-inspection, e.g., more adequate preparation and more careful publicity. This study concludes that most of the expected benefits of retro-inspection can be empirically confirmed in this enterprise and its value on the progress to continuous maturity can be recognized organization-wide. The experiences on executing this altered practice in a large enterprise provide reference value on code quality assurance to other software organizations.",Stimulation,Variation in Life,The paper highlights that retro-inspection identifies more complicated and underlying defects; providing more indicative and suggestive comments. This aligns with the value item Variation in Life and its corresponding value Stimulation.,"The alignment of 'Paper X' with the value item Variation in Life (v2.2) and its corresponding value Stimulation (v2) from a ""Software User"" perspective is justified by the paper's mention that retro-inspection identifies more complicated and underlying defects, providing more indicative and suggestive comments. This implies a more stimulating and diverse software development process, as it involves uncovering and addressing complex issues, which aligns with the value of seeking stimulation and variation in life.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2632,ICSE-SEIP,Code Generation & Analysis,Automated Labeling and Classification of Business Rules from Software Requirement Specifications,"Business Rules (BRs) are a critical artifact in the requirements elicitation phase of the software development life cycle. Several taxonomies have been proposed for classification of BRs. In this paper, we utilize Ross&#x27;s BR classification schema and present an approach to automatically label and classify BRs along this schema. Our approach uses Data Programming (DP) for generating labeled training data needed for training two deep learning-based models to classify the BRs. We obtained an average labeling accuracy of 0.73 for all the BR classes using DP. Upon evaluating the approach on industryspecific dataset, we obtained highest weighted F-score (0.69) with a Bi-LSTM with attention-based model.",Achievement,Successful,The paper presents an approach to automatically label and classify Business Rules (BRs) in software development; which demonstrates a focus on competence and striving for success in accurately classifying BRs.,"In 'Paper X', the authors propose an approach to automatically label and classify Business Rules (BRs) in software development. This directly aligns with the value item v4.3 and its corresponding value v4 from a ""Software User"" perspective because accurately classifying BRs demonstrates competence and success in understanding and organizing the rules that govern the software, ultimately leading to better software development outcomes.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2632,ICSE-SEIP,Code Generation & Analysis,Automated Labeling and Classification of Business Rules from Software Requirement Specifications,"Business Rules (BRs) are a critical artifact in the requirements elicitation phase of the software development life cycle. Several taxonomies have been proposed for classification of BRs. In this paper, we utilize Ross&#x27;s BR classification schema and present an approach to automatically label and classify BRs along this schema. Our approach uses Data Programming (DP) for generating labeled training data needed for training two deep learning-based models to classify the BRs. We obtained an average labeling accuracy of 0.73 for all the BR classes using DP. Upon evaluating the approach on industryspecific dataset, we obtained highest weighted F-score (0.69) with a Bi-LSTM with attention-based model.",Benevolence,Helpful,The paper utilizes Data Programming (DP) to generate labeled training data for training deep learning-based models to classify BRs; emphasizing the helpfulness of the software in assisting users with the classification process.,"In the paper, the authors propose an approach to automatically label and classify Business Rules (BRs) in the software development life cycle. They utilize Data Programming (DP) and deep learning-based models to assist users in the classification process, which aligns with the value item v9.3 of Schwartz's Taxonomy (""Helpful""). This alignment indicates that the main contribution of the paper is in providing a software solution that is beneficial and supportive to software users by assisting them with the classification of BRs, ultimately enhancing their efficiency and effectiveness in the requirements elicitation phase.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2632,ICSE-SEIP,Code Generation & Analysis,Automated Labeling and Classification of Business Rules from Software Requirement Specifications,"Business Rules (BRs) are a critical artifact in the requirements elicitation phase of the software development life cycle. Several taxonomies have been proposed for classification of BRs. In this paper, we utilize Ross&#x27;s BR classification schema and present an approach to automatically label and classify BRs along this schema. Our approach uses Data Programming (DP) for generating labeled training data needed for training two deep learning-based models to classify the BRs. We obtained an average labeling accuracy of 0.73 for all the BR classes using DP. Upon evaluating the approach on industryspecific dataset, we obtained highest weighted F-score (0.69) with a Bi-LSTM with attention-based model.",Power,Wealth,The paper evaluates the approach on an industry-specific dataset and achieves the highest weighted F-score; indicating a focus on recognition and success in the software's performance and effectiveness.,"In the paper abstract, it is mentioned that the approach is evaluated on an industry-specific dataset and achieves the highest weighted F-score. This demonstrates a clear focus on the software's performance and effectiveness, aligning with the value item v5.2 (Wealth) and its corresponding value v5 (Power) from a ""Software User"" perspective. This suggests that the main contribution of 'Paper X' is centered around achieving recognition and success in the software context, which is in line with the value of power and the desire for social recognition and authority.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2634,ICSE-SEIP,Code Generation & Analysis,Automatically Identifying Shared Root Causes of Test Breakages in SAP HANA,"Continuous Integration (CI) of a largescale software system such as SAP HANA can produce a non-trivial number of test breakages. Each breakage that newly occurs from daily runs needs to be manually inspected, triaged, and eventually assigned to developers for debugging. However, not all new breakages are unique, as some test breakages would share the same root cause; in addition, human errors can produce duplicate bug tickets for the same root cause. An automated identification of breakages with shared root causes will be able to significantly reduce the cost of the (typically manual) post-breakage steps. This paper investigates multiple similarity functions between test breakages to assist and automate the identification of test breakages that are caused by the same root cause. We consider multiple information sources, such as static (i.e., the code itself), historical (i.e., whether the test results have changed in a similar way in the past), as well as dynamic (i.e., whether the coverage of test cases are similar to each other), for the purpose of such automation. We evaluate a total of 27 individual similarity functions, using realworld CI data of SAP HANA from a six-month period. Further, using these individual similarity functions as in-put features, we construct a classification model that can predict whether two test breakages share the same root cause or not. When trained using ground truth labels extracted from the issue tracker of SAP HANA, our model achieves an F1 score of 0.743 when evaluated using a set of unseen test breakages collected over three months. Our results show that a classification model based on test similarity functions can successfully support the bug triage stage of a CI pipeline.",Power,Authority,The paper contributes to the automation of bug triage in a CI pipeline. This aligns with the value item Authority and its corresponding value Power. By automating the identification of test breakages with shared root causes; the software empowers the software users to take charge and make decisions; reducing the need for manual intervention and oversight.,"The justification provided aligns with the value item Authority (v5.3) and its corresponding value Power (v5) from a ""Software User"" perspective because the automation of bug triage in the CI pipeline empowers software users to have control and make decisions. This is achieved by reducing the manual intervention and oversight needed in identifying test breakages with shared root causes. Thus, the software provides a sense of authority and power to the software users by enabling them to take charge of the bug triage process.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2634,ICSE-SEIP,Code Generation & Analysis,Automatically Identifying Shared Root Causes of Test Breakages in SAP HANA,"Continuous Integration (CI) of a largescale software system such as SAP HANA can produce a non-trivial number of test breakages. Each breakage that newly occurs from daily runs needs to be manually inspected, triaged, and eventually assigned to developers for debugging. However, not all new breakages are unique, as some test breakages would share the same root cause; in addition, human errors can produce duplicate bug tickets for the same root cause. An automated identification of breakages with shared root causes will be able to significantly reduce the cost of the (typically manual) post-breakage steps. This paper investigates multiple similarity functions between test breakages to assist and automate the identification of test breakages that are caused by the same root cause. We consider multiple information sources, such as static (i.e., the code itself), historical (i.e., whether the test results have changed in a similar way in the past), as well as dynamic (i.e., whether the coverage of test cases are similar to each other), for the purpose of such automation. We evaluate a total of 27 individual similarity functions, using realworld CI data of SAP HANA from a six-month period. Further, using these individual similarity functions as in-put features, we construct a classification model that can predict whether two test breakages share the same root cause or not. When trained using ground truth labels extracted from the issue tracker of SAP HANA, our model achieves an F1 score of 0.743 when evaluated using a set of unseen test breakages collected over three months. Our results show that a classification model based on test similarity functions can successfully support the bug triage stage of a CI pipeline.",Security,Social Order,The paper contributes to the improvement of social order in a CI pipeline. This aligns with the value item Social Order and its corresponding value Security. By automating the identification of test breakages with shared root causes; the software helps maintain the order and efficiency of the bug triage process; reducing the potential chaos and confusion caused by duplicate bug tickets.,"The paper aligns with the value item v6.3 and its corresponding value v6 from a ""Software User"" perspective by contributing to the improvement of social order in a CI pipeline. Through the automation of identifying test breakages with shared root causes, the software ensures that the bug triage process is efficient and organized, reducing the potential chaos and confusion caused by duplicate bug tickets. This fosters a sense of security for software users as it ensures that issues are properly addressed and resolved in an orderly manner, promoting a stable and reliable software environment.",Agreed-Justified,Disagree,"Related to Achievement: capable more, as it automatically identifies leakages",Disagree,The justification is not related to social order,
Exp A,Exp H,2634,ICSE-SEIP,Code Generation & Analysis,Automatically Identifying Shared Root Causes of Test Breakages in SAP HANA,"Continuous Integration (CI) of a largescale software system such as SAP HANA can produce a non-trivial number of test breakages. Each breakage that newly occurs from daily runs needs to be manually inspected, triaged, and eventually assigned to developers for debugging. However, not all new breakages are unique, as some test breakages would share the same root cause; in addition, human errors can produce duplicate bug tickets for the same root cause. An automated identification of breakages with shared root causes will be able to significantly reduce the cost of the (typically manual) post-breakage steps. This paper investigates multiple similarity functions between test breakages to assist and automate the identification of test breakages that are caused by the same root cause. We consider multiple information sources, such as static (i.e., the code itself), historical (i.e., whether the test results have changed in a similar way in the past), as well as dynamic (i.e., whether the coverage of test cases are similar to each other), for the purpose of such automation. We evaluate a total of 27 individual similarity functions, using realworld CI data of SAP HANA from a six-month period. Further, using these individual similarity functions as in-put features, we construct a classification model that can predict whether two test breakages share the same root cause or not. When trained using ground truth labels extracted from the issue tracker of SAP HANA, our model achieves an F1 score of 0.743 when evaluated using a set of unseen test breakages collected over three months. Our results show that a classification model based on test similarity functions can successfully support the bug triage stage of a CI pipeline.",Benevolence,Responsibility,The paper contributes to the cultivation of responsibility in software users. This aligns with the value item Responsibility and its corresponding value Benevolence. By providing a classification model that can predict whether two test breakages share the same root cause; the software enables software users to take responsibility for the identification and resolution of bugs; fostering a sense of duty and care towards the software development process.,"In the context of the ""Software User"", 'Paper X' aligns with the value item v9.8 (Responsibility) and its corresponding value v9 (Benevolence) by providing a classification model that allows software users to take responsibility for the identification and resolution of bugs. This contribution fosters a sense of duty and care towards the software development process as users are empowered to actively participate in bug triage and contribute to the overall improvement of the software's quality. By automating the identification of test breakages with shared root causes, the software enables software users to play a responsible role in resolving issues, promoting a greater sense of responsibility and benevolence in their engagement with the software.",Agreed-Clarified,Agree,,Agree,,
Exp A,Exp H,2634,ICSE-SEIP,Code Generation & Analysis,Automatically Identifying Shared Root Causes of Test Breakages in SAP HANA,"Continuous Integration (CI) of a largescale software system such as SAP HANA can produce a non-trivial number of test breakages. Each breakage that newly occurs from daily runs needs to be manually inspected, triaged, and eventually assigned to developers for debugging. However, not all new breakages are unique, as some test breakages would share the same root cause; in addition, human errors can produce duplicate bug tickets for the same root cause. An automated identification of breakages with shared root causes will be able to significantly reduce the cost of the (typically manual) post-breakage steps. This paper investigates multiple similarity functions between test breakages to assist and automate the identification of test breakages that are caused by the same root cause. We consider multiple information sources, such as static (i.e., the code itself), historical (i.e., whether the test results have changed in a similar way in the past), as well as dynamic (i.e., whether the coverage of test cases are similar to each other), for the purpose of such automation. We evaluate a total of 27 individual similarity functions, using realworld CI data of SAP HANA from a six-month period. Further, using these individual similarity functions as in-put features, we construct a classification model that can predict whether two test breakages share the same root cause or not. When trained using ground truth labels extracted from the issue tracker of SAP HANA, our model achieves an F1 score of 0.743 when evaluated using a set of unseen test breakages collected over three months. Our results show that a classification model based on test similarity functions can successfully support the bug triage stage of a CI pipeline.",Self Direction,Privacy,"This value-item can be inferred by one of the key phrases ""An automated identification of breakages with shared root causes will be able to significantly reduce the cost of the (typically manual) post-breakage steps"" and also ""Our results show that a classification model based on test similarity functions can successfully support the bug triage stage of a CI pipeline."" By providing an automated mechanism for identifying shared root causes of test breakages; the paper aims to reduce the manual effort required; allowing software users to have more privacy in handling the triage process.","The value item v1.6, Privacy, can be inferred from the phrase ""An automated identification of breakages with shared root causes will be able to significantly reduce the cost of the (typically manual) post-breakage steps."" This implies that by automating the identification of shared root causes in test breakages, the need for manual inspection and triage is reduced. This, in turn, provides software users with more privacy in handling the triage process by minimizing the need for human intervention and potentially exposing sensitive information.",Agreed-Justified,Disagree,reconcile with coder_2,Disagree,The justification claims that the paper aligns with the value item of privacy but  the paper aims to reduce the manual effort,
Exp A,Exp H,2635,ICSE-SEIP,Code Generation & Analysis,Automatic Anti-Pattern Detection in Microservice Architectures Based on Distributed Tracing,The successful use of microservice-based applications by large companies has popularized this architectural style. One problem with the microservice architecture is that current techniques for visualising- and detecting anti-pattern are inadequate. This study contributes a method and tool for detecting anti-patterns in microservice architecture based on distributed execution traces. We demonstrate this on an industrial case study.,Achievement,Successful,The paper contributes a method and tool for detecting anti-patterns in microservice architecture. This aligns with the value item Successful and its corresponding value Achievement.,"In the abstract of 'Paper X,' it is stated that the paper contributes a method and tool for detecting anti-patterns in microservice architecture. This directly aligns with the value item v4.3 (Successful) and its corresponding value v4 (Achievement) from the perspective of a software user. By providing a method and tool for detecting anti-patterns, it implies that the paper aims to enhance the success and achievement of utilizing microservice architecture by addressing the problem of inadequate techniques. This directly aligns with the value item of Successful and its corresponding value of Achievement as achieving success in software development often involves identifying and addressing potential issues and challenges in the architecture.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2635,ICSE-SEIP,Code Generation & Analysis,Automatic Anti-Pattern Detection in Microservice Architectures Based on Distributed Tracing,The successful use of microservice-based applications by large companies has popularized this architectural style. One problem with the microservice architecture is that current techniques for visualising- and detecting anti-pattern are inadequate. This study contributes a method and tool for detecting anti-patterns in microservice architecture based on distributed execution traces. We demonstrate this on an industrial case study.,Benevolence,Helpful,The paper contributes a method and tool for detecting anti-patterns in microservice architecture. This aligns with the value item Helpful and its corresponding value Benevolence.,"In 'Paper X', the contribution of a method and tool for detecting anti-patterns in microservice architecture aligns with the value item Helpful (v9.3) and its corresponding value Benevolence (v9) because it aims to address the challenges faced by software users (not developers) in identifying and resolving issues in their microservice-based applications. By providing a way to detect anti-patterns, the paper assists users in improving the performance and reliability of their software systems, ultimately helping them achieve their goals and fulfill their responsibilities in a more effective and beneficial manner.",Agreed-Clarified,Agree,,Agree,,
Exp A,Exp H,2638,ICSE-SEIP,Software Engineering Practices,Bus Factor in Practice,"Bus factor is a metric that identifies how resilient is the project to the sudden engineer turnover. It states the minimal number of engineers that have to be hit by a bus for a project to be stalled. Even though the metric is often discussed in the community, few studies consider its general relevance. Moreover, the existing tools for bus factor estimation focus solely on the data from version control systems, even though there exists other channels for knowledge generation and distribution. With a survey of 269 engineers, we find that the bus factor is perceived as an important problem in collective development, and determine the highest impact channels of knowledge generation and distribution in software development teams. We also propose a multimodal bus factor estimation algorithm that uses data on code reviews and meetings together with the VCS data. We test the algorithm on 13 projects developed at JetBrains and compared its results to the results of the state-of-the-art tool by Avelino et al. against the ground truth collected in a survey of the engineers working on these projects. Our algorithm is slightly better in terms of both predicting the bus factor as well as key developers compared to the results of Avelino et al. Finally, we use the interviews and the surveys to derive a set of best practices to address the bus factor issue and proposals for the possible bus factor assessment tool.",Achievement,Intelligent,The paper proposes a multimodal bus factor estimation algorithm that uses data on code reviews and meetings; which improves the software development teams' ability to identify and mitigate the risk of sudden engineer turnover. This aligns with the value item Intelligent and its corresponding value Achievement.,"The paper's proposal of a multimodal bus factor estimation algorithm, using data on code reviews and meetings, directly contributes to the achievement of the value item ""v4.1 Intelligent"" from Schwartz's Taxonomy. This is because the algorithm improves the ability of software development teams to identify and mitigate the risk of sudden engineer turnover, demonstrating intelligence in analyzing and addressing potential disruptions to project continuity. Software users, who rely on stable and reliable software products, would value this alignment as it ensures a higher level of quality and continuity in their software experience.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2638,ICSE-SEIP,Software Engineering Practices,Bus Factor in Practice,"Bus factor is a metric that identifies how resilient is the project to the sudden engineer turnover. It states the minimal number of engineers that have to be hit by a bus for a project to be stalled. Even though the metric is often discussed in the community, few studies consider its general relevance. Moreover, the existing tools for bus factor estimation focus solely on the data from version control systems, even though there exists other channels for knowledge generation and distribution. With a survey of 269 engineers, we find that the bus factor is perceived as an important problem in collective development, and determine the highest impact channels of knowledge generation and distribution in software development teams. We also propose a multimodal bus factor estimation algorithm that uses data on code reviews and meetings together with the VCS data. We test the algorithm on 13 projects developed at JetBrains and compared its results to the results of the state-of-the-art tool by Avelino et al. against the ground truth collected in a survey of the engineers working on these projects. Our algorithm is slightly better in terms of both predicting the bus factor as well as key developers compared to the results of Avelino et al. Finally, we use the interviews and the surveys to derive a set of best practices to address the bus factor issue and proposals for the possible bus factor assessment tool.",Power,Social Recognition,The paper addresses the bus factor issue; which measures the resilience of a project to sudden engineer turnover. By proposing a multimodal bus factor estimation algorithm and best practices to tackle this problem; the paper contributes to the software users' desire for social recognition and the preservation of their public image; aligning with the value item Social Recognition and its corresponding value Power.,"In the abstract of 'Paper X', the authors mention that they propose a multimodal bus factor estimation algorithm and best practices to address the bus factor issue. The bus factor, which measures the resilience of a project to sudden engineer turnover, is seen as an important problem in collective development. By developing solutions to this issue, the paper aligns with the value item v5.1 (Social Recognition) and its corresponding value v5 (Power) from a ""Software User"" perspective. Software users, who rely on stable and well-maintained software, value social recognition and the preservation of their public image, which can be achieved through addressing the bus factor problem and ensuring the continuous development and support of the software.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2638,ICSE-SEIP,Software Engineering Practices,Bus Factor in Practice,"Bus factor is a metric that identifies how resilient is the project to the sudden engineer turnover. It states the minimal number of engineers that have to be hit by a bus for a project to be stalled. Even though the metric is often discussed in the community, few studies consider its general relevance. Moreover, the existing tools for bus factor estimation focus solely on the data from version control systems, even though there exists other channels for knowledge generation and distribution. With a survey of 269 engineers, we find that the bus factor is perceived as an important problem in collective development, and determine the highest impact channels of knowledge generation and distribution in software development teams. We also propose a multimodal bus factor estimation algorithm that uses data on code reviews and meetings together with the VCS data. We test the algorithm on 13 projects developed at JetBrains and compared its results to the results of the state-of-the-art tool by Avelino et al. against the ground truth collected in a survey of the engineers working on these projects. Our algorithm is slightly better in terms of both predicting the bus factor as well as key developers compared to the results of Avelino et al. Finally, we use the interviews and the surveys to derive a set of best practices to address the bus factor issue and proposals for the possible bus factor assessment tool.",Universalism,Protecting the Environment,The paper acknowledges that there exist other channels for knowledge generation and distribution in software development teams besides version control systems. By proposing a multimodal bus factor estimation algorithm that incorporates data from code reviews; meetings; and version control systems; the paper aims to improve the understanding and management of the bus factor in collective software development. This aligns with the value item Protecting the Environment and its corresponding value Universalism.,"In the paper, the authors propose a multimodal bus factor estimation algorithm that incorporates data from code reviews, meetings, and version control systems to improve the understanding and management of the bus factor in collective software development. This aligns with the value item v10.4 ""Protecting the Environment"" because by considering multiple channels for knowledge generation and distribution, the algorithm enhances the resilience and sustainability of software development processes, which contributes to the overall goal of protecting the environment in the software context. This alignment is based on the direct evidence provided in the abstract regarding the paper's focus on different knowledge channels and their impact on the bus factor, making it a clear and relevant connection.",Agreed-Clarified,Agree,,Agree,,
Exp A,Exp H,2640,ICSE-SEIP,Code Generation & Analysis,Code Reviewer Recommendation in Tencent: Practice Challenge and Direction,"Code review is essential for assuring system quality in software engineering. Over decades in practice, code review has evolved to be a lightweight tool-based process focusing on code change: the smallest unit of the development cycle, and we refer to it as Modern Code Review (MCR). MCR involves code contributors committing code changes and code reviewers reviewing the assigned code changes. Such a reviewer assigning process is challenged by efficiently finding appropriate reviewers. Recent studies propose automated code reviewer recommendation (CRR) approaches to resolve such challenges. These approaches are often evaluated on open-source projects and obtain promising performance. However, the code reviewer recommendation systems are not widely used on proprietary projects, and most current reviewer selecting practice is still manual or, at best, semi-manual. No previous work systematically evaluated these approachesaEUR(tm) effectiveness and compared each other on proprietary projects in practice. In this paper, we performed a quantitative analysis of typical recommendation approaches on proprietary projects in Tencent. The results show an imperfect performance of these approaches on proprietary projects and reveal practical challenges like the aEURoecold start problemaEUR. To better understand practical challenges, we interviewed practitioners about the expectations of applying reviewer recommendations to a production environment. The interview involves the current systemsaEUR(tm) limitations, expected application scenario, and information requirements. Finally, we discuss the implications and the direction of practical code reviewer recommendation tools.",Stimulation,Excitement in Life,The paper contributes to the development of automated code reviewer recommendation approaches; which can bring excitement and stimulation to software users by improving the code review process and system quality in software engineering.,"I apologize for the confusion. The justification for aligning 'Paper X' with the value item v2.3 (Excitement in Life) and its corresponding value v2 (Stimulation) from a ""Software User"" perspective is that the development of automated code reviewer recommendation approaches can introduce excitement and stimulation for software users. By improving the code review process and system quality, users can experience the excitement of more efficient and effective code reviews, leading to improved software outcomes and a more engaging software development experience.",Agreed-Clarified,Agree,,Agree,,
Exp A,Exp H,2640,ICSE-SEIP,Code Generation & Analysis,Code Reviewer Recommendation in Tencent: Practice Challenge and Direction,"Code review is essential for assuring system quality in software engineering. Over decades in practice, code review has evolved to be a lightweight tool-based process focusing on code change: the smallest unit of the development cycle, and we refer to it as Modern Code Review (MCR). MCR involves code contributors committing code changes and code reviewers reviewing the assigned code changes. Such a reviewer assigning process is challenged by efficiently finding appropriate reviewers. Recent studies propose automated code reviewer recommendation (CRR) approaches to resolve such challenges. These approaches are often evaluated on open-source projects and obtain promising performance. However, the code reviewer recommendation systems are not widely used on proprietary projects, and most current reviewer selecting practice is still manual or, at best, semi-manual. No previous work systematically evaluated these approachesaEUR(tm) effectiveness and compared each other on proprietary projects in practice. In this paper, we performed a quantitative analysis of typical recommendation approaches on proprietary projects in Tencent. The results show an imperfect performance of these approaches on proprietary projects and reveal practical challenges like the aEURoecold start problemaEUR. To better understand practical challenges, we interviewed practitioners about the expectations of applying reviewer recommendations to a production environment. The interview involves the current systemsaEUR(tm) limitations, expected application scenario, and information requirements. Finally, we discuss the implications and the direction of practical code reviewer recommendation tools.",Achievement,Intelligent,The paper evaluates the effectiveness of different code reviewer recommendation approaches on proprietary projects; which aligns with the value item Intelligent and its corresponding value Achievement. The evaluation of these approaches demonstrates the software user's aspiration to excel and be acknowledged for their skills and accomplishments in the context of code review.,"In the context of code review, aligning with the value item Intelligent and its corresponding value Achievement implies the desire for software users to excel and be recognized for their skills and achievements. 'Paper X' directly aligns with these values through its evaluation of code reviewer recommendation approaches on proprietary projects. By analyzing the effectiveness of different approaches, the paper addresses the software user's aspiration to excel in code review and be acknowledged for their abilities and accomplishments, contributing to the value of Achievement.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2640,ICSE-SEIP,Code Generation & Analysis,Code Reviewer Recommendation in Tencent: Practice Challenge and Direction,"Code review is essential for assuring system quality in software engineering. Over decades in practice, code review has evolved to be a lightweight tool-based process focusing on code change: the smallest unit of the development cycle, and we refer to it as Modern Code Review (MCR). MCR involves code contributors committing code changes and code reviewers reviewing the assigned code changes. Such a reviewer assigning process is challenged by efficiently finding appropriate reviewers. Recent studies propose automated code reviewer recommendation (CRR) approaches to resolve such challenges. These approaches are often evaluated on open-source projects and obtain promising performance. However, the code reviewer recommendation systems are not widely used on proprietary projects, and most current reviewer selecting practice is still manual or, at best, semi-manual. No previous work systematically evaluated these approachesaEUR(tm) effectiveness and compared each other on proprietary projects in practice. In this paper, we performed a quantitative analysis of typical recommendation approaches on proprietary projects in Tencent. The results show an imperfect performance of these approaches on proprietary projects and reveal practical challenges like the aEURoecold start problemaEUR. To better understand practical challenges, we interviewed practitioners about the expectations of applying reviewer recommendations to a production environment. The interview involves the current systemsaEUR(tm) limitations, expected application scenario, and information requirements. Finally, we discuss the implications and the direction of practical code reviewer recommendation tools.",Conformity,Self-Discipline,The paper discusses the practical challenges of finding appropriate code reviewers and proposes automated code reviewer recommendation approaches to address this challenge. This aligns with the value item Self-Discipline and its corresponding value Conformity; as the paper aims to improve the discipline and adherence to best practices in the software development process.,"I apologize for any confusion in my previous justification. To further clarify, the alignment of 'Paper X' with the value item v8.1 (Self-Discipline) and its corresponding value v8 (Conformity) is based on the fact that the paper aims to improve the software development process by implementing automated code reviewer recommendation approaches. This aligns with the concept of self-discipline and adherence to best practices, as the automation process helps ensure that developers follow established guidelines and conform to industry standards.",Agreed-Clarified,Agree,,Agree,,
Exp A,Exp H,2643,ICSE-SEIP,AI & Machine Learning,Dependency Tracking for Risk Mitigation in Machine Learning (ML) Systems,"In a Machine Learning (ML) system, characteristics of the ML components create new challenges for software system design and development activities. Data-dependent behavior causes risks in ML systems. Dealing with such risks in the development phase requires non-trivial costs due to un-controllable data generation processes in the test phase. In addition, ML systems often need continuous monitoring and validation in run-time. In this paper, we propose an integrated dependency tracking system that balances the cost and risks in the development stage and operation stage. Our solution uses blockchain (an immutable data store) to track the co-evolution of the models and the corresponding datasets. The provenance of data and models provides a trustworthy trace for dependencies between datasets and models at the development phase, and predictions at the operation phase. A graph database is used to provide visualization and query of the provenance information, and enables explainability for the model-data co-evolution.",Universalism,Protecting the Environment,"The paper proposes a system that uses blockchain to track the provenance of data and models in a Machine Learning system. This aligns with the value item ""Protecting the Environment"" as it emphasizes the importance of sustainable practices in software development.","In my justification, I mentioned that the paper's proposal aligns with the value item ""Protecting the Environment"" because it emphasizes sustainable practices in software development. By using blockchain to track the provenance of data and models, the paper aims to ensure transparency and accountability in ML systems. This approach promotes responsible data usage, reduces the risk of bias, and ultimately contributes to the goal of protecting the environment by minimizing negative impacts caused by irresponsible data handling in software systems.",Agreed-Clarified,Agree,,Agree,,
Exp A,Exp H,2643,ICSE-SEIP,AI & Machine Learning,Dependency Tracking for Risk Mitigation in Machine Learning (ML) Systems,"In a Machine Learning (ML) system, characteristics of the ML components create new challenges for software system design and development activities. Data-dependent behavior causes risks in ML systems. Dealing with such risks in the development phase requires non-trivial costs due to un-controllable data generation processes in the test phase. In addition, ML systems often need continuous monitoring and validation in run-time. In this paper, we propose an integrated dependency tracking system that balances the cost and risks in the development stage and operation stage. Our solution uses blockchain (an immutable data store) to track the co-evolution of the models and the corresponding datasets. The provenance of data and models provides a trustworthy trace for dependencies between datasets and models at the development phase, and predictions at the operation phase. A graph database is used to provide visualization and query of the provenance information, and enables explainability for the model-data co-evolution.",Achievement,Intelligent,"The paper presents a solution to address risks in ML systems and achieve a balance between cost and risks in the development stage and operation stage. This aligns with the value item ""Intelligent"" as it demonstrates the software user's aspiration to excel and be acknowledged for their skills and accomplishments.","In the context of a software user, the alignment of 'Paper X' with the value item v4.1 (""Intelligent"") from Schwartz's Taxonomy is evident in the abstract. The paper acknowledges the risks associated with data-dependent behavior in Machine Learning (ML) systems and proposes an integrated dependency tracking system to mitigate these risks. By addressing these challenges and providing a solution that ensures the traceability and reliability of the models and datasets, the software user demonstrates their desire to be knowledgeable and capable in utilizing ML systems effectively, which aligns with the value of intelligence.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2643,ICSE-SEIP,AI & Machine Learning,Dependency Tracking for Risk Mitigation in Machine Learning (ML) Systems,"In a Machine Learning (ML) system, characteristics of the ML components create new challenges for software system design and development activities. Data-dependent behavior causes risks in ML systems. Dealing with such risks in the development phase requires non-trivial costs due to un-controllable data generation processes in the test phase. In addition, ML systems often need continuous monitoring and validation in run-time. In this paper, we propose an integrated dependency tracking system that balances the cost and risks in the development stage and operation stage. Our solution uses blockchain (an immutable data store) to track the co-evolution of the models and the corresponding datasets. The provenance of data and models provides a trustworthy trace for dependencies between datasets and models at the development phase, and predictions at the operation phase. A graph database is used to provide visualization and query of the provenance information, and enables explainability for the model-data co-evolution.",Stimulation,Excitement in Life,"The paper highlights the need for continuous monitoring and validation in run-time for ML systems. This aligns with the value item ""Excitement in Life"" as it emphasizes the desire for stimulation and new challenges in software usage.","The continuous monitoring and validation in run-time for ML systems, mentioned in the paper, aligns with the value item ""Excitement in Life"" as it reflects the desire for novel and stimulating experiences in the context of software usage. This aligns with the value of seeking stimulation and new challenges, which can be attributed to the excitement of exploring and improving ML models and their corresponding datasets in real-time.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2644,ICSE-SEIP,Software Engineering Practices,Dozer: Migrating Shell Commands to Ansible Modules via Execution Profiling and Synthesis,"Software developers frequently use the system shell to perform configuration management tasks. Unfortunately, the shell does not scale well to large systems, and configuration management tools like Ansible are more difficult to learn. We address this problem with Dozer, a technique to help developers push their shell commands into Ansible task definitions. It operates by tracing and comparing system calls to find Ansible modules with similar behaviors to shell commands, then generating and validating migrations to find the task which produces the most similar changes to the system. Dozer is syntax agnostic, which should allow it to generalize to other configuration management platforms. We evaluate Dozer using datasets from open source configuration scripts.",Self Direction,Independent,The paper proposes a technique; Dozer; that allows developers to push their shell commands into Ansible task definitions. This aligns with the value item Independent and its corresponding value Self Direction. By providing a way for developers to directly control and manage their system configurations without relying solely on the shell; Dozer empowers them to be more independent in their software development processes.,"In the abstract of 'Paper X', it is stated that the technique, Dozer, enables developers to push their shell commands into Ansible task definitions. This aligns with the value item v1.1, Independent, and its corresponding value v1, Self Direction, as it empowers software users to have control and autonomy in managing their system configurations, rather than solely relying on the system shell. By providing a way for developers to have more independence and freedom in their software development processes, Dozer encourages self-direction and aligns with the values of autonomy and self-respect.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2644,ICSE-SEIP,Software Engineering Practices,Dozer: Migrating Shell Commands to Ansible Modules via Execution Profiling and Synthesis,"Software developers frequently use the system shell to perform configuration management tasks. Unfortunately, the shell does not scale well to large systems, and configuration management tools like Ansible are more difficult to learn. We address this problem with Dozer, a technique to help developers push their shell commands into Ansible task definitions. It operates by tracing and comparing system calls to find Ansible modules with similar behaviors to shell commands, then generating and validating migrations to find the task which produces the most similar changes to the system. Dozer is syntax agnostic, which should allow it to generalize to other configuration management platforms. We evaluate Dozer using datasets from open source configuration scripts.",Achievement,Intelligent,The paper introduces Dozer as a technique to help developers with configuration management tasks. By making it easier for developers to transition from using the shell to Ansible; Dozer contributes to the software user's pursuit of intelligence and competence in managing large system configurations. It enables users to expand their skills and knowledge in using configuration management tools like Ansible; aligning with the value item Intelligent and its corresponding value Achievement.,"The justification for labeling 'Paper X' as aligning with value item v4.1 (Intelligent) and its corresponding value v4 (Achievement) from a ""Software User"" perspective is based on the fact that Dozer enables developers to transition from using the shell to Ansible more easily. This transition empowers software users to enhance their capabilities and expertise in managing complex system configurations, which directly contributes to their pursuit of intelligence and competence in the software field. By mastering configuration management tools like Ansible, software users can achieve greater success and influence in their work, aligning with the value of Achievement.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2644,ICSE-SEIP,Software Engineering Practices,Dozer: Migrating Shell Commands to Ansible Modules via Execution Profiling and Synthesis,"Software developers frequently use the system shell to perform configuration management tasks. Unfortunately, the shell does not scale well to large systems, and configuration management tools like Ansible are more difficult to learn. We address this problem with Dozer, a technique to help developers push their shell commands into Ansible task definitions. It operates by tracing and comparing system calls to find Ansible modules with similar behaviors to shell commands, then generating and validating migrations to find the task which produces the most similar changes to the system. Dozer is syntax agnostic, which should allow it to generalize to other configuration management platforms. We evaluate Dozer using datasets from open source configuration scripts.",Benevolence,Helpful,The paper presents Dozer as a technique that assists developers in managing system configurations. By simplifying the learning curve of Ansible and allowing developers to leverage their existing knowledge of shell commands; Dozer facilitates helpfulness among software users. It enables them to share their expertise and contribute to the well-being of other developers by making the transition to Ansible smoother; aligning with the value item Helpful and its corresponding value Benevolence.,"Dozer's alignment with the value item v9.3 and its corresponding value v9 (Helpful and Benevolence) is justified by the fact that it enables software users (not developers) to help other developers by simplifying the learning curve of Ansible. The user's expertise in shell commands can be shared and leveraged through Dozer, contributing to the well-being of other developers and facilitating a smoother transition to Ansible. This alignment directly supports the value of being helpful and benevolent within the software context.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2645,ICSE-SEIP,Code Generation & Analysis,Field-Based Static Taint Analysis for Industrial Microservices,"Taint analysis is widely used for tracing sensitive data. However, the state-of-the-art taint analyzers face challenges on recall, scalability, and precision when applied on industrial microservices. To overcome these challenges, we present a field-based static taint analysis approach, which does not distinguish different instances of the same type but distinguishes fields of the same kind for tracing sensitive data on industrial microservices. The experimental results demonstrate that our approach is practical in industrial scenarios.",Universalism,Protecting the Environment,The paper proposes a field-based static taint analysis approach that focuses on tracing sensitive data on industrial microservices. By considering the fields of the same kind; the approach contributes to the preservation of the environment by avoiding unnecessary resources and optimizing the analysis process. This aligns with the value item Protecting the Environment from Universalism.,"In 'Paper X', the field-based static taint analysis approach proposed explicitly aims to improve recall, scalability, and precision when applied to industrial microservices. By considering fields of the same kind instead of different instances of the same type, the approach optimizes the analysis process and avoids using unnecessary resources. This efficient approach directly aligns with the value item v10.4 of Protecting the Environment from Schwartz's Taxonomy, as it contributes to reducing resource consumption and aligns with the principle of sustainability in software development.",Agreed-Clarified,Agree,,Agree,,
Exp A,Exp H,2646,ICSE-SEIP,Software Engineering Practices,How Does Code Reviewing Feedback Evolve?: A Longitudinal Study at Dell EMC,"Code review is an integral part of modern software development, where fellow developers critique the content, premise, and structure of code changes. Organizations like DellEMC have made considerable investment in code reviews, yet tracking the characteristics of feedback that code reviews provide (a primary product of the code reviewing process) is still a difficult process. To understand community and personal feedback trends, we perform a longitudinal study of 39,249 reviews that contain 248,695 review comments from a proprietary project that is developed by DellEMC. To investigate generalizability, we replicate our study on the OpenStackn Ova project. Through an analysis guided by topic models, we observe that more context-specific, technical feedback is introduced as the studied projects and communities age and as the reviewers within those communities accrue experience. This suggests that communities are reaping a larger return on investment in code review as they grow accustomed to the practice and as reviewers hone their skills. The code review trends uncovered by our models present opportunities for enterprises to monitor reviewing tendencies and improve knowledge transfer and reviewer skills.",Power,Social Power,The paper contributes to the understanding and improvement of code review practices; which can lead to increased social power for software users. This aligns with the value item Social Power and its corresponding value Power.,"The justification for aligning 'Paper X' with value item v5.4 (Social Power) and its corresponding value v5 (Power) from a ""Software User"" perspective is that the paper focuses on the understanding and improvement of code review practices. By enhancing code review processes, software users gain more control and influence over the development and quality of software. This increased control and influence can be seen as a form of social power, allowing software users to have a greater impact on the software they use, which aligns with the value of Power.",Agreed-Clarified,Agree,,Agree,,
Exp A,Exp H,2646,ICSE-SEIP,Software Engineering Practices,How Does Code Reviewing Feedback Evolve?: A Longitudinal Study at Dell EMC,"Code review is an integral part of modern software development, where fellow developers critique the content, premise, and structure of code changes. Organizations like DellEMC have made considerable investment in code reviews, yet tracking the characteristics of feedback that code reviews provide (a primary product of the code reviewing process) is still a difficult process. To understand community and personal feedback trends, we perform a longitudinal study of 39,249 reviews that contain 248,695 review comments from a proprietary project that is developed by DellEMC. To investigate generalizability, we replicate our study on the OpenStackn Ova project. Through an analysis guided by topic models, we observe that more context-specific, technical feedback is introduced as the studied projects and communities age and as the reviewers within those communities accrue experience. This suggests that communities are reaping a larger return on investment in code review as they grow accustomed to the practice and as reviewers hone their skills. The code review trends uncovered by our models present opportunities for enterprises to monitor reviewing tendencies and improve knowledge transfer and reviewer skills.",Benevolence,Helpful,The paper investigates the characteristics of feedback in code reviews and presents opportunities for enterprises to improve knowledge transfer. By improving knowledge transfer; software users can be more helpful towards other users; aligning with the value item Helpful and its corresponding value Benevolence.,"In the paper abstract, it is explicitly stated that the study of code reviews aims to understand community and personal feedback trends, and to improve knowledge transfer and reviewer skills. By improving knowledge transfer, software users can gain a deeper understanding of the code and become more knowledgeable and helpful towards other users. This aligns with the value item ""Helpful"" from Schwartz's Taxonomy and its corresponding value, ""Benevolence."" Therefore, the main contribution of ""Paper X"" directly aligns with the value item v9.3 - Helpful and its corresponding value v9 - Benevolence, from the perspective of a software user.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2647,ICSE-SEIP,Code Generation & Analysis,Improving Code Autocompletion with Transfer Learning,"Software language models have achieved promising results predicting code completion usages, and several industry studies have described successful IDE integration. Recently, accuracy in autocompletion prediction improved 12.8%[2] from training on a real-world dataset collected from programmersaEUR(tm) IDE activities. But what if the number of examples of IDE autocompletion in the target programming language is inadequate for model training? In this paper, we highlight practical reasons for this inadequacy, and make a call to action in using transfer learning to overcome the issue.",Benevolence,Honesty,The paper proposes using transfer learning to overcome the issue of inadequate examples for model training. This aligns with the value item Honesty; as the paper highlights the practical reasons for the inadequacy and addresses the issue directly.,"The justification for aligning 'Paper X' with the value item v9.7 (Honesty) from a ""Software User"" perspective is that the paper explicitly addresses the issue of inadequate examples for model training and highlights the practical reasons behind it. By acknowledging this limitation and proposing the use of transfer learning to overcome it, the paper demonstrates an honest approach towards improving code completion predictions. This alignment with Honesty is important in ensuring transparency and trustworthiness in software development.",Agreed-Justified,Disagree,reconcile with coder_2,Disagree,The justification claims that the paper aligns with the value 'Benevolence' but the text does not align with the value item Honesty,
Exp A,Exp H,2647,ICSE-SEIP,Code Generation & Analysis,Improving Code Autocompletion with Transfer Learning,"Software language models have achieved promising results predicting code completion usages, and several industry studies have described successful IDE integration. Recently, accuracy in autocompletion prediction improved 12.8%[2] from training on a real-world dataset collected from programmersaEUR(tm) IDE activities. But what if the number of examples of IDE autocompletion in the target programming language is inadequate for model training? In this paper, we highlight practical reasons for this inadequacy, and make a call to action in using transfer learning to overcome the issue.",Universalism,Inner Harmony,The paper emphasizes the use of transfer learning to improve code completion prediction. This aligns with the value item Inner Harmony; as it promotes a more harmonious and efficient coding experience for software users.,"The paper's emphasis on using transfer learning to improve code completion prediction aligns with the value item v10.6 - Inner Harmony. By leveraging transfer learning, software users can benefit from a more harmonious and efficient coding experience. This aligns with the value of inner harmony as it contributes to a sense of balance and coherence in the software development process, ultimately enhancing the overall user experience.",Agreed-Clarified,Disagree,reconcile with coder_2,Disagree,The justification does not align with  Inner Harmony,
Exp A,Exp H,2648,ICSE-SEIP,Software Engineering Practices,Industry&#x27;s Cry for Tools that Support Large-Scale Refactoring,"Software refactoring plays an important role in software engineering. Developers often turn to refactoring when they want to restructure software to improve its quality without changing its external behavior. Compared to small-scale (floss) refactoring, many refactoring efforts are much larger, requiring entire teams and months of effort, and the role of tools in these efforts is not as well studied. This short paper introduces an industry survey that we conducted. Results from 107 developers demonstrate that projects commonly go through multiple large-scale refactorings, each of which requires considerable effort. While there is often a desire to refactor, other business concerns such as developing new features often take higher priority. Our study finds that developers use several categories of tools to support large-scale refactoring and rely more heavily on general-purpose tools like IDEs than on tools designed specifically to support refactoring. Tool support varies across the different activities (spanning communication, reasoning, and technical activities), with some particularly challenging activities seeing little use of tools in practice. Our study demonstrates a clear need for better large-scale refactoring tools.",Achievement,Capable,The paper acknowledges the importance of improving software quality without changing its external behavior; which aligns with the value item Capable and its corresponding value Achievement.,"In 'Paper X', the authors discuss the importance of software quality improvement without changing its external behavior. This aligns with the value item v4.2 (Capable) and its corresponding value v4 (Achievement) from a ""Software User"" perspective. The ability to ensure that the software performs its intended functions effectively and efficiently, without any negative impact on its external behavior, reflects the value of achieving competence and excellence in the software product. This alignment emphasizes the significance of maintaining software quality and achieving successful outcomes for users, thereby supporting the value of achievement.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2648,ICSE-SEIP,Software Engineering Practices,Industry&#x27;s Cry for Tools that Support Large-Scale Refactoring,"Software refactoring plays an important role in software engineering. Developers often turn to refactoring when they want to restructure software to improve its quality without changing its external behavior. Compared to small-scale (floss) refactoring, many refactoring efforts are much larger, requiring entire teams and months of effort, and the role of tools in these efforts is not as well studied. This short paper introduces an industry survey that we conducted. Results from 107 developers demonstrate that projects commonly go through multiple large-scale refactorings, each of which requires considerable effort. While there is often a desire to refactor, other business concerns such as developing new features often take higher priority. Our study finds that developers use several categories of tools to support large-scale refactoring and rely more heavily on general-purpose tools like IDEs than on tools designed specifically to support refactoring. Tool support varies across the different activities (spanning communication, reasoning, and technical activities), with some particularly challenging activities seeing little use of tools in practice. Our study demonstrates a clear need for better large-scale refactoring tools.",Achievement,Influential,The paper recognizes the role of tools in supporting large-scale refactoring; which aligns with the value item Influential and its corresponding value Achievement.,"The justification for aligning 'Paper X' with the value item v4.4 (Influential) and its corresponding value v4 (Achievement) is based on the recognition of the role of tools in supporting large-scale refactoring. By acknowledging the importance of tools in the refactoring process, the paper suggests that developers can achieve significant improvements in software quality and potentially influence the outcome of their projects. This aligns with the value of Achievement, as developers aim to successfully refactor their software and make a notable impact in improving its quality. The use of tools in this context can empower developers to accomplish their goals and exert influence over the software development process, thus aligning with the value of being Influential.",Agreed-Clarified,Agree,,Agree,,
Exp A,Exp H,2649,ICSE-SEIP,Code Generation & Analysis,InspectJS: Leveraging Code Similarity and User-Feedback for Effective Taint Specification Inference for JavaScript,"Static analysis has established itself as a weapon of choice for detecting security vulnerabilities. Taint analysis in particular is a very general and powerful technique, where security policies are expressed in terms of forbidden flows, either from untrusted input sources to sensitive sinks (in integrity policies) or from sensitive sources to untrusted sinks (in confidentiality policies). The appeal of this approach is that the taint-tracking mechanism has to be implemented only once, and can then be parameterized with different taint specifications (that is, sets of sources and sinks, as well as any sanitizers that render otherwise problematic flows innocuous) to detect many different kinds of vulnerabilities. But while techniques for implementing scalable inter-procedural static taint tracking are fairly well established, crafting taint specifications is still more of an art than a science, and in practice tends to involve a lot of manual effort. Past work has focussed on automated techniques for inferring taint specifications for libraries either from their implementation or from the way they tend to be used in client code. Among the latter, machine learning-based approaches have shown great promise. In this work we present our experience combining an existing machine-learning approach to mining sink specifications for JavaScript libraries with manual taint modelling in the context of GitHub&#x27;s CodeQL analysis framework. We show that the machine-learning component can successfully infer many new taint sinks that either are not part of the manual modelling or are not detected due to analysis incompleteness. Moreover, we present techniques for organizing sink predictions using automated ranking and code-similarity metrics that allow an analysis engineer to efficiently sift through large numbers of predictions to identify true positives.",Benevolence,Helpful,The paper presents an approach for inferring taint sinks in JavaScript libraries; which contributes to the overall goal of creating helpful software. This aligns with the value item Helpful and its corresponding value Benevolence.,"The justification for labeling 'Paper X' with the value item v9.3 (Helpful) and its corresponding value v9 (Benevolence) is based on the fact that the paper presents a method for inferring taint sinks in JavaScript libraries. By identifying potential vulnerabilities and providing insights on how to prevent them, the paper contributes to the development of software that is helpful in terms of protecting user data and ensuring their security. This aligns with the value of Benevolence, as it emphasizes the importance of creating software that promotes the well-being and benefit of its users.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2649,ICSE-SEIP,Code Generation & Analysis,InspectJS: Leveraging Code Similarity and User-Feedback for Effective Taint Specification Inference for JavaScript,"Static analysis has established itself as a weapon of choice for detecting security vulnerabilities. Taint analysis in particular is a very general and powerful technique, where security policies are expressed in terms of forbidden flows, either from untrusted input sources to sensitive sinks (in integrity policies) or from sensitive sources to untrusted sinks (in confidentiality policies). The appeal of this approach is that the taint-tracking mechanism has to be implemented only once, and can then be parameterized with different taint specifications (that is, sets of sources and sinks, as well as any sanitizers that render otherwise problematic flows innocuous) to detect many different kinds of vulnerabilities. But while techniques for implementing scalable inter-procedural static taint tracking are fairly well established, crafting taint specifications is still more of an art than a science, and in practice tends to involve a lot of manual effort. Past work has focussed on automated techniques for inferring taint specifications for libraries either from their implementation or from the way they tend to be used in client code. Among the latter, machine learning-based approaches have shown great promise. In this work we present our experience combining an existing machine-learning approach to mining sink specifications for JavaScript libraries with manual taint modelling in the context of GitHub&#x27;s CodeQL analysis framework. We show that the machine-learning component can successfully infer many new taint sinks that either are not part of the manual modelling or are not detected due to analysis incompleteness. Moreover, we present techniques for organizing sink predictions using automated ranking and code-similarity metrics that allow an analysis engineer to efficiently sift through large numbers of predictions to identify true positives.",Achievement,Successful,The paper discusses the successful inference of taint sinks that were not part of manual modeling or were missed due to analysis incompleteness. This aligns with the value item Successful and its corresponding value Achievement.,"The paper aligns with the value item ""Successful"" and its corresponding value ""Achievement"" because it presents the successful inference of taint sinks that were not part of the manual modeling or were missed due to analysis incompleteness. This demonstrates the accomplishment of effectively identifying and addressing security vulnerabilities, which can be considered a significant achievement in the software context. The paper's contributions directly align with the value of achieving success in the field of software security.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2649,ICSE-SEIP,Code Generation & Analysis,InspectJS: Leveraging Code Similarity and User-Feedback for Effective Taint Specification Inference for JavaScript,"Static analysis has established itself as a weapon of choice for detecting security vulnerabilities. Taint analysis in particular is a very general and powerful technique, where security policies are expressed in terms of forbidden flows, either from untrusted input sources to sensitive sinks (in integrity policies) or from sensitive sources to untrusted sinks (in confidentiality policies). The appeal of this approach is that the taint-tracking mechanism has to be implemented only once, and can then be parameterized with different taint specifications (that is, sets of sources and sinks, as well as any sanitizers that render otherwise problematic flows innocuous) to detect many different kinds of vulnerabilities. But while techniques for implementing scalable inter-procedural static taint tracking are fairly well established, crafting taint specifications is still more of an art than a science, and in practice tends to involve a lot of manual effort. Past work has focussed on automated techniques for inferring taint specifications for libraries either from their implementation or from the way they tend to be used in client code. Among the latter, machine learning-based approaches have shown great promise. In this work we present our experience combining an existing machine-learning approach to mining sink specifications for JavaScript libraries with manual taint modelling in the context of GitHub&#x27;s CodeQL analysis framework. We show that the machine-learning component can successfully infer many new taint sinks that either are not part of the manual modelling or are not detected due to analysis incompleteness. Moreover, we present techniques for organizing sink predictions using automated ranking and code-similarity metrics that allow an analysis engineer to efficiently sift through large numbers of predictions to identify true positives.",Self Direction,Curiosity,The paper combines machine-learning techniques with manual taint modeling to improve the process of crafting taint specifications. This aligns with the value item Curiosity and its corresponding value Self Direction.,"The paper's approach aligns with the value item Curiosity and its corresponding value Self Direction because it combines machine-learning techniques with manual taint modeling in order to explore new methods for improving the process of crafting taint specifications. By incorporating machine-learning algorithms, the paper demonstrates a pursuit of curiosity by utilizing innovative approaches to address the challenges of manual effort in taint specification crafting. This aligns with the value of self direction as it empowers software users to explore new avenues in identifying and mitigating security vulnerabilities within software systems.",Agreed-Clarified,Agree,,Agree,,
Exp A,Exp H,2650,ICSE-SEIP,Software Development Methodologies,Issues in the Adoption of the Scaled Agile Framework,"Agile methods were originally introduced for small sized, colocated teams. Their successful products immediately brought up the issue of adapting the methods also for large and distributed organizations engaged in projects to build major, complex products. Currently the most popular multi-teams agile method is the Scaled Agile Framework (SAFe) which, however, is subject to criticism: it appears to be quite demanding and expensive in terms of human resource and project management practices. Moreover, SAFe allegedly goes against some of the principles of agility. This research attempts to gather a deeper understanding of the matter first reviewing and analysing the studies published on this topic via a multivocal literature review and then with an extended empirical investigation on the matters that appear most controversial via the direct analysis of the work of 25 respondents from 17 different companies located in eight countries. Thus, the originality of this research is in the systemic assessment of the aEURoelevel of flexibilityaEUR of SAFe, highlighting the challenges of adopting this framework as it relates to decision making, structure, and the technical and managerial competencies of the company. The results show that SAFe can be an effective and adequate approach if the company is ready to invest a significant effort and resources into it both in the form of providing time for SAFe to be properly absorbed and specific training for individuals.",Power,Social Recognition,The paper examines the challenges of adopting the Scaled Agile Framework (SAFe) and its impact on decision making; structure; and the technical and managerial competencies of the company. This aligns with the value item Social Recognition and its corresponding value Power.,"The justification for aligning 'Paper X' with the value item v5.1 (Social Recognition) and its corresponding value v5 (Power) is based on the examination of the challenges of adopting the Scaled Agile Framework (SAFe) and its impact on decision making, structure, and the technical and managerial competencies of the company. The focus on these aspects suggests that achieving social recognition and power within the software context is an important consideration for the success of implementing SAFe. This aligns with the value item v5.1 (Social Recognition) which emphasizes the desire for recognition, esteem, and attention from others, and the corresponding value v5 (Power) which emphasizes the desire to have an impact on people and events, and to be influential and respected.",Agreed-Clarified,Agree,,Agree,,
Exp A,Exp H,2650,ICSE-SEIP,Software Development Methodologies,Issues in the Adoption of the Scaled Agile Framework,"Agile methods were originally introduced for small sized, colocated teams. Their successful products immediately brought up the issue of adapting the methods also for large and distributed organizations engaged in projects to build major, complex products. Currently the most popular multi-teams agile method is the Scaled Agile Framework (SAFe) which, however, is subject to criticism: it appears to be quite demanding and expensive in terms of human resource and project management practices. Moreover, SAFe allegedly goes against some of the principles of agility. This research attempts to gather a deeper understanding of the matter first reviewing and analysing the studies published on this topic via a multivocal literature review and then with an extended empirical investigation on the matters that appear most controversial via the direct analysis of the work of 25 respondents from 17 different companies located in eight countries. Thus, the originality of this research is in the systemic assessment of the aEURoelevel of flexibilityaEUR of SAFe, highlighting the challenges of adopting this framework as it relates to decision making, structure, and the technical and managerial competencies of the company. The results show that SAFe can be an effective and adequate approach if the company is ready to invest a significant effort and resources into it both in the form of providing time for SAFe to be properly absorbed and specific training for individuals.",Achievement,Intelligent,The paper investigates the criticisms of SAFe and its compatibility with agile principles. It aims to gather a deeper understanding of the framework's flexibility and the challenges of adopting it. This aligns with the value item Intelligent and its corresponding value Achievement.,"The main contribution of 'Paper X' is investigating the flexibility of the Scaled Agile Framework (SAFe) and the challenges of adopting it. This aligns with the value item ""Intelligent"" from the perspective of a software user because it shows an effort towards gaining a deeper understanding of the framework and its compatibility with agile principles, reflecting a thoughtful approach that prioritizes intelligence and knowledge in decision-making processes. This aligns with the value of ""Achievement"" as it aims to assess and improve the adoption of SAFe, which can lead to successful and influential outcomes for organizations using the framework.",Agreed-Reconciled,Agree,,Agree,,
Exp A,Exp H,2650,ICSE-SEIP,Software Development Methodologies,Issues in the Adoption of the Scaled Agile Framework,"Agile methods were originally introduced for small sized, colocated teams. Their successful products immediately brought up the issue of adapting the methods also for large and distributed organizations engaged in projects to build major, complex products. Currently the most popular multi-teams agile method is the Scaled Agile Framework (SAFe) which, however, is subject to criticism: it appears to be quite demanding and expensive in terms of human resource and project management practices. Moreover, SAFe allegedly goes against some of the principles of agility. This research attempts to gather a deeper understanding of the matter first reviewing and analysing the studies published on this topic via a multivocal literature review and then with an extended empirical investigation on the matters that appear most controversial via the direct analysis of the work of 25 respondents from 17 different companies located in eight countries. Thus, the originality of this research is in the systemic assessment of the aEURoelevel of flexibilityaEUR of SAFe, highlighting the challenges of adopting this framework as it relates to decision making, structure, and the technical and managerial competencies of the company. The results show that SAFe can be an effective and adequate approach if the company is ready to invest a significant effort and resources into it both in the form of providing time for SAFe to be properly absorbed and specific training for individuals.",Security,Social Order,The paper discusses the resource and project management practices associated with SAFe and the investments needed from companies; including providing time for proper absorption and specific training. This aligns with the value item Social Order and its corresponding value Security.,"In the abstract of 'Paper X', it is mentioned that the adoption of SAFe requires companies to invest significant effort and resources, including providing time for absorption and specific training. This emphasis on resource allocation and structured practices aligns with the value item v6.3 (Social Order) from Schwartz's Taxonomy. Social Order emphasizes the importance of maintaining a stable and organized society, which in this context relates to the need for structured project management practices and processes. By investing in these practices, companies aim to establish a sense of security and order within their software development projects, aligning with the corresponding value of Security.",Agreed-Clarified,Agree,,Agree,,
Exp A,Exp H,2651,ICSE-SEIP,Software Testing & QA,Looking for Lacunae in Bitcoin Core&#x27;s Fuzzing Efforts,"Bitcoin is one of the most prominent distributed software systems in the world. This paper describes an effort to investigate and enhance the effectiveness of the Bitcoin Core fuzzing effort. The effort initially began as a query about how to escape saturation in the fuzzing effort, but developed into a more general exploration. This paper summarizes the outcomes of a two-week focused effort. While the effort found no smoking guns indicating major test/fuzz weaknesses, it produced a large number of additional fuzz corpus entries, increased the set of fuzzers used for Bitcoin Core, and ran mutation analysis of Bitcoin Core fuzz targets, with a comparison to Bitcoin functional tests and other cryptocurrenciesaEUR(tm) tests. Our conclusion is that for high quality fuzzing efforts, improvements to the oracle may be the best way to get more out of fuzzing.",Achievement,Successful,The paper contributes to the enhancement of the Bitcoin Core fuzzing effort; which demonstrates the software user's competence and the pursuit of success in achieving a high quality fuzzing system.,"The enhancement of the Bitcoin Core fuzzing effort, as described in the paper, aligns with the value item v4.3 (Successful) and its corresponding value v4 (Achievement) from a ""Software User"" perspective. This contribution demonstrates the software user's competence and pursuit of success in achieving a high-quality fuzzing system. By expanding the fuzz corpus, increasing the set of fuzzers, and conducting mutation analysis, the paper aims to improve the effectiveness of the fuzzing effort, ultimately achieving a more robust and secure software system. This directly aligns with the value of achievement, as the software user seeks to accomplish their goals and ensure the success of the system.",Agreed-Clarified,Agree,,Agree,,
Exp A,Exp H,2651,ICSE-SEIP,Software Testing & QA,Looking for Lacunae in Bitcoin Core&#x27;s Fuzzing Efforts,"Bitcoin is one of the most prominent distributed software systems in the world. This paper describes an effort to investigate and enhance the effectiveness of the Bitcoin Core fuzzing effort. The effort initially began as a query about how to escape saturation in the fuzzing effort, but developed into a more general exploration. This paper summarizes the outcomes of a two-week focused effort. While the effort found no smoking guns indicating major test/fuzz weaknesses, it produced a large number of additional fuzz corpus entries, increased the set of fuzzers used for Bitcoin Core, and ran mutation analysis of Bitcoin Core fuzz targets, with a comparison to Bitcoin functional tests and other cryptocurrenciesaEUR(tm) tests. Our conclusion is that for high quality fuzzing efforts, improvements to the oracle may be the best way to get more out of fuzzing.",Achievement,Ambitious,The paper describes the effort to investigate and enhance the effectiveness of the Bitcoin Core fuzzing effort; which reflects the software user's ambition in pursuing improvements and advancements in software testing.,"The alignment of 'Paper X' with value item v4.5 and its corresponding value v4 is justified by the fact that the paper explicitly states the effort to investigate and enhance the effectiveness of the Bitcoin Core fuzzing effort. As a software user, one's ambition is to have access to high-quality and well-tested software, and the pursuit of improvements and advancements in software testing directly aligns with the value of ambition.",Agreed-Clarified,Agree,,Agree,,
Exp A,Exp H,2651,ICSE-SEIP,Software Testing & QA,Looking for Lacunae in Bitcoin Core&#x27;s Fuzzing Efforts,"Bitcoin is one of the most prominent distributed software systems in the world. This paper describes an effort to investigate and enhance the effectiveness of the Bitcoin Core fuzzing effort. The effort initially began as a query about how to escape saturation in the fuzzing effort, but developed into a more general exploration. This paper summarizes the outcomes of a two-week focused effort. While the effort found no smoking guns indicating major test/fuzz weaknesses, it produced a large number of additional fuzz corpus entries, increased the set of fuzzers used for Bitcoin Core, and ran mutation analysis of Bitcoin Core fuzz targets, with a comparison to Bitcoin functional tests and other cryptocurrenciesaEUR(tm) tests. Our conclusion is that for high quality fuzzing efforts, improvements to the oracle may be the best way to get more out of fuzzing.",Security,National Security,The paper highlights the need for improvements to the oracle in order to achieve high quality fuzzing efforts; which aligns with the software user's concern for national security and the desire for social order and stability in software systems.,"In the context of a ""Software User,"" the concern for national security in software systems relates to the need for software to be secure, reliable, and free from vulnerabilities that could be exploited by malicious actors. The desire for social order and stability aligns with the notion that software should function smoothly and consistently, without causing disruptions or instability. Therefore, the paper's emphasis on improving the oracle in fuzzing aligns with these concerns by aiming to enhance the effectiveness and strength of the Bitcoin Core software, addressing potential vulnerabilities, and ensuring the overall security and stability of the system.",Agreed-Clarified,Agree,,Agree,,
Exp A,Exp H,2653,ICSE-SEIP,Software Engineering Practices,Mining Root Cause Knowledge from Cloud Service Incident Investigations for AIOps,"Root Cause Analysis (RCA) of any service-disrupting incident is one of the most critical as well as complex tasks in IT processes, especially for cloud industry leaders like Salesforce. Typically RCA investigation leverages data-sources like application error logs or service call traces. However a rich goldmine of root cause information is also hidden in the natural language documentation of the past incidents investigations by domain experts. This is generally termed as Problem Review Board (PRB) Data which constitute a core component of IT Incident Management. However, owing to the raw unstructured nature of PRBs, such root cause knowledge is not directly reusable by manual or automated pipelines for RCA of new incidents. This motivates us to leverage this widely-available data-source to build an Incident Causation Analysis (ICA) engine, using SoTA neural NLP techniques to extract targeted information and construct a structured Causal Knowledge Graph from PRB documents. ICA forms the backbone of a simple-yet-effective Retrieval based RCA for new incidents, through an Information Retrieval system to search and rank past incidents and detect likely root causes from them, given the incident symptom. In this work, we present ICA and the downstream Incident Search and Retrieval based RCA pipeline, built at Salesforce, over 2K documented cloud service incident investigations collected over a few years. We also establish the effectiveness of ICA and the downstream tasks through various quantitative benchmarks, qualitative analysis as well as domain expert&#x27;s validation and real incident case studies after deployment.",Achievement,Ambitious,The paper's main contribution is the development of an Incident Causation Analysis (ICA) engine; which helps in the retrieval-based Root Cause Analysis (RCA) for new incidents in the cloud industry. This aligns with the value item Ambitious and its corresponding value Achievement; as it demonstrates the software user's aspiration to excel and be acknowledged for their skills and accomplishments.,"I apologize for any confusion. My justification aligning 'Paper X' with the value item v4.5 (Ambitious) and its corresponding value v4 (Achievement) is based on the fact that the paper's main contribution, the development of an Incident Causation Analysis (ICA) engine for Root Cause Analysis (RCA) in the cloud industry, demonstrates the software user's ambition to excel and achieve recognition for their skills and accomplishments in solving complex issues and improving incident management processes. This aligns with the ambition and drive for personal and professional success that is associated with the value of achievement.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2653,ICSE-SEIP,Software Engineering Practices,Mining Root Cause Knowledge from Cloud Service Incident Investigations for AIOps,"Root Cause Analysis (RCA) of any service-disrupting incident is one of the most critical as well as complex tasks in IT processes, especially for cloud industry leaders like Salesforce. Typically RCA investigation leverages data-sources like application error logs or service call traces. However a rich goldmine of root cause information is also hidden in the natural language documentation of the past incidents investigations by domain experts. This is generally termed as Problem Review Board (PRB) Data which constitute a core component of IT Incident Management. However, owing to the raw unstructured nature of PRBs, such root cause knowledge is not directly reusable by manual or automated pipelines for RCA of new incidents. This motivates us to leverage this widely-available data-source to build an Incident Causation Analysis (ICA) engine, using SoTA neural NLP techniques to extract targeted information and construct a structured Causal Knowledge Graph from PRB documents. ICA forms the backbone of a simple-yet-effective Retrieval based RCA for new incidents, through an Information Retrieval system to search and rank past incidents and detect likely root causes from them, given the incident symptom. In this work, we present ICA and the downstream Incident Search and Retrieval based RCA pipeline, built at Salesforce, over 2K documented cloud service incident investigations collected over a few years. We also establish the effectiveness of ICA and the downstream tasks through various quantitative benchmarks, qualitative analysis as well as domain expert&#x27;s validation and real incident case studies after deployment.",Security,Cleanliness,The paper explores the use of Problem Review Board (PRB) Data in IT Incident Management; which is a core component of ensuring social order and cleanliness in the software ecosystem. This aligns with the value item Cleanliness and its corresponding value Security; as it contributes to maintaining a clean and well-organized incident management process.,"The use of Problem Review Board (PRB) Data in IT Incident Management aligns with the value item Cleanliness and its corresponding value Security because it helps maintain a clean and well-organized incident management process. By leveraging PRB Data, the paper aims to extract targeted information and construct a structured Causal Knowledge Graph, which in turn enables efficient retrieval of past incidents and identification of likely root causes for new incidents. This systematic approach contributes to the security and cleanliness of the software ecosystem by ensuring that incidents are properly addressed and resolved, leading to a more organized and secure software environment.",Agreed-Clarified,Disagree,reconcile with coder_2,Disagree,this justification does not align with the value item Cleanliness,
Exp A,Exp H,2653,ICSE-SEIP,Software Engineering Practices,Mining Root Cause Knowledge from Cloud Service Incident Investigations for AIOps,"Root Cause Analysis (RCA) of any service-disrupting incident is one of the most critical as well as complex tasks in IT processes, especially for cloud industry leaders like Salesforce. Typically RCA investigation leverages data-sources like application error logs or service call traces. However a rich goldmine of root cause information is also hidden in the natural language documentation of the past incidents investigations by domain experts. This is generally termed as Problem Review Board (PRB) Data which constitute a core component of IT Incident Management. However, owing to the raw unstructured nature of PRBs, such root cause knowledge is not directly reusable by manual or automated pipelines for RCA of new incidents. This motivates us to leverage this widely-available data-source to build an Incident Causation Analysis (ICA) engine, using SoTA neural NLP techniques to extract targeted information and construct a structured Causal Knowledge Graph from PRB documents. ICA forms the backbone of a simple-yet-effective Retrieval based RCA for new incidents, through an Information Retrieval system to search and rank past incidents and detect likely root causes from them, given the incident symptom. In this work, we present ICA and the downstream Incident Search and Retrieval based RCA pipeline, built at Salesforce, over 2K documented cloud service incident investigations collected over a few years. We also establish the effectiveness of ICA and the downstream tasks through various quantitative benchmarks, qualitative analysis as well as domain expert&#x27;s validation and real incident case studies after deployment.",Benevolence,Responsibility,The paper presents the effectiveness of the Incident Causation Analysis (ICA) engine and Retrieval-based RCA pipeline through various benchmarks; validation by domain experts; and real incident case studies. This aligns with the value item Responsibility and its corresponding value Benevolence; as it demonstrates the software user's responsibility in ensuring the well-being and welfare of others by providing an effective incident management system.,"The justification for aligning 'Paper X' with the value item v9.8 (Responsibility) and its corresponding value v9 (Benevolence) is based on the paper's demonstration of the Incident Causation Analysis (ICA) engine and Retrieval-based RCA pipeline. These contributions highlight the software user's responsibility in ensuring the well-being and welfare of others by providing an effective incident management system. By leveraging the PRB data and constructing a structured Causal Knowledge Graph, the ICA engine enables the software user to identify root causes of incidents, leading to prompt resolution and prevention of service disruptions. This responsibility aligns with the value of Benevolence, as it emphasizes the software user's commitment to being helpful, responsible, and ensuring the overall well-being of the software system and its users.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2656,ICSE-SEIP,Software Engineering Practices,aEURoeProject smellsaEUR - Experiences in Analysing the Software Quality of ML Projects with mllint,"Machine Learning (ML) projects incur novel challenges in their development and productionisation over traditional software applications, though established principles and best practices in ensuring the project&#x27;s software quality still apply. While using static analysis to catch code smells has been shown to improve software quality attributes, it is only a small piece of the software quality puzzle, especially in the case of ML projects given their additional challenges and lower degree of Software Engineering (SE) experience in the data scientists that develop them. We introduce the novel concept of project smells which consider deficits in project management as a more holistic perspective on software quality in ML projects. An open-source static analysis tool mllint was also implemented to help detect and mitigate these. Our research evaluates this novel concept of project smells in the industrial context of ING, a global bank and large software- and data-intensive organisation. We also investigate the perceived importance of these project smells for proof-of-concept versus production-ready ML projects, as well as the perceived obstructions and benefits to using static analysis tools such as mllint. Our findings indicate a need for context-aware static analysis tools, that fit the needs of the project at its current stage of development, while requiring minimal configuration effort from the user.",Achievement,Capable,The paper discusses the deficits in project management in ML projects and introduces the concept of project smells to provide a more holistic perspective on software quality. This aligns with the value item Capable and its corresponding value Achievement; as it addresses the need for effective project management to ensure the successful development and productionization of ML projects.,"I apologize for the confusion. My justification is based on the fact that effective project management, as addressed in 'Paper X', is a crucial aspect of achieving successful outcomes in ML projects. By introducing the concept of project smells, the paper aims to improve software quality and mitigate deficits in project management. This aligns with the value item Capable and its corresponding value Achievement, as it emphasizes the importance of being capable and achieving successful outcomes through effective project management in the context of ML projects.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2662,ICSE-SEIP,AI & Machine Learning,Testing Machine Learning Systems in Industry: An Empirical Study,"Machine learning becomes increasingly prevalent and integrated into a wide range of software systems. These systems, named ML systems, must be adequately tested to gain confidence that they behave correctly. Although many research efforts have been devoted to testing technologies for ML systems, the industrial teams are faced with new challenges on testing the ML systems in real-world settings. To absorb inspirations from the industry on the problems in ML testing, we conducted an empirical study including a survey with 87 responses and interviews with 7 senior ML practitioners from well-known IT companies. Our study uncovers significant industrial concerns on major testing activities, i.e., test data collection, test execution, and test result analysis, and also the good practices and open challenges from the perspective of the industry. (1) Test data collection is conducted in different ways on ML model, data, and code and faced with different challenges. (2) Test execution in ML systems suffers from two major problems: entanglement among the components and the regression on model performance. (3) Test result analysis centers on quantitative methods, e.g., metric-based evaluation, and is combined with some qualitative methods based on practitionersaEUR(tm) experience. Based on our findings, we highlight the research opportunities and also provide some implications for practitioners.",Achievement,Intelligent,The paper's contribution of testing technologies for ML systems aligns with the value item Intelligent. This is because the testing technologies demonstrate a level of intelligence in ensuring that ML systems behave correctly.,"In the context of 'Paper X', the alignment between the paper's contribution of testing technologies for ML systems and the value item Intelligent can be justified as follows: ML systems are complex and require intelligent testing approaches to ensure that they behave correctly. The development and implementation of testing technologies for ML systems require a deep understanding of the underlying algorithms and models, as well as the ability to design and execute tests that can effectively identify potential issues or errors. By addressing the challenges in testing ML systems in real-world settings, 'Paper X' contributes to the development of intelligent testing methodologies that uphold the value of intelligence in ensuring the accuracy and reliability of ML systems.",Agreed-Clarified,Agree,,Agree,,
Exp A,Exp H,2662,ICSE-SEIP,AI & Machine Learning,Testing Machine Learning Systems in Industry: An Empirical Study,"Machine learning becomes increasingly prevalent and integrated into a wide range of software systems. These systems, named ML systems, must be adequately tested to gain confidence that they behave correctly. Although many research efforts have been devoted to testing technologies for ML systems, the industrial teams are faced with new challenges on testing the ML systems in real-world settings. To absorb inspirations from the industry on the problems in ML testing, we conducted an empirical study including a survey with 87 responses and interviews with 7 senior ML practitioners from well-known IT companies. Our study uncovers significant industrial concerns on major testing activities, i.e., test data collection, test execution, and test result analysis, and also the good practices and open challenges from the perspective of the industry. (1) Test data collection is conducted in different ways on ML model, data, and code and faced with different challenges. (2) Test execution in ML systems suffers from two major problems: entanglement among the components and the regression on model performance. (3) Test result analysis centers on quantitative methods, e.g., metric-based evaluation, and is combined with some qualitative methods based on practitionersaEUR(tm) experience. Based on our findings, we highlight the research opportunities and also provide some implications for practitioners.",Achievement,Capable,The paper's contribution of testing technologies for ML systems aligns with the value item Capable. This is because the testing technologies showcase the capability of effectively testing ML systems in real-world settings.,"The paper's contribution aligns with the value item Capable (v4.2) and its corresponding value Achievement (v4) from a ""Software User"" perspective. This is evident as the paper focuses on addressing the challenges faced by industrial teams in testing ML systems, aiming to gain confidence in their correct behavior. By providing insights into major testing activities such as test data collection, test execution, and test result analysis, the paper demonstrates the capability of testing technologies to effectively tackle these challenges in real-world settings. This aligns with the value of achievement as it showcases the successful application of testing technologies to ensure the proper functioning and reliability of ML systems.",Agreed-Clarified,Agree,,Agree,,
Exp A,Exp H,2662,ICSE-SEIP,AI & Machine Learning,Testing Machine Learning Systems in Industry: An Empirical Study,"Machine learning becomes increasingly prevalent and integrated into a wide range of software systems. These systems, named ML systems, must be adequately tested to gain confidence that they behave correctly. Although many research efforts have been devoted to testing technologies for ML systems, the industrial teams are faced with new challenges on testing the ML systems in real-world settings. To absorb inspirations from the industry on the problems in ML testing, we conducted an empirical study including a survey with 87 responses and interviews with 7 senior ML practitioners from well-known IT companies. Our study uncovers significant industrial concerns on major testing activities, i.e., test data collection, test execution, and test result analysis, and also the good practices and open challenges from the perspective of the industry. (1) Test data collection is conducted in different ways on ML model, data, and code and faced with different challenges. (2) Test execution in ML systems suffers from two major problems: entanglement among the components and the regression on model performance. (3) Test result analysis centers on quantitative methods, e.g., metric-based evaluation, and is combined with some qualitative methods based on practitionersaEUR(tm) experience. Based on our findings, we highlight the research opportunities and also provide some implications for practitioners.",Achievement,Successful,The paper's contribution of testing technologies for ML systems aligns with the value item Successful. This is because the testing technologies contribute to the success of ML systems by ensuring their correct behavior.,"Certainly! In the context of a software user, the value item v4.3 ""Successful"" aligns with the main contribution of 'Paper X' in testing technologies for ML systems. The successful implementation and application of these testing technologies ensure that ML systems behave correctly, providing users with the confidence that the software will function as intended. By addressing the challenges faced by industrial teams when testing ML systems, 'Paper X' contributes to the success of these systems by mitigating potential issues and ensuring their effectiveness. This alignment with the value item ""Successful"" highlights the importance of reliable and well-tested ML systems, ultimately benefiting the software user's experience and satisfaction.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2667,ICSE-SEIP,Software Engineering Practices,Towards Build Verifiability for Java-based Systems,"Build verifiability refers to the property that the build of a software system can be verified by independent third parties and it is crucial for the trustworthiness of a software system. Various efforts towards build verifiability have been made to $\mathrm{C}/\mathrm{C}++$ -based systems, yet the techniques for Java-based systems are not systematic and are often specific to a particular build tool (e.g., Maven). In this study, we present a systematic approach towards build verifiability on Java-based systems. Our approach consists of three parts: a unified build process, a tool that dynamically controls non-determinism during the build process, and another tool that eliminates non-equivalences by post-processing the build artifacts. We apply our approach on 46 unverified open source projects from Reproducible Central and 13 open source projects that are widely used by Huawei commercial products. As a result, 91% of the unverified Reproducible Central projects and 100% of the commercially adopted OSS projects are successfully verified with our approach. In addition, based on our experience in analyzing thousands of builds for both commercial and open source Java-based systems, we present 14 patterns that introduce non-equivalences in generated build artifacts and their respective mitigation strategies. Among these patterns, 11 (78%) are unique for Java-based system, whereas the remaining 3 (22%) are common with $\mathrm{C}/\mathrm{C}++$ -based systems. The approach and the findings of this paper are useful for both practitioners and researchers who are interested in build verifiability.",Universalism,Protecting the Environment,The paper presents a systematic approach towards build verifiability on Java-based systems; which includes a tool that eliminates non-equivalences in the build artifacts. This aligns with the value item Protecting the Environment and its corresponding value Universalism; as it focuses on ensuring the trustworthiness and reliability of software systems; contributing to the preservation of the environment by reducing wastefulness and inefficiency in software builds.,"The alignment of 'Paper X' with the value item v10.4 (Protecting the Environment) and its corresponding value v10 (Universalism) is justified by the paper's focus on build verifiability, which contributes to the trustworthiness and reliability of software systems. By eliminating non-equivalences in build artifacts, the paper aims to reduce wastefulness and inefficiency in software builds, ultimately promoting sustainable practices in software development. This aligns directly with the value item v10.4 and the broader value of Universalism, which emphasizes the importance of protecting the environment and seeking harmony with nature.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2669,ICSE-SEIP,Security & Privacy,Using a Semantic Knowledge Base to Improve the Management of Security Reports in Industrial DevOps Projects,"Integrating security activities into the software development lifecycle to detect security flaws is essential for any project. These activities produce reports that must be managed and looped back to project stakeholders like developers to enable security improvements. This so-called Feedback Loop is a crucial part of any project and is required by various industrial security standards and models. However, the operation of this loop presents a variety of challenges. These challenges range from ensuring that feedback data is of sufficient quality over providing different stakeholders with the information they need to the enormous effort to manage the reports. In this paper, we propose a novel approach for treating findings from security activity reports as belief in a Knowledge Base (KB). By utilizing continuous logical inferences, we derive information necessary for practitioners and address existing challenges in the industry. This approach is currently evaluated in industrial DevOps projects, using data from continuous security testing.",Power,Preserving My Public Image,The paper proposes a novel approach for managing security activity reports; which is crucial for preserving the public image of the software users. This aligns with the value item Preserving My Public Image and its corresponding value Power.,"The paper's proposal of a novel approach for managing security activity reports directly aligns with the value item ""Preserving My Public Image"" and its corresponding value ""Power"" from the perspective of a software user. By addressing the challenges in managing these reports, the paper acknowledges the importance of maintaining a positive public image, as security flaws can negatively impact user trust and perception. The ability to effectively manage and improve security measures demonstrates power and control over the software's impact on the user's public image. Thus, the alignment between the main contributions of 'Paper X' and the value item v5.5 and its corresponding value v5 is evident.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2669,ICSE-SEIP,Security & Privacy,Using a Semantic Knowledge Base to Improve the Management of Security Reports in Industrial DevOps Projects,"Integrating security activities into the software development lifecycle to detect security flaws is essential for any project. These activities produce reports that must be managed and looped back to project stakeholders like developers to enable security improvements. This so-called Feedback Loop is a crucial part of any project and is required by various industrial security standards and models. However, the operation of this loop presents a variety of challenges. These challenges range from ensuring that feedback data is of sufficient quality over providing different stakeholders with the information they need to the enormous effort to manage the reports. In this paper, we propose a novel approach for treating findings from security activity reports as belief in a Knowledge Base (KB). By utilizing continuous logical inferences, we derive information necessary for practitioners and address existing challenges in the industry. This approach is currently evaluated in industrial DevOps projects, using data from continuous security testing.",Security,Social Order,The paper addresses the challenges of managing feedback data in security activities; which is essential for maintaining social order within the software context. This aligns with the value item Social Order and its corresponding value Security.,"In the paper, the authors highlight the challenges of managing feedback data in security activities for software development. This directly relates to the value item v6.3, which is Social Order, from Schwartz's Taxonomy. Social Order refers to the value of maintaining stability and order in society. In the software context, ensuring that feedback data is managed effectively contributes to the overall stability and orderliness of the software development process. By addressing this challenge, the paper aligns with the value of Social Order, as it aims to improve the management of feedback data and promote a more efficient and orderly software development lifecycle.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2669,ICSE-SEIP,Security & Privacy,Using a Semantic Knowledge Base to Improve the Management of Security Reports in Industrial DevOps Projects,"Integrating security activities into the software development lifecycle to detect security flaws is essential for any project. These activities produce reports that must be managed and looped back to project stakeholders like developers to enable security improvements. This so-called Feedback Loop is a crucial part of any project and is required by various industrial security standards and models. However, the operation of this loop presents a variety of challenges. These challenges range from ensuring that feedback data is of sufficient quality over providing different stakeholders with the information they need to the enormous effort to manage the reports. In this paper, we propose a novel approach for treating findings from security activity reports as belief in a Knowledge Base (KB). By utilizing continuous logical inferences, we derive information necessary for practitioners and address existing challenges in the industry. This approach is currently evaluated in industrial DevOps projects, using data from continuous security testing.",Universalism,Protecting the Environment,The paper emphasizes the importance of integrating security activities into the software development lifecycle to ensure the protection and preservation of the environment in the software industry. This aligns with the value item Protecting the Environment and its corresponding value Universalism.,"In the paper abstract, it is explicitly stated that the proposed approach of treating findings from security activity reports as belief in a Knowledge Base aims to address existing challenges in the industry. By utilizing continuous logical inferences, the approach allows practitioners to derive information necessary for making security improvements. This focus on improving security aligns with the value item v10.4, ""Protecting the Environment,"" as it addresses the need to safeguard the software environment from security flaws and vulnerabilities. Additionally, this value item corresponds to the value of Universalism, which emphasizes the importance of considering the broader impact and well-being of the software ecosystem.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2670,ICSE-SEIP,Software Testing & QA,Using Natural Language Processing Techniques to Improve Manual Test Case Descriptions,"Despite the recent advancements in test automation, testing often remains a manual, and costly, activity in many industries. Manual test cases, often described only in natural language, consist of one or more test steps, which are instructions that must be performed to achieve the testing objective. Having different employees specifying test cases might result in redundant, unclear, or incomplete test cases. Manually reviewing and validating newly-specified test cases is time-consuming and becomes impractical in a scenario with a large test suite. Therefore, in this paper, we propose an automated framework to automatically analyze test cases that are specified in natural language and provide actionable recommendations on how to improve the test cases. Our framework consists of configurable components and modules for analysis, which are capable of recommending improvements to the following: (1) the terminology of a new test case through language modeling, (2) potentially missing test steps for a new test case through frequent itemset and association rule mining, and (3) recommendation of similar test cases that already exist in the test suite through text embedding and clustering. We thoroughly evaluated the three modules on data from our industry partner. Our framework can provide actionable recommendations, which is an important challenge given the widespread occurrence of test cases that are described only in natural language in the software industry (in particular, the game industry).",Benevolence,Helpful,The paper proposes an automated framework to analyze test cases specified in natural language and provide actionable recommendations for improvement. This aligns with the value item Helpful; as it aims to assist software users in creating better test cases that lead to more effective testing.,"In 'Paper X', the proposed automated framework aims to assist software users in creating better test cases by analyzing them and providing actionable recommendations. This aligns with the value item Helpful from Schwartz's Taxonomy, as it clearly demonstrates the intention to provide support and assistance to software users in their testing activities. By improving the effectiveness of test cases, the framework directly contributes to the value of Helpful by helping users achieve their testing objectives more efficiently and accurately.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2670,ICSE-SEIP,Software Testing & QA,Using Natural Language Processing Techniques to Improve Manual Test Case Descriptions,"Despite the recent advancements in test automation, testing often remains a manual, and costly, activity in many industries. Manual test cases, often described only in natural language, consist of one or more test steps, which are instructions that must be performed to achieve the testing objective. Having different employees specifying test cases might result in redundant, unclear, or incomplete test cases. Manually reviewing and validating newly-specified test cases is time-consuming and becomes impractical in a scenario with a large test suite. Therefore, in this paper, we propose an automated framework to automatically analyze test cases that are specified in natural language and provide actionable recommendations on how to improve the test cases. Our framework consists of configurable components and modules for analysis, which are capable of recommending improvements to the following: (1) the terminology of a new test case through language modeling, (2) potentially missing test steps for a new test case through frequent itemset and association rule mining, and (3) recommendation of similar test cases that already exist in the test suite through text embedding and clustering. We thoroughly evaluated the three modules on data from our industry partner. Our framework can provide actionable recommendations, which is an important challenge given the widespread occurrence of test cases that are described only in natural language in the software industry (in particular, the game industry).",Achievement,Capable,The paper's framework includes components and modules for analyzing test cases and recommending improvements; such as identifying missing test steps and improving the terminology used. This aligns with the value item Capable; as it empowers software users to enhance their testing capabilities and create more thorough test cases.,"The alignment of 'Paper X' with the value item Capable (v4.2) and its corresponding value (v4) from a software user perspective is evident in the paper's proposal of an automated framework that analyzes test cases and provides actionable recommendations for improvement. By empowering software users to enhance their testing capabilities and create more thorough test cases, the paper directly addresses the value of being capable in the context of software usage.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2670,ICSE-SEIP,Software Testing & QA,Using Natural Language Processing Techniques to Improve Manual Test Case Descriptions,"Despite the recent advancements in test automation, testing often remains a manual, and costly, activity in many industries. Manual test cases, often described only in natural language, consist of one or more test steps, which are instructions that must be performed to achieve the testing objective. Having different employees specifying test cases might result in redundant, unclear, or incomplete test cases. Manually reviewing and validating newly-specified test cases is time-consuming and becomes impractical in a scenario with a large test suite. Therefore, in this paper, we propose an automated framework to automatically analyze test cases that are specified in natural language and provide actionable recommendations on how to improve the test cases. Our framework consists of configurable components and modules for analysis, which are capable of recommending improvements to the following: (1) the terminology of a new test case through language modeling, (2) potentially missing test steps for a new test case through frequent itemset and association rule mining, and (3) recommendation of similar test cases that already exist in the test suite through text embedding and clustering. We thoroughly evaluated the three modules on data from our industry partner. Our framework can provide actionable recommendations, which is an important challenge given the widespread occurrence of test cases that are described only in natural language in the software industry (in particular, the game industry).",Self Direction,Curiosity,The paper's framework includes a module for recommending similar test cases that already exist in the test suite. This aligns with the value item Curiosity; as it encourages software users to explore and learn from existing test cases; fostering a sense of intellectual curiosity in the testing process.,"In the paper abstract, it is stated that the framework includes a module for recommending similar test cases that already exist in the test suite. This aligns with the value item v1.3 (Curiosity) and its corresponding value v1 (Self Direction) from a ""Software User"" perspective. By recommending similar test cases, the framework encourages software users to explore and learn from existing cases, which fosters a sense of intellectual curiosity in the testing process. This alignment directly addresses the value of self-directed exploration and creativity, as users are empowered to choose and investigate similar test cases to enhance their understanding and improve their own test cases.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2673,ICSE-SEIP,Software Engineering Practices,What&#x27;s bothering developers in code review?,"The practice of code review is widely adopted in industry and has been studied to an increasing degree in the research community. However, the developer experience of code review has received limited attention. Here, we report on initial results from a mixed-method exploratory study of the developer experience.",Achievement,Capable,The paper explores the developer experience of code review; which aligns with the value item Capable and its corresponding value Achievement. This is because code review is a practice that allows developers to demonstrate their competence and improve their skills in software development.,"I apologize for the confusion. The alignment of 'Paper X' with the value item v4.2 (Capable) and its corresponding value v4 (Achievement) is justified because code review is a platform for developers to showcase their competence and prove their abilities in software development. Through code review, developers have the opportunity to demonstrate their skills and knowledge, receive feedback to improve their work, and ultimately achieve success in their software development endeavors. This aligns with the value of Achievement as it highlights the importance of personal growth, accomplishment, and the pursuit of excellence for software users within a software context.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2673,ICSE-SEIP,Software Engineering Practices,What&#x27;s bothering developers in code review?,"The practice of code review is widely adopted in industry and has been studied to an increasing degree in the research community. However, the developer experience of code review has received limited attention. Here, we report on initial results from a mixed-method exploratory study of the developer experience.",Power,Social Recognition,The paper discusses the developer experience of code review; which aligns with the value item Social Recognition and its corresponding value Power. Code review is a way for developers to gain recognition and respect from their peers and demonstrate their authority in software development.,"In the context of software development, the practice of code review allows developers to showcase their expertise, knowledge, and skills in evaluating and providing feedback on code written by their peers. By participating in code reviews, developers aim to gain recognition and respect from their peers, establishing their authority and social recognition in the software development community. This aligns with the value item v5.1 (Social Recognition) and the corresponding value v5 (Power) in Schwartz's Taxonomy, as developers seek recognition and authority through their involvement in code review processes.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2673,ICSE-SEIP,Software Engineering Practices,What&#x27;s bothering developers in code review?,"The practice of code review is widely adopted in industry and has been studied to an increasing degree in the research community. However, the developer experience of code review has received limited attention. Here, we report on initial results from a mixed-method exploratory study of the developer experience.",Benevolence,Responsibility,The paper explores the developer experience of code review; which aligns with the value item Responsibility and its corresponding value Benevolence. Code review is a way for developers to take responsibility for their work and ensure the quality and reliability of the software they are developing.,"In the context of a software user, the alignment between 'Paper X' and the value item Responsibility (v9.8) from Schwartz's Taxonomy is evident. Code review, as explored in the paper, highlights the responsibility developers have in ensuring the quality and reliability of the software. By actively participating in code review processes, developers demonstrate their accountability and commitment to providing a dependable software experience for the users. This alignment reflects how developers take responsibility for their work and prioritize the well-being and satisfaction of software users.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2674,ICSE-SEIP,AI & Machine Learning,When Cyber-Physical Systems Meet AI: A Benchmark an Evaluation and a Way Forward,"Cyber-Physical Systems (CPS) have been broadly deployed in safety-critical domains, such as automotive systems, avionics, medical devices, etc. In recent years, Artificial Intelligence (AI) has been increasingly adopted to control CPS. Despite the popularity of AI-enabled CPS, few benchmarks are publicly available. There is also a lack of deep understanding on the performance and reliability of AI-enabled CPS across different industrial domains. To bridge this gap, we present a public benchmark of industry-level CPS in seven domains and build AI controllers for them via state-of-the-art deep reinforcement learning (DRL) methods. Based on that, we further perform a systematic evaluation of these AI-enabled systems with their traditional counterparts to identify current challenges and future opportunities. Our key findings include (1) AI controllers do not always outperform traditional controllers, (2) existing CPS testing techniques (falsification, specifically) fall short of analyzing AI-enabled CPS, and (3) building a hybrid system that strategically combines and switches between AI controllers and traditional controllers can achieve better performance across different domains. Our results highlight the need for new testing techniques for AI-enabled CPS and the need for more investigations into hybrid CPS to achieve optimal performance and reliability. Our benchmark, code, detailed evaluation results, and experiment scripts are available on https://sites.google.com/view/ai-cps-benchmark.",Achievement,Intelligent,The paper contributes to the achievement value item by presenting a benchmark and evaluation of AI-enabled CPS in various industrial domains. This aligns with the pursuit of intelligence and competence in software systems.,"The paper's contribution aligns with the value item v4.1 (Intelligent) and its corresponding value v4 (Achievement) from a ""Software User"" perspective because it presents a benchmark and evaluation of AI-enabled CPS across different industrial domains. By leveraging state-of-the-art deep reinforcement learning methods, the paper aims to build AI controllers for industry-level CPS, showcasing the pursuit of intelligence and competence in software systems. This aligns with the value of Achievement as it highlights the goal of achieving success, capability, and influence in the development and implementation of AI-enabled CPS.",Agreed-Clarified,Agree,,Agree,,
Exp A,Exp H,2674,ICSE-SEIP,AI & Machine Learning,When Cyber-Physical Systems Meet AI: A Benchmark an Evaluation and a Way Forward,"Cyber-Physical Systems (CPS) have been broadly deployed in safety-critical domains, such as automotive systems, avionics, medical devices, etc. In recent years, Artificial Intelligence (AI) has been increasingly adopted to control CPS. Despite the popularity of AI-enabled CPS, few benchmarks are publicly available. There is also a lack of deep understanding on the performance and reliability of AI-enabled CPS across different industrial domains. To bridge this gap, we present a public benchmark of industry-level CPS in seven domains and build AI controllers for them via state-of-the-art deep reinforcement learning (DRL) methods. Based on that, we further perform a systematic evaluation of these AI-enabled systems with their traditional counterparts to identify current challenges and future opportunities. Our key findings include (1) AI controllers do not always outperform traditional controllers, (2) existing CPS testing techniques (falsification, specifically) fall short of analyzing AI-enabled CPS, and (3) building a hybrid system that strategically combines and switches between AI controllers and traditional controllers can achieve better performance across different domains. Our results highlight the need for new testing techniques for AI-enabled CPS and the need for more investigations into hybrid CPS to achieve optimal performance and reliability. Our benchmark, code, detailed evaluation results, and experiment scripts are available on https://sites.google.com/view/ai-cps-benchmark.",Universalism,Protecting the Environment,"The paper aligns with the value item ""Protecting the Environment"" as it addresses the need for new testing techniques for AI-enabled CPS; which can contribute to the preservation of the environment by ensuring reliable and efficient systems.","In the paper, the authors highlight the need for new testing techniques for AI-enabled CPS, which can contribute to the preservation of the environment by ensuring reliable and efficient systems. By developing benchmarks and evaluating AI-enabled CPS in various domains, the paper aims to identify current challenges and future opportunities. This aligns with the value item ""Protecting the Environment"" (v10.4) from Schwartz's Taxonomy, as it emphasizes the importance of creating systems that are environmentally conscious and sustainable.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2682,ICSE-SEIP,Software Engineering Practices,Scaling Web API Integrations,"In ASAPP, a company that offers AI solutions to enterprise customers, internal services consume data from our customersaEUR(tm) web APIs. Implementing and maintaining integrations between our customersaEUR(tm) APIs and internal services is a major effort for the company. In this paper, we present a scalable approach for integrating web APIs in enterprise software that is lightweight and semi-automatic. It leverages a combination of Ontology-Based Data Access architectures (OBDA), a Domain Specific Language (DSL) called IBL, Natural Language Processing (NLP) models, and Automated Planning techniques. The OBDA architecture decouples our platform from our customersaEUR(tm) APIs via an ontology that acts as a single internal data access point. IBL is a functional and graphical DSL that enables domain experts to implement integrations, even if they donaEUR(tm)t have software development expertise. To reduce the effort of manually writing the IBL code, an NLP model suggests correspondences from each web API to the ontology. Given the API, ontology, and selected mappings for a set of desired fields from the ontology, we define an Automated Planning problem. The resulting policy is finally fed to a code synthesizer that generates the appropriate IBL method implementing the desired integration.This approach has been in production in ASAPP for 2 years with more than 300 integrations already implemented. Results indicate a a%0^ 50% reduction in effort due to implementing integrations with IBL. Preliminary results on the IBL automatic code generation show an encouraging further a%0^ 25% reduction so far.",Achievement,Capable,The paper presents a scalable approach for integrating web APIs in enterprise software; which enables software users to demonstrate their competence and intelligence in implementing these integrations. This aligns with the value item Capable and its corresponding value Achievement.,"My justification for aligning 'Paper X' with the value item v4.2 (Capable) and its corresponding value v4 (Achievement) is based on the fact that the paper presents a scalable approach for integrating web APIs in enterprise software. By enabling software users to implement these integrations, it gives them the opportunity to showcase their competence and intelligence in successfully completing this task. This aligns with the value item Capable, as it indicates the capability and expertise of software users in handling complex integration processes. Additionally, the achievement of successfully implementing these integrations aligns with the value of Achievement, as it represents the accomplishment of a challenging task.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2682,ICSE-SEIP,Software Engineering Practices,Scaling Web API Integrations,"In ASAPP, a company that offers AI solutions to enterprise customers, internal services consume data from our customersaEUR(tm) web APIs. Implementing and maintaining integrations between our customersaEUR(tm) APIs and internal services is a major effort for the company. In this paper, we present a scalable approach for integrating web APIs in enterprise software that is lightweight and semi-automatic. It leverages a combination of Ontology-Based Data Access architectures (OBDA), a Domain Specific Language (DSL) called IBL, Natural Language Processing (NLP) models, and Automated Planning techniques. The OBDA architecture decouples our platform from our customersaEUR(tm) APIs via an ontology that acts as a single internal data access point. IBL is a functional and graphical DSL that enables domain experts to implement integrations, even if they donaEUR(tm)t have software development expertise. To reduce the effort of manually writing the IBL code, an NLP model suggests correspondences from each web API to the ontology. Given the API, ontology, and selected mappings for a set of desired fields from the ontology, we define an Automated Planning problem. The resulting policy is finally fed to a code synthesizer that generates the appropriate IBL method implementing the desired integration.This approach has been in production in ASAPP for 2 years with more than 300 integrations already implemented. Results indicate a a%0^ 50% reduction in effort due to implementing integrations with IBL. Preliminary results on the IBL automatic code generation show an encouraging further a%0^ 25% reduction so far.",Security,Social Order,The paper describes how the OBDA architecture decouples the platform from customers' APIs by using an ontology as a single internal data access point. This ensures social order within the software system and aligns with the value item Social Order and its corresponding value Security.,"In 'Paper X', the use of the OBDA architecture to decouple the platform from customers' APIs and establish an ontology as a single internal data access point contributes to maintaining social order within the software system. This alignment with the value item Social Order from Schwartz's Taxonomy emphasizes the importance of organizing and structuring the software environment in a way that promotes stability, reliability, and predictability. By ensuring social order, the paper addresses the need for a secure and structured software system, which aligns with the value of Security. Overall, 'Paper X' directly aligns with the value item v6.3 (Social Order) and its corresponding value v6 (Security) from a ""Software User"" perspective.",Agreed-Clarified,Agree,,Agree,,
Exp A,Exp H,2682,ICSE-SEIP,Software Engineering Practices,Scaling Web API Integrations,"In ASAPP, a company that offers AI solutions to enterprise customers, internal services consume data from our customersaEUR(tm) web APIs. Implementing and maintaining integrations between our customersaEUR(tm) APIs and internal services is a major effort for the company. In this paper, we present a scalable approach for integrating web APIs in enterprise software that is lightweight and semi-automatic. It leverages a combination of Ontology-Based Data Access architectures (OBDA), a Domain Specific Language (DSL) called IBL, Natural Language Processing (NLP) models, and Automated Planning techniques. The OBDA architecture decouples our platform from our customersaEUR(tm) APIs via an ontology that acts as a single internal data access point. IBL is a functional and graphical DSL that enables domain experts to implement integrations, even if they donaEUR(tm)t have software development expertise. To reduce the effort of manually writing the IBL code, an NLP model suggests correspondences from each web API to the ontology. Given the API, ontology, and selected mappings for a set of desired fields from the ontology, we define an Automated Planning problem. The resulting policy is finally fed to a code synthesizer that generates the appropriate IBL method implementing the desired integration.This approach has been in production in ASAPP for 2 years with more than 300 integrations already implemented. Results indicate a a%0^ 50% reduction in effort due to implementing integrations with IBL. Preliminary results on the IBL automatic code generation show an encouraging further a%0^ 25% reduction so far.",Benevolence,Forgiving,The paper introduces the IBL DSL that enables domain experts without software development expertise to implement integrations. This highlights the helpfulness of software users towards other users or others; aligning with the value item Helpful and its corresponding value Benevolence.,"In the abstract, the paper specifically mentions that the IBL DSL enables domain experts without software development expertise to implement integrations. This indicates that the main contribution of the paper is focused on providing a tool or approach that allows software users to be helpful towards other users or others, aligning with the value item Helpful and its corresponding value Benevolence. By empowering non-experts to implement integrations, the paper enables software users to contribute to the development and functionality of enterprise software in a helpful and benevolent manner.",Agreed-Reconciled,Agree,,Agree,,
Exp A,Exp H,2683,ICSE-SEIP,Security & Privacy,DAppHunter: Identifying Inconsistent Behaviors of Blockchain-based Decentralized Applications,"A blockchain-based decentralized application (DApp) refers to an application typically using web pages or mobile applications as the front-end and smart contracts as the back-end. The front-end of the DApp helps users generate transactions and send them to the useraEUR(tm)s blockchain wallet. After the user signs and confirms the transaction using the blockchain wallet, the transaction will invoke the smart contract of the DApp. However, users bear the following risks when using DApps because of the potential inconsistent behaviors in DApps. First, the DApp front-end may generate incorrect transactions inconsistent with usersaEUR(tm) intentions. Second, the smart contract may have misbehaviors when executing the transactions. Inconsistent behaviors of DApps not only lead to user confusion but also cause significant financial losses. In this paper, we proposed a novel approach to identify inconsistent behaviors of DApps on EVM-compatible blockchains by contrasting the behaviors of DApps that derived from the front-end, blockchain wallet, and smart contracts, respectively. We implemented our approach into a prototype named DAppHunter. We have applied DAppHunter on 92 real-world DApps of Ethereum and Binance Smart Chain and successfully identified 37 DApps with inconsistent behaviors. We confirmed that 35 of them are scam DApps and over 5 million blockchain addresses are at risk of becoming victims of these inconsistent DApps.",Self Direction,Choosing Own Goals,The paper proposes a novel approach to identify inconsistent behaviors of DApps; which helps users to choose their own goals and have control over their transactions. This aligns with the value item Choosing Own Goals and its corresponding value Self Direction.,"The justification for labeling 'Paper X' with the value item v1.5 (Choosing Own Goals) and its corresponding value v1 (Self Direction) is based on the fact that the paper proposes a novel approach to identify inconsistent behaviors of DApps. By doing so, it empowers users to have control over their transactions and choose their own goals, ensuring that their intentions are not compromised. This aligns directly with the value of Self Direction, as it allows users to exercise their autonomy and make informed decisions in the software context.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2683,ICSE-SEIP,Security & Privacy,DAppHunter: Identifying Inconsistent Behaviors of Blockchain-based Decentralized Applications,"A blockchain-based decentralized application (DApp) refers to an application typically using web pages or mobile applications as the front-end and smart contracts as the back-end. The front-end of the DApp helps users generate transactions and send them to the useraEUR(tm)s blockchain wallet. After the user signs and confirms the transaction using the blockchain wallet, the transaction will invoke the smart contract of the DApp. However, users bear the following risks when using DApps because of the potential inconsistent behaviors in DApps. First, the DApp front-end may generate incorrect transactions inconsistent with usersaEUR(tm) intentions. Second, the smart contract may have misbehaviors when executing the transactions. Inconsistent behaviors of DApps not only lead to user confusion but also cause significant financial losses. In this paper, we proposed a novel approach to identify inconsistent behaviors of DApps on EVM-compatible blockchains by contrasting the behaviors of DApps that derived from the front-end, blockchain wallet, and smart contracts, respectively. We implemented our approach into a prototype named DAppHunter. We have applied DAppHunter on 92 real-world DApps of Ethereum and Binance Smart Chain and successfully identified 37 DApps with inconsistent behaviors. We confirmed that 35 of them are scam DApps and over 5 million blockchain addresses are at risk of becoming victims of these inconsistent DApps.",Power,Social Power,The paper aims to identify scam DApps with inconsistent behaviors; which can prevent users from being victims and preserve their public image. This aligns with the value item Social Power and its corresponding value Power.,"The justification for aligning 'Paper X' with the value item v5.4 (Social Power) and its corresponding value v5 (Power) is that the paper addresses the issue of inconsistent behaviors in DApps, particularly in relation to scam DApps. By identifying and exposing these inconsistent behaviors, the paper empowers software users to protect themselves from potential financial losses and maintain their social power by preserving their public image. The ability to discern and avoid scam DApps gives software users the power to make informed decisions and establish a sense of control over their digital interactions, aligning with the value of power as defined in Schwartz's Taxonomy.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2683,ICSE-SEIP,Security & Privacy,DAppHunter: Identifying Inconsistent Behaviors of Blockchain-based Decentralized Applications,"A blockchain-based decentralized application (DApp) refers to an application typically using web pages or mobile applications as the front-end and smart contracts as the back-end. The front-end of the DApp helps users generate transactions and send them to the useraEUR(tm)s blockchain wallet. After the user signs and confirms the transaction using the blockchain wallet, the transaction will invoke the smart contract of the DApp. However, users bear the following risks when using DApps because of the potential inconsistent behaviors in DApps. First, the DApp front-end may generate incorrect transactions inconsistent with usersaEUR(tm) intentions. Second, the smart contract may have misbehaviors when executing the transactions. Inconsistent behaviors of DApps not only lead to user confusion but also cause significant financial losses. In this paper, we proposed a novel approach to identify inconsistent behaviors of DApps on EVM-compatible blockchains by contrasting the behaviors of DApps that derived from the front-end, blockchain wallet, and smart contracts, respectively. We implemented our approach into a prototype named DAppHunter. We have applied DAppHunter on 92 real-world DApps of Ethereum and Binance Smart Chain and successfully identified 37 DApps with inconsistent behaviors. We confirmed that 35 of them are scam DApps and over 5 million blockchain addresses are at risk of becoming victims of these inconsistent DApps.",Benevolence,Helpful,By identifying scam DApps; the paper contributes to software that promotes helpfulness among software users towards others; protecting them from financial losses. This aligns with the value item Helpful and its corresponding value Benevolence.,"The justification for aligning 'Paper X' with the value item v9.3 (Helpful) and its corresponding value v9 (Benevolence) is based on the paper's contribution in identifying scam DApps and protecting software users from financial losses. By addressing the inconsistent behaviors of DApps and identifying those with malicious intent, the paper helps create software that promotes helpfulness among users towards others, emphasizing their well-being and safeguarding them from potential harm. This aligns with the value of Benevolence, as it emphasizes the importance of software contributing to the welfare and protection of users.",Agreed-Clarified,Agree,,Agree,,
Exp A,Exp H,2684,ICSE-SEIP,Software Testing & QA,Evolutionary Approach for Concurrency Testing of Ripple Blockchain Consensus Algorithm,"Blockchain systems are prone to concurrency bugs due to the nondeterminism in the delivery order of messages between the distributed nodes. These bugs are hard to detect since they can only be triggered by a specific order or timing of concurrent events in the execution. Systematic concurrency testing techniques, which explore all possible delivery orderings of messages to uncover concurrency bugs, are not scalable to large distributed systems such as blockchains. Random concurrency testing methods search for bugs in a randomly generated set of executions and offer a practical testing method.In this paper, we investigate the effectiveness of random concurrency testing on blockchain systems using a case study on the XRP Ledger of the Ripple blockchain, which maintains one of the most popular cryptocurrencies in the market today. We test the Ripple consensus algorithm of the XRP Ledger by exploring different delivery orderings of consensus protocol messages. Moreover, we design an evolutionary algorithm to guide the random test case generation toward certain system behaviors to discover concurrency bugs more efficiently. Our case study shows that random concurrency testing is effective at detecting concurrency bugs in blockchains, and the evolutionary approach for test generation improves test efficiency. Our experiments could successfully detect the bugs we seeded in the Ripple source code. Moreover, we discovered a previously unknown concurrency bug in the production implementation of Ripple.",Achievement,Successful,"Paper X contributes to the value item ""Achievement"" through its investigation of the effectiveness of random concurrency testing on blockchain systems. The paper demonstrates the ability of this testing method to detect concurrency bugs in blockchains; including a previously unknown bug in the production implementation of Ripple.","The justification for aligning 'Paper X' with the value item v4.3 and its corresponding value v4 from a ""Software User"" perspective is based on the paper's investigation of the effectiveness of random concurrency testing on blockchain systems. By demonstrating that this testing method can successfully detect concurrency bugs, including discovering a previously unknown bug in the production implementation of Ripple, the paper highlights the achievement of improving the reliability and security of blockchain systems. This aligns with the value of Achievement as it emphasizes the importance of intelligence, capability, and success in the software context, ultimately benefiting the software user by ensuring the integrity of the blockchain system they interact with.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2684,ICSE-SEIP,Software Testing & QA,Evolutionary Approach for Concurrency Testing of Ripple Blockchain Consensus Algorithm,"Blockchain systems are prone to concurrency bugs due to the nondeterminism in the delivery order of messages between the distributed nodes. These bugs are hard to detect since they can only be triggered by a specific order or timing of concurrent events in the execution. Systematic concurrency testing techniques, which explore all possible delivery orderings of messages to uncover concurrency bugs, are not scalable to large distributed systems such as blockchains. Random concurrency testing methods search for bugs in a randomly generated set of executions and offer a practical testing method.In this paper, we investigate the effectiveness of random concurrency testing on blockchain systems using a case study on the XRP Ledger of the Ripple blockchain, which maintains one of the most popular cryptocurrencies in the market today. We test the Ripple consensus algorithm of the XRP Ledger by exploring different delivery orderings of consensus protocol messages. Moreover, we design an evolutionary algorithm to guide the random test case generation toward certain system behaviors to discover concurrency bugs more efficiently. Our case study shows that random concurrency testing is effective at detecting concurrency bugs in blockchains, and the evolutionary approach for test generation improves test efficiency. Our experiments could successfully detect the bugs we seeded in the Ripple source code. Moreover, we discovered a previously unknown concurrency bug in the production implementation of Ripple.",Power,Social Recognition,"The paper aligns with the value item ""Power"" through its exploration of the delivery orderings of consensus protocol messages in the Ripple blockchain. By designing an evolutionary algorithm to guide test case generation; the paper aims to improve the efficiency of detecting concurrency bugs. This emphasis on influencing and controlling the process aligns with the value of ""Social Recognition"" under the Power value item.","The alignment of 'Paper X' with the value item ""Power"" is justified based on its emphasis on influencing and controlling the process of detecting concurrency bugs in blockchain systems. By designing an evolutionary algorithm to guide test case generation, the paper aims to improve the efficiency of bug detection, aligning with the value of ""Social Recognition"" under the Power value item. This demonstrates the paper's focus on exerting influence and authority in the software context, contributing to the value of Power from a Software User perspective.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2685,ICSE-SEIP,Software Engineering Practices,A Model for Understanding and Reducing Developer Burnout,"Job burnout is a type of work-related stress associated with a state of physical or emotional exhaustion that also involves a sense of reduced accomplishment and loss of personal identity. Burnt out can affect oneaEUR(tm)s physical and mental health and has become a leading industry concern and can result in high workforce turnover. Through an empirical study at Globant, a large multi-national company, we created a theoretical model to evaluate the complex interplay among organizational culture, work satisfaction, and team climate, and how they impact developer burnout. We conducted a survey of developers in software delivery teams (n&#x3D;3,281) to test our model and analyzed the data using structural equation modeling, moderation, and multi-group analysis. Our results show that Organizational Culture, Climate for Learning, Sense of Belonging, and Inclusiveness are positively associated with Work Satisfaction, which in turn is associated with Reduced Burnout. Our model generated through a large-scale survey can guide organizations in how to reduce workforce burnout by creating a climate for learning, inclusiveness in teams, and a generative organizational culture where new ideas are welcome, information is actively sought and bad news can be shared without fear.",Security,Sense of Belonging,The paper contributes to creating a sense of belonging within software delivery teams; which aligns with the value item Sense of Belonging and its corresponding value Security.,"The paper's contribution to creating a sense of belonging within software delivery teams aligns with the value item of Sense of Belonging and its corresponding value of Security because when software users feel a sense of belonging, they perceive a secure environment where they belong and are accepted. This contributes to their overall sense of security and enhances their experience as software users within the team.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2685,ICSE-SEIP,Software Engineering Practices,A Model for Understanding and Reducing Developer Burnout,"Job burnout is a type of work-related stress associated with a state of physical or emotional exhaustion that also involves a sense of reduced accomplishment and loss of personal identity. Burnt out can affect oneaEUR(tm)s physical and mental health and has become a leading industry concern and can result in high workforce turnover. Through an empirical study at Globant, a large multi-national company, we created a theoretical model to evaluate the complex interplay among organizational culture, work satisfaction, and team climate, and how they impact developer burnout. We conducted a survey of developers in software delivery teams (n&#x3D;3,281) to test our model and analyzed the data using structural equation modeling, moderation, and multi-group analysis. Our results show that Organizational Culture, Climate for Learning, Sense of Belonging, and Inclusiveness are positively associated with Work Satisfaction, which in turn is associated with Reduced Burnout. Our model generated through a large-scale survey can guide organizations in how to reduce workforce burnout by creating a climate for learning, inclusiveness in teams, and a generative organizational culture where new ideas are welcome, information is actively sought and bad news can be shared without fear.",Achievement,Ambitious,The paper emphasizes the importance of creating a generative organizational culture where new ideas are welcome and actively sought. This aligns with the value item Ambitious and its corresponding value Achievement.,"The alignment between the paper's emphasis on creating a generative organizational culture that welcomes new ideas and actively seeks them and the value item Ambitious and its corresponding value Achievement can be justified by considering that an ambitious individual seeks to challenge the status quo and strive for success. By promoting a climate for learning, inclusiveness, and a generative organizational culture, the paper aligns with the ambition of individuals to achieve their goals and make a significant impact within the software context.",Agreed-Clarified,Agree,,Agree,,
Exp A,Exp H,2688,ICSE-SEIP,Code Generation & Analysis,Daisy: Effective Fuzz Driver Synthesis with Object Usage Sequence Analysis,"Fuzzing is increasingly used in industrial settings for vulnerability detection due to its scalability and effectiveness. Libraries require driver programs to feed the fuzzer-generated inputs into library-provided interfaces. Writing such drivers manually is tedious and error-prone, thus greatly hindering the widespread use of fuzzing in practical situations. Previous attempts at automatic driver synthesis perform static analysis on the libraries and their consumers. However, a lack of dynamic object usage information renders them ineffective at generating interface function calls with correct parameters and meaningful sequences. This severely limits fuzzingaEUR(tm)s bug-finding capabilities and can produce faulty drivers.In this paper, we propose Daisy, a driver synthesis framework, which extracts dynamic object usage sequences of library consumers to synthesize significantly more effective drivers. Daisy uses the following two steps to synthesize a fuzz driver for a library. First, it models each objectaEUR(tm)s behaviors into an object usage sequence during the execution of its consumers. Next, it merges all the extracted sequences and constructs a series of interface calls with valid object usages based on the merged sequence. We implemented Daisy and evaluated its effectiveness on real-world libraries selected from both the Android Open Source Project (AOSP) and GoogleaEUR(tm)s FuzzBench. DaisyaEUR(tm)s synthesized drivers significantly outperform drivers produced by other state-of-the-art fuzz driver synthesizers. In addition, on applying Daisy to the latest versions of those extensively-fuzzed real-world libraries of the benchmark, e.g. libaom and freetype2, we also found 9 previously-unknown bugs with 3 CVEs assigned.",Achievement,Influential,The paper proposes a driver synthesis framework called Daisy that improves the bug-finding capabilities of fuzzing. By synthesizing more effective drivers; Daisy contributes to the software user's pursuit of influence and recognition; aligning with the value item Influential and its corresponding value Achievement.,"In the paper, Daisy is presented as a driver synthesis framework that improves the bug-finding capabilities of fuzzing by synthesizing more effective drivers. By enhancing the effectiveness of fuzzing and enabling the discovery of previously unknown bugs, Daisy empowers software users with the ability to influence and make an impact in the software development process. Through the value item v4.4 (Influential) and its corresponding value v4 (Achievement), the paper's contribution directly aligns with the software user's pursuit of recognition and influence in the software domain.",Agreed-Clarified,Agree,,Agree,,
Exp A,Exp H,2690,ICSE-SEIP,Code Generation & Analysis,Scalable Compositional Static Taint Analysis for Sensitive Data Tracing on Industrial Micro-Services,"In recent years, there has been an increasing demand for sensitive data tracing for industrial microservices; these include change of governance, data breach detection, to data consistency validation. As an information tracking technique, Taint analysis is widely used to address these demands. This paper aims to share our experience in developing a scalable static taint analyzer on sensitive data tracing for large-scale industrial microservices. Although several taint analyzers have been proposed for Java applications, our experiments show that existing approaches are inefficient and/or ineffective (in terms of low recall/precision rates) for analyzing large-scale industrial microservices.Instead, we present CFTaint, a compositional field-based taint analyzer, to address the challenges for popular microservices running on industrial Fintech applications. CFTaint improves scalability by using a fast compositional function summary, which summarizes the data propagation of each function during the on-the-fly taint analysis. CFTaint also uses a novel filed-based algorithm to analyze the taint propagation based on specified sensitive fields to reduce false negatives. Our field-based algorithm maximizes the soundness of our approach even when the taint tracking is performed on an unsound call graph. Furthermore, we also propose an efficient code transformation method to model the behaviours of the containers, which allows our analysis to trace data propagation in a container environment. Experiments on numerous production microservices demonstrate the high recall (96.09%) rates and precision (93.51% for tracing sensitive data) of CFTaint with low time complexity (121.73 seconds).",Self Direction,Curiosity,The paper presents CFTaint; a compositional field-based taint analyzer that improves scalability. This aligns with the value item Curiosity; as the paper demonstrates an innovative approach to address the challenges in analyzing large-scale industrial microservices.,"I apologize for the confusion. The justification for aligning the main contribution of 'Paper X' with the value item Curiosity is based on the fact that the paper presents a new approach, CFTaint, which is a compositional field-based taint analyzer. This innovative technique addresses the challenges of analyzing large-scale industrial microservices. By introducing a novel method and demonstrating its effectiveness, the paper encourages curiosity and exploration among software users who are interested in finding new solutions to complex problems.",Agreed-Clarified,Agree,,Agree,,
Exp A,Exp H,2690,ICSE-SEIP,Code Generation & Analysis,Scalable Compositional Static Taint Analysis for Sensitive Data Tracing on Industrial Micro-Services,"In recent years, there has been an increasing demand for sensitive data tracing for industrial microservices; these include change of governance, data breach detection, to data consistency validation. As an information tracking technique, Taint analysis is widely used to address these demands. This paper aims to share our experience in developing a scalable static taint analyzer on sensitive data tracing for large-scale industrial microservices. Although several taint analyzers have been proposed for Java applications, our experiments show that existing approaches are inefficient and/or ineffective (in terms of low recall/precision rates) for analyzing large-scale industrial microservices.Instead, we present CFTaint, a compositional field-based taint analyzer, to address the challenges for popular microservices running on industrial Fintech applications. CFTaint improves scalability by using a fast compositional function summary, which summarizes the data propagation of each function during the on-the-fly taint analysis. CFTaint also uses a novel filed-based algorithm to analyze the taint propagation based on specified sensitive fields to reduce false negatives. Our field-based algorithm maximizes the soundness of our approach even when the taint tracking is performed on an unsound call graph. Furthermore, we also propose an efficient code transformation method to model the behaviours of the containers, which allows our analysis to trace data propagation in a container environment. Experiments on numerous production microservices demonstrate the high recall (96.09%) rates and precision (93.51% for tracing sensitive data) of CFTaint with low time complexity (121.73 seconds).",Achievement,Intelligent,The paper proposes CFTaint; a taint analyzer for sensitive data tracing in industrial microservices. This aligns with the value item Intelligent; as the paper contributes a solution that requires sophisticated analysis techniques to achieve accurate and efficient results.,"The main contribution of 'Paper X' aligns with the value item 'Intelligent' because it introduces CFTaint, a taint analyzer that utilizes advanced analysis techniques to accurately and efficiently trace sensitive data in industrial microservices. By developing a compositional field-based algorithm and implementing a fast compositional function summary, the paper demonstrates a sophisticated approach that requires intelligence and expertise in order to achieve the desired results. This aligns with the value of intelligence as it emphasizes the use of smart and clever techniques to address the challenges in sensitive data tracing.",Agreed-Clarified,Agree,,Agree,,
Exp A,Exp H,2690,ICSE-SEIP,Code Generation & Analysis,Scalable Compositional Static Taint Analysis for Sensitive Data Tracing on Industrial Micro-Services,"In recent years, there has been an increasing demand for sensitive data tracing for industrial microservices; these include change of governance, data breach detection, to data consistency validation. As an information tracking technique, Taint analysis is widely used to address these demands. This paper aims to share our experience in developing a scalable static taint analyzer on sensitive data tracing for large-scale industrial microservices. Although several taint analyzers have been proposed for Java applications, our experiments show that existing approaches are inefficient and/or ineffective (in terms of low recall/precision rates) for analyzing large-scale industrial microservices.Instead, we present CFTaint, a compositional field-based taint analyzer, to address the challenges for popular microservices running on industrial Fintech applications. CFTaint improves scalability by using a fast compositional function summary, which summarizes the data propagation of each function during the on-the-fly taint analysis. CFTaint also uses a novel filed-based algorithm to analyze the taint propagation based on specified sensitive fields to reduce false negatives. Our field-based algorithm maximizes the soundness of our approach even when the taint tracking is performed on an unsound call graph. Furthermore, we also propose an efficient code transformation method to model the behaviours of the containers, which allows our analysis to trace data propagation in a container environment. Experiments on numerous production microservices demonstrate the high recall (96.09%) rates and precision (93.51% for tracing sensitive data) of CFTaint with low time complexity (121.73 seconds).",Security,Social Order,The paper introduces CFTaint; a taint analyzer for data tracing in microservices; which contributes to maintaining social order within the software context. This aligns with the value item Social Order and its corresponding value Security.,"The paper's contribution of CFTaint, a taint analyzer for data tracing in microservices, aligns with the value item Social Order and its corresponding value Security because it helps maintain the integrity and stability of the software systems by ensuring that sensitive data is traced and protected, ultimately leading to a more secure and organized software environment.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2691,ICSE-SEIP,Software Testing & QA,Simulation-Driven Automated End-to-End Test and Oracle Inference,"This is the first work to report on inferential testing at scale in industry. Specifically, it reports the experience of automated testing of integrity systems at Meta. We built an internal tool called ALPACAS for automated inference of end-to-end integrity tests. Integrity tests are designed to keep users safe online by checking that interventions take place when harmful behaviour occurs on a platform. ALPACAS infers not only the test input, but also the oracle, by observing production interventions to prevent harmful behaviour. This approach allows Meta to automate the process of generating integrity tests for its platforms, such as Facebook and Instagram, which consist of hundreds of millions of lines of production code. We outline the design and deployment of ALPACAS, and report results for its coverage, number of tests produced at each stage of the test inference process, and their pass rates. Specifically, we demonstrate that using ALPACAS significantly improves coverage from a manual test design for the particular aspect of integrity end-to-end testing it was applied to. Further, from a pool of 3 million data points, ALPACAS automatically yields 39 production-ready end-to-end integrity tests. We also report that the ALPACAS-inferred test suite enjoys exceptionally low flakiness for end-to-end testing with its average in-production pass rate of 99.84%.",Achievement,Capable,The paper's contribution of ALPACAS; an internal tool for automated inference of end-to-end integrity tests; aligns with the value item Capable and its corresponding value Achievement. The development of ALPACAS demonstrates the software user's aspiration to excel in automated testing and the demonstration of software user competence in ensuring the integrity of online platforms like Facebook and Instagram.,"The development of ALPACAS, an internal tool for automated inference of end-to-end integrity tests, aligns with the value item Capable and its corresponding value Achievement from a ""Software User"" perspective. By automating the process of generating integrity tests for platforms like Facebook and Instagram, ALPACAS demonstrates the software user's ability to excel in automated testing and ensures the competence of the software user in maintaining the integrity of these online platforms. This alignment with Capable and Achievement highlights the software user's desire to accomplish tasks efficiently and effectively, contributing to their overall sense of achievement and success in the software context.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2691,ICSE-SEIP,Software Testing & QA,Simulation-Driven Automated End-to-End Test and Oracle Inference,"This is the first work to report on inferential testing at scale in industry. Specifically, it reports the experience of automated testing of integrity systems at Meta. We built an internal tool called ALPACAS for automated inference of end-to-end integrity tests. Integrity tests are designed to keep users safe online by checking that interventions take place when harmful behaviour occurs on a platform. ALPACAS infers not only the test input, but also the oracle, by observing production interventions to prevent harmful behaviour. This approach allows Meta to automate the process of generating integrity tests for its platforms, such as Facebook and Instagram, which consist of hundreds of millions of lines of production code. We outline the design and deployment of ALPACAS, and report results for its coverage, number of tests produced at each stage of the test inference process, and their pass rates. Specifically, we demonstrate that using ALPACAS significantly improves coverage from a manual test design for the particular aspect of integrity end-to-end testing it was applied to. Further, from a pool of 3 million data points, ALPACAS automatically yields 39 production-ready end-to-end integrity tests. We also report that the ALPACAS-inferred test suite enjoys exceptionally low flakiness for end-to-end testing with its average in-production pass rate of 99.84%.",Benevolence,Helpful,The paper's contribution of ALPACAS; which automates the process of generating integrity tests; aligns with the value item Helpful and its corresponding value Benevolence. By automating the generation of integrity tests and checking for harmful behavior on the platform; ALPACAS helps in keeping users safe online; contributing to the well-being and welfare of software users.,"In the paper, the authors specifically state that the purpose of integrity tests is to keep users safe online by checking for harmful behavior on the platform. By automating the process of generating these tests, ALPACAS contributes to the well-being and welfare of software users. This aligns with the value item ""Helpful"" from Schwartz's Taxonomy, which emphasizes being helpful and contributing to the welfare and well-being of others. Therefore, the alignment between ALPACAS and the value item v9.3 from a ""Software User"" perspective is evident.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2691,ICSE-SEIP,Software Testing & QA,Simulation-Driven Automated End-to-End Test and Oracle Inference,"This is the first work to report on inferential testing at scale in industry. Specifically, it reports the experience of automated testing of integrity systems at Meta. We built an internal tool called ALPACAS for automated inference of end-to-end integrity tests. Integrity tests are designed to keep users safe online by checking that interventions take place when harmful behaviour occurs on a platform. ALPACAS infers not only the test input, but also the oracle, by observing production interventions to prevent harmful behaviour. This approach allows Meta to automate the process of generating integrity tests for its platforms, such as Facebook and Instagram, which consist of hundreds of millions of lines of production code. We outline the design and deployment of ALPACAS, and report results for its coverage, number of tests produced at each stage of the test inference process, and their pass rates. Specifically, we demonstrate that using ALPACAS significantly improves coverage from a manual test design for the particular aspect of integrity end-to-end testing it was applied to. Further, from a pool of 3 million data points, ALPACAS automatically yields 39 production-ready end-to-end integrity tests. We also report that the ALPACAS-inferred test suite enjoys exceptionally low flakiness for end-to-end testing with its average in-production pass rate of 99.84%.",Universalism,Protecting the Environment,The paper's contribution of ALPACAS; which infers the oracle for integrity tests by observing production interventions to prevent harmful behavior; aligns with the value item Protecting the Environment and its corresponding value Universalism. By checking for harmful behavior and enforcing interventions; ALPACAS contributes to the preservation of a safe and healthy online environment; exemplifying a broader concern for ecological well-being in the software context.,"In the paper abstract, 'Paper X' describes how ALPACAS, an automated testing tool, infers the oracle for integrity tests by observing production interventions to prevent harmful behavior. This aligns with the value item v10.4, ""Protecting the Environment,"" and its corresponding value v10, ""Universalism,"" as the tool's purpose is to ensure a safe and healthy online environment. By checking for harmful behavior and enforcing interventions, ALPACAS contributes to the preservation of a secure online space, exemplifying a broader concern for ecological well-being in the software context.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2691,ICSE-SEIP,Software Testing & QA,Simulation-Driven Automated End-to-End Test and Oracle Inference,"This is the first work to report on inferential testing at scale in industry. Specifically, it reports the experience of automated testing of integrity systems at Meta. We built an internal tool called ALPACAS for automated inference of end-to-end integrity tests. Integrity tests are designed to keep users safe online by checking that interventions take place when harmful behaviour occurs on a platform. ALPACAS infers not only the test input, but also the oracle, by observing production interventions to prevent harmful behaviour. This approach allows Meta to automate the process of generating integrity tests for its platforms, such as Facebook and Instagram, which consist of hundreds of millions of lines of production code. We outline the design and deployment of ALPACAS, and report results for its coverage, number of tests produced at each stage of the test inference process, and their pass rates. Specifically, we demonstrate that using ALPACAS significantly improves coverage from a manual test design for the particular aspect of integrity end-to-end testing it was applied to. Further, from a pool of 3 million data points, ALPACAS automatically yields 39 production-ready end-to-end integrity tests. We also report that the ALPACAS-inferred test suite enjoys exceptionally low flakiness for end-to-end testing with its average in-production pass rate of 99.84%.",Stimulation,Variation in Life,The paper's contribution of ALPACAS; which significantly improves coverage and produces a large number of end-to-end integrity tests; aligns with the value item Variation in Life and its corresponding value Stimulation. The development and usage of ALPACAS introduce variation and excitement in the testing process; stimulating the software user's experience and providing new and automated ways to ensure platform integrity.,"In the paper, the development and usage of ALPACAS for automated inference of end-to-end integrity tests significantly improve coverage and produce a large number of tests. This aligns with the value item ""Variation in Life"" and its corresponding value ""Stimulation"" from a ""Software User"" perspective. By introducing variation and excitement in the testing process, ALPACAS stimulates the software user's experience by providing new and automated ways to ensure platform integrity, enhancing their overall interaction and satisfaction with the software.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2693,ICSE-SEIP,Software Engineering Practices,CONAN: Diagnosing Batch Failures for Cloud Systems,"Failure diagnosis is critical to the maintenance of large-scale cloud systems, which has attracted tremendous attention from academia and industry over the last decade. In this paper, we focus on diagnosing batch failures, which occur to a batch of instances of the same subject (e.g., API requests, VMs, nodes, etc.), resulting in degraded service availability and performance. Manual investigation over a large volume of high-dimensional telemetry data (e.g., logs, traces, and metrics) is labor-intensive and time-consuming, like finding a needle in a haystack. Meanwhile, existing proposed approaches are usually tailored for specific scenarios, which hinders their applications in diverse scenarios. According to our experience with Azure and Microsoft 365 aEUR"" two world-leading cloud systems, when batch failures happen, the procedure of finding the root cause can be abstracted as looking for contrast patterns by comparing two groups of instances, such as failed vs. succeeded, slow vs. normal, or during vs. before an anomaly. We thus propose CONAN, an efficient and flexible framework that can automatically extract contrast patterns from contextual data. CONAN has been successfully integrated into multiple diagnostic tools for various products, which proves its usefulness in diagnosing real-world batch failures.",Achievement,Successful,The paper proposes a framework called CONAN that can automatically extract contrast patterns from contextual data for diagnosing batch failures in cloud systems. This aligns with the value item Successful from the Achievement value; as the framework aims to improve the success rate of diagnosing failures in large-scale cloud systems.,"In the paper abstract, it is explicitly stated that the proposed framework CONAN has been successfully integrated into multiple diagnostic tools for various products, proving its usefulness in diagnosing real-world batch failures. This directly aligns with the value item v4.3 ""Successful"" from the Achievement value. By improving the success rate of diagnosing failures in large-scale cloud systems, CONAN contributes to the achievement of successful outcomes in the context of software usage, which is highly desirable from a software user's perspective.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2693,ICSE-SEIP,Software Engineering Practices,CONAN: Diagnosing Batch Failures for Cloud Systems,"Failure diagnosis is critical to the maintenance of large-scale cloud systems, which has attracted tremendous attention from academia and industry over the last decade. In this paper, we focus on diagnosing batch failures, which occur to a batch of instances of the same subject (e.g., API requests, VMs, nodes, etc.), resulting in degraded service availability and performance. Manual investigation over a large volume of high-dimensional telemetry data (e.g., logs, traces, and metrics) is labor-intensive and time-consuming, like finding a needle in a haystack. Meanwhile, existing proposed approaches are usually tailored for specific scenarios, which hinders their applications in diverse scenarios. According to our experience with Azure and Microsoft 365 aEUR"" two world-leading cloud systems, when batch failures happen, the procedure of finding the root cause can be abstracted as looking for contrast patterns by comparing two groups of instances, such as failed vs. succeeded, slow vs. normal, or during vs. before an anomaly. We thus propose CONAN, an efficient and flexible framework that can automatically extract contrast patterns from contextual data. CONAN has been successfully integrated into multiple diagnostic tools for various products, which proves its usefulness in diagnosing real-world batch failures.",Power,Authority,The CONAN framework proposed in the paper has been successfully integrated into multiple diagnostic tools for various products; indicating its usefulness in diagnosing real-world batch failures. This aligns with the value item Authority from the Power value; as the framework empowers software users to have authority in diagnosing and resolving failures.,"In 'Paper X', the CONAN framework empowers software users by providing them with the authority to diagnose and resolve batch failures in real-world scenarios. This aligns with the value item Authority from the Power value in Schwartz's Taxonomy. By integrating CONAN into diagnostic tools, software users are given the power and control to identify the root causes of failures, leading to improved service availability and performance. This direct alignment demonstrates the significance of the paper's contribution in enabling software users to have authority and take charge of failure diagnosis in cloud systems.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2693,ICSE-SEIP,Software Engineering Practices,CONAN: Diagnosing Batch Failures for Cloud Systems,"Failure diagnosis is critical to the maintenance of large-scale cloud systems, which has attracted tremendous attention from academia and industry over the last decade. In this paper, we focus on diagnosing batch failures, which occur to a batch of instances of the same subject (e.g., API requests, VMs, nodes, etc.), resulting in degraded service availability and performance. Manual investigation over a large volume of high-dimensional telemetry data (e.g., logs, traces, and metrics) is labor-intensive and time-consuming, like finding a needle in a haystack. Meanwhile, existing proposed approaches are usually tailored for specific scenarios, which hinders their applications in diverse scenarios. According to our experience with Azure and Microsoft 365 aEUR"" two world-leading cloud systems, when batch failures happen, the procedure of finding the root cause can be abstracted as looking for contrast patterns by comparing two groups of instances, such as failed vs. succeeded, slow vs. normal, or during vs. before an anomaly. We thus propose CONAN, an efficient and flexible framework that can automatically extract contrast patterns from contextual data. CONAN has been successfully integrated into multiple diagnostic tools for various products, which proves its usefulness in diagnosing real-world batch failures.",Security,Family Security,By proposing the CONAN framework; the paper addresses the labor-intensive and time-consuming nature of manual investigation over a large volume of telemetry data for diagnosing batch failures. This aligns with the value item Family Security from the Security value; as the framework helps to ensure the availability and performance of cloud systems; providing a sense of security to software users.,"In the abstract of 'Paper X', it is mentioned that the CONAN framework proposed in the paper addresses the labor-intensive and time-consuming nature of manual investigation for diagnosing batch failures. This aligns with the value item v6.2 (Family Security) and its corresponding value v6 (Security) from a ""Software User"" perspective because the framework helps ensure the availability and performance of cloud systems, providing a sense of security to software users who rely on these systems for their work.",Agreed-Justified,Disagree,"Related to Achievement: capable more, as it automatically identifies failures",Disagree,This is not aligns with the value item Family Security from the Security value; as the framework helps to ensure the availability and performance of cloud systems,
Exp A,Exp H,2693,ICSE-SEIP,Software Engineering Practices,CONAN: Diagnosing Batch Failures for Cloud Systems,"Failure diagnosis is critical to the maintenance of large-scale cloud systems, which has attracted tremendous attention from academia and industry over the last decade. In this paper, we focus on diagnosing batch failures, which occur to a batch of instances of the same subject (e.g., API requests, VMs, nodes, etc.), resulting in degraded service availability and performance. Manual investigation over a large volume of high-dimensional telemetry data (e.g., logs, traces, and metrics) is labor-intensive and time-consuming, like finding a needle in a haystack. Meanwhile, existing proposed approaches are usually tailored for specific scenarios, which hinders their applications in diverse scenarios. According to our experience with Azure and Microsoft 365 aEUR"" two world-leading cloud systems, when batch failures happen, the procedure of finding the root cause can be abstracted as looking for contrast patterns by comparing two groups of instances, such as failed vs. succeeded, slow vs. normal, or during vs. before an anomaly. We thus propose CONAN, an efficient and flexible framework that can automatically extract contrast patterns from contextual data. CONAN has been successfully integrated into multiple diagnostic tools for various products, which proves its usefulness in diagnosing real-world batch failures.",Benevolence,Helpful,The CONAN framework contributes to improving the diagnosing capabilities of software users; enabling them to effectively identify and resolve batch failures. This aligns with the value item Helpful from the Benevolence value; as the framework allows software users to be helpful towards others by ensuring the availability and performance of cloud systems.,"I apologize for any confusion. My justification is that the CONAN framework, proposed in 'Paper X', aligns with the value item v9.3 (Helpful) and its corresponding value v9 (Benevolence) from a ""Software User"" perspective because the framework enables software users to be helpful towards others by facilitating the identification and resolution of batch failures in cloud systems. This helps to ensure the availability and performance of the systems, benefiting both users and potentially other stakeholders who rely on the services provided by those systems.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2693,ICSE-SEIP,Software Engineering Practices,CONAN: Diagnosing Batch Failures for Cloud Systems,"Failure diagnosis is critical to the maintenance of large-scale cloud systems, which has attracted tremendous attention from academia and industry over the last decade. In this paper, we focus on diagnosing batch failures, which occur to a batch of instances of the same subject (e.g., API requests, VMs, nodes, etc.), resulting in degraded service availability and performance. Manual investigation over a large volume of high-dimensional telemetry data (e.g., logs, traces, and metrics) is labor-intensive and time-consuming, like finding a needle in a haystack. Meanwhile, existing proposed approaches are usually tailored for specific scenarios, which hinders their applications in diverse scenarios. According to our experience with Azure and Microsoft 365 aEUR"" two world-leading cloud systems, when batch failures happen, the procedure of finding the root cause can be abstracted as looking for contrast patterns by comparing two groups of instances, such as failed vs. succeeded, slow vs. normal, or during vs. before an anomaly. We thus propose CONAN, an efficient and flexible framework that can automatically extract contrast patterns from contextual data. CONAN has been successfully integrated into multiple diagnostic tools for various products, which proves its usefulness in diagnosing real-world batch failures.",Universalism,Equality,The CONAN framework contributes to improving the diagnosing capabilities of software users; providing them with the ability to extract contrast patterns from contextual data for diagnosing batch failures. This aligns with the value item Equality from the Universalism value; as the framework empowers software users with equal access to diagnostic tools regardless of their specific scenarios.,"In the context of Schwartz's Taxonomy, the alignment between the CONAN framework and the value item v10.2 (Equality) is justified because CONAN enables software users to have equal access to diagnostic tools regardless of the specific scenarios they encounter. By allowing users to extract contrast patterns from contextual data for diagnosing batch failures, CONAN ensures that software users can benefit equally from the framework's capabilities, promoting fairness and equal opportunities in the realm of software diagnostics.",Agreed-Clarified,Agree,,Agree,,
Exp A,Exp H,2694,ICSE-SEIP,Code Generation & Analysis,Please fix this mutant: How do developers resolve mutants surfaced during code review?,"Mutation testing has been demonstrated to motivate developers to write more tests when presented with undetected, actionable mutants. To facilitate this effect, modern mutation systems aim to generate and surface only actionable mutantsaEUR""few in numbers but highly valuable to the developer. This requires a deeper understanding of the extent to which developers resolve surfaced mutants and how: If they decide not to resolve an undetected mutant, why not? On the other hand, if they do resolve a mutant, do they simply add a test that detects it, or do they also improve the code?In order to answer these questions we compiled and analyzed a dataset of 1,538 merge requests with corresponding mutants surfaced during the code review phase. Our analysis reveals that determining whether a mutant is indeed resolved during code review is actually a non-trivial problem: for 64% of mutants, the mutated code changes as the merge request evolves, requiring dedicated techniques to precisely resurface the same mutants and to discover which of them remain unresolved after a code change. Overall, our analysis demonstrates that 38% of all surfaced mutants are resolved via code changes or test additions. Out of all mutants that are endorsed by a reviewer, 60% are resolved and result in additional tests, code refactorings, and improved documentation. Unresolved, yet endorsed, mutants stem from developers questioning the value of adding tests for surfaced mutants, later resolving mutants in deferred code changes (atomicity of merge requests), and false positives (mutants being resolved by tests not considered when creating the mutants, e.g., in integration test suites).",Achievement,Successful,Our analysis reveals that... 60% are resolved and result in additional tests; code refactorings; and improved documentation. This aligns with the value item Successful and its corresponding value Achievement.,"In the abstract of 'Paper X', it is stated that 60% of the surfaced mutants, which are endorsed by a reviewer, are resolved and result in additional tests, code refactorings, and improved documentation. This aligns with the value item v4.3 - Successful, and its corresponding value Achievement, because the resolution of these mutants demonstrates the developer's competence and capability in improving the software by adding tests, refactoring code, and enhancing documentation, thus achieving success in the software development process.",Agreed-Clarified,Agree,,Agree,,
Exp A,Exp H,2695,ICSE-SEIP,Software Engineering Practices,Using Large-scale Heterogeneous Graph Representation Learning for Code Review Recommendations at Microsoft,"Code review is an integral part of any mature software development process, and identifying the best reviewer for a code change is a well-accepted problem within the software engineering community. Selecting a reviewer who lacks expertise and understanding can slow development or result in more defects. To date, most reviewer recommendation systems rely primarily on historical file change and review information; those who changed or reviewed a file in the past are the best positioned to review in the future.We posit that while these approaches are able to identify and suggest qualified reviewers, they may be blind to reviewers who have the needed expertise and have simply never interacted with the changed files before. Fortunately, at Microsoft, we have a wealth of work artifacts across many repositories that can yield valuable information about our developers. To address the aforementioned problem, we present Coral, a novel approach to reviewer recommendation that leverages a socio-technical graph built from the rich set of entities (developers, repositories, files, pull requests (PRs), work items, etc.) and their relationships in modern source code management systems. We employ a graph convolutional neural network on this graph and train it on two and a half years of history on 332 repositories within Microsoft.We show that Coral is able to model the manual history of reviewer selection remarkably well. Further, based on an extensive user study, we demonstrate that this approach identifies relevant and qualified reviewers who traditional reviewer recommenders miss, and that these developers desire to be included in the review process. Finally, we find that &quot;classical&quot; reviewer recommendation systems perform better on smaller (in terms of developers) software projects while Coral excels on larger projects, suggesting that there is &quot;no one model to rule them all.&quot;",Achievement,Influential,The paper's contribution of Coral; a novel approach to reviewer recommendation; aligns with the value item Influential from the Achievement value. By recommending relevant and qualified reviewers who may have been missed by traditional reviewer recommenders; the software empowers software users to have an influence on the code review process and ensure high-quality reviews.,"In the context of 'Paper X', Coral, the reviewer recommendation system, aligns with the value item v4.4 Influential from the Achievement value. This alignment is evident as the system empowers software users by recommending relevant and qualified reviewers who may have been missed by traditional systems. By enabling software users to have a say in the code review process and ensuring high-quality reviews, Coral allows them to exert influence and contribute to the achievement of successful code changes.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2695,ICSE-SEIP,Software Engineering Practices,Using Large-scale Heterogeneous Graph Representation Learning for Code Review Recommendations at Microsoft,"Code review is an integral part of any mature software development process, and identifying the best reviewer for a code change is a well-accepted problem within the software engineering community. Selecting a reviewer who lacks expertise and understanding can slow development or result in more defects. To date, most reviewer recommendation systems rely primarily on historical file change and review information; those who changed or reviewed a file in the past are the best positioned to review in the future.We posit that while these approaches are able to identify and suggest qualified reviewers, they may be blind to reviewers who have the needed expertise and have simply never interacted with the changed files before. Fortunately, at Microsoft, we have a wealth of work artifacts across many repositories that can yield valuable information about our developers. To address the aforementioned problem, we present Coral, a novel approach to reviewer recommendation that leverages a socio-technical graph built from the rich set of entities (developers, repositories, files, pull requests (PRs), work items, etc.) and their relationships in modern source code management systems. We employ a graph convolutional neural network on this graph and train it on two and a half years of history on 332 repositories within Microsoft.We show that Coral is able to model the manual history of reviewer selection remarkably well. Further, based on an extensive user study, we demonstrate that this approach identifies relevant and qualified reviewers who traditional reviewer recommenders miss, and that these developers desire to be included in the review process. Finally, we find that &quot;classical&quot; reviewer recommendation systems perform better on smaller (in terms of developers) software projects while Coral excels on larger projects, suggesting that there is &quot;no one model to rule them all.&quot;",Power,Social Recognition,The paper's contribution of Coral; a reviewer recommendation system that leverages a socio-technical graph; aligns with the value item Social Recognition from the Power value. By identifying and suggesting qualified reviewers; the software helps software users gain recognition from their peers for their expertise and contributions in the code review process.,"By leveraging Coral, a reviewer recommendation system, software users can gain social recognition through the identification and suggestion of qualified reviewers in the code review process. This aligns with the value item Social Recognition from the Power value in Schwartz's Taxonomy. The system helps software users be acknowledged and respected by their peers for their expertise and contributions, reinforcing their social status within the software community.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2695,ICSE-SEIP,Software Engineering Practices,Using Large-scale Heterogeneous Graph Representation Learning for Code Review Recommendations at Microsoft,"Code review is an integral part of any mature software development process, and identifying the best reviewer for a code change is a well-accepted problem within the software engineering community. Selecting a reviewer who lacks expertise and understanding can slow development or result in more defects. To date, most reviewer recommendation systems rely primarily on historical file change and review information; those who changed or reviewed a file in the past are the best positioned to review in the future.We posit that while these approaches are able to identify and suggest qualified reviewers, they may be blind to reviewers who have the needed expertise and have simply never interacted with the changed files before. Fortunately, at Microsoft, we have a wealth of work artifacts across many repositories that can yield valuable information about our developers. To address the aforementioned problem, we present Coral, a novel approach to reviewer recommendation that leverages a socio-technical graph built from the rich set of entities (developers, repositories, files, pull requests (PRs), work items, etc.) and their relationships in modern source code management systems. We employ a graph convolutional neural network on this graph and train it on two and a half years of history on 332 repositories within Microsoft.We show that Coral is able to model the manual history of reviewer selection remarkably well. Further, based on an extensive user study, we demonstrate that this approach identifies relevant and qualified reviewers who traditional reviewer recommenders miss, and that these developers desire to be included in the review process. Finally, we find that &quot;classical&quot; reviewer recommendation systems perform better on smaller (in terms of developers) software projects while Coral excels on larger projects, suggesting that there is &quot;no one model to rule them all.&quot;",Benevolence,Helpful,The paper's contribution of Coral; which identifies relevant and qualified reviewers; aligns with the value item Helpful from the Benevolence value. By recommending reviewers who have the needed expertise; the software enables software users to contribute to the well-being and welfare of other software users by providing valuable feedback during code reviews.,"I apologize for any confusion in my previous response. The alignment between the paper's contribution of Coral, which identifies relevant and qualified reviewers, and the value item Helpful from the Benevolence value, stems from the fact that by recommending reviewers with expertise, the software enables software users, who act as reviewers, to contribute their knowledge and insights to the well-being and welfare of other software users by providing valuable feedback during code reviews. This aligns with the idea of helping others and promoting a sense of collective support and growth within the software community.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2696,ICSE-SEIP,Code Generation & Analysis,Widget Detection-based Testing for Industrial Mobile Games,"The fast advances in mobile hardware and widespread smartphone usage have fueled the growth of global mobile gaming in the past decade. As a result, the need for quality assurance of mobile gaming has become increasingly pressing. While general-purpose testing methods have been developed for mobile applications, they become struggling when being applied to mobile games due to the unique characteristics of mobile games, such as dynamic loading and stunning visual effects. There comes a growing industrial demand for automated testing techniques with high compatibility (compatible with various resolutions, and platforms) and non-intrusive characteristics (without packaging external modules into the source code, e.g., POCO). To fulfill these demands, in this paper, we introduce our experience in adopting the widget detection-based testing technique WDTEST, for mobile games at NetEase Games. To this end, we have constructed by far the largest graphical user interface (GUI) dataset for mobile games and conducted comprehensive evaluations on the performance of state-of-the-art widget detection techniques in the context of mobile gaming.We leverage widget detection techniques to develop WDTEST, which performs automated testing using only screenshots as input. Our evaluation shows that WDTEST outperforms the widely used tool Monkey in achieving three times more coverage of unique UI in gaming scenarios. Our further experiments demonstrate that WDTEST can be applied to general mobile applications without additional fine-tuning. Furthermore, we conducted a thorough survey at NetEase Games to gain a comprehensive understanding of widget detection-based testing techniques and identify challenges in industrial mobile game testing. The results show that testers are overall satisfied with the compatibility testing aspect of widget detection-based testing, but not much with functionality testing. This survey also highlights several unique characteristics of mobile games, providing valuable insights for future research directions.",Hedonism,Self Indulgent,The paper focuses on the need for quality assurance in mobile gaming; which implies a desire for self-indulgence and enjoying life. This aligns with the value item Hedonism and its corresponding value Pleasure.,"The paper's emphasis on the need for quality assurance in mobile gaming reflects the desire for a pleasurable and enjoyable gaming experience for users. This aligns with the value item v3.2 (Self Indulgent) and its corresponding value v3 (Pleasure) from Schwartz's Taxonomy. The focus on creating a seamless and visually stunning gaming experience suggests that the main contributions of 'Paper X' directly align with the value of Hedonism, as users seek pleasure and enjoyment from their mobile gaming interactions.",Agreed-Clarified,Agree,,Agree,,
Exp A,Exp H,2696,ICSE-SEIP,Code Generation & Analysis,Widget Detection-based Testing for Industrial Mobile Games,"The fast advances in mobile hardware and widespread smartphone usage have fueled the growth of global mobile gaming in the past decade. As a result, the need for quality assurance of mobile gaming has become increasingly pressing. While general-purpose testing methods have been developed for mobile applications, they become struggling when being applied to mobile games due to the unique characteristics of mobile games, such as dynamic loading and stunning visual effects. There comes a growing industrial demand for automated testing techniques with high compatibility (compatible with various resolutions, and platforms) and non-intrusive characteristics (without packaging external modules into the source code, e.g., POCO). To fulfill these demands, in this paper, we introduce our experience in adopting the widget detection-based testing technique WDTEST, for mobile games at NetEase Games. To this end, we have constructed by far the largest graphical user interface (GUI) dataset for mobile games and conducted comprehensive evaluations on the performance of state-of-the-art widget detection techniques in the context of mobile gaming.We leverage widget detection techniques to develop WDTEST, which performs automated testing using only screenshots as input. Our evaluation shows that WDTEST outperforms the widely used tool Monkey in achieving three times more coverage of unique UI in gaming scenarios. Our further experiments demonstrate that WDTEST can be applied to general mobile applications without additional fine-tuning. Furthermore, we conducted a thorough survey at NetEase Games to gain a comprehensive understanding of widget detection-based testing techniques and identify challenges in industrial mobile game testing. The results show that testers are overall satisfied with the compatibility testing aspect of widget detection-based testing, but not much with functionality testing. This survey also highlights several unique characteristics of mobile games, providing valuable insights for future research directions.",Achievement,Capable,The paper introduces a widget detection-based testing technique for mobile games; which aims to improve the performance and coverage of unique UI in gaming scenarios. This aligns with the value item Achievement and its corresponding value Capable.,"The justification for aligning 'Paper X' with the value item v4.2 (Capable) and its corresponding value v4 (Achievement) from a ""Software User"" perspective is that the paper specifically addresses the need for quality assurance in mobile gaming, which is a significant achievement in itself. By introducing a widget detection-based testing technique that outperforms widely used tools and achieves higher coverage of unique UI in gaming scenarios, the paper provides a solution that enhances the capability of mobile game testing. This aligns with the value of being capable and achieving success in the software user's context.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2696,ICSE-SEIP,Code Generation & Analysis,Widget Detection-based Testing for Industrial Mobile Games,"The fast advances in mobile hardware and widespread smartphone usage have fueled the growth of global mobile gaming in the past decade. As a result, the need for quality assurance of mobile gaming has become increasingly pressing. While general-purpose testing methods have been developed for mobile applications, they become struggling when being applied to mobile games due to the unique characteristics of mobile games, such as dynamic loading and stunning visual effects. There comes a growing industrial demand for automated testing techniques with high compatibility (compatible with various resolutions, and platforms) and non-intrusive characteristics (without packaging external modules into the source code, e.g., POCO). To fulfill these demands, in this paper, we introduce our experience in adopting the widget detection-based testing technique WDTEST, for mobile games at NetEase Games. To this end, we have constructed by far the largest graphical user interface (GUI) dataset for mobile games and conducted comprehensive evaluations on the performance of state-of-the-art widget detection techniques in the context of mobile gaming.We leverage widget detection techniques to develop WDTEST, which performs automated testing using only screenshots as input. Our evaluation shows that WDTEST outperforms the widely used tool Monkey in achieving three times more coverage of unique UI in gaming scenarios. Our further experiments demonstrate that WDTEST can be applied to general mobile applications without additional fine-tuning. Furthermore, we conducted a thorough survey at NetEase Games to gain a comprehensive understanding of widget detection-based testing techniques and identify challenges in industrial mobile game testing. The results show that testers are overall satisfied with the compatibility testing aspect of widget detection-based testing, but not much with functionality testing. This survey also highlights several unique characteristics of mobile games, providing valuable insights for future research directions.",Benevolence,Helpful,The paper addresses the compatibility testing aspect of widget detection-based testing in mobile games; showing the satisfaction of testers in this aspect. This aligns with the value item Benevolence and its corresponding value Helpful.,"In 'Paper X', the authors discuss the satisfaction of testers with the compatibility testing aspect of widget detection-based testing in mobile games. This aligns with the value item v9.3 (Helpful) from the perspective of a ""Software User"" within a software context. The paper demonstrates the helpfulness of the automated testing technique in providing a reliable method for ensuring compatibility in mobile gaming, which ultimately benefits the users of the software.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2697,ICSE-SEIP,Software Engineering Practices,Towards More Effective AI-Assisted Programming: A Systematic Design Exploration to Improve Visual Studio IntelliCodeaEUR(tm)s User Experience,"AI-driven code editor extensions such as Visual Studio IntelliCode and Github CoPilot have become extremely popular. These tools recommend inserting chunks of code, with the lines to be inserted presented inline at the current cursor location as gray text. In contrast to their popularity, other AI-driven code recommendation tools that suggest code changes (as opposed to code completions) have remained woefully underused. We conducted lab studies at Microsoft to understand this disparity and found one major cause: discoverability. Code change suggestions are hard to surface through bold, inline interfaces and hence, developers often do not even notice them.Towards a systematic understanding of code change interfaces, we performed a thorough design exploration for various categories of code changes: additive single-line changes, single-line changes, and multi-line changes. Overall, we explored 19 designs through a series of 7 laboratory studies involving 61 programmers and distilled our findings into a set of 5 design principles. To validate our results, we built and deployed a new version of IntelliCode with two of our new inline interfaces in Microsoft Visual Studio 2022 and found that they lead to a significant increase in usage of the corresponding tools.",Achievement,Capable,The paper explores different designs for code changes and identifies design principles. This aligns with the value item Capable and its corresponding value Achievement.,"The justification for labeling 'Paper X' as aligning with the value item v4.2 (Capable) and its corresponding value v4 (Achievement) from a ""Software User"" perspective is based on the fact that the paper explores various designs for code changes and provides design principles. By doing so, it enables software users to enhance their coding abilities and achieve a sense of accomplishment by effectively implementing code changes. The paper's focus on improving the usability and discoverability of code change interfaces ultimately empowers software users to become more capable in their development tasks, which is in line with the value of Achievement.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2697,ICSE-SEIP,Software Engineering Practices,Towards More Effective AI-Assisted Programming: A Systematic Design Exploration to Improve Visual Studio IntelliCodeaEUR(tm)s User Experience,"AI-driven code editor extensions such as Visual Studio IntelliCode and Github CoPilot have become extremely popular. These tools recommend inserting chunks of code, with the lines to be inserted presented inline at the current cursor location as gray text. In contrast to their popularity, other AI-driven code recommendation tools that suggest code changes (as opposed to code completions) have remained woefully underused. We conducted lab studies at Microsoft to understand this disparity and found one major cause: discoverability. Code change suggestions are hard to surface through bold, inline interfaces and hence, developers often do not even notice them.Towards a systematic understanding of code change interfaces, we performed a thorough design exploration for various categories of code changes: additive single-line changes, single-line changes, and multi-line changes. Overall, we explored 19 designs through a series of 7 laboratory studies involving 61 programmers and distilled our findings into a set of 5 design principles. To validate our results, we built and deployed a new version of IntelliCode with two of our new inline interfaces in Microsoft Visual Studio 2022 and found that they lead to a significant increase in usage of the corresponding tools.",Power,Authority,The paper introduces new inline interfaces for code change suggestions; which improve the developer's authority and control over code changes. This aligns with the value item Authority and its corresponding value Power.,"In 'Paper X', the introduction of new inline interfaces for code change suggestions aligns with the value item Authority and its corresponding value Power from a ""Software User"" perspective. By providing developers with the ability to make code changes with greater control and ownership, these interfaces enhance their authority and decision-making power over the software development process. This empowers software users to have a sense of control and influence over their code, aligning with the value of Power.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2697,ICSE-SEIP,Software Engineering Practices,Towards More Effective AI-Assisted Programming: A Systematic Design Exploration to Improve Visual Studio IntelliCodeaEUR(tm)s User Experience,"AI-driven code editor extensions such as Visual Studio IntelliCode and Github CoPilot have become extremely popular. These tools recommend inserting chunks of code, with the lines to be inserted presented inline at the current cursor location as gray text. In contrast to their popularity, other AI-driven code recommendation tools that suggest code changes (as opposed to code completions) have remained woefully underused. We conducted lab studies at Microsoft to understand this disparity and found one major cause: discoverability. Code change suggestions are hard to surface through bold, inline interfaces and hence, developers often do not even notice them.Towards a systematic understanding of code change interfaces, we performed a thorough design exploration for various categories of code changes: additive single-line changes, single-line changes, and multi-line changes. Overall, we explored 19 designs through a series of 7 laboratory studies involving 61 programmers and distilled our findings into a set of 5 design principles. To validate our results, we built and deployed a new version of IntelliCode with two of our new inline interfaces in Microsoft Visual Studio 2022 and found that they lead to a significant increase in usage of the corresponding tools.",Security,Cleanliness,The paper addresses the issue of code change suggestions being hard to notice; which can contribute to a lack of cleanliness in the code. By providing new inline interfaces; the paper promotes a sense of cleanliness and orderliness in the code. This aligns with the value item Cleanliness and its corresponding value Security.,"In 'Paper X', the main contribution of introducing new inline interfaces for code change suggestions aligns with the value item Cleanliness (v6.4) and its corresponding value Security (v6) from the perspective of a Software User. By addressing the issue of discoverability and making code change suggestions more noticeable and accessible, the paper promotes a sense of cleanliness and orderliness in the code, leading to a more secure and reliable software experience for the user.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2698,ICSE-SEIP,Code Generation & Analysis,Code Librarian: A Software Package Recommendation System,"The use of packaged libraries can significantly shorten the software development life cycle by improving the quality and readability of code. In this paper, we present a recommendation engine called Code Librarian for open source libraries. A candidate library package is recommended for a given context if: 1) it has been frequently used with the imported libraries in the program; 2) it has similar functionality to the imported libraries in the program; 3) it has similar functionality to the developeraEUR(tm)s implementation, and 4) it can be used efficiently in the context of the provided code. We apply the state of the art CodeBERT-based model for analysing the context of the source code to deliver relevant library recommendations to users.",Achievement,Capable,The paper's main contribution is the development of a recommendation engine called Code Librarian for open source libraries. This aligns with the value item Capable and its corresponding value Achievement; as the software allows software users to improve the quality and readability of their code; showcasing their competence and skills.,"Certainly! The development of Code Librarian aligns with the value item Capable and its corresponding value Achievement from a ""Software User"" perspective because the recommendation engine empowers users to improve the quality and readability of their code. By utilizing the recommended library packages, software users are able to showcase their competence and skills in developing efficient and effective software solutions. This alignment directly addresses the value of personal achievement and the ability to demonstrate one's capabilities in the software development process.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2698,ICSE-SEIP,Code Generation & Analysis,Code Librarian: A Software Package Recommendation System,"The use of packaged libraries can significantly shorten the software development life cycle by improving the quality and readability of code. In this paper, we present a recommendation engine called Code Librarian for open source libraries. A candidate library package is recommended for a given context if: 1) it has been frequently used with the imported libraries in the program; 2) it has similar functionality to the imported libraries in the program; 3) it has similar functionality to the developeraEUR(tm)s implementation, and 4) it can be used efficiently in the context of the provided code. We apply the state of the art CodeBERT-based model for analysing the context of the source code to deliver relevant library recommendations to users.",Benevolence,Helpful,The paper's recommendation engine; Code Librarian; aims to provide relevant library recommendations to software users. By helping users find suitable library packages that can improve their software development process; the paper contributes to the value item Helpful and its corresponding value Benevolence; as it enables software users to contribute to the well-being and welfare of others in the software development community.,"By providing relevant library recommendations to software users, the paper's recommendation engine, Code Librarian, directly aligns with the value item v9.3 (Helpful) and its corresponding value v9 (Benevolence) from the perspective of a software user. This alignment is evident because the paper's main contribution is enabling software users to easily find suitable library packages that can improve their software development process. By doing so, users are empowered to be helpful to others in the software development community, as they can contribute to their well-being and welfare by utilizing efficient and quality code libraries.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2698,ICSE-SEIP,Code Generation & Analysis,Code Librarian: A Software Package Recommendation System,"The use of packaged libraries can significantly shorten the software development life cycle by improving the quality and readability of code. In this paper, we present a recommendation engine called Code Librarian for open source libraries. A candidate library package is recommended for a given context if: 1) it has been frequently used with the imported libraries in the program; 2) it has similar functionality to the imported libraries in the program; 3) it has similar functionality to the developeraEUR(tm)s implementation, and 4) it can be used efficiently in the context of the provided code. We apply the state of the art CodeBERT-based model for analysing the context of the source code to deliver relevant library recommendations to users.",Power,Authority,The paper's development of a recommendation engine; Code Librarian; contributes to the value item Authority and its corresponding value Power. By providing software users with the power to make informed decisions about the libraries they use and recommending suitable packages; the paper empowers software users and enhances their sense of authority and control in the software development process.,"Based on the development of Code Librarian, the paper directly aligns with the value item v5.3 - Authority, and its corresponding value Power, from a ""Software User"" perspective. By providing software users with the ability to make informed decisions about the libraries they use and recommending suitable packages, the paper empowers software users and enhances their sense of authority and control in the software development process. This aligns directly with the value of Power, as it enables users to have influence and control over their software development choices, ultimately enhancing their sense of authority in the decision-making process.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2698,ICSE-SEIP,Code Generation & Analysis,Code Librarian: A Software Package Recommendation System,"The use of packaged libraries can significantly shorten the software development life cycle by improving the quality and readability of code. In this paper, we present a recommendation engine called Code Librarian for open source libraries. A candidate library package is recommended for a given context if: 1) it has been frequently used with the imported libraries in the program; 2) it has similar functionality to the imported libraries in the program; 3) it has similar functionality to the developeraEUR(tm)s implementation, and 4) it can be used efficiently in the context of the provided code. We apply the state of the art CodeBERT-based model for analysing the context of the source code to deliver relevant library recommendations to users.",Self Direction,Choosing Own Goals,The paper's recommendation engine; Code Librarian; assists software users in choosing suitable library packages for their specific needs and goals. This aligns with the value item Choosing Own Goals and its corresponding value Self Direction; as it enables software users to make independent choices and set their own goals in the software development process.,"The recommendation engine, Code Librarian, described in 'Paper X' aligns with the value item Choosing Own Goals and its corresponding value Self Direction from the perspective of a Software User. This alignment is evident in the paper's statement that the recommendation engine assists software users in choosing suitable library packages for their specific needs and goals. By providing relevant recommendations, the software users are empowered to make independent choices and set their own goals in the software development process, thus promoting self-direction.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2698,ICSE-SEIP,Code Generation & Analysis,Code Librarian: A Software Package Recommendation System,"The use of packaged libraries can significantly shorten the software development life cycle by improving the quality and readability of code. In this paper, we present a recommendation engine called Code Librarian for open source libraries. A candidate library package is recommended for a given context if: 1) it has been frequently used with the imported libraries in the program; 2) it has similar functionality to the imported libraries in the program; 3) it has similar functionality to the developeraEUR(tm)s implementation, and 4) it can be used efficiently in the context of the provided code. We apply the state of the art CodeBERT-based model for analysing the context of the source code to deliver relevant library recommendations to users.",Hedonism,Enjoying Life,The paper's development of a recommendation engine; Code Librarian; contributes to the value item Enjoying Life and its corresponding value Hedonism. By improving the quality and readability of code through the use of recommended library packages; the paper enhances the software user's enjoyment and satisfaction in the software development process.,"In 'Paper X', the development of the recommendation engine, Code Librarian, contributes to the value item Enjoying Life and its corresponding value Hedonism. The paper explicitly states that the use of packaged libraries can significantly improve the quality and readability of code, thereby shortening the software development life cycle. By providing relevant library recommendations to software users, the paper enhances their experience by reducing the time and effort required to develop software and improving the overall satisfaction and enjoyment in the software development process. This aligns with the value of Hedonism, which emphasizes pleasure and enjoying life.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2698,ICSE-SEIP,Code Generation & Analysis,Code Librarian: A Software Package Recommendation System,"The use of packaged libraries can significantly shorten the software development life cycle by improving the quality and readability of code. In this paper, we present a recommendation engine called Code Librarian for open source libraries. A candidate library package is recommended for a given context if: 1) it has been frequently used with the imported libraries in the program; 2) it has similar functionality to the imported libraries in the program; 3) it has similar functionality to the developeraEUR(tm)s implementation, and 4) it can be used efficiently in the context of the provided code. We apply the state of the art CodeBERT-based model for analysing the context of the source code to deliver relevant library recommendations to users.",Security,Healthy,The paper's focus on improving the quality of code through the use of recommended library packages aligns with the value item Healthy and its corresponding value Security. By utilizing recommended packages; software users can ensure the health and stability of their software; enhancing the overall security of their projects.,"In the context of a software user, the alignment of 'Paper X' with the value item v6.1 (Healthy) and its corresponding value v6 (Security) is justified by the fact that using recommended library packages can contribute to the overall health and stability of the software. By relying on these packages, software users can minimize the risks of vulnerabilities, bugs, and errors, ultimately enhancing the security of their projects. This alignment is based on the explicit statement in the abstract that the paper focuses on improving code quality and readability, which directly correlates to the value of security for a software user.",Agreed-Clarified,Agree,,Agree,,
Exp A,Exp H,2698,ICSE-SEIP,Code Generation & Analysis,Code Librarian: A Software Package Recommendation System,"The use of packaged libraries can significantly shorten the software development life cycle by improving the quality and readability of code. In this paper, we present a recommendation engine called Code Librarian for open source libraries. A candidate library package is recommended for a given context if: 1) it has been frequently used with the imported libraries in the program; 2) it has similar functionality to the imported libraries in the program; 3) it has similar functionality to the developeraEUR(tm)s implementation, and 4) it can be used efficiently in the context of the provided code. We apply the state of the art CodeBERT-based model for analysing the context of the source code to deliver relevant library recommendations to users.",Tradition,Detachment,The paper's recommendation engine; Code Librarian; assists software users in finding suitable library packages based on the functionality required by their projects. This aligns with the value item Detachment and its corresponding value Tradition; as it allows software users to detach themselves from any specific implementation and instead rely on established and traditional practices in software development.,"The justification for aligning 'Paper X' with the value item v7.2 (Detachment) and its corresponding value v7 (Tradition) is based on the fact that the recommendation engine, Code Librarian, enables software users to detach themselves from specific implementations and instead rely on established and traditional practices in software development. By recommending library packages that have been frequently used in similar contexts and have similar functionality to the imported libraries in the user's program, Code Librarian promotes a traditional approach to selecting libraries based on past usage and functionality. This aligns with the value of Tradition, as it encourages software users to follow established practices and rely on proven solutions rather than reinventing the wheel. The value item v7.2 (Detachment) further emphasizes this alignment, as it indicates that software users can detach themselves from individual implementations and prioritize adherence to traditional practices. Therefore, the main contributions of 'Paper X' align directly with the values of Tradition and Detachment from a software user's perspective.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2700,ICSE-SEIP,Code Generation & Analysis,Investigating a NASA Cyclomatic Complexity Policy on Maintenance Risk of a Critical System,"Monte is a mission critical system used by NASA for navigation and design of deep space missions has been used in over 40 missions over the last 18 years. Its continuous, reliable operation is considered critical to the operation for over 18 ongoing missions. Recently Monte has been escalated to safety- critical software and subject to NASA Software Assurance and Software Safety Standard requirements. One of these requirements mandates a policy that the cyclomatic complexity (CC) for safety-critical components be under 16 or given a technically detailed explanation as to why it cannot or should not be lower. Conformance to this requirement would be costly and we had doubts about its benefit and efficacy in managing our maintenance risk (defect proneness and defect repair effort). The requirement was not substantiated either empirically or in principle, and guidance in the literature for use of CC as in indicator of maintenance risk is limited and often speculative or have contradictory empirical or non-definitive results.This led us to rigorously investigate the impact the CC policy would have on the practical management of maintenance risk for Monte. The effect of CC on maintenance risk for Monte is explored using a variety of statistical methods and machine learning with aim of provide objective empirical evidence to support our decision as to what extent we will conform to the NASA CC policy of maintaining a CC under 16. This paper presents the conclusions and insights gained from this investigation. Practical questions related to the policy are addressed such as: Does higher CC associate with higher defect proneness? More effort to repair defects? If so, is there a CC after which action should be taken to reduce defect proneness and repair effort?We conclude that the impacts of CC on maintenance risk are, with high confidence, consistent with the risk management expectations of the NASA policy. We can quantify the benefit and weight it against the cost to inform our decision in conforming to the policy. Furthermore, we gained insight into how CC affects maintenance risk and is a useful indicator for maintenance risk management.",Achievement,Capable,The paper investigates the impact of cyclomatic complexity (CC) on maintenance risk in software. This aligns with the value item Capable and its corresponding value Achievement. By examining the relationship between CC and defect proneness; the paper contributes to the software user's aspiration to demonstrate their competence and excel in managing maintenance risk.,"The justification for aligning 'Paper X' with the value item v4.2 and its corresponding value v4 is based on the fact that the paper investigates the impact of cyclomatic complexity (CC) on maintenance risk in software. This aligns with the value item Capable and its corresponding value Achievement because by examining the relationship between CC and defect proneness, the paper contributes to the software user's aspiration to demonstrate their competence and excel in managing maintenance risk. The ability to understand and analyze the impact of CC on risk management showcases the user's capability and achievement in effectively handling software maintenance.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2700,ICSE-SEIP,Code Generation & Analysis,Investigating a NASA Cyclomatic Complexity Policy on Maintenance Risk of a Critical System,"Monte is a mission critical system used by NASA for navigation and design of deep space missions has been used in over 40 missions over the last 18 years. Its continuous, reliable operation is considered critical to the operation for over 18 ongoing missions. Recently Monte has been escalated to safety- critical software and subject to NASA Software Assurance and Software Safety Standard requirements. One of these requirements mandates a policy that the cyclomatic complexity (CC) for safety-critical components be under 16 or given a technically detailed explanation as to why it cannot or should not be lower. Conformance to this requirement would be costly and we had doubts about its benefit and efficacy in managing our maintenance risk (defect proneness and defect repair effort). The requirement was not substantiated either empirically or in principle, and guidance in the literature for use of CC as in indicator of maintenance risk is limited and often speculative or have contradictory empirical or non-definitive results.This led us to rigorously investigate the impact the CC policy would have on the practical management of maintenance risk for Monte. The effect of CC on maintenance risk for Monte is explored using a variety of statistical methods and machine learning with aim of provide objective empirical evidence to support our decision as to what extent we will conform to the NASA CC policy of maintaining a CC under 16. This paper presents the conclusions and insights gained from this investigation. Practical questions related to the policy are addressed such as: Does higher CC associate with higher defect proneness? More effort to repair defects? If so, is there a CC after which action should be taken to reduce defect proneness and repair effort?We conclude that the impacts of CC on maintenance risk are, with high confidence, consistent with the risk management expectations of the NASA policy. We can quantify the benefit and weight it against the cost to inform our decision in conforming to the policy. Furthermore, we gained insight into how CC affects maintenance risk and is a useful indicator for maintenance risk management.",Stimulation,Daring,The paper explores the impact of CC on maintenance risk using statistical methods and machine learning. This aligns with the value item Daring and its corresponding value Stimulation. By challenging the NASA CC policy and investigating its efficacy; the paper contributes to the software user's desire for variation and excitement in the field of software development.,"The justification for aligning 'Paper X' with the value item v2.1 (Daring) and its corresponding value v2 (Stimulation) is based on the fact that the paper challenges the NASA CC policy and investigates its impact using statistical methods and machine learning. This exploration and questioning of the established policy aligns with the software user's desire for variation and excitement in the field of software development, as it goes against the traditional norms and aims to bring about new insights and perspectives in managing maintenance risk. This aligns with the value of Stimulation, as it seeks to explore new possibilities and challenge existing norms to stimulate innovation and growth.",Agreed-Clarified,Agree,,Agree,,
Exp A,Exp H,2705,ICSE-SEIP,Software Engineering Practices,You DonaEUR(tm)t Know Search: Helping Users Find Code by Automatically Evaluating Alternative Queries,"Tens of thousands of engineers use Sourcegraph day-to-day to search for code and rely on it to make progress on software development tasks. We face a key challenge in designing a query language that accommodates the needs of a broad spectrum of users. Our experience shows that users express different and often contradictory preferences for how queries should be interpreted. These preferences stem from users with differing usage contexts, technical experience, and implicit expectations from using prior tools. At the same time, designing a code search query language poses unique challenges because it intersects traditional search engines and full-fledged programming languages. For example, code search queries adopt certain syntactic conventions in the interest of simplicity and terseness but invariably risk encoding implicit semantics that are ambiguous at face-value (a single space in a query could mean three or more semantically different things depending on surrounding terms). Users often need to disambiguate intent with additional syntax so that a query expresses what they actually want to search. This need to disambiguate is one of the primary frustrations weaEUR(tm)ve seen users experience with writing search queries in the last three years. We share our observations that lead us to a fresh perspective where code search behavior can straddle seemingly ambiguous queries. We develop Automated Query Evaluation (AQE), a new technique that automatically generates and adaptively runs alternative query interpretations in frustration-prone conditions. We evaluate AQE with an A/B test across more than 10,000 unique users on our publicly-available code search instance. Our main result shows that relative to the control group, users are on average 22% more likely to click on a search result at all on any given day when AQE is active. We share our technique, learnings, and implementation that made it possible for a substantial number of users to now see and click on results that they would not have seen otherwise.",Achievement,Intelligent,The paper aims to design a query language that accommodates the needs of a broad spectrum of software users. This aligns with the value item Intelligent; as it demonstrates the software users' aspiration to excel and be acknowledged for their skills and accomplishments.,"The main contribution of 'Paper X' is the development of Automated Query Evaluation (AQE), a technique that generates alternative query interpretations to address the frustrations faced by software users when writing search queries. By accommodating the needs of a broad spectrum of software users, the paper aligns with the value item Intelligent in Schwartz's Taxonomy. This alignment is evident as software users aspire to excel and be acknowledged for their skills and accomplishments in the software development domain. The development of AQE demonstrates the effort to enhance the intelligence and effectiveness of the query language, enabling software users to search for code more efficiently and successfully.",Agreed-Reconciled,Agree,,Agree,,
Exp A,Exp H,2705,ICSE-SEIP,Software Engineering Practices,You DonaEUR(tm)t Know Search: Helping Users Find Code by Automatically Evaluating Alternative Queries,"Tens of thousands of engineers use Sourcegraph day-to-day to search for code and rely on it to make progress on software development tasks. We face a key challenge in designing a query language that accommodates the needs of a broad spectrum of users. Our experience shows that users express different and often contradictory preferences for how queries should be interpreted. These preferences stem from users with differing usage contexts, technical experience, and implicit expectations from using prior tools. At the same time, designing a code search query language poses unique challenges because it intersects traditional search engines and full-fledged programming languages. For example, code search queries adopt certain syntactic conventions in the interest of simplicity and terseness but invariably risk encoding implicit semantics that are ambiguous at face-value (a single space in a query could mean three or more semantically different things depending on surrounding terms). Users often need to disambiguate intent with additional syntax so that a query expresses what they actually want to search. This need to disambiguate is one of the primary frustrations weaEUR(tm)ve seen users experience with writing search queries in the last three years. We share our observations that lead us to a fresh perspective where code search behavior can straddle seemingly ambiguous queries. We develop Automated Query Evaluation (AQE), a new technique that automatically generates and adaptively runs alternative query interpretations in frustration-prone conditions. We evaluate AQE with an A/B test across more than 10,000 unique users on our publicly-available code search instance. Our main result shows that relative to the control group, users are on average 22% more likely to click on a search result at all on any given day when AQE is active. We share our technique, learnings, and implementation that made it possible for a substantial number of users to now see and click on results that they would not have seen otherwise.",Achievement,Capable,The paper addresses the challenge of users expressing different preferences for how queries should be interpreted. By developing the Automated Query Evaluation technique; the paper helps software users demonstrate their capability and competence in using code search tools.,"Based on the abstract of 'Paper X', the paper addresses the challenge of accommodating the needs of a broad spectrum of users in designing a query language for code search. This aligns with the value item v4.2 of the Achievement value, which represents being capable. By developing the Automated Query Evaluation technique, the paper enables software users to effectively search and navigate code, thereby demonstrating their capability and competence in using code search tools. This alignment directly addresses the stated challenge and the value of being capable in the context of software users.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2705,ICSE-SEIP,Software Engineering Practices,You DonaEUR(tm)t Know Search: Helping Users Find Code by Automatically Evaluating Alternative Queries,"Tens of thousands of engineers use Sourcegraph day-to-day to search for code and rely on it to make progress on software development tasks. We face a key challenge in designing a query language that accommodates the needs of a broad spectrum of users. Our experience shows that users express different and often contradictory preferences for how queries should be interpreted. These preferences stem from users with differing usage contexts, technical experience, and implicit expectations from using prior tools. At the same time, designing a code search query language poses unique challenges because it intersects traditional search engines and full-fledged programming languages. For example, code search queries adopt certain syntactic conventions in the interest of simplicity and terseness but invariably risk encoding implicit semantics that are ambiguous at face-value (a single space in a query could mean three or more semantically different things depending on surrounding terms). Users often need to disambiguate intent with additional syntax so that a query expresses what they actually want to search. This need to disambiguate is one of the primary frustrations weaEUR(tm)ve seen users experience with writing search queries in the last three years. We share our observations that lead us to a fresh perspective where code search behavior can straddle seemingly ambiguous queries. We develop Automated Query Evaluation (AQE), a new technique that automatically generates and adaptively runs alternative query interpretations in frustration-prone conditions. We evaluate AQE with an A/B test across more than 10,000 unique users on our publicly-available code search instance. Our main result shows that relative to the control group, users are on average 22% more likely to click on a search result at all on any given day when AQE is active. We share our technique, learnings, and implementation that made it possible for a substantial number of users to now see and click on results that they would not have seen otherwise.",Benevolence,Helpful,The paper introduces the Automated Query Evaluation technique; which automatically generates and adaptively runs alternative query interpretations to improve user experience with writing search queries. This aligns with the value item Helpful; as it contributes to the well-being and welfare of software users by making the search process more efficient and effective.,"The Automated Query Evaluation technique introduced in 'Paper X' aligns with the value item Helpful (v9.3) and its corresponding value (v9) from a ""Software User"" perspective because it directly contributes to the well-being and welfare of software users. By automatically generating and adaptively running alternative query interpretations, the technique improves the user experience with writing search queries, making the search process more efficient and effective. This ultimately helps software users find the information they need more easily, saving them time and effort in their software development tasks.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2709,ICSE-SEIP,Code Generation & Analysis,Long-term Static Analysis Rule Quality Monitoring Using True Negatives,"Static application security testing (SAST) tools have found broad adoption in modern software development work-flows. These tools employ a variety of static analysis rules to generate recommendations on how to improve the code of an application.Every recommendation consumes the time of the engineer that is investigating it, so it is important to measure how useful these rules are in the long term. But what is a good metric for monitoring rule quality over time? Counting the number of recommendations rewards noisy rules and ignores developersaEUR(tm) reactions. Measuring fix rate is not ideal either, because it over-emphasizes rules that are easy to fix.In this paper, we report on an experiment where we use the frequency of true negatives to quantify if developers are able to learn a static analysis rule. We consider a static analysis rule to be ideal if its recommendations are not only addressed, but also internalized by the developer in a way that prevents the bug from recurring. That is, the rule contributes to code quality not only at present, but also in the future. We measure how often developers produce true negatives, that is, code changes that are relevant to a rule but do not trigger a recommendation, and we compare true-negative rate against other metrics. Our results show that measuring true negatives provides insights that cannot be provided by metrics such as fix rate or developer feedback.",Benevolence,Meaning in Life,"The paper explores how the frequency of true negatives can be used as a metric to evaluate the effectiveness of static analysis rules in improving code quality. By measuring how often developers produce true negatives; the paper aims to determine if static analysis rules are internalized by developers in a way that prevents the recurrence of bugs. This aligns with the value item ""Meaning in Life"" as it emphasizes the importance of creating software that has long-term value and contributes to the overall quality and reliability of applications.","The alignment of 'Paper X' with the value item v9.6, ""Meaning in Life,"" is justified by the paper's focus on evaluating the long-term effectiveness of static analysis rules in improving code quality. By measuring the frequency of true negatives, the paper addresses the importance of creating software that has lasting value and contributes to the overall quality and reliability of applications. This aligns with the value of v9, which emphasizes the significance of developing software that holds meaning and purpose in users' lives.",Agreed-Justified,Disagree,reconcile with coder_2,Disagree,the paper addresses the importance of creating software that has lasting value and contributes to the overall quality and reliability of applications. This is not aligns with the value item of Meaning in Life,
Exp A,Exp H,2709,ICSE-SEIP,Code Generation & Analysis,Long-term Static Analysis Rule Quality Monitoring Using True Negatives,"Static application security testing (SAST) tools have found broad adoption in modern software development work-flows. These tools employ a variety of static analysis rules to generate recommendations on how to improve the code of an application.Every recommendation consumes the time of the engineer that is investigating it, so it is important to measure how useful these rules are in the long term. But what is a good metric for monitoring rule quality over time? Counting the number of recommendations rewards noisy rules and ignores developersaEUR(tm) reactions. Measuring fix rate is not ideal either, because it over-emphasizes rules that are easy to fix.In this paper, we report on an experiment where we use the frequency of true negatives to quantify if developers are able to learn a static analysis rule. We consider a static analysis rule to be ideal if its recommendations are not only addressed, but also internalized by the developer in a way that prevents the bug from recurring. That is, the rule contributes to code quality not only at present, but also in the future. We measure how often developers produce true negatives, that is, code changes that are relevant to a rule but do not trigger a recommendation, and we compare true-negative rate against other metrics. Our results show that measuring true negatives provides insights that cannot be provided by metrics such as fix rate or developer feedback.",Power,Social Recognition,"The paper focuses on measuring the frequency of true negatives as a metric to assess the quality of static analysis rules. By identifying code changes that are relevant to a rule but do not trigger a recommendation; the paper aims to determine if developers are able to learn and internalize the rule effectively. This aligns with the value item ""Social Recognition"" as it recognizes the importance of developers' ability to understand and apply static analysis rules correctly; which contributes to their professional reputation and recognition within the software development community.","The justification for aligning 'Paper X' with the value item ""Social Recognition"" (v5.1) is based on the acknowledgement and recognition of developers' ability to correctly understand and apply static analysis rules. By measuring the frequency of true negatives, the paper aims to assess if developers have internalized these rules effectively, highlighting their competence and expertise. This aligns with the value item as it emphasizes the importance of developers' professional reputation and recognition within the software development community, particularly in relation to their ability to effectively utilize static analysis tools.",Agreed-Justified,Disagree,reconcile with coder_2,Disagree,The justification is not related to social recognition,
Exp A,Exp H,2709,ICSE-SEIP,Code Generation & Analysis,Long-term Static Analysis Rule Quality Monitoring Using True Negatives,"Static application security testing (SAST) tools have found broad adoption in modern software development work-flows. These tools employ a variety of static analysis rules to generate recommendations on how to improve the code of an application.Every recommendation consumes the time of the engineer that is investigating it, so it is important to measure how useful these rules are in the long term. But what is a good metric for monitoring rule quality over time? Counting the number of recommendations rewards noisy rules and ignores developersaEUR(tm) reactions. Measuring fix rate is not ideal either, because it over-emphasizes rules that are easy to fix.In this paper, we report on an experiment where we use the frequency of true negatives to quantify if developers are able to learn a static analysis rule. We consider a static analysis rule to be ideal if its recommendations are not only addressed, but also internalized by the developer in a way that prevents the bug from recurring. That is, the rule contributes to code quality not only at present, but also in the future. We measure how often developers produce true negatives, that is, code changes that are relevant to a rule but do not trigger a recommendation, and we compare true-negative rate against other metrics. Our results show that measuring true negatives provides insights that cannot be provided by metrics such as fix rate or developer feedback.",Achievement,Capable,"The paper aims to evaluate the long-term usefulness of static analysis rules in improving code quality. By considering the frequency of true negatives as a metric; the paper assesses if developers have internalized the recommendations of a rule and prevent the recurrence of bugs. This aligns with the value item ""Capable"" as it emphasizes the software users' desire to demonstrate their competence and excel in their coding practices by effectively applying static analysis rules to improve code quality.","I apologize for any confusion caused. The alignment between 'Paper X' and the value item v4.2 (Capable) is justified by the fact that the paper focuses on evaluating the long-term usefulness of static analysis rules in improving code quality. By measuring the frequency of true negatives, the paper assesses if developers have internalized the recommendations of a rule and effectively prevented the recurrence of bugs. This aligns with the value v4 (Achievement) because it emphasizes the software users' desire to demonstrate their competence and excel in their coding practices. The ability to effectively apply static analysis rules and improve code quality aligns with the software users' aspiration to achieve success and excellence in their software development endeavors.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2710,ICSE-SEIP,Code Generation & Analysis,A Language-agnostic Framework for Mining Static Analysis Rules from Code Changes,"Static analysis tools detect a wide range of code defects, including code quality issues, security vulnerabilities, operational risks, and best-practice violations. Creating and maintaining a set of high-quality static analysis rules that detect misuses of popular libraries and SDKs across multiple languages is challenging. One of the mechanisms for inferring static analysis rules is by leveraging frequently occurring bug-fix code changes in the wild that are committed by multiple developers and into different software repositories. The intuition is that code changes following a common pattern correspond to recurring mistakes, from which deriving best practices could likely be of high value and accepted by the community.Automating the process of mining and clustering code changes enables a scalable mechanism to source and generate best-practices rules. From a coverage standpoint, the rules are derived from real-world code changes, which ensures that popular libraries and application domains are accounted for.In this paper, we present a language-agnostic framework for mining and clustering code changes from software repositories using a graph-based representation dubbed MU (kh 1/4 ). Unlike language-specific ASTs, the MU representation generalizes across languages by modeling programs at a higher semantic level, which enables grouping of code changes that are semantically similar yet syntactically distinct. We have mined a total of 62 high-quality static analysis rules across Java, JavaScript, and Python from less than 600 code change clusters. These cover multiple libraries, including the AWS Java and Python SDKs, as well as libraries like pandas, React, Android libraries, Json parsing libraries, and many more. These rules are integrated into a cloud-based static analyzer, Amazon CodeGuru Reviewer. Developers have accepted 73% of recommendations from these rules during code review, which signifies the value of these rules to help improve developer productivity, make code secure, and improve code hygiene.",Achievement,Intelligent,The paper contributes a language-agnostic framework for mining and clustering code changes; which enables software users to demonstrate their intelligence and competence in software development. This aligns with the value item Intelligent and its corresponding value Achievement.,"I apologize for the confusion. The justification for the alignment of 'Paper X' with the value item v4.1 (Intelligent) and its corresponding value v4 (Achievement) from a ""Software User"" perspective is that the paper's language-agnostic framework for mining and clustering code changes allows software users to improve their code quality, detect and fix various code defects, and adhere to best practices. By utilizing this framework, software users can demonstrate their intelligence and competence in software development by effectively identifying and addressing issues in their code, thereby achieving success in their programming tasks.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2710,ICSE-SEIP,Code Generation & Analysis,A Language-agnostic Framework for Mining Static Analysis Rules from Code Changes,"Static analysis tools detect a wide range of code defects, including code quality issues, security vulnerabilities, operational risks, and best-practice violations. Creating and maintaining a set of high-quality static analysis rules that detect misuses of popular libraries and SDKs across multiple languages is challenging. One of the mechanisms for inferring static analysis rules is by leveraging frequently occurring bug-fix code changes in the wild that are committed by multiple developers and into different software repositories. The intuition is that code changes following a common pattern correspond to recurring mistakes, from which deriving best practices could likely be of high value and accepted by the community.Automating the process of mining and clustering code changes enables a scalable mechanism to source and generate best-practices rules. From a coverage standpoint, the rules are derived from real-world code changes, which ensures that popular libraries and application domains are accounted for.In this paper, we present a language-agnostic framework for mining and clustering code changes from software repositories using a graph-based representation dubbed MU (kh 1/4 ). Unlike language-specific ASTs, the MU representation generalizes across languages by modeling programs at a higher semantic level, which enables grouping of code changes that are semantically similar yet syntactically distinct. We have mined a total of 62 high-quality static analysis rules across Java, JavaScript, and Python from less than 600 code change clusters. These cover multiple libraries, including the AWS Java and Python SDKs, as well as libraries like pandas, React, Android libraries, Json parsing libraries, and many more. These rules are integrated into a cloud-based static analyzer, Amazon CodeGuru Reviewer. Developers have accepted 73% of recommendations from these rules during code review, which signifies the value of these rules to help improve developer productivity, make code secure, and improve code hygiene.",Power,Social Power,The paper integrates high-quality static analysis rules into a cloud-based static analyzer; which allows software users to gain social power and authority in the software development community. This aligns with the value item Social Power and its corresponding value Power.,"In the paper abstract, it is mentioned that the high-quality static analysis rules derived from real-world code changes are integrated into a cloud-based static analyzer. This implies that software users who utilize the static analyzer can now have access to these rules, giving them the power and authority in the software development community. By following these rules, software users can demonstrate their knowledge and adherence to best practices, positioning themselves as experts and gaining social recognition among their peers. This aligns with the value item v5.4 (Social Power) and its corresponding value v5 (Power) from the perspective of a software user.",Agreed-Clarified,Agree,,Agree,,
Exp A,Exp H,2710,ICSE-SEIP,Code Generation & Analysis,A Language-agnostic Framework for Mining Static Analysis Rules from Code Changes,"Static analysis tools detect a wide range of code defects, including code quality issues, security vulnerabilities, operational risks, and best-practice violations. Creating and maintaining a set of high-quality static analysis rules that detect misuses of popular libraries and SDKs across multiple languages is challenging. One of the mechanisms for inferring static analysis rules is by leveraging frequently occurring bug-fix code changes in the wild that are committed by multiple developers and into different software repositories. The intuition is that code changes following a common pattern correspond to recurring mistakes, from which deriving best practices could likely be of high value and accepted by the community.Automating the process of mining and clustering code changes enables a scalable mechanism to source and generate best-practices rules. From a coverage standpoint, the rules are derived from real-world code changes, which ensures that popular libraries and application domains are accounted for.In this paper, we present a language-agnostic framework for mining and clustering code changes from software repositories using a graph-based representation dubbed MU (kh 1/4 ). Unlike language-specific ASTs, the MU representation generalizes across languages by modeling programs at a higher semantic level, which enables grouping of code changes that are semantically similar yet syntactically distinct. We have mined a total of 62 high-quality static analysis rules across Java, JavaScript, and Python from less than 600 code change clusters. These cover multiple libraries, including the AWS Java and Python SDKs, as well as libraries like pandas, React, Android libraries, Json parsing libraries, and many more. These rules are integrated into a cloud-based static analyzer, Amazon CodeGuru Reviewer. Developers have accepted 73% of recommendations from these rules during code review, which signifies the value of these rules to help improve developer productivity, make code secure, and improve code hygiene.",Benevolence,Helpful,The paper presents a scalable mechanism to source and generate best-practices rules; which helps software users to be helpful towards other users or others by improving the overall quality and security of code. This aligns with the value item Helpful and its corresponding value Benevolence.,"In the paper, the authors describe how their framework for mining and clustering code changes enables the generation of high-quality static analysis rules, which in turn improve code quality and security. By providing developers with these rules, software users can be helpful towards others, including the community and other users, by promoting better coding practices and preventing code defects. This aligns with the value item v9.3 (Helpful) and its corresponding value v9 (Benevolence) as it emphasizes the intention to contribute to the well-being and improvement of others in the software context.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2712,ICSE-SEIP,Software Testing & QA,Achieving Last-Mile Functional Coverage in Testing Chip Design Software Implementations,"Defective chips may cause huge losses (even disasters), and thus ensuring the reliability of chips is fundamentally important. To ensure the functional correctness of chips, adequate testing is essential on the chip design implementation (CDI), which is the software implementation of the chip under design in hardware description languages, before putting on fabrication. Over the years, while some techniques targeting CDI functional testing have been proposed, there are still a number of hard-to-cover functionality points due to huge input space and complex constraints among variables in a test input. We call the coverage of these points last-mile functional coverage.Here, we propose the first technique targeting the significant challenge of improving last-mile functional coverage in CDI functional testing, called LMT, which does not rely on domain knowledge and CDI internal information. LMT first identifies the relevant variables in test inputs to the coverage of last-mile functionality points inspired by the idea of feature selection in machine learning, so as to largely reduce the search space. It then incorporates Generative Adversarial Network (GAN) to learn to generate valid test inputs (that satisfy complex constraints among variables) with a larger possibility. We conducted a practical study on two industrial CDIs in Huawei to evaluate LMT. The results show that LMT achieves at least 49.27% and 75.09% higher last-mile functional coverage than the state-of-the-art CDI test input generation techniques under the same number of test inputs, and saves at least 94.24% and 84.45% testing time to achieve the same functional coverage.",Achievement,Successful,The paper proposes a technique called LMT that achieves higher last-mile functional coverage in CDI functional testing compared to the state-of-the-art techniques. This aligns with the value item Achievement and its corresponding value Successful; as it demonstrates the software user's aspiration to excel and be acknowledged for their skills and accomplishments.,"The justification for the alignment of 'Paper X' with the value item v4.3 and its corresponding value v4 is based on the fact that the paper's proposed technique, LMT, achieves higher last-mile functional coverage in CDI functional testing compared to state-of-the-art techniques. This demonstrates the software user's desire to excel and be acknowledged for their skills and accomplishments, aligning with the value item Achievement and its corresponding value Successful. This alignment is evident as the paper highlights the improved performance and effectiveness of LMT, providing tangible evidence of achieving higher levels of success in terms of functional testing coverage.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2712,ICSE-SEIP,Software Testing & QA,Achieving Last-Mile Functional Coverage in Testing Chip Design Software Implementations,"Defective chips may cause huge losses (even disasters), and thus ensuring the reliability of chips is fundamentally important. To ensure the functional correctness of chips, adequate testing is essential on the chip design implementation (CDI), which is the software implementation of the chip under design in hardware description languages, before putting on fabrication. Over the years, while some techniques targeting CDI functional testing have been proposed, there are still a number of hard-to-cover functionality points due to huge input space and complex constraints among variables in a test input. We call the coverage of these points last-mile functional coverage.Here, we propose the first technique targeting the significant challenge of improving last-mile functional coverage in CDI functional testing, called LMT, which does not rely on domain knowledge and CDI internal information. LMT first identifies the relevant variables in test inputs to the coverage of last-mile functionality points inspired by the idea of feature selection in machine learning, so as to largely reduce the search space. It then incorporates Generative Adversarial Network (GAN) to learn to generate valid test inputs (that satisfy complex constraints among variables) with a larger possibility. We conducted a practical study on two industrial CDIs in Huawei to evaluate LMT. The results show that LMT achieves at least 49.27% and 75.09% higher last-mile functional coverage than the state-of-the-art CDI test input generation techniques under the same number of test inputs, and saves at least 94.24% and 84.45% testing time to achieve the same functional coverage.",Benevolence,Helpful,The paper introduces LMT; which incorporates Generative Adversarial Network (GAN) to learn to generate valid test inputs that satisfy complex constraints among variables. This aligns with the value item Benevolence and its corresponding value Helpful; as it contributes to the well-being and welfare of software users by providing a technique that aids in the successful generation of test inputs.,"The justification for aligning 'Paper X' with the value item v9.3 (Helpful) and its corresponding value v9 (Benevolence) is based on the direct contribution of the paper in providing a technique (LMT) that aids in the generation of valid test inputs to ensure the functional correctness of chips. This directly aligns with the value of being helpful because it contributes to the well-being and welfare of software users by providing a method that saves testing time and improves functional coverage, ultimately reducing the potential for defective chips and the associated losses or disasters that may occur.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2713,ICSE-SEIP,Software Deployment & Operations,Auto-tuning elastic applications in production,"Modern cloud applications must be tuned for high performance. Yet, a single static configuration is insufficient since a cloud application must deal with changes in workload, varying numbers of replicas due to auto-scaling, and upgrades to the environment and the application code itself. These dynamics can only be observed altogether during the application execution and affects different layers of the application stack. In this paper, we describe SmartTuning, a technique and tool to auto-tune cloud applications on the fly, improving resource utilization and performance under dynamic workloads. SmartTuning reacts to different workloads over time and automatically explores and adapts the application&#x27;s configuration through Bayesian Optimization. SmartTuning searches for configurations that better use resources when the application is subject to auto-scaling and dynamic workloads. It minimizes the need for the operations team to instrument code or manually try out configurations in testing environments. Our evaluation of three industrial applications indicates that SmartTuning can, on average, improve application efficiency by 58% and reduce cost by 27%.",Achievement,Capable,The paper discusses SmartTuning; a technique and tool to auto-tune cloud applications on the fly; which improves resource utilization and performance. This aligns with the value item Capable and its corresponding value Achievement; as it demonstrates the software user's competence in optimizing and improving the performance of their applications.,"In the context of a ""Software User,"" the alignment between 'Paper X' and the value item v4.2 (Capable) and its corresponding value v4 (Achievement) is evident in the paper's focus on SmartTuning, a technique and tool that enables users to auto-tune cloud applications on the fly, resulting in improved resource utilization and performance. By empowering users to optimize and enhance their applications, SmartTuning showcases the user's capability to successfully achieve their desired outcomes and goals, thereby aligning with the value item of being capable and the broader value of achievement.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2713,ICSE-SEIP,Software Deployment & Operations,Auto-tuning elastic applications in production,"Modern cloud applications must be tuned for high performance. Yet, a single static configuration is insufficient since a cloud application must deal with changes in workload, varying numbers of replicas due to auto-scaling, and upgrades to the environment and the application code itself. These dynamics can only be observed altogether during the application execution and affects different layers of the application stack. In this paper, we describe SmartTuning, a technique and tool to auto-tune cloud applications on the fly, improving resource utilization and performance under dynamic workloads. SmartTuning reacts to different workloads over time and automatically explores and adapts the application&#x27;s configuration through Bayesian Optimization. SmartTuning searches for configurations that better use resources when the application is subject to auto-scaling and dynamic workloads. It minimizes the need for the operations team to instrument code or manually try out configurations in testing environments. Our evaluation of three industrial applications indicates that SmartTuning can, on average, improve application efficiency by 58% and reduce cost by 27%.",Power,Wealth,The paper proposes SmartTuning as a solution to automatically explore and adapt the application's configuration; minimizing the need for manual intervention by the operations team. This aligns with the value item Wealth and its corresponding value Power; as it empowers the software user to have control over their resources and potentially reduce costs.,"My justification is based on the fact that the paper's main contribution, SmartTuning, provides the software user with the power to control their resources and potentially reduce costs. By automatically exploring and adapting the application's configuration, SmartTuning minimizes the need for manual intervention from the operations team. This aligns with the value item Wealth and its corresponding value Power, as it empowers the software user to have control over their resources, which can potentially lead to financial benefits.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2713,ICSE-SEIP,Software Deployment & Operations,Auto-tuning elastic applications in production,"Modern cloud applications must be tuned for high performance. Yet, a single static configuration is insufficient since a cloud application must deal with changes in workload, varying numbers of replicas due to auto-scaling, and upgrades to the environment and the application code itself. These dynamics can only be observed altogether during the application execution and affects different layers of the application stack. In this paper, we describe SmartTuning, a technique and tool to auto-tune cloud applications on the fly, improving resource utilization and performance under dynamic workloads. SmartTuning reacts to different workloads over time and automatically explores and adapts the application&#x27;s configuration through Bayesian Optimization. SmartTuning searches for configurations that better use resources when the application is subject to auto-scaling and dynamic workloads. It minimizes the need for the operations team to instrument code or manually try out configurations in testing environments. Our evaluation of three industrial applications indicates that SmartTuning can, on average, improve application efficiency by 58% and reduce cost by 27%.",Benevolence,Forgiving,The paper introduces SmartTuning; which aims to improve resource utilization and performance under dynamic workloads. By automatically exploring and adapting the application's configuration; it minimizes the need for manual intervention and reduces the workload for the operations team. This aligns with the value item Forgiving and its corresponding value Benevolence; as it helps alleviate the burden on the software user and fosters forgiveness for potential performance issues or inefficiencies.,"In aligning with the value item v9.4 (Forgiving) and its corresponding value v9 (Benevolence), the main contribution of 'Paper X', which is the introduction of SmartTuning, directly addresses the needs and concerns of software users. By automatically adapting the application's configuration to dynamic workloads, SmartTuning reduces the burden on the software user and the operations team, allowing for forgiveness towards potential performance issues or inefficiencies. This alignment emphasizes the user-centric approach of the paper, promoting a benevolent and forgiving software environment that prioritizes the user's experience and alleviates their workload.",Agreed-Clarified,Agree,,Agree,,
Exp A,Exp H,2714,ICSE-SEIP,AI & Machine Learning,Runtime Performance Prediction for Deep Learning Models with Graph Neural Network,"Deep learning models have been widely adopted in many application domains. Predicting the runtime performance of deep learning models, such as GPU memory consumption and training time, is important for boosting development productivity and reducing resource waste. The reason is that improper configurations of hyperparameters and neural architectures can result in many failed training jobs or unsatisfactory models. However, the runtime performance prediction of deep learning models is challenging because of the hybrid programming paradigm, complicated hidden factors within the framework runtime, enormous model configuration space, and broad differences among models. In this paper, we propose DNNPerf, a novel ML-based tool for predicting the runtime performance of deep learning models using Graph Neural Network. DNNPerf represents a model as a directed acyclic computation graph and incorporates a rich set of performance-related features based on the computational semantics of both nodes and edges. We also propose a new Attention-based Node-Edge Encoder for the node and edge features. DNNPerf is evaluated on thousands of configurations of real-world and synthetic deep learning models to predict their GPU memory consumption and training time. The experimental results show that DNNPerf achieves accurate predictions, with an overall error of 7.4% for the training time prediction and an overall error of 13.7% for the GPU memory consumption prediction, confirming its effectiveness.",Achievement,Successful,The paper proposes a novel ML-based tool; DNNPerf; for predicting the runtime performance of deep learning models. This aligns with the value item Successful and its corresponding value Achievement.,"The alignment of the paper with the value item v4.3 (Successful) and its corresponding value Achievement is justified by the fact that the paper proposes a novel ML-based tool, DNNPerf, which aims to accurately predict the runtime performance of deep learning models. The ability to accurately predict performance metrics such as GPU memory consumption and training time can significantly contribute to the success of deep learning projects by boosting development productivity and reducing resource waste. Therefore, the main contribution of the paper directly aligns with the value of Achievement, as it offers a practical solution for achieving successful outcomes in the context of developing and optimizing deep learning models.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2714,ICSE-SEIP,AI & Machine Learning,Runtime Performance Prediction for Deep Learning Models with Graph Neural Network,"Deep learning models have been widely adopted in many application domains. Predicting the runtime performance of deep learning models, such as GPU memory consumption and training time, is important for boosting development productivity and reducing resource waste. The reason is that improper configurations of hyperparameters and neural architectures can result in many failed training jobs or unsatisfactory models. However, the runtime performance prediction of deep learning models is challenging because of the hybrid programming paradigm, complicated hidden factors within the framework runtime, enormous model configuration space, and broad differences among models. In this paper, we propose DNNPerf, a novel ML-based tool for predicting the runtime performance of deep learning models using Graph Neural Network. DNNPerf represents a model as a directed acyclic computation graph and incorporates a rich set of performance-related features based on the computational semantics of both nodes and edges. We also propose a new Attention-based Node-Edge Encoder for the node and edge features. DNNPerf is evaluated on thousands of configurations of real-world and synthetic deep learning models to predict their GPU memory consumption and training time. The experimental results show that DNNPerf achieves accurate predictions, with an overall error of 7.4% for the training time prediction and an overall error of 13.7% for the GPU memory consumption prediction, confirming its effectiveness.",Conformity,Self-Discipline,The paper introduces DNNPerf; which represents a model as a directed acyclic computation graph and incorporates a rich set of performance-related features based on the computational semantics of both nodes and edges. This aligns with the value item Self-Discipline and its corresponding value Conformity.,"I apologize for any confusion caused. My justification for aligning 'Paper X' with the value item v8.1 (Self-Discipline) and its corresponding value v8 (Conformity) is based on the approach taken by DNNPerf in representing a model as a directed acyclic computation graph and incorporating performance-related features. This approach demonstrates conformity to established practices and guidelines within the deep learning community, indicating self-discipline in adhering to standard conventions and frameworks. This alignment aligns with the software user's perspective as it emphasizes the importance of reliability, consistency, and following established norms in the development and performance prediction of deep learning models.",Agreed-Clarified,Agree,,Agree,,
Exp A,Exp H,2714,ICSE-SEIP,AI & Machine Learning,Runtime Performance Prediction for Deep Learning Models with Graph Neural Network,"Deep learning models have been widely adopted in many application domains. Predicting the runtime performance of deep learning models, such as GPU memory consumption and training time, is important for boosting development productivity and reducing resource waste. The reason is that improper configurations of hyperparameters and neural architectures can result in many failed training jobs or unsatisfactory models. However, the runtime performance prediction of deep learning models is challenging because of the hybrid programming paradigm, complicated hidden factors within the framework runtime, enormous model configuration space, and broad differences among models. In this paper, we propose DNNPerf, a novel ML-based tool for predicting the runtime performance of deep learning models using Graph Neural Network. DNNPerf represents a model as a directed acyclic computation graph and incorporates a rich set of performance-related features based on the computational semantics of both nodes and edges. We also propose a new Attention-based Node-Edge Encoder for the node and edge features. DNNPerf is evaluated on thousands of configurations of real-world and synthetic deep learning models to predict their GPU memory consumption and training time. The experimental results show that DNNPerf achieves accurate predictions, with an overall error of 7.4% for the training time prediction and an overall error of 13.7% for the GPU memory consumption prediction, confirming its effectiveness.",Benevolence,Helpful,The paper evaluates DNNPerf on thousands of configurations of real-world and synthetic deep learning models to predict their GPU memory consumption and training time. By helping software users improve the runtime performance of their models; this aligns with the value item Helpful and its corresponding value Benevolence.,"The main contribution of 'Paper X' is the development of DNNPerf, a tool that predicts the runtime performance of deep learning models. By providing accurate predictions of GPU memory consumption and training time, DNNPerf assists software users in optimizing their models and avoiding failed training jobs or unsatisfactory results. This directly aligns with the value item v9.3 (Helpful) and its corresponding value v9 (Benevolence) from Schwartz's Taxonomy, as the paper's contributions aim to be beneficial and supportive to software users in the context of improving their models' performance.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2716,ICSE-SEIP,Software Deployment & Operations,Hybrid Cloudification of Legacy Software for Efficient Simulation of Gas Turbine Designs,"When developing aeroderivative gas turbines at Siemens Energy, engine models are subject to complex simulation campaigns for finite element analysis carried out by a legacy simulation tool. This paper presents results of a multi-year software modernization project to provide a software-as-a-service (SaaS) framework that enables the distributed and automated execution of simulation jobs over a hybrid cloud platform containing both private cloud and public cloud nodes. Our framework allows to significantly reduce the net time required for completing complex simulation campaigns, thus increasing the effectiveness of engineers. The performance of our framework is evaluated in various cloud configurations with complex simulation campaigns performed in the context of a real simulation task.",Achievement,Ambitious,The paper contributes to the development of a software framework that enables the efficient execution of complex simulation campaigns. This aligns with the value item Ambitious; as it allows software users to achieve their goals of completing these campaigns effectively.,"In 'Paper X', the development of a software framework that enables the efficient execution of complex simulation campaigns aligns with the value item Ambitious from Schwartz's Taxonomy. The ambition lies in the ability of software users to accomplish their goals of completing these campaigns effectively by utilizing the framework. This aligns with the value of achievement and the pursuit of success, as engineers can enhance their effectiveness in completing simulation campaigns within shorter timeframes. By leveraging the framework, they can strive for excellence in their work, demonstrating ambition in achieving their objectives.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2716,ICSE-SEIP,Software Deployment & Operations,Hybrid Cloudification of Legacy Software for Efficient Simulation of Gas Turbine Designs,"When developing aeroderivative gas turbines at Siemens Energy, engine models are subject to complex simulation campaigns for finite element analysis carried out by a legacy simulation tool. This paper presents results of a multi-year software modernization project to provide a software-as-a-service (SaaS) framework that enables the distributed and automated execution of simulation jobs over a hybrid cloud platform containing both private cloud and public cloud nodes. Our framework allows to significantly reduce the net time required for completing complex simulation campaigns, thus increasing the effectiveness of engineers. The performance of our framework is evaluated in various cloud configurations with complex simulation campaigns performed in the context of a real simulation task.",Benevolence,Helpful,The paper presents a software-as-a-service (SaaS) framework that automates the execution of simulation jobs; reducing the net time required for completing complex campaigns. This aligns with the value item Helpful; as it enables software users to contribute to the well-being and success of their engineering projects.,"The alignment of 'Paper X' with the value item v9.3 (Helpful) from a ""Software User"" perspective is justified by the fact that the software-as-a-service (SaaS) framework presented in the paper allows users to automate and streamline the execution of simulation jobs. By reducing the net time required for completing complex campaigns, the framework enables software users to be more productive and efficient in their engineering projects, ultimately contributing to the well-being and success of those projects. This direct alignment corresponds to the value of being helpful, as it empowers software users to actively contribute to the achievement of their goals and the overall success of their work.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2716,ICSE-SEIP,Software Deployment & Operations,Hybrid Cloudification of Legacy Software for Efficient Simulation of Gas Turbine Designs,"When developing aeroderivative gas turbines at Siemens Energy, engine models are subject to complex simulation campaigns for finite element analysis carried out by a legacy simulation tool. This paper presents results of a multi-year software modernization project to provide a software-as-a-service (SaaS) framework that enables the distributed and automated execution of simulation jobs over a hybrid cloud platform containing both private cloud and public cloud nodes. Our framework allows to significantly reduce the net time required for completing complex simulation campaigns, thus increasing the effectiveness of engineers. The performance of our framework is evaluated in various cloud configurations with complex simulation campaigns performed in the context of a real simulation task.",Power,Authority,The paper introduces a hybrid cloud platform that includes both private and public cloud nodes for the execution of simulation jobs. This aligns with the value item Authority; as it empowers software users with the ability to control and have authority over their simulation tasks by leveraging the cloud resources.,"The alignment of 'Paper X' with value item v5.3 and its corresponding value v5 (Authority) is evident in the introduction of a hybrid cloud platform. This platform gives software users the authority and control over their simulation tasks by leveraging both private and public cloud nodes. They have the power to choose and manage their resources, enabling them to make decisions and have control over their simulation campaigns. This aligns directly with the value of Authority, as it empowers software users in their decision-making process and gives them the ability to exercise control over their tasks.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2717,ICSE-SEIP,Code Generation & Analysis,Automated Misconfiguration Repair of Configurable Cyber-Physical Systems with Search: an Industrial Case Study on Elevator Dispatching Algorithms,"Real-world Cyber-Physical Systems (CPSs) are usually configurable. Through parameters, it is possible to configure, select or unselect different system functionalities. While this provides high flexibility, it also becomes a source for failures due to misconfigurations. The large number of parameters these systems have and the long test execution time in this context due to the use of simulation-based testing make the manual repair process a cumbersome activity. Subsequently, in this context, automated repairing methods are paramount. In this paper, we propose an approach to automatically repair CPSsaEUR(tm) misconfigurations. Our approach is evaluated with an industrial CPS case study from the elevation domain. Experiments with a real building and data obtained from operation suggests that our approach outperforms a baseline algorithm as well as the state of the practice (i.e., manual repair carried out by domain experts).",Achievement,Successful,The paper proposes an approach to automatically repair misconfigurations in Cyber-Physical Systems (CPSs). This aligns with the value item Successful and its corresponding value Achievement.,"The paper's proposed approach to automatically repair misconfigurations in CPSs is aligned with the value item v4.3 and its corresponding value v4 because successfully addressing misconfigurations in these systems demonstrates intelligence, capability, and ambition, ultimately leading to achieving successful outcomes.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2717,ICSE-SEIP,Code Generation & Analysis,Automated Misconfiguration Repair of Configurable Cyber-Physical Systems with Search: an Industrial Case Study on Elevator Dispatching Algorithms,"Real-world Cyber-Physical Systems (CPSs) are usually configurable. Through parameters, it is possible to configure, select or unselect different system functionalities. While this provides high flexibility, it also becomes a source for failures due to misconfigurations. The large number of parameters these systems have and the long test execution time in this context due to the use of simulation-based testing make the manual repair process a cumbersome activity. Subsequently, in this context, automated repairing methods are paramount. In this paper, we propose an approach to automatically repair CPSsaEUR(tm) misconfigurations. Our approach is evaluated with an industrial CPS case study from the elevation domain. Experiments with a real building and data obtained from operation suggests that our approach outperforms a baseline algorithm as well as the state of the practice (i.e., manual repair carried out by domain experts).",Conformity,Self-Discipline,The paper addresses the issue of manual repair process for misconfigurations in CPSs; which can be a cumbersome activity. By proposing an automated repairing method; the paper contributes to making the repair process more efficient and systematic. This aligns with the value item Self-Discipline and its corresponding value Conformity.,"I apologize for any confusion caused by my previous justification. In aligning 'Paper X' with the value item v8.1 (Self-Discipline) and its corresponding value v8 (Conformity), my reasoning is based on the aspect of the paper addressing the need for a more systematic and efficient automated repair process. By replacing the manual repair process, which can be cumbersome and prone to errors, with an automated method, the paper highlights the importance of adhering to a standardized and disciplined approach to ensure accurate and reliable repairs. This emphasis on conformity aligns with the value item of Self-Discipline, as it focuses on following established rules and procedures to achieve desired outcomes.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2717,ICSE-SEIP,Code Generation & Analysis,Automated Misconfiguration Repair of Configurable Cyber-Physical Systems with Search: an Industrial Case Study on Elevator Dispatching Algorithms,"Real-world Cyber-Physical Systems (CPSs) are usually configurable. Through parameters, it is possible to configure, select or unselect different system functionalities. While this provides high flexibility, it also becomes a source for failures due to misconfigurations. The large number of parameters these systems have and the long test execution time in this context due to the use of simulation-based testing make the manual repair process a cumbersome activity. Subsequently, in this context, automated repairing methods are paramount. In this paper, we propose an approach to automatically repair CPSsaEUR(tm) misconfigurations. Our approach is evaluated with an industrial CPS case study from the elevation domain. Experiments with a real building and data obtained from operation suggests that our approach outperforms a baseline algorithm as well as the state of the practice (i.e., manual repair carried out by domain experts).",Security,Healthy,The paper's approach to automatically repair misconfigurations in CPSs aims to ensure the overall system's health and reliability. By reducing the failures caused by misconfigurations; the paper contributes to the value item Healthy and its corresponding value Security.,"In ""Paper X,"" the approach to automatically repair misconfigurations in CPSs directly aligns with the value item v6.1 (Healthy) and its corresponding value v6 (Security) from a ""Software User"" perspective. The main contribution of the paper is to address the source of failures in CPSs caused by misconfigurations, providing automated repairing methods to ensure system health and reliability. By reducing these failures, the paper aligns with the value item Healthy, as it aims to enhance the overall system's security and stability.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2719,ICSE-SEIP,Code Generation & Analysis,Identifying Defect Injection Risks from Analysis and Design Diagrams: An Industrial Case Study at Sony,"Identifying the origins of potentially injected defects in implementation activities during requirement analysis and design activities is challenging, but leads to the prevention of defects, which are burdensome to correct. This study investigates whether such defect injection risks can be generalized and defined by risk occurrence conditions of the objects in the existing analysis and design diagrams and whether the defined defect injection risks are applicable to other analysis and design diagrams. Specifically, we identify defect categories, which are injected after analysis and design activities and subsequently detected during system testing of commercial products developed at Sony. Then, regarding the defect categories as the exposed defect injection risks, we define defect injection risks with the objects defined in the analysis and design diagrams of the products. Each defect injection risk consists of risk description, diagram type, and occurrence conditions of objects in the diagrams. Afterwards, we evaluate whether the defined defect injection risks appear in the analysis and design diagrams of three different products under development and five publicly available analysis and design diagrams. The results showed that three defect injection risks were defined and that the two risks appear in the analysis and design diagrams of the three products and the remaining one appears in the analysis and design diagrams of two products. The results also showed that one defect injection risk is present in all five publicly available analysis and design diagrams, and two risks appear in four of the diagrams. The three defect injection risks are general enough to identify risks in analysis and design diagrams from other domains. Developers can be more cautious about the risks and prevent defect injections with the defect injection risks.",Achievement,Capable,The paper contributes to the identification of potential defects during requirement analysis and design activities; which aligns with the value item Capable and its corresponding value Achievement. The software users can demonstrate their competence and skills by effectively preventing and addressing defects in the software development process.,"The main contribution of 'Paper X' is the identification and prevention of defects during requirement analysis and design activities in software development. This directly aligns with the value item Capable and its corresponding value Achievement from a ""Software User"" perspective. By effectively preventing and addressing defects, software users can demonstrate their competence and skills in ensuring the quality and success of the software they use.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2719,ICSE-SEIP,Code Generation & Analysis,Identifying Defect Injection Risks from Analysis and Design Diagrams: An Industrial Case Study at Sony,"Identifying the origins of potentially injected defects in implementation activities during requirement analysis and design activities is challenging, but leads to the prevention of defects, which are burdensome to correct. This study investigates whether such defect injection risks can be generalized and defined by risk occurrence conditions of the objects in the existing analysis and design diagrams and whether the defined defect injection risks are applicable to other analysis and design diagrams. Specifically, we identify defect categories, which are injected after analysis and design activities and subsequently detected during system testing of commercial products developed at Sony. Then, regarding the defect categories as the exposed defect injection risks, we define defect injection risks with the objects defined in the analysis and design diagrams of the products. Each defect injection risk consists of risk description, diagram type, and occurrence conditions of objects in the diagrams. Afterwards, we evaluate whether the defined defect injection risks appear in the analysis and design diagrams of three different products under development and five publicly available analysis and design diagrams. The results showed that three defect injection risks were defined and that the two risks appear in the analysis and design diagrams of the three products and the remaining one appears in the analysis and design diagrams of two products. The results also showed that one defect injection risk is present in all five publicly available analysis and design diagrams, and two risks appear in four of the diagrams. The three defect injection risks are general enough to identify risks in analysis and design diagrams from other domains. Developers can be more cautious about the risks and prevent defect injections with the defect injection risks.",Achievement,Successful,The paper aims to prevent defects by identifying their origins and providing a framework for risk evaluation. This aligns with the value item Successful and its corresponding value Achievement. By successfully implementing the defined defect injection risks; software users can ensure the quality and reliability of the developed software.,"The justification for aligning 'Paper X' with the value item v4.3, Successful, and its corresponding value Achievement, is based on the paper's aim to prevent defects by identifying their origins and providing a framework for risk evaluation. By successfully implementing the defined defect injection risks, software users can ensure the quality and reliability of the developed software, which directly aligns with the value of Achievement. The paper's focus on achieving successful outcomes in terms of defect prevention and quality improvement reflects the value of Achievement from a software user's perspective.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2719,ICSE-SEIP,Code Generation & Analysis,Identifying Defect Injection Risks from Analysis and Design Diagrams: An Industrial Case Study at Sony,"Identifying the origins of potentially injected defects in implementation activities during requirement analysis and design activities is challenging, but leads to the prevention of defects, which are burdensome to correct. This study investigates whether such defect injection risks can be generalized and defined by risk occurrence conditions of the objects in the existing analysis and design diagrams and whether the defined defect injection risks are applicable to other analysis and design diagrams. Specifically, we identify defect categories, which are injected after analysis and design activities and subsequently detected during system testing of commercial products developed at Sony. Then, regarding the defect categories as the exposed defect injection risks, we define defect injection risks with the objects defined in the analysis and design diagrams of the products. Each defect injection risk consists of risk description, diagram type, and occurrence conditions of objects in the diagrams. Afterwards, we evaluate whether the defined defect injection risks appear in the analysis and design diagrams of three different products under development and five publicly available analysis and design diagrams. The results showed that three defect injection risks were defined and that the two risks appear in the analysis and design diagrams of the three products and the remaining one appears in the analysis and design diagrams of two products. The results also showed that one defect injection risk is present in all five publicly available analysis and design diagrams, and two risks appear in four of the diagrams. The three defect injection risks are general enough to identify risks in analysis and design diagrams from other domains. Developers can be more cautious about the risks and prevent defect injections with the defect injection risks.",Stimulation,Excitement in Life,The paper investigates the occurrence and generalization of defect injection risks in analysis and design diagrams. This aligns with the value item Excitement in Life and its corresponding value Stimulation. The software users can find excitement and stimulation in the process of exploring and addressing potential risks and defects in their software development projects.,"In the context of software development, the process of identifying and addressing potential risks and defects can be an exciting and stimulating experience for software users. By investigating and generalizing defect injection risks in analysis and design diagrams, 'Paper X' directly aligns with the value item v2.3 (Excitement in Life) and its corresponding value v2 (Stimulation) from a ""Software User"" perspective. This alignment suggests that software users can derive excitement and stimulation from actively participating in the prevention and resolution of defects in their software projects, contributing to a more engaging and fulfilling software development experience.",Agreed-Clarified,Agree,,Agree,,
Exp A,Exp H,2721,ICSE-SEIP,Code Generation & Analysis,Code Compliance Assessment as a Learning Problem,"Manual code reviews and static code analyzers are the traditional mechanisms to verify if source code complies with coding policies. However, they are hard to scale. We formulate code compliance assessment as a machine learning (ML) problem, to take as input a natural language policy and code, and generate a prediction on the codeaEUR(tm)s compliance, non-compliance, or irrelevance. Our intention for ML-based automation is to scale the development of Amazon CodeGuru, a commercial code analyzer. We explore key research questions on model formulation, training data, and evaluation setup. We obtain a joint code-text representation space (embeddings) which preserves compliance relationships via the vector distance of code and policy embeddings. As there is no task-specific data, we re-interpret and filter commonly available software datasets with additional pre-training and pre-finetuning tasks that reduce the semantic gap. We benchmarked our approach on two listings of coding policies (CWE and CBP). This is a zero-shot evaluation as none of the policies occur in the training set. On CWE and CBP respectively, our tool Policy2Code achieves classification accuracies of (59%, 71%) and search MRR of (0.05, 0.21) compared to CodeBERT with classification accuracies of (37%, 54%) and MRR of (0.02, 0.02). In a user study, 24% Policy2Code detections were accepted compared to 7% for CodeBERT. Policy2Code is considered a useful ML-based aid to supplement manual efforts.",Achievement,Successful,The paper's main contribution is the development of a machine learning model for code compliance assessment. This aligns with the value item Successful and its corresponding value Achievement.,"In 'Paper X', the main contribution is the development of a machine learning model for code compliance assessment, with the aim of automating the verification of source code compliance with coding policies. This aligns with the value item v4.3 (Successful) and its corresponding value v4 (Achievement) from a ""Software User"" perspective because the successful implementation of this ML model would enable a more efficient and scalable approach to ensuring code compliance, which can lead to improved software quality and successful software development projects.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2721,ICSE-SEIP,Code Generation & Analysis,Code Compliance Assessment as a Learning Problem,"Manual code reviews and static code analyzers are the traditional mechanisms to verify if source code complies with coding policies. However, they are hard to scale. We formulate code compliance assessment as a machine learning (ML) problem, to take as input a natural language policy and code, and generate a prediction on the codeaEUR(tm)s compliance, non-compliance, or irrelevance. Our intention for ML-based automation is to scale the development of Amazon CodeGuru, a commercial code analyzer. We explore key research questions on model formulation, training data, and evaluation setup. We obtain a joint code-text representation space (embeddings) which preserves compliance relationships via the vector distance of code and policy embeddings. As there is no task-specific data, we re-interpret and filter commonly available software datasets with additional pre-training and pre-finetuning tasks that reduce the semantic gap. We benchmarked our approach on two listings of coding policies (CWE and CBP). This is a zero-shot evaluation as none of the policies occur in the training set. On CWE and CBP respectively, our tool Policy2Code achieves classification accuracies of (59%, 71%) and search MRR of (0.05, 0.21) compared to CodeBERT with classification accuracies of (37%, 54%) and MRR of (0.02, 0.02). In a user study, 24% Policy2Code detections were accepted compared to 7% for CodeBERT. Policy2Code is considered a useful ML-based aid to supplement manual efforts.",Power,Social Recognition,The paper mentions the development of Amazon CodeGuru; a commercial code analyzer. This aligns with the value item Social Recognition and its corresponding value Power.,"In the given abstract, the main focus of 'Paper X' is on the development of Amazon CodeGuru, a commercial code analyzer. The paper mentions that they aim to scale the development of this tool through ML-based automation. This aligns with the value item v5.1 (Social Recognition) and its corresponding value v5 (Power) from a ""Software User"" perspective because the development of a commercial code analyzer like CodeGuru can provide users with social recognition and power within their software development community. By automating code compliance assessment, users can demonstrate their expertise and influence in ensuring code quality and adherence to policies.",Agreed-Clarified,Agree,,Agree,,
Exp A,Exp H,2723,ICSE-SEIP,AI & Machine Learning,Automated Metamorphic Testing using Transitive Relations for Specializing Stance Detection Models,"In machine-learning-based natural language processing, methods with high accuracy have been proposed for stance detection tasks. However, when they are applied to specific domains, they are often inaccurate due to domain-specific expressions. We propose an automated metamorphic testing method using transitive relations for creating training data that specializes stance detection in a specific domain. By specializing IBM DebateraEUR(tm)s stance detection in currency exchange domain, we confirmed our proposed method can improve the accuracy of judging the currency exchange-related sentences.",Achievement,Successful,The paper proposes an automated metamorphic testing method for creating training data that specializes stance detection in a specific domain. This aligns with the value item Successful and its corresponding value Achievement.,"The justification for aligning 'Paper X' with the value item v4.3 (Successful) and its corresponding value v4 (Achievement) is that the paper's proposed method of automated metamorphic testing specifically aims to improve the accuracy of stance detection in a specific domain. By developing a method that addresses the challenge of domain-specific expressions, the paper contributes to the successful achievement of accurate stance detection in that domain. This aligns with the value of Achievement as it focuses on the success and effectiveness of the proposed method in improving the accuracy of judging currency exchange-related sentences, which is directly aligned with the value item v4.3 (Successful).",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2723,ICSE-SEIP,AI & Machine Learning,Automated Metamorphic Testing using Transitive Relations for Specializing Stance Detection Models,"In machine-learning-based natural language processing, methods with high accuracy have been proposed for stance detection tasks. However, when they are applied to specific domains, they are often inaccurate due to domain-specific expressions. We propose an automated metamorphic testing method using transitive relations for creating training data that specializes stance detection in a specific domain. By specializing IBM DebateraEUR(tm)s stance detection in currency exchange domain, we confirmed our proposed method can improve the accuracy of judging the currency exchange-related sentences.",Power,Preserving My Public Image,The paper's proposed method of improving the accuracy of judging currency exchange-related sentences implies a concern for preserving the public image of users. This aligns with the value item Preserving My Public Image and its corresponding value Power.,"In the context of a software user, the ability to accurately judge currency exchange-related sentences is important as it directly impacts the user's financial decisions. By improving the accuracy of this judgement, the paper's proposed method indirectly aligns with the value item of preserving the user's public image. This is because accurate judgement and decision-making in currency exchange can contribute to the user's perception of being knowledgeable and influential in financial matters, thereby preserving their public image as someone who is competent in financial decision-making.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2723,ICSE-SEIP,AI & Machine Learning,Automated Metamorphic Testing using Transitive Relations for Specializing Stance Detection Models,"In machine-learning-based natural language processing, methods with high accuracy have been proposed for stance detection tasks. However, when they are applied to specific domains, they are often inaccurate due to domain-specific expressions. We propose an automated metamorphic testing method using transitive relations for creating training data that specializes stance detection in a specific domain. By specializing IBM DebateraEUR(tm)s stance detection in currency exchange domain, we confirmed our proposed method can improve the accuracy of judging the currency exchange-related sentences.",Benevolence,Helpful,The paper's proposed method aims to improve the accuracy of stance detection in a specific domain; which can contribute to users being helpful towards other users or others. This aligns with the value item Helpful and its corresponding value Benevolence.,"The proposed metamorphic testing method in the paper aims to improve the accuracy of stance detection in a specific domain, which can contribute to users being helpful towards other users or others. This aligns with the value item Helpful and its corresponding value Benevolence, as it promotes the intention of being supportive and assisting others in achieving better outcomes or understanding in a software context.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2724,ICSE-SEIP,Code Generation & Analysis,Incremental Call Graph Construction in Industrial Practice,"Interprocedural program analysis is critical in finding hidden program defects and vulnerabilities in CI/CD pipelines. A pre-constructed call graph is a prerequisite for interprocedural analysis. However, the exhaustive call graph construction, i.e., analyzing the target program as a whole and constructing from scratch, often takes too much time. We made a scalable empirical study on both industrial and open-source projects and observed that most program updates only involve a very limited part of the code. The observation inspires an efficient approach that not wholely re-constructs a call graph but incrementally patches the old one with the partial graph affected by the update. We propose a sound incremental call graph construction algorithm that works in a reset-recompute way: first, prune invalid nodes and edges from the old call graph, then analyze the new code to patch it to construct the new one. We implemented the algorithm and built a benchmark suite consisting of 20 industrial and 10 open-source projects. The experimental evaluation shows that the efficiency improvement is encouraging. Compared with the exhaustive construction algorithm, the incremental way can speed up the construction by 20.0 times and reduce the memory and storage consumption to 58.1% and 10.4%, respectively.",Achievement,Intelligent,The paper's proposal of a sound incremental call graph construction algorithm aligns with the value item Intelligent; as it demonstrates a sophisticated approach to interprocedural program analysis in CI/CD pipelines.,"The paper's proposal of a sound incremental call graph construction algorithm aligns with the value item ""Intelligent"" as it showcases a thoughtful and advanced methodology for analyzing programs in CI/CD pipelines. By efficiently patching the existing call graph with the parts affected by updates, it demonstrates a high level of intelligence in the approach, optimizing the analysis process and improving efficiency. This aligns with the value of intelligence as it indicates the use of clever techniques to solve complex software analysis problems.",Agreed-Clarified,Disagree,reconcile with coder_2,Disagree,"The justification doesn't align with value item of the Intelligent
",
Exp A,Exp H,2724,ICSE-SEIP,Code Generation & Analysis,Incremental Call Graph Construction in Industrial Practice,"Interprocedural program analysis is critical in finding hidden program defects and vulnerabilities in CI/CD pipelines. A pre-constructed call graph is a prerequisite for interprocedural analysis. However, the exhaustive call graph construction, i.e., analyzing the target program as a whole and constructing from scratch, often takes too much time. We made a scalable empirical study on both industrial and open-source projects and observed that most program updates only involve a very limited part of the code. The observation inspires an efficient approach that not wholely re-constructs a call graph but incrementally patches the old one with the partial graph affected by the update. We propose a sound incremental call graph construction algorithm that works in a reset-recompute way: first, prune invalid nodes and edges from the old call graph, then analyze the new code to patch it to construct the new one. We implemented the algorithm and built a benchmark suite consisting of 20 industrial and 10 open-source projects. The experimental evaluation shows that the efficiency improvement is encouraging. Compared with the exhaustive construction algorithm, the incremental way can speed up the construction by 20.0 times and reduce the memory and storage consumption to 58.1% and 10.4%, respectively.",Achievement,Capable,The paper's emphasis on efficiency improvement in constructing call graphs aligns with the value item Capable; as it demonstrates a capability to optimize the process and reduce time and resource consumption.,"My justification aligning 'Paper X' with the value item v4.2 (Capable) and its corresponding value v4 (Achievement) is based on the explicit contributions mentioned in the abstract. The paper proposes an efficient approach for constructing call graphs in interprocedural program analysis, specifically focused on reducing time and resource consumption. This optimization demonstrates a capability to achieve improved efficiency and success in the analysis process, thereby aligning with the value of Achievement, as it strives for capable and accomplished outcomes in software use.",Agreed-Justified,Disagree,reconcile with coder_2,Disagree,"The justification claiming that the paper aligns with the value 'Capable',but the abstract, neither explains how nor confirms this.",
Exp A,Exp H,2724,ICSE-SEIP,Code Generation & Analysis,Incremental Call Graph Construction in Industrial Practice,"Interprocedural program analysis is critical in finding hidden program defects and vulnerabilities in CI/CD pipelines. A pre-constructed call graph is a prerequisite for interprocedural analysis. However, the exhaustive call graph construction, i.e., analyzing the target program as a whole and constructing from scratch, often takes too much time. We made a scalable empirical study on both industrial and open-source projects and observed that most program updates only involve a very limited part of the code. The observation inspires an efficient approach that not wholely re-constructs a call graph but incrementally patches the old one with the partial graph affected by the update. We propose a sound incremental call graph construction algorithm that works in a reset-recompute way: first, prune invalid nodes and edges from the old call graph, then analyze the new code to patch it to construct the new one. We implemented the algorithm and built a benchmark suite consisting of 20 industrial and 10 open-source projects. The experimental evaluation shows that the efficiency improvement is encouraging. Compared with the exhaustive construction algorithm, the incremental way can speed up the construction by 20.0 times and reduce the memory and storage consumption to 58.1% and 10.4%, respectively.",Security,Social Order,The paper's focus on analyzing limited parts of the code rather than the entire program aligns with the value item Social Order; as it promotes an organized and efficient approach to program analysis that maintains the integrity and structure of the codebase.,"The focus of 'Paper X' on analyzing limited parts of the code aligns with the value item Social Order because it promotes an organized and efficient approach to program analysis. By only analyzing the parts of the code that are affected by updates, the paper contributes to maintaining the overall structure and integrity of the codebase, thereby promoting social order within the software context. This approach avoids unnecessary overhead and ensures that the analysis process is streamlined and optimized for efficiency in maintaining social order within the software development process.",Agreed-Clarified,Disagree,reconcile with coder_2,Disagree,"The justification claims that the paper aligns with the value of 'Security' , but it is not supported by the evidence in the abstract",
Exp A,Exp H,2758,ICSE-SEIS,Accessibility & User Experience,A New Approach Towards Ensuring Gender Inclusive SE Job Advertisements,"A majority of software engineers are male, perhaps, because of the very way that software engineering (SE) roles are advertised is gender biased. Thus far, only word-based checking tools are available to identify gender biases (e.g., aEURoeanalystaEUR is considered a masculine word). However, such word-based analyses end up identifying the skills required for SE job positions as masculine words, and therefore, not sufficient. In this work, we present a more nu-anced mechanism to check for gender bias in SE job advertisements by building on the GenderMag method, which has proven to be successful in gender bias detection in software interfaces. From a survey of 44 software practitioners, we identified 16 factors where male and female participants differ and based on a thematic analysis we derived three SE job applicant persona facets. We verified the facets with a small survey where SE candidates related to the descriptions of those factors. We conducted a pilot study using these facets to evaluate four SE job advertisements and identified gender related biases in two of those. The software engineering (SE) workforce is dominated by male employees. This is potentially because SE job advertisements are gender biased toward men. SE job advertisements are often checked for potential gender bias using word-based bias detection tools. However, these do not take software engineering specific words into consideration. For example the wordaEUR(tm) analystaEUR(tm) is considered male using the word-based tools, and therefore any SE analyst position would be seen (incorrectly) to be biased toward men. As a result, such word-based tools are not good enough to accurately assess for gender bias in SE job ads. We present a more nuanced mechanism to check for gender bias in SE job advertisements by building on the GenderMag method. GenderMag has proven to be successful in gender bias detection in software interfaces. This method identifies five dimensions of problem solving (referred to as facets) on which male and female software users differ. From a survey of software practitioners, we identified factors where male and female SE job applicants differ. We grouped the similar factors together and derived three facets that were important to predict job application behaviour of SE job applicants. We validated these facets with some SE candidates and used them to develop aEURkhnewaEUR(tm) Tim and Abi personas from the facets and evaluated four SE job advertisements. We identified gender related biases in two of these.",Stimulation,Daring,The paper presents a more nuanced mechanism to check for gender bias in software engineering job advertisements; aligning with the value item Daring from the Stimulation value.,"I apologize for any confusion caused. My justification for aligning 'Paper X' with the value item v2.1 (Daring) and its corresponding value v2 (Stimulation) is based on the fact that the paper introduces a new approach to identifying gender bias in software engineering job advertisements. This demonstrates a willingness to challenge existing norms and take risks in addressing a significant problem. By pushing the boundaries of traditional word-based bias detection tools and utilizing the GenderMag method, the authors of 'Paper X' are showing a daring approach to stimulate change in the software engineering industry and promote more inclusive job advertisements.",Agreed-Clarified,Agree,,Agree,,
Exp A,Exp H,2758,ICSE-SEIS,Accessibility & User Experience,A New Approach Towards Ensuring Gender Inclusive SE Job Advertisements,"A majority of software engineers are male, perhaps, because of the very way that software engineering (SE) roles are advertised is gender biased. Thus far, only word-based checking tools are available to identify gender biases (e.g., aEURoeanalystaEUR is considered a masculine word). However, such word-based analyses end up identifying the skills required for SE job positions as masculine words, and therefore, not sufficient. In this work, we present a more nu-anced mechanism to check for gender bias in SE job advertisements by building on the GenderMag method, which has proven to be successful in gender bias detection in software interfaces. From a survey of 44 software practitioners, we identified 16 factors where male and female participants differ and based on a thematic analysis we derived three SE job applicant persona facets. We verified the facets with a small survey where SE candidates related to the descriptions of those factors. We conducted a pilot study using these facets to evaluate four SE job advertisements and identified gender related biases in two of those. The software engineering (SE) workforce is dominated by male employees. This is potentially because SE job advertisements are gender biased toward men. SE job advertisements are often checked for potential gender bias using word-based bias detection tools. However, these do not take software engineering specific words into consideration. For example the wordaEUR(tm) analystaEUR(tm) is considered male using the word-based tools, and therefore any SE analyst position would be seen (incorrectly) to be biased toward men. As a result, such word-based tools are not good enough to accurately assess for gender bias in SE job ads. We present a more nuanced mechanism to check for gender bias in SE job advertisements by building on the GenderMag method. GenderMag has proven to be successful in gender bias detection in software interfaces. This method identifies five dimensions of problem solving (referred to as facets) on which male and female software users differ. From a survey of software practitioners, we identified factors where male and female SE job applicants differ. We grouped the similar factors together and derived three facets that were important to predict job application behaviour of SE job applicants. We validated these facets with some SE candidates and used them to develop aEURkhnewaEUR(tm) Tim and Abi personas from the facets and evaluated four SE job advertisements. We identified gender related biases in two of these.",Benevolence,Helpful,The paper's contribution of identifying gender biases in software engineering job advertisements aims to promote helpfulness and contribute to the well-being of software users; aligning with the value item Helpful from the Benevolence value.,"The contribution of 'Paper X' in identifying gender biases in software engineering job advertisements aligns with the value item ""Helpful"" from the Benevolence value because it aims to address and rectify gender bias in the industry. By recognizing and addressing these biases, the paper seeks to create a more inclusive and supportive environment for software users, contributing to their overall well-being. This aligns with the value of helpfulness as it focuses on promoting the welfare and fairness of individuals within the software context.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2758,ICSE-SEIS,Accessibility & User Experience,A New Approach Towards Ensuring Gender Inclusive SE Job Advertisements,"A majority of software engineers are male, perhaps, because of the very way that software engineering (SE) roles are advertised is gender biased. Thus far, only word-based checking tools are available to identify gender biases (e.g., aEURoeanalystaEUR is considered a masculine word). However, such word-based analyses end up identifying the skills required for SE job positions as masculine words, and therefore, not sufficient. In this work, we present a more nu-anced mechanism to check for gender bias in SE job advertisements by building on the GenderMag method, which has proven to be successful in gender bias detection in software interfaces. From a survey of 44 software practitioners, we identified 16 factors where male and female participants differ and based on a thematic analysis we derived three SE job applicant persona facets. We verified the facets with a small survey where SE candidates related to the descriptions of those factors. We conducted a pilot study using these facets to evaluate four SE job advertisements and identified gender related biases in two of those. The software engineering (SE) workforce is dominated by male employees. This is potentially because SE job advertisements are gender biased toward men. SE job advertisements are often checked for potential gender bias using word-based bias detection tools. However, these do not take software engineering specific words into consideration. For example the wordaEUR(tm) analystaEUR(tm) is considered male using the word-based tools, and therefore any SE analyst position would be seen (incorrectly) to be biased toward men. As a result, such word-based tools are not good enough to accurately assess for gender bias in SE job ads. We present a more nuanced mechanism to check for gender bias in SE job advertisements by building on the GenderMag method. GenderMag has proven to be successful in gender bias detection in software interfaces. This method identifies five dimensions of problem solving (referred to as facets) on which male and female software users differ. From a survey of software practitioners, we identified factors where male and female SE job applicants differ. We grouped the similar factors together and derived three facets that were important to predict job application behaviour of SE job applicants. We validated these facets with some SE candidates and used them to develop aEURkhnewaEUR(tm) Tim and Abi personas from the facets and evaluated four SE job advertisements. We identified gender related biases in two of these.",Universalism,Equality,The paper's focus on identifying and addressing gender biases in software engineering job advertisements reflects a commitment to equality; aligning with the value item Equality from the Universalism value.,"The justification for labeling 'Paper X' as aligning with the value item v10.2 and its corresponding value v10 from a ""Software User"" perspective is that the paper explicitly addresses the issue of gender bias in software engineering job advertisements and aims to provide a more nuanced mechanism to check for such bias. By acknowledging and attempting to rectify these biases, the paper demonstrates a concern for equality, which is a central component of the Universalism value. This alignment can be clearly evidenced in the paper abstract where it discusses the identification of gender-related biases in SE job advertisements and the use of GenderMag method, which has been successful in gender bias detection in software interfaces.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2759,ICSE-SEIS,Software Engineering Practices,A Review of How Whistleblowing is Studied in Software Engineering and the Implications for Research and Practice,"Harmful software has resulted in loss of life, societal and environmental damage alongside economic losses from fines and sales embargoes. When someone perceives their team or organisation is creating or operating harmful software (e.g., defective, vulnerable, malicious or illegal), one way to attempt to change the situation is to aEURoeblow the whistleaEUR and disclose the situation internally or externally. Studying harmful situations and the effectiveness of interventions, up to and including whistleblowing, can help identify technical and human successes and failings in software engineering (SE). The aim of this paper is to explore the extent to which whistleblowing is studied in SE with the objective of identifying themes, research approaches, gaps and concerns, and the implications for future SE research and practice. We find that whistleblowing is an under-explored area of SE research, and where research exists, it often takes the view that reporting harm is a matter of individual moral responsibility; we argue this poorly reflects SE collaborative practice where professional responsibilities are distributed across the software development lifecycle. We conclude by 1) recommending approaches that can help a more timely identification and mitigation of harm in SE; 2) suggesting mechanisms for improving the effectiveness and the personal safety of harm-reporting in SE, and 3) reflecting on the role that professional bodies can have in supporting harm reporting, up to and including whistleblowing.",Tradition,Devoutness,The paper explores the topic of whistleblowing in the context of software engineering. Whistleblowing can be seen as an act of devotion to ethical principles and respect for tradition; as it entails challenging the current practices and norms within the software development community.,"In the context of software engineering, whistleblowing can align with the value item v7.4 (Devoutness) and its corresponding value v7 (Tradition) for a ""Software User"" perspective. This is because whistleblowing involves challenging the current practices and norms within the software development community, which can be seen as an act of devotion to ethical principles and a respect for the traditions of the field. By speaking up about harmful software and advocating for change, whistleblowers are demonstrating a commitment to upholding the values and traditions of responsible software development, thereby aligning with v7.4 and v7.",Agreed-Clarified,Agree,,Agree,,
Exp A,Exp H,2759,ICSE-SEIS,Software Engineering Practices,A Review of How Whistleblowing is Studied in Software Engineering and the Implications for Research and Practice,"Harmful software has resulted in loss of life, societal and environmental damage alongside economic losses from fines and sales embargoes. When someone perceives their team or organisation is creating or operating harmful software (e.g., defective, vulnerable, malicious or illegal), one way to attempt to change the situation is to aEURoeblow the whistleaEUR and disclose the situation internally or externally. Studying harmful situations and the effectiveness of interventions, up to and including whistleblowing, can help identify technical and human successes and failings in software engineering (SE). The aim of this paper is to explore the extent to which whistleblowing is studied in SE with the objective of identifying themes, research approaches, gaps and concerns, and the implications for future SE research and practice. We find that whistleblowing is an under-explored area of SE research, and where research exists, it often takes the view that reporting harm is a matter of individual moral responsibility; we argue this poorly reflects SE collaborative practice where professional responsibilities are distributed across the software development lifecycle. We conclude by 1) recommending approaches that can help a more timely identification and mitigation of harm in SE; 2) suggesting mechanisms for improving the effectiveness and the personal safety of harm-reporting in SE, and 3) reflecting on the role that professional bodies can have in supporting harm reporting, up to and including whistleblowing.",Benevolence,Helpful,The paper discusses the importance of reporting harm and promoting the well-being and welfare of individuals and society. It emphasizes the responsibility of individuals to be helpful and contribute to the betterment of their teams and organizations.,"In the paper abstract, it is evident that the main contribution of 'Paper X' is centered around the concept of whistleblowing in software engineering. As a ""Software User,"" the value item v9.3, which corresponds to being helpful, aligns with the idea that individuals have a responsibility to report harm and contribute to the betterment of their teams and organizations. By emphasizing the importance of harm reporting and promoting the well-being of individuals and society, the paper highlights the value of being helpful and responsible in a software context.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2759,ICSE-SEIS,Software Engineering Practices,A Review of How Whistleblowing is Studied in Software Engineering and the Implications for Research and Practice,"Harmful software has resulted in loss of life, societal and environmental damage alongside economic losses from fines and sales embargoes. When someone perceives their team or organisation is creating or operating harmful software (e.g., defective, vulnerable, malicious or illegal), one way to attempt to change the situation is to aEURoeblow the whistleaEUR and disclose the situation internally or externally. Studying harmful situations and the effectiveness of interventions, up to and including whistleblowing, can help identify technical and human successes and failings in software engineering (SE). The aim of this paper is to explore the extent to which whistleblowing is studied in SE with the objective of identifying themes, research approaches, gaps and concerns, and the implications for future SE research and practice. We find that whistleblowing is an under-explored area of SE research, and where research exists, it often takes the view that reporting harm is a matter of individual moral responsibility; we argue this poorly reflects SE collaborative practice where professional responsibilities are distributed across the software development lifecycle. We conclude by 1) recommending approaches that can help a more timely identification and mitigation of harm in SE; 2) suggesting mechanisms for improving the effectiveness and the personal safety of harm-reporting in SE, and 3) reflecting on the role that professional bodies can have in supporting harm reporting, up to and including whistleblowing.",Universalism,A World at Peace,The paper examines the role of professional bodies in supporting harm reporting and whistleblowing in the software engineering field. It highlights the need for a world at peace; where ethical conduct and justice prevail in software development practices.,"In the given abstract of 'Paper X', the author explicitly states that they reflect on the role that professional bodies can have in supporting harm reporting, up to and including whistleblowing. This aligns with value item v10.7, which represents ""A World at Peace"" in Schwartz's Taxonomy. The paper emphasizes the importance of ethical conduct and justice to prevail in software development practices, contributing to a world where conflicts and harm are mitigated. Hence, the alignment with v10.7 and its corresponding value v10 from a ""Software User"" perspective is evident.",Agreed-Clarified,Agree,,Agree,,
Exp A,Exp H,2760,ICSE-SEIS,Software Engineering Practices,An Empirical Investigation on the Challenges Faced by Women in the Software Industry: A Case Study,"Context: Addressing women's under-representation in the soft-ware industry, a widely recognized concern, requires attracting as well as retaining more women. Hearing from women practitioners, particularly those positioned in multi-cultural settings, about their challenges and and adopting their lived experienced solutions can support the design of programs to resolve the under-representation issue. Goal: We investigated the challenges women face in global software development teams, particularly what motivates women to leave their company; how those challenges might break down according to demographics; and strategies to mitigate the identified challenges. Method: To achieve this goal, we conducted an ex-ploratory case study in Ericsson, a global technology company. We surveyed 94 women and employed mixed-methods to analyze the data. Results: Our findings reveal that women face socio-cultural challenges, including work-life balance issues, benevolent and hos-tile sexism, lack of recognition and peer parity, impostor syndrome, glass ceiling bias effects, the prove-it-again phenomenon, and the maternal wall. The participants of our research provided different suggestions to address/mitigate the reported challenges, including sabbatical policies, flexibility of location and time, parenthood support, soft skills training for managers, equality of payment and opportunities between genders, mentoring and role models to sup-port career growth, directives to hire more women, inclusive groups and events, women's empowerment, and recognition for women's success. The framework of challenges and suggestions can inspire further initiatives both in academia and industry to onboard and retain women. Women represent less than 24% of employees in software development industry and experience various types of prejudice and bias. Even in companies that care about Diversity & Inclusion, aEURoeuntying the mooring ropesaEUR of socio-cultural problems is hard. Hearing from women, especially those working in a multi-cultural organization, about their challenges and adopting their suggestions can be vital to design programs and resolve the under-representation issue. In this work we work closely with a large software development or-ganization which invests and believes in diversity and inclusion. We listened to women and the challenges they face in global soft-ware development teams of this company and what these women suggest reduce the problems and increase retention. Our research showed that women face work-life balance issues and encounter invisible barriers that prevent them from rising to top positions. They also suffer micro-aggression and sexism, need to show com-petence constantly, be supervised in essential tasks, and receive less work after becoming mothers. Moreover, women miss having more female colleagues, lack self-confidence and recognition. The women from the company suggested sabbatical policies, the flexibil-ity of location and time, parenthood support, soft skills training for managers, equality of opportunities, role models to support career growth, directives to hire more women, support groups, and more interaction between women, inclusive groups and events, women's empowerment by publishing their success stories in media and recognizing their achievements. Our results had been shared with the company Human Resources department and management and they considered the diagnosis helpful and will work on actions to mitigate the challenges that women still perceive.",Power,Social Power,The paper addresses socio-cultural challenges faced by women in the software industry; such as sexist biases and lack of recognition. This aligns with the value item Social Power and its corresponding value Power.,"The paper 'Paper X' directly aligns with the value item v5.4 and its corresponding value v5 from the perspective of a ""Software User"" because it addresses socio-cultural challenges, such as sexist biases and lack of recognition, that women face in the software industry. By addressing these challenges, the paper highlights the importance of social power in creating an inclusive and equitable environment for software users.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2760,ICSE-SEIS,Software Engineering Practices,An Empirical Investigation on the Challenges Faced by Women in the Software Industry: A Case Study,"Context: Addressing women's under-representation in the soft-ware industry, a widely recognized concern, requires attracting as well as retaining more women. Hearing from women practitioners, particularly those positioned in multi-cultural settings, about their challenges and and adopting their lived experienced solutions can support the design of programs to resolve the under-representation issue. Goal: We investigated the challenges women face in global software development teams, particularly what motivates women to leave their company; how those challenges might break down according to demographics; and strategies to mitigate the identified challenges. Method: To achieve this goal, we conducted an ex-ploratory case study in Ericsson, a global technology company. We surveyed 94 women and employed mixed-methods to analyze the data. Results: Our findings reveal that women face socio-cultural challenges, including work-life balance issues, benevolent and hos-tile sexism, lack of recognition and peer parity, impostor syndrome, glass ceiling bias effects, the prove-it-again phenomenon, and the maternal wall. The participants of our research provided different suggestions to address/mitigate the reported challenges, including sabbatical policies, flexibility of location and time, parenthood support, soft skills training for managers, equality of payment and opportunities between genders, mentoring and role models to sup-port career growth, directives to hire more women, inclusive groups and events, women's empowerment, and recognition for women's success. The framework of challenges and suggestions can inspire further initiatives both in academia and industry to onboard and retain women. Women represent less than 24% of employees in software development industry and experience various types of prejudice and bias. Even in companies that care about Diversity & Inclusion, aEURoeuntying the mooring ropesaEUR of socio-cultural problems is hard. Hearing from women, especially those working in a multi-cultural organization, about their challenges and adopting their suggestions can be vital to design programs and resolve the under-representation issue. In this work we work closely with a large software development or-ganization which invests and believes in diversity and inclusion. We listened to women and the challenges they face in global soft-ware development teams of this company and what these women suggest reduce the problems and increase retention. Our research showed that women face work-life balance issues and encounter invisible barriers that prevent them from rising to top positions. They also suffer micro-aggression and sexism, need to show com-petence constantly, be supervised in essential tasks, and receive less work after becoming mothers. Moreover, women miss having more female colleagues, lack self-confidence and recognition. The women from the company suggested sabbatical policies, the flexibil-ity of location and time, parenthood support, soft skills training for managers, equality of opportunities, role models to support career growth, directives to hire more women, support groups, and more interaction between women, inclusive groups and events, women's empowerment by publishing their success stories in media and recognizing their achievements. Our results had been shared with the company Human Resources department and management and they considered the diagnosis helpful and will work on actions to mitigate the challenges that women still perceive.",Security,Sense of Belonging,The paper suggests strategies to improve work-life balance and provide support for parenthood in the software industry. This aligns with the value item Sense of Belonging and its corresponding value Security.,"The paper's contribution in addressing work-life balance issues and providing support for parenthood in the software industry directly aligns with the value item of Sense of Belonging and its corresponding value of Security. By implementing strategies such as sabbatical policies, flexibility of location and time, and parenthood support, the paper aims to create an environment where software users, specifically women, feel a sense of belonging and security, knowing that their personal lives can be accommodated alongside their professional responsibilities. This contributes to their overall well-being and satisfaction in their software user roles.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2764,ICSE-SEIS,Software Engineering Practices,Good Fences Make Good Neighbours? On the Impact of Cultural and Geographical Dispersion on Community Smells,"Software development is de facto a social activity that often involves people from all places to join forces globally. In such common instances, project managers must face social challenges, e.g., personality conflicts and language barriers, which often amount literally to aEURoeculture shockaEUR. In this paper, we seek to analyze and illustrate how cultural and geographical dispersion-that is, how much a com-munity is diverse in terms of its membersaEURkh cultural attitudes and geographical collocation-influence the emergence of collaboration and communication problems in open-source communities, a.k.a. community smells, the socio-technical precursors of unforeseen, often nasty organizational conditions amounting collectively to the phenomenon called social debt. We perform an extensive empirical study on cultural characteristics of GITHUB developers, and build a regression model relating the two types of dispersion-cultural and geographical-with the emergence of four types of commu-nity smells, i.e., Organizational Silo, Lone Wolf, Radio Silence, and Black Cloud. Results indicate that cultural and geographical factors influence collaboration and communication within open-source communities, to an extent which incites-or even more interestingly mitigates, in some cases-community smells, e.g., Lone Wolf, in development teams. Managers can use these findings to address their own organizational structure and tentatively diagnose any nasty phenomena related to the conditions under study. To what extent does the global and multi-cultural nature of software engineering influence software processes welfare? More specifically, does an increase in aEURoeglobalizationaEUR of software activities negatively or positively influence known nasty effects common in the pro-cess of software construction? Rotating around these questions, this research finds that there is in fact evidence of the aforementioned in-fluence but it does not provide for positive effects only. Specifically, a decrease of globalization does not necessarily bode positively on conditions such as lone developers working in an individualistic fashion-a phenomenon known as aEURoelone wolfaEUR effect-and other nasty organizational phenomena potentially slowing down or halting software construction and maintenance activities.",Universalism,A World at Peace,"The paper explores the influence of cultural and geographical dispersion on collaboration and communication problems in open-source communities. This aligns with the value item ""A World at Peace"" and its corresponding value ""A World at Peace"" as it aims to understand the factors that contribute to a more harmonious and cooperative software development process.","Certainly! The alignment of the paper with the value item ""A World at Peace"" and its corresponding value ""A World at Peace"" is justified because the paper specifically investigates how cultural and geographical dispersion affects collaboration and communication in open-source communities. By understanding these factors and their impact on community smells and social debt, the paper aims to address and mitigate any negative organizational conditions that may arise. This in turn contributes to creating a more harmonious and cooperative software development process, which can be seen as aligning with the value of ""A World at Peace"" as it promotes collaboration, understanding, and effective communication among global software users.",Agreed-Clarified,Disagree,reconcile with coder_2,Disagree,The justification doesn't align with the a world at peace,
Exp A,Exp H,2764,ICSE-SEIS,Software Engineering Practices,Good Fences Make Good Neighbours? On the Impact of Cultural and Geographical Dispersion on Community Smells,"Software development is de facto a social activity that often involves people from all places to join forces globally. In such common instances, project managers must face social challenges, e.g., personality conflicts and language barriers, which often amount literally to aEURoeculture shockaEUR. In this paper, we seek to analyze and illustrate how cultural and geographical dispersion-that is, how much a com-munity is diverse in terms of its membersaEURkh cultural attitudes and geographical collocation-influence the emergence of collaboration and communication problems in open-source communities, a.k.a. community smells, the socio-technical precursors of unforeseen, often nasty organizational conditions amounting collectively to the phenomenon called social debt. We perform an extensive empirical study on cultural characteristics of GITHUB developers, and build a regression model relating the two types of dispersion-cultural and geographical-with the emergence of four types of commu-nity smells, i.e., Organizational Silo, Lone Wolf, Radio Silence, and Black Cloud. Results indicate that cultural and geographical factors influence collaboration and communication within open-source communities, to an extent which incites-or even more interestingly mitigates, in some cases-community smells, e.g., Lone Wolf, in development teams. Managers can use these findings to address their own organizational structure and tentatively diagnose any nasty phenomena related to the conditions under study. To what extent does the global and multi-cultural nature of software engineering influence software processes welfare? More specifically, does an increase in aEURoeglobalizationaEUR of software activities negatively or positively influence known nasty effects common in the pro-cess of software construction? Rotating around these questions, this research finds that there is in fact evidence of the aforementioned in-fluence but it does not provide for positive effects only. Specifically, a decrease of globalization does not necessarily bode positively on conditions such as lone developers working in an individualistic fashion-a phenomenon known as aEURoelone wolfaEUR effect-and other nasty organizational phenomena potentially slowing down or halting software construction and maintenance activities.",Benevolence,Loyalty,"The paper investigates the cultural characteristics of GitHub developers and their impact on collaboration and communication within open-source communities. This aligns with the value item ""Loyalty"" and its corresponding value ""Loyalty"" as it seeks to understand how the commitment and dedication of developers to their community can enhance software development processes.","In the paper, the authors analyze the cultural characteristics of GitHub developers and how it influences collaboration and communication within open-source communities. This aligns with the value item ""Loyalty"" and its corresponding value ""Loyalty"" as the authors seek to understand the commitment and dedication of developers to their community, indicating a willingness to contribute and support the software development processes. By studying the impact of cultural characteristics on collaboration, the paper directly addresses the value of loyalty within the software user perspective.",Agreed-Justified,Disagree,reconcile with coder_2,Disagree,"The justification claiming that the paper aligns with the value item 'Loyalty',but the text neither explains how nor confirms this.",
Exp A,Exp H,2764,ICSE-SEIS,Software Engineering Practices,Good Fences Make Good Neighbours? On the Impact of Cultural and Geographical Dispersion on Community Smells,"Software development is de facto a social activity that often involves people from all places to join forces globally. In such common instances, project managers must face social challenges, e.g., personality conflicts and language barriers, which often amount literally to aEURoeculture shockaEUR. In this paper, we seek to analyze and illustrate how cultural and geographical dispersion-that is, how much a com-munity is diverse in terms of its membersaEURkh cultural attitudes and geographical collocation-influence the emergence of collaboration and communication problems in open-source communities, a.k.a. community smells, the socio-technical precursors of unforeseen, often nasty organizational conditions amounting collectively to the phenomenon called social debt. We perform an extensive empirical study on cultural characteristics of GITHUB developers, and build a regression model relating the two types of dispersion-cultural and geographical-with the emergence of four types of commu-nity smells, i.e., Organizational Silo, Lone Wolf, Radio Silence, and Black Cloud. Results indicate that cultural and geographical factors influence collaboration and communication within open-source communities, to an extent which incites-or even more interestingly mitigates, in some cases-community smells, e.g., Lone Wolf, in development teams. Managers can use these findings to address their own organizational structure and tentatively diagnose any nasty phenomena related to the conditions under study. To what extent does the global and multi-cultural nature of software engineering influence software processes welfare? More specifically, does an increase in aEURoeglobalizationaEUR of software activities negatively or positively influence known nasty effects common in the pro-cess of software construction? Rotating around these questions, this research finds that there is in fact evidence of the aforementioned in-fluence but it does not provide for positive effects only. Specifically, a decrease of globalization does not necessarily bode positively on conditions such as lone developers working in an individualistic fashion-a phenomenon known as aEURoelone wolfaEUR effect-and other nasty organizational phenomena potentially slowing down or halting software construction and maintenance activities.",Security,Social Order,"The paper explores the social challenges faced by project managers in globally distributed software development teams; such as personality conflicts and language barriers. This aligns with the value item ""Social Order"" and its corresponding value ""Social Order"" as it aims to understand how to manage and maintain a sense of order and cohesion within diverse software development teams.","The justification for aligning 'Paper X' with the value item v6.3 and its corresponding value v6, ""Social Order,"" is based on the fact that the paper aims to understand and address social challenges in globally distributed software development teams. By exploring issues such as personality conflicts and language barriers, the paper seeks to find ways to maintain order and cohesion within diverse teams. This directly aligns with the value of social order, as it emphasizes the importance of creating a harmonious and organized environment for software development, which ultimately benefits software users by ensuring the smooth and efficient delivery of high-quality software products.",Agreed-Justified,Disagree,reconcile with coder_2,Disagree,"The justification claims that the paper aligns with the value 'Security', but the text neither explains how nor confirms this",
Exp A,Exp H,2767,ICSE-SEIS,Accessibility & User Experience,Impact of animated objects on autistic and non-autistic users,"Interface usability and accessibility are crucial for autistic people because of the challenges they may confront while navigating the Internet. Nevertheless, limited empirical evidence is available from investigations of software accessibility and usability for autistic users. The core aim of this work was to empirically investigate the effects of irrelevant animation in user interfaces (UI). We analysed the impact of animation on autistic and non-autistic individuals, as well as the impact of animated UI objects on the performance of both cohorts while conducting various task types. Our findings suggest that animation significantly affects task performance for both cohorts, with autistic users more severely affected: autistic individuals are distracted to a greater extent, exacerbating frustration and necessitating greater mental exertion. The results of our study will help practitioners to establish greater comprehension of the autistic population's particular traits, as well as provide a basis for UI design guidelines so that autistic people will find interfaces more usable and accessible. LAY ABSTRACT There are legal guidelines requiring web sites to be accessible to everyone. Our research focuses on making software interfaces inclusive for autistic people. About 1% of the overall population are known to be on the autism spectrum, and many autistic users are very sensitive to visual overload in everyday life. They may also get distracted more easily, potentially making it harder to do typical tasks using software. However, there were very studies on the accessability of software interfaces for autistic people, in particular on whether and how animated (moving) images might impact them. Our study compared how autistic and non-autistic users completed tasks such as writing emails, on-line shopping and search when there were images that rotated on the screen. We also experi-mented with different rotation speeds and image sizes. We found that on average, people have more trouble when there is a rotating image on the screen, but that autistic users struggled more.",Benevolence,Meaning in Life,The paper contributes to making software interfaces more usable and accessible for autistic individuals; aligning with the value item Meaning in Life and its corresponding value Benevolence. By conducting empirical investigations on the effects of irrelevant animation on autistic and non-autistic individuals; the study aims to establish greater comprehension of the autistic population's particular traits and provide UI design guidelines for the betterment of autistic users' experience.,"I apologize for any confusion. To clarify, by aligning with the value item v9.6 (Meaning in Life) and its corresponding value v9 (Benevolence), 'Paper X' contributes to making software interfaces more usable and accessible for autistic individuals. Through empirical investigations of the effects of irrelevant animation on autistic and non-autistic individuals, the paper aims to gain a better understanding of the unique traits of the autistic population and provide UI design guidelines specifically tailored to enhance their experience. This aligns with the value of Benevolence, as the study seeks to improve the well-being and inclusivity of autistic software users by addressing their particular needs and challenges.",Agreed-Clarified,Agree,,Agree,,
Exp A,Exp H,2767,ICSE-SEIS,Accessibility & User Experience,Impact of animated objects on autistic and non-autistic users,"Interface usability and accessibility are crucial for autistic people because of the challenges they may confront while navigating the Internet. Nevertheless, limited empirical evidence is available from investigations of software accessibility and usability for autistic users. The core aim of this work was to empirically investigate the effects of irrelevant animation in user interfaces (UI). We analysed the impact of animation on autistic and non-autistic individuals, as well as the impact of animated UI objects on the performance of both cohorts while conducting various task types. Our findings suggest that animation significantly affects task performance for both cohorts, with autistic users more severely affected: autistic individuals are distracted to a greater extent, exacerbating frustration and necessitating greater mental exertion. The results of our study will help practitioners to establish greater comprehension of the autistic population's particular traits, as well as provide a basis for UI design guidelines so that autistic people will find interfaces more usable and accessible. LAY ABSTRACT There are legal guidelines requiring web sites to be accessible to everyone. Our research focuses on making software interfaces inclusive for autistic people. About 1% of the overall population are known to be on the autism spectrum, and many autistic users are very sensitive to visual overload in everyday life. They may also get distracted more easily, potentially making it harder to do typical tasks using software. However, there were very studies on the accessability of software interfaces for autistic people, in particular on whether and how animated (moving) images might impact them. Our study compared how autistic and non-autistic users completed tasks such as writing emails, on-line shopping and search when there were images that rotated on the screen. We also experi-mented with different rotation speeds and image sizes. We found that on average, people have more trouble when there is a rotating image on the screen, but that autistic users struggled more.",Hedonism,Pleasure,The paper discusses the impact of animation on task performance for both autistic and non-autistic individuals. It highlights that animation significantly affects task performance; with autistic users being more severely affected. This aligns with the value item Pleasure and its corresponding value Hedonism. The study acknowledges that animation can exacerbate frustration and necessitate greater mental exertion for autistic users; indicating that the absence of irrelevant animation can contribute to a more pleasurable and enjoyable software experience for individuals on the autism spectrum.,"My justification for labeling 'Paper X' as aligning with the value item v3.3 and its corresponding value v3 is based on the direct evidence provided in the paper abstract. The paper clearly states that animation significantly affects task performance for both autistic and non-autistic individuals, with autistic users being more severely affected. This aligns with the value of Hedonism because the paper acknowledges that the presence of irrelevant animation can exacerbate frustration and require greater mental exertion for autistic users, indicating that the absence of such animation can contribute to a more pleasurable and enjoyable software experience for individuals on the autism spectrum.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2767,ICSE-SEIS,Accessibility & User Experience,Impact of animated objects on autistic and non-autistic users,"Interface usability and accessibility are crucial for autistic people because of the challenges they may confront while navigating the Internet. Nevertheless, limited empirical evidence is available from investigations of software accessibility and usability for autistic users. The core aim of this work was to empirically investigate the effects of irrelevant animation in user interfaces (UI). We analysed the impact of animation on autistic and non-autistic individuals, as well as the impact of animated UI objects on the performance of both cohorts while conducting various task types. Our findings suggest that animation significantly affects task performance for both cohorts, with autistic users more severely affected: autistic individuals are distracted to a greater extent, exacerbating frustration and necessitating greater mental exertion. The results of our study will help practitioners to establish greater comprehension of the autistic population's particular traits, as well as provide a basis for UI design guidelines so that autistic people will find interfaces more usable and accessible. LAY ABSTRACT There are legal guidelines requiring web sites to be accessible to everyone. Our research focuses on making software interfaces inclusive for autistic people. About 1% of the overall population are known to be on the autism spectrum, and many autistic users are very sensitive to visual overload in everyday life. They may also get distracted more easily, potentially making it harder to do typical tasks using software. However, there were very studies on the accessability of software interfaces for autistic people, in particular on whether and how animated (moving) images might impact them. Our study compared how autistic and non-autistic users completed tasks such as writing emails, on-line shopping and search when there were images that rotated on the screen. We also experi-mented with different rotation speeds and image sizes. We found that on average, people have more trouble when there is a rotating image on the screen, but that autistic users struggled more.",Universalism,Protecting the Environment,The paper addresses the accessibility and usability of software interfaces for autistic individuals. By conducting experiments with rotating images on screens; the study investigates how animated images impact task completion for autistic and non-autistic users. This aligns with the value item Protecting the Environment and its corresponding value Universalism. While the value item primarily refers to ecological well-being; the paper's focus on creating inclusive software interfaces for autistic individuals can be seen as a form of protecting the social environment and promoting inclusivity and equity in technology.,"My justification for aligning 'Paper X' with the value item v10.4 (Protecting the Environment) and its corresponding value v10 (Universalism) is based on the idea that promoting inclusivity and equity in technology is a way of protecting the social environment. Autistic individuals, who are part of the software user population, often face challenges in accessing and using software interfaces. By conducting experiments on animated images and their impact on task completion for autistic and non-autistic users, the paper aims to improve the usability and accessibility of software interfaces for autistic individuals, thus creating a more inclusive and equitable digital environment. This aligns with the broader value of Universalism, which emphasizes equality, social justice, and the well-being of all individuals. Therefore, the paper's focus on creating inclusive software interfaces can be seen as promoting a social environment that protects the needs and rights of autistic individuals, aligning with the value item v10.4 and its corresponding value v10.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2767,ICSE-SEIS,Accessibility & User Experience,Impact of animated objects on autistic and non-autistic users,"Interface usability and accessibility are crucial for autistic people because of the challenges they may confront while navigating the Internet. Nevertheless, limited empirical evidence is available from investigations of software accessibility and usability for autistic users. The core aim of this work was to empirically investigate the effects of irrelevant animation in user interfaces (UI). We analysed the impact of animation on autistic and non-autistic individuals, as well as the impact of animated UI objects on the performance of both cohorts while conducting various task types. Our findings suggest that animation significantly affects task performance for both cohorts, with autistic users more severely affected: autistic individuals are distracted to a greater extent, exacerbating frustration and necessitating greater mental exertion. The results of our study will help practitioners to establish greater comprehension of the autistic population's particular traits, as well as provide a basis for UI design guidelines so that autistic people will find interfaces more usable and accessible. LAY ABSTRACT There are legal guidelines requiring web sites to be accessible to everyone. Our research focuses on making software interfaces inclusive for autistic people. About 1% of the overall population are known to be on the autism spectrum, and many autistic users are very sensitive to visual overload in everyday life. They may also get distracted more easily, potentially making it harder to do typical tasks using software. However, there were very studies on the accessability of software interfaces for autistic people, in particular on whether and how animated (moving) images might impact them. Our study compared how autistic and non-autistic users completed tasks such as writing emails, on-line shopping and search when there were images that rotated on the screen. We also experi-mented with different rotation speeds and image sizes. We found that on average, people have more trouble when there is a rotating image on the screen, but that autistic users struggled more.",Power,Preserving My Public Image,Not found,"In the abstract of 'Paper X', the main contributions mentioned are related to investigating the effects of irrelevant animation on user interfaces for both autistic and non-autistic individuals. It is stated that animation significantly affects task performance for both cohorts, with autistic users being more severely affected. This aligns with the value item v5.5 ""Preserving My Public Image"" and its corresponding value v5 ""Power"" from a ""Software User"" perspective. Autistic individuals may struggle with distraction and frustration caused by animation, which can impact their ability to effectively navigate and use software interfaces. By recognizing this issue and providing guidelines for UI design, the paper aims to empower autistic users, allowing them to have more control over their experiences and preserving their public image by enabling them to use interfaces more effectively.",Agreed-Clarified,Agree,,Agree,,
Exp A,Exp H,2768,ICSE-SEIS,Software Architecture & Design,Lowering Barriers to Application Development With Cloud-Native Domain-Specific Functions,"Creating and maintaining a modern, heterogeneous set of client applications remains an obstacle for many businesses and individu-als. While simple domain-specific graphical languages and libraries can empower a variety of users to create application behaviors and logic, using these languages to produce and maintain a set of heterogeneous client applications is a challenge. Primarily because each client typically requires the developers to both understand and embed the domain-specific logic. This is because application logic must be encoded to some extent in both the server and client sides. In this paper, we propose an alternative approach, which allows the specification of application logic to reside solely on the cloud. We have built a system where reusable application components can be assembled on the cloud in different logical chains and the client is largely decoupled from this logic and is solely concerned with how data is displayed and gathered from users of the application. In this way, the chaining of requests and responses is done by the cloud and the client side has no knowledge of the application logic. This means that the experts in the domain can build these modular cloud components, arrange them in various logical chains, generate a simple user interface to test the application, and later leave it to client-side developers to customize the presentation and gathering of data types to and from the user. An additional effect of our approach is that the client side developer is able to immediately see any changes they make, while executing the logic residing on the cloud. This further allows more novice programmers to perform these customizations, as they do not need to aEURkhget the full application workingaEUR(tm) and are able to see the results of their code as they go, thereby lowering the obstacles to businesses and individuals to produce and maintain applications. Furthermore, this decoupling enables the quick generation and customization of a variety of application clients, ranging from web to mobile devices and personal assistants, while customizing one or more as needed. Creating new computer applications, distributing them to users, and maintaining them, is complex and time-consuming. Building even a simple application requires years of study. Furthermore, given the number of devices we have today (from smartphones to voice assistants), the task is further complicated and difficult even for experienced programmers. The world is undergoing a digital transformation, and computer applications are now part of our everyday existence and building them is a requirement for many organizations and businesses. How-ever, only companies and individuals with large budgets and skills can build these applications, a consequence of which is an increased digital gap between those with means and those without. We see a clear need to lower the barriers to building these applications so a more diverse set of voices are able to participate and shape our digital futures, as underrepresented groups are often left out of designing and building these platforms. Previous researchers have proposed approaches where applications can be easily specified by people without (or with few) technical skills. However, handling different device platforms and maintenance/distribution were neglected in these efforts. In this paper we propose an approach where the program is executed on the cloud, while the client only needs to display and gather data from users. A result of our approach is that generic front-end applications adapt to any changes to the program being executed on the cloud. Our programs work with end-user apps on any device, the application that the user sees automatically adapts to any changes.",Achievement,Ambitious,The paper proposes an alternative approach that allows experts in the domain to build modular cloud components and arrange them in various logical chains. This aligns with the value item Ambitious and its corresponding value Achievement; as it enables the generation and customization of a variety of application clients; ranging from web to mobile devices and personal assistants; while customizing one or more as needed.,"The justification for aligning 'Paper X' with the value item v4.5 (Ambitious) and its corresponding value v4 (Achievement) is based on the fact that the paper proposes an alternative approach that enables the generation and customization of a variety of application clients. This demonstrates ambition in the sense that it allows for the development of a wide range of applications across different platforms, which can be seen as a significant achievement within the software context.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2768,ICSE-SEIS,Software Architecture & Design,Lowering Barriers to Application Development With Cloud-Native Domain-Specific Functions,"Creating and maintaining a modern, heterogeneous set of client applications remains an obstacle for many businesses and individu-als. While simple domain-specific graphical languages and libraries can empower a variety of users to create application behaviors and logic, using these languages to produce and maintain a set of heterogeneous client applications is a challenge. Primarily because each client typically requires the developers to both understand and embed the domain-specific logic. This is because application logic must be encoded to some extent in both the server and client sides. In this paper, we propose an alternative approach, which allows the specification of application logic to reside solely on the cloud. We have built a system where reusable application components can be assembled on the cloud in different logical chains and the client is largely decoupled from this logic and is solely concerned with how data is displayed and gathered from users of the application. In this way, the chaining of requests and responses is done by the cloud and the client side has no knowledge of the application logic. This means that the experts in the domain can build these modular cloud components, arrange them in various logical chains, generate a simple user interface to test the application, and later leave it to client-side developers to customize the presentation and gathering of data types to and from the user. An additional effect of our approach is that the client side developer is able to immediately see any changes they make, while executing the logic residing on the cloud. This further allows more novice programmers to perform these customizations, as they do not need to aEURkhget the full application workingaEUR(tm) and are able to see the results of their code as they go, thereby lowering the obstacles to businesses and individuals to produce and maintain applications. Furthermore, this decoupling enables the quick generation and customization of a variety of application clients, ranging from web to mobile devices and personal assistants, while customizing one or more as needed. Creating new computer applications, distributing them to users, and maintaining them, is complex and time-consuming. Building even a simple application requires years of study. Furthermore, given the number of devices we have today (from smartphones to voice assistants), the task is further complicated and difficult even for experienced programmers. The world is undergoing a digital transformation, and computer applications are now part of our everyday existence and building them is a requirement for many organizations and businesses. How-ever, only companies and individuals with large budgets and skills can build these applications, a consequence of which is an increased digital gap between those with means and those without. We see a clear need to lower the barriers to building these applications so a more diverse set of voices are able to participate and shape our digital futures, as underrepresented groups are often left out of designing and building these platforms. Previous researchers have proposed approaches where applications can be easily specified by people without (or with few) technical skills. However, handling different device platforms and maintenance/distribution were neglected in these efforts. In this paper we propose an approach where the program is executed on the cloud, while the client only needs to display and gather data from users. A result of our approach is that generic front-end applications adapt to any changes to the program being executed on the cloud. Our programs work with end-user apps on any device, the application that the user sees automatically adapts to any changes.",Universalism,Unity with Nature,The paper suggests that the chaining of requests and responses is done by the cloud; and the client side has no knowledge of the application logic. This aligns with the value item Unity with Nature and its corresponding value Universalism; as it promotes a harmonious and interconnected system where the client-side developers can focus solely on how data is displayed and gathered from users.,"The alignment of 'Paper X' with the value item Unity with Nature and its corresponding value Universalism is evident in the paper's proposition of decoupling the client-side from the application logic and relying on the cloud for the chaining of requests and responses. This approach promotes a more holistic and interconnected system, where the client-side developers can focus solely on the user interface and data handling, thereby contributing to a more unified and harmonious software experience.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2769,ICSE-SEIS,Accessibility & User Experience,Open Data Inclusion through Narrative Approaches,"Open data is published with the intention that it can be used by everyone. In reality, various barriers exclude some people from its use. This short paper will examine common reasons why people struggle to make use of open data and propose a conceptual framework to guide narrative-driven curation of open data sets to overcome these barriers. These principles are currently being used to guide the creation of new interfaces to open data that can be used by people with differing levels of data literacy, thereby increasing inclusion of open data sets.",Universalism,Protecting the Environment,"The paper proposes a conceptual framework to guide the curation of open data sets; aiming to overcome the barriers that exclude some people from using open data. This aligns with the value item ""Protecting the Environment"" as the paper emphasizes the importance of sustainable practices and actions to increase the inclusion of open data sets.","The justification for aligning 'Paper X' with the value item v10.4 (""Protecting the Environment"") is based on the paper's focus on creating new interfaces for open data that can be used by individuals with varying levels of data literacy. By considering the inclusion of open data sets for all users, the paper indirectly emphasizes the importance of ensuring sustainable practices and actions in the curation of these datasets, which directly aligns with the value of protecting the environment.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2769,ICSE-SEIS,Accessibility & User Experience,Open Data Inclusion through Narrative Approaches,"Open data is published with the intention that it can be used by everyone. In reality, various barriers exclude some people from its use. This short paper will examine common reasons why people struggle to make use of open data and propose a conceptual framework to guide narrative-driven curation of open data sets to overcome these barriers. These principles are currently being used to guide the creation of new interfaces to open data that can be used by people with differing levels of data literacy, thereby increasing inclusion of open data sets.",Benevolence,Helpful,"The paper mentions that the conceptual framework being proposed is used to guide the creation of new interfaces to open data that can be used by people with differing levels of data literacy. This aligns with the value item ""Helpful"" as it highlights the intention to contribute to the well-being and welfare of others by facilitating the use of open data for a wider range of users.","The alignment of 'Paper X' with the value item v9.3 ""Helpful"" is justified by the explicit mention of the intention to create interfaces that can be used by people with differing levels of data literacy, thereby increasing inclusion of open data sets. This aligns with the value of v9 ""Benevolence,"" as it demonstrates a desire to contribute to the well-being and welfare of others by facilitating access to and utilization of open data for a wider range of users.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2769,ICSE-SEIS,Accessibility & User Experience,Open Data Inclusion through Narrative Approaches,"Open data is published with the intention that it can be used by everyone. In reality, various barriers exclude some people from its use. This short paper will examine common reasons why people struggle to make use of open data and propose a conceptual framework to guide narrative-driven curation of open data sets to overcome these barriers. These principles are currently being used to guide the creation of new interfaces to open data that can be used by people with differing levels of data literacy, thereby increasing inclusion of open data sets.",Self Direction,Freedom,"The paper addresses the barriers that exclude some people from using open data and aims to increase inclusion. This aligns with the value item ""Freedom"" as the paper promotes the idea of providing equal opportunities and access to open data for everyone; allowing individuals to have the freedom to use and benefit from it.","The main contribution of 'Paper X' is to propose a conceptual framework for narrative-driven curation of open data sets, aiming to overcome barriers and increase inclusion. This aligns with the value item ""Freedom"" in Schwartz's Taxonomy because it promotes the idea of providing equal opportunities and access to open data for everyone, allowing individuals to have the freedom to use and benefit from it without being excluded or limited due to barriers. In a software context, this aligns with the value of ""Freedom"" because it advocates for software users to have the freedom to access and utilize open data without restrictions or exclusions.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2771,ICSE-SEIS,Accessibility & User Experience,SCRATCH as Social Network: Topic Modeling and Sentiment Analysis in SCRATCH Projects,"Societal matters like the Black Lives Matter (BLM) movement influence software engineering, as the recent debate on replacing certain discriminatory terms such as whitelist/blacklist has shown. Identifying relevant and trending societal matters is important, and often done for traditional social media channels such as Twitter. In this paper we explore whether this type of analysis can also be used for introspection of the software world, by looking at the thriving scene of Scratch programmers. The educational programming language Scratch is not only used for teaching programming concepts, but also offers a platform for young programmers to express and share their creativity on any topics of relevance. By automatically analyzing titles and project comments in a dataset of 106.032 Scratch projects, we explore which topics are common in the Scratch community, whether socially relevant events are reflected, and how the sentiment in the comments discussing these topics is. It turns out that the diversity of topics within the Scratch projects makes the analysis process challenging. Our results nevertheless show that topics from pop and net culture are present, and even recent societal events such as the Covid-19 pandemic or BLM are to some extent reflected in Scratch. The tone in the comments is mostly positive with catchy youth language. Hence, despite the challenges, Scratch projects can be studied in the same way as social networks, which opens up new possibilities to improve our understanding of the behavior and motivation of novice programmers. Analyzing the content and emotions within a social network provides insights on how software engineers communicate and what they talk about. The Scratch programming environment is extremely popular with young, learning programmers. In this paper we investigate whether social network analysis in terms of automatically identifying topics and sentiments can also be ap-plied to Scratch. We analyze the topics of the projects that young programmers create and share, and determine the tone of their conversations. Although the process turns out to be technically challenging, we encounter pop and net culture references but also societal matters, and the overall tone of communication is positive.",Universalism,Equality,The paper explores the topics and sentiments within the Scratch programming community; including socially relevant events such as the Black Lives Matter movement and the Covid-19 pandemic. This aligns with the value item Equality and its corresponding value Universalism.,"The main contribution of 'Paper X' is the analysis of topics and sentiments within the Scratch programming community, including socially relevant events. This aligns with the value item v10.2 - Equality, as it suggests that the Scratch community allows for diverse topics and perspectives to be expressed and discussed. This promotes the value of Universalism, as it encourages inclusivity, equality, and respect for different viewpoints within the software context.",Agreed-Clarified,Agree,,Agree,,
Exp A,Exp H,2771,ICSE-SEIS,Accessibility & User Experience,SCRATCH as Social Network: Topic Modeling and Sentiment Analysis in SCRATCH Projects,"Societal matters like the Black Lives Matter (BLM) movement influence software engineering, as the recent debate on replacing certain discriminatory terms such as whitelist/blacklist has shown. Identifying relevant and trending societal matters is important, and often done for traditional social media channels such as Twitter. In this paper we explore whether this type of analysis can also be used for introspection of the software world, by looking at the thriving scene of Scratch programmers. The educational programming language Scratch is not only used for teaching programming concepts, but also offers a platform for young programmers to express and share their creativity on any topics of relevance. By automatically analyzing titles and project comments in a dataset of 106.032 Scratch projects, we explore which topics are common in the Scratch community, whether socially relevant events are reflected, and how the sentiment in the comments discussing these topics is. It turns out that the diversity of topics within the Scratch projects makes the analysis process challenging. Our results nevertheless show that topics from pop and net culture are present, and even recent societal events such as the Covid-19 pandemic or BLM are to some extent reflected in Scratch. The tone in the comments is mostly positive with catchy youth language. Hence, despite the challenges, Scratch projects can be studied in the same way as social networks, which opens up new possibilities to improve our understanding of the behavior and motivation of novice programmers. Analyzing the content and emotions within a social network provides insights on how software engineers communicate and what they talk about. The Scratch programming environment is extremely popular with young, learning programmers. In this paper we investigate whether social network analysis in terms of automatically identifying topics and sentiments can also be ap-plied to Scratch. We analyze the topics of the projects that young programmers create and share, and determine the tone of their conversations. Although the process turns out to be technically challenging, we encounter pop and net culture references but also societal matters, and the overall tone of communication is positive.",Universalism,A World at Peace,The paper investigates the presence of recent societal events such as the Black Lives Matter movement and the Covid-19 pandemic in Scratch projects. This aligns with the value item A World at Peace and its corresponding value Universalism.,"The presence of recent societal events such as the Black Lives Matter movement and the Covid-19 pandemic in Scratch projects reflects the value of A World at Peace from a Universalism perspective. By analyzing the Scratch projects, the paper demonstrates how young programmers are engaging with and reflecting on these societal matters, potentially fostering empathy, understanding, and unity among the users. This aligns with the Universalism value which emphasizes the importance of social justice, unity, and a world at peace.",Agreed-Clarified,Agree,,Agree,,
Exp A,Exp H,2771,ICSE-SEIS,Accessibility & User Experience,SCRATCH as Social Network: Topic Modeling and Sentiment Analysis in SCRATCH Projects,"Societal matters like the Black Lives Matter (BLM) movement influence software engineering, as the recent debate on replacing certain discriminatory terms such as whitelist/blacklist has shown. Identifying relevant and trending societal matters is important, and often done for traditional social media channels such as Twitter. In this paper we explore whether this type of analysis can also be used for introspection of the software world, by looking at the thriving scene of Scratch programmers. The educational programming language Scratch is not only used for teaching programming concepts, but also offers a platform for young programmers to express and share their creativity on any topics of relevance. By automatically analyzing titles and project comments in a dataset of 106.032 Scratch projects, we explore which topics are common in the Scratch community, whether socially relevant events are reflected, and how the sentiment in the comments discussing these topics is. It turns out that the diversity of topics within the Scratch projects makes the analysis process challenging. Our results nevertheless show that topics from pop and net culture are present, and even recent societal events such as the Covid-19 pandemic or BLM are to some extent reflected in Scratch. The tone in the comments is mostly positive with catchy youth language. Hence, despite the challenges, Scratch projects can be studied in the same way as social networks, which opens up new possibilities to improve our understanding of the behavior and motivation of novice programmers. Analyzing the content and emotions within a social network provides insights on how software engineers communicate and what they talk about. The Scratch programming environment is extremely popular with young, learning programmers. In this paper we investigate whether social network analysis in terms of automatically identifying topics and sentiments can also be ap-plied to Scratch. We analyze the topics of the projects that young programmers create and share, and determine the tone of their conversations. Although the process turns out to be technically challenging, we encounter pop and net culture references but also societal matters, and the overall tone of communication is positive.",Stimulation,Excitement in Life,The paper finds that the tone in the comments discussing various topics in Scratch projects is mostly positive with catchy youth language. This aligns with the value item Excitement in Life and its corresponding value Stimulation.,"The positive tone and catchy youth language in the comments of Scratch projects indicate that the software users, particularly young programmers, are finding excitement and stimulation in expressing their creativity through programming. This aligns with the value item ""Excitement in Life"" and its corresponding value ""Stimulation"" from Schwartz's Taxonomy. The paper explores how the Scratch community reflects socially relevant events and topics, and the positive tone in the comments suggests that the users are enthusiastic and stimulated by their engagement with the software.",Agreed-Clarified,Agree,,Agree,,
Exp A,Exp H,2773,ICSE-SEIS,Software Project Management,Why do projects join the Apache Software Foundation?,"While numerous open source projects operate on their own, others decide to join well-established foundations such as the Apache Software Foundation (ASF) and the Eclipse Foundation. Although many studies have investigated the motivations of individuals and companies contributing to open source, it remains unknown why projects decide to join software foundations. In this paper, we study the motivators behind the projectsaEURkh decision to join the ASF, the geographical and organizational characteristics of these projects, and the differences between projects in terms of their motivations. To this aim, we analyzed 292 proposals submitted to ASF, and we found that there is an increasing number of company-based and Asia-based projects joining the ASF in recent years. Furthermore, we found that more than half of the projects are motivated by the desire to foster their community, strengthen the outcome of the project, increase interaction with other communities, and boost technical development. Our work shed some light on projectsaEURkh expectations from the ASF. Having this understanding can help foundations to identify ways of supporting newly joined projects, while the prospective joiners can better decide on whether ASF is the right place for them by checking the alignment of their motivations and motivations of projects that have joined in the past. Open Source Software (OSS) is free to be used and modified by any-one in the world for any purpose. Nowadays, OSS is widely used in all kinds of products and hence plays an important role in our daily life. For example, as the most used OSS, the Linux kernel runs on 85% of all smartphones. To create a better environment for OSS de-velopers to collaborate and for OSS projects to grow, many software foundations have been established, such as the Apache Software Foundation (ASF). We have seen that many OSS projects joined the ASF over the years. To join the ASF, OSS projects have to donate all their assets to the foundation, adhering to the foundation's rules and culture. In this paper, we study why these projects decide to join the ASF to understand their expectations from such foundations so that the expected technical and non-technical support can be provided to them. We identified their motivations by analyzing 292 proposals that these projects submitted when applying for joining the ASF. We observed that more than half of the projects try to join the ASF with motivations related to fostering a community, strengthening the project's outcome, increasing interactions with other OSS projects in the ASF, and boosting technical development.",Benevolence,Mature Love,More than half of the projects are motivated by the desire to foster their community; strengthen the outcome of the project; increase interaction with other communities; and boost technical development. This aligns with the value item Mature Love; which includes building and nurturing relationships; and its corresponding value Helpful. The projects joining the ASF are motivated by a desire to contribute to the well-being of their community and strengthen the outcomes of their projects; demonstrating a sense of helpfulness towards others.,"The alignment between the value item v9.1 (Mature Love) and its corresponding value v9 (Helpful) in Schwartz's Taxonomy and the main contributions of 'Paper X' is evident in the motivations of the projects joining the ASF. The desire to foster their community, strengthen the outcome of the project, increase interaction with other communities, and boost technical development reflects a sense of helpfulness towards others and a commitment to building and nurturing relationships. By joining the ASF, these projects are demonstrating a proactive stance in contributing to the well-being of their community and the success of their projects, which aligns with the value of helpfulness in the context of a Software User.",Agreed-Justified,Disagree,reconcile with coder_2,Disagree,This does not align with the value item of Mature Love,
Exp A,Exp H,2773,ICSE-SEIS,Software Project Management,Why do projects join the Apache Software Foundation?,"While numerous open source projects operate on their own, others decide to join well-established foundations such as the Apache Software Foundation (ASF) and the Eclipse Foundation. Although many studies have investigated the motivations of individuals and companies contributing to open source, it remains unknown why projects decide to join software foundations. In this paper, we study the motivators behind the projectsaEURkh decision to join the ASF, the geographical and organizational characteristics of these projects, and the differences between projects in terms of their motivations. To this aim, we analyzed 292 proposals submitted to ASF, and we found that there is an increasing number of company-based and Asia-based projects joining the ASF in recent years. Furthermore, we found that more than half of the projects are motivated by the desire to foster their community, strengthen the outcome of the project, increase interaction with other communities, and boost technical development. Our work shed some light on projectsaEURkh expectations from the ASF. Having this understanding can help foundations to identify ways of supporting newly joined projects, while the prospective joiners can better decide on whether ASF is the right place for them by checking the alignment of their motivations and motivations of projects that have joined in the past. Open Source Software (OSS) is free to be used and modified by any-one in the world for any purpose. Nowadays, OSS is widely used in all kinds of products and hence plays an important role in our daily life. For example, as the most used OSS, the Linux kernel runs on 85% of all smartphones. To create a better environment for OSS de-velopers to collaborate and for OSS projects to grow, many software foundations have been established, such as the Apache Software Foundation (ASF). We have seen that many OSS projects joined the ASF over the years. To join the ASF, OSS projects have to donate all their assets to the foundation, adhering to the foundation's rules and culture. In this paper, we study why these projects decide to join the ASF to understand their expectations from such foundations so that the expected technical and non-technical support can be provided to them. We identified their motivations by analyzing 292 proposals that these projects submitted when applying for joining the ASF. We observed that more than half of the projects try to join the ASF with motivations related to fostering a community, strengthening the project's outcome, increasing interactions with other OSS projects in the ASF, and boosting technical development.",Achievement,Capable,"The paper states that ""more than half of the projects try to join the ASF with motivations related to fostering a community; strengthening the project's outcome; increasing interactions with other OSS projects in the ASF; and boosting technical development."" This aligns with the value item Capable; which represents the software user's aspiration to demonstrate competence and excel in their endeavors. By joining the ASF; projects aim to strengthen their projects and technical development; showcasing their capabilities.","The justification for aligning the main contributions of 'Paper X' with the value item v4.2 and its corresponding value v4 from a ""Software User"" perspective is based on the statement in the abstract that projects joining the ASF aim to strengthen their projects and technical development. This aligns with the value item Capable as software users aspire to demonstrate competence in their endeavors. Joining the ASF allows projects to showcase their capabilities by fostering a community and increasing interactions with other OSS projects, ultimately leading to stronger outcomes and technical advancements.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2774,ICSE-SEIS,Software Engineering Practices,Worldwide Gender Differences in Public Code Contributions and how they have been affected by the COVID-19 pandemic,"Gender imbalance is a well-known phenomenon observed throughout sciences which is particularly severe in software development and Free/Open Source Software communities. Little is know yet about the geography of this phenomenon in particular when considering large scales for both its time and space dimensions. We contribute to fill this gap with a longitudinal study of the population of contributors to publicly available software source code. We analyze the development history of 160 million software projects for a total of 2.2 billion commits contributed by 43 million distinct authors over a period of 50 years. We classify author names by gender using name frequencies and author geographical locations using heuristics based on email addresses and time zones. We study the evolution over time of contributions to public code by gender and by world region. For the world overall, we confirm previous findings about the low but steadily increasing ratio of contributions by female authors. When breaking down by world regions we find that the long-term growth of female participation is a world-wide phenomenon. We also observe a decrease in the ratio of female participation during the COVID-19 pandemic, suggesting that women's ability to contribute to public code has been more hindered than that of men. Software developers around the world work together to produce publicly available software (or public code). They do so using public identities and disclosing information about their work that include their names and when a software change was made. We use this information to characterize the gender gap in public code, that is, the difference in participation to public software development between men and women. Specifically, we study the development history of 160 million pieces of public software, developed over a period of 50 years by 43 million authors. We characterize the gender gap on this corpus over time and by world region. To determine author genders we rely on public data about name frequencies by gender around the world. To determine author locations we use email addresses, name frequencies around the world, and the time zone associated to each software change. We confirm that the gender gap in public code is huge. Female authors are only 8.1 % of the total and have authored only 13.5% software versions. The gender gap is however shrinking, with women participation having increased steadily over the past 12 years. This improvement is a global phenomenon, observable in most world regions. We also observe a decrease in the ratio of female participation during the COVID-19 pandemic, suggesting that women have been more hindered than men in their ability to contribute to public code.",Conformity,Politeness,The paper contributes to software development practices that promote politeness and respect for elders. This aligns with the value item Politeness and its corresponding value Conformity.,"In 'Paper X', the contribution of promoting politeness and respect for elders in software development practices aligns with the value item v8.2 (Politeness) and its corresponding value v8 (Conformity). By emphasizing these practices, the paper highlights the importance of adhering to social norms and cultural expectations within the software development community. This alignment reflects the value of maintaining harmonious relationships and demonstrating respectful behavior, which can enhance the overall user experience and collaboration within the software context.",Agreed-Clarified,Agree,,Agree,,
Exp A,Exp H,2774,ICSE-SEIS,Software Engineering Practices,Worldwide Gender Differences in Public Code Contributions and how they have been affected by the COVID-19 pandemic,"Gender imbalance is a well-known phenomenon observed throughout sciences which is particularly severe in software development and Free/Open Source Software communities. Little is know yet about the geography of this phenomenon in particular when considering large scales for both its time and space dimensions. We contribute to fill this gap with a longitudinal study of the population of contributors to publicly available software source code. We analyze the development history of 160 million software projects for a total of 2.2 billion commits contributed by 43 million distinct authors over a period of 50 years. We classify author names by gender using name frequencies and author geographical locations using heuristics based on email addresses and time zones. We study the evolution over time of contributions to public code by gender and by world region. For the world overall, we confirm previous findings about the low but steadily increasing ratio of contributions by female authors. When breaking down by world regions we find that the long-term growth of female participation is a world-wide phenomenon. We also observe a decrease in the ratio of female participation during the COVID-19 pandemic, suggesting that women's ability to contribute to public code has been more hindered than that of men. Software developers around the world work together to produce publicly available software (or public code). They do so using public identities and disclosing information about their work that include their names and when a software change was made. We use this information to characterize the gender gap in public code, that is, the difference in participation to public software development between men and women. Specifically, we study the development history of 160 million pieces of public software, developed over a period of 50 years by 43 million authors. We characterize the gender gap on this corpus over time and by world region. To determine author genders we rely on public data about name frequencies by gender around the world. To determine author locations we use email addresses, name frequencies around the world, and the time zone associated to each software change. We confirm that the gender gap in public code is huge. Female authors are only 8.1 % of the total and have authored only 13.5% software versions. The gender gap is however shrinking, with women participation having increased steadily over the past 12 years. This improvement is a global phenomenon, observable in most world regions. We also observe a decrease in the ratio of female participation during the COVID-19 pandemic, suggesting that women have been more hindered than men in their ability to contribute to public code.",Benevolence,Helpful,The paper contributes to the creation of software that facilitates helpfulness towards others in the software user community. This aligns with the value item Helpful and its corresponding value Benevolence.,"In 'Paper X', the contributions mentioned focus on the analysis of gender imbalance and the participation of women in software development. By studying the evolution of contributions to public software code, the paper provides insights into the gender gap in the software community. While the abstract does not explicitly mention the creation of software that facilitates helpfulness, it indirectly aligns with the value item 'Helpful' from a ""Software User"" perspective. Understanding and addressing gender imbalance can lead to creating a more inclusive and supportive software environment, thereby enabling helpfulness and benevolence among software users.",Agreed-Clarified,Agree,,Agree,,
Exp A,Exp H,2774,ICSE-SEIS,Software Engineering Practices,Worldwide Gender Differences in Public Code Contributions and how they have been affected by the COVID-19 pandemic,"Gender imbalance is a well-known phenomenon observed throughout sciences which is particularly severe in software development and Free/Open Source Software communities. Little is know yet about the geography of this phenomenon in particular when considering large scales for both its time and space dimensions. We contribute to fill this gap with a longitudinal study of the population of contributors to publicly available software source code. We analyze the development history of 160 million software projects for a total of 2.2 billion commits contributed by 43 million distinct authors over a period of 50 years. We classify author names by gender using name frequencies and author geographical locations using heuristics based on email addresses and time zones. We study the evolution over time of contributions to public code by gender and by world region. For the world overall, we confirm previous findings about the low but steadily increasing ratio of contributions by female authors. When breaking down by world regions we find that the long-term growth of female participation is a world-wide phenomenon. We also observe a decrease in the ratio of female participation during the COVID-19 pandemic, suggesting that women's ability to contribute to public code has been more hindered than that of men. Software developers around the world work together to produce publicly available software (or public code). They do so using public identities and disclosing information about their work that include their names and when a software change was made. We use this information to characterize the gender gap in public code, that is, the difference in participation to public software development between men and women. Specifically, we study the development history of 160 million pieces of public software, developed over a period of 50 years by 43 million authors. We characterize the gender gap on this corpus over time and by world region. To determine author genders we rely on public data about name frequencies by gender around the world. To determine author locations we use email addresses, name frequencies around the world, and the time zone associated to each software change. We confirm that the gender gap in public code is huge. Female authors are only 8.1 % of the total and have authored only 13.5% software versions. The gender gap is however shrinking, with women participation having increased steadily over the past 12 years. This improvement is a global phenomenon, observable in most world regions. We also observe a decrease in the ratio of female participation during the COVID-19 pandemic, suggesting that women have been more hindered than men in their ability to contribute to public code.",Universalism,Protecting the Environment,The paper contributes to software development practices that focus on protecting the environment and promoting sustainability. This aligns with the value item Protecting the Environment and its corresponding value Universalism.,"The justification for aligning 'Paper X' with the value item v10.4 (Protecting the Environment) and its corresponding value v10 (Universalism) from a ""Software User"" perspective is that the paper analyzes the gender imbalance in software development and open-source communities, which contributes to understanding and addressing issues related to inclusivity, diversity, and fairness. By examining the demographics of contributors and their geographical locations, the paper highlights the importance of promoting equal opportunities and creating a more sustainable and environmentally conscious software development ecosystem. This aligns with the value of Universalism, which emphasizes the importance of broadmindedness, equality, and protecting the environment for the overall well-being and sustainability of society.",Agreed-Clarified,Agree,,Agree,,
Exp A,Exp H,2779,ICSE-SEIS,Security & Privacy,Security Thinking in Online Freelance Software Development,"Online freelance software development (OFSD) is a significant part of the software industry and is a thriving online economy; a recent survey by Stack Overflow reported that nearly 15% of developers are independent contractors, freelancers, or self-employed. Although security is an important quality requirement for the social sustainability of software, existing studies have shown differences in the way security issues are handled by developers working in OFSD compared to those working in organisational environments. This paper investigates the security culture of OFSD developers, and identifies significant themes in how security is conceived, practiced, and compensated. Based on in-depth interviews with 20 freelance (FL) developers, we report that (a) security thinking is evident in descriptions of their work, (b) security thinking manifests in different ways within OFSD practice, and (c) the dynamics of the freelance development ecosystem influence financial investment in secure development. Our findings help to understand the reasons why insecure software development is evident in freelance development, and they contribute toward developing security interventions that are tailored to the needs of freelance software developers.General Summary- Online freelance software development (OFSD) is a significant part of the software industry and is a thriving online economy. Although security is an important quality requirement for the social sustainability of software, existing studies have shown differences in the way security issues are handled by developers working in OFSD compared to those working in organisational environments. Based on in-depth interviews with 20 freelance developers, this paper investigates the security culture of OFSD developers, and identifies significant themes in how security is conceived, practiced, and compensated.",Security,Social Order,The paper investigates the security culture of freelance software developers and how security issues are handled in online freelance software development (OFSD). This aligns with the value item Social Order and its corresponding value Security. The paper aims to understand the reasons why insecure software development is evident in freelance development and contribute towards developing security interventions tailored to the needs of freelance software developers.,"In the paper abstract, the authors mention that they investigate the security culture of freelance software developers and how security issues are handled in online freelance software development. This aligns with the value item v6.3, which is Social Order, and its corresponding value v6, Security, since security is an important quality requirement for the social sustainability of software. The paper aims to understand the reasons behind insecure software development in freelance development and contribute toward developing security interventions tailored to the needs of freelance software developers. Therefore, by addressing the security culture and issues in freelance development, the main contributions of the paper directly align with the value item v6.3 and its corresponding value v6 from the perspective of a ""Software User"" within a software context.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2779,ICSE-SEIS,Security & Privacy,Security Thinking in Online Freelance Software Development,"Online freelance software development (OFSD) is a significant part of the software industry and is a thriving online economy; a recent survey by Stack Overflow reported that nearly 15% of developers are independent contractors, freelancers, or self-employed. Although security is an important quality requirement for the social sustainability of software, existing studies have shown differences in the way security issues are handled by developers working in OFSD compared to those working in organisational environments. This paper investigates the security culture of OFSD developers, and identifies significant themes in how security is conceived, practiced, and compensated. Based on in-depth interviews with 20 freelance (FL) developers, we report that (a) security thinking is evident in descriptions of their work, (b) security thinking manifests in different ways within OFSD practice, and (c) the dynamics of the freelance development ecosystem influence financial investment in secure development. Our findings help to understand the reasons why insecure software development is evident in freelance development, and they contribute toward developing security interventions that are tailored to the needs of freelance software developers.General Summary- Online freelance software development (OFSD) is a significant part of the software industry and is a thriving online economy. Although security is an important quality requirement for the social sustainability of software, existing studies have shown differences in the way security issues are handled by developers working in OFSD compared to those working in organisational environments. Based on in-depth interviews with 20 freelance developers, this paper investigates the security culture of OFSD developers, and identifies significant themes in how security is conceived, practiced, and compensated.",Universalism,Protecting the Environment,The paper explores the security culture of freelance software developers and the dynamics of the freelance development ecosystem in relation to financial investment in secure development. This aligns with the value item Protecting the Environment and its corresponding value Universalism. By addressing security issues in OFSD; the paper contributes to the preservation of the software development environment and promotes sustainable practices in the software industry.,"In the context of software development, the paper's focus on exploring the security culture of freelance developers and the dynamics of the freelance development ecosystem aligns with the value item of ""Protecting the Environment"" and its corresponding value of ""Universalism"" from a software user perspective. By addressing security issues in online freelance software development, the paper contributes to the preservation of a secure and sustainable software development environment, ultimately promoting practices that benefit the larger software industry as a whole.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2781,ICSE-SEIS,Software Engineering Practices,Draw a Software Engineer Test - An Investigation into ChildrenaEUR(tm)s Perceptions of Software Engineering Profession,"Context: The gender gap is particularly affecting the software engineering community, as both academia and industry are dominated by men. Literature reports how the lack of women is a consequence of gender stereotypes around certain figures that begin in the early stages of education, affecting childrenaEUR(tm)s perceptions of the role they can play across scientific fields.Objective: In this study, we asked children to draw a software engineer in order to collect their perceptions and let us check whether gender stereotypes still persist.Methods: We asked a total of 371 children to draw a person who works in the software engineering field. We analyzed the drawings based on a set of parameters extracted from literature and inspected the results through a cross-sectional study.Results: Children agreed on their representations of a software engineer: 51% drew a man and 44% drew a woman, while 5% a non-recognizable figure. The main differences emerged when the data were grouped by age and gender: only 23% of eleven-year-old girls drew a woman software engineer, while 54% drew a man, and in 23% gender was non-recognizable.Conclusion: The findings revealed a favorable gender balance in childrenaEUR(tm)s perceptions of software engineering. They seem more willing to recognize diversity, an improvement compared with what was reported in previous studies. ChildrenaEUR(tm)s perceptions of technology may have become more accessible as a result of the COVID-19 situation. These findings may draw positive comparisons with the current gender gap in software engineering, encouraging future developments.",Self Direction,Curiosity,The paper contributes to challenging gender stereotypes by examining children's perceptions of a software engineer. This aligns with the value item Curiosity and its corresponding value Self Direction.,"The paper's examination of children's perceptions of a software engineer challenges gender stereotypes by encouraging curiosity and self-direction in understanding the role. By analyzing the drawings, the paper provides insight into how children perceive and recognize diversity in the field, promoting an inclusive and open-minded perspective that aligns with the value item Curiosity and its corresponding value Self Direction.",Agreed-Clarified,Agree,,Agree,,
Exp A,Exp H,2781,ICSE-SEIS,Software Engineering Practices,Draw a Software Engineer Test - An Investigation into ChildrenaEUR(tm)s Perceptions of Software Engineering Profession,"Context: The gender gap is particularly affecting the software engineering community, as both academia and industry are dominated by men. Literature reports how the lack of women is a consequence of gender stereotypes around certain figures that begin in the early stages of education, affecting childrenaEUR(tm)s perceptions of the role they can play across scientific fields.Objective: In this study, we asked children to draw a software engineer in order to collect their perceptions and let us check whether gender stereotypes still persist.Methods: We asked a total of 371 children to draw a person who works in the software engineering field. We analyzed the drawings based on a set of parameters extracted from literature and inspected the results through a cross-sectional study.Results: Children agreed on their representations of a software engineer: 51% drew a man and 44% drew a woman, while 5% a non-recognizable figure. The main differences emerged when the data were grouped by age and gender: only 23% of eleven-year-old girls drew a woman software engineer, while 54% drew a man, and in 23% gender was non-recognizable.Conclusion: The findings revealed a favorable gender balance in childrenaEUR(tm)s perceptions of software engineering. They seem more willing to recognize diversity, an improvement compared with what was reported in previous studies. ChildrenaEUR(tm)s perceptions of technology may have become more accessible as a result of the COVID-19 situation. These findings may draw positive comparisons with the current gender gap in software engineering, encouraging future developments.",Benevolence,True Friendship,The paper contributes to promoting diversity and inclusivity in the software engineering field by highlighting the representation of women as software engineers. This aligns with the value item True Friendship and its corresponding value Benevolence.,"By promoting the representation of women as software engineers, 'Paper X' aligns with the value item True Friendship and its corresponding value Benevolence. This alignment is evident through the paper's focus on challenging gender stereotypes that limit the participation of women in the field. By recognizing the importance of inclusivity and diversity, the paper aims to create an environment where true friendship and benevolence can thrive, fostering equal opportunities and supporting the software user community as a whole.",Agreed-Clarified,Agree,,Agree,,
Exp A,Exp H,2781,ICSE-SEIS,Software Engineering Practices,Draw a Software Engineer Test - An Investigation into ChildrenaEUR(tm)s Perceptions of Software Engineering Profession,"Context: The gender gap is particularly affecting the software engineering community, as both academia and industry are dominated by men. Literature reports how the lack of women is a consequence of gender stereotypes around certain figures that begin in the early stages of education, affecting childrenaEUR(tm)s perceptions of the role they can play across scientific fields.Objective: In this study, we asked children to draw a software engineer in order to collect their perceptions and let us check whether gender stereotypes still persist.Methods: We asked a total of 371 children to draw a person who works in the software engineering field. We analyzed the drawings based on a set of parameters extracted from literature and inspected the results through a cross-sectional study.Results: Children agreed on their representations of a software engineer: 51% drew a man and 44% drew a woman, while 5% a non-recognizable figure. The main differences emerged when the data were grouped by age and gender: only 23% of eleven-year-old girls drew a woman software engineer, while 54% drew a man, and in 23% gender was non-recognizable.Conclusion: The findings revealed a favorable gender balance in childrenaEUR(tm)s perceptions of software engineering. They seem more willing to recognize diversity, an improvement compared with what was reported in previous studies. ChildrenaEUR(tm)s perceptions of technology may have become more accessible as a result of the COVID-19 situation. These findings may draw positive comparisons with the current gender gap in software engineering, encouraging future developments.",Universalism,Equality,The paper contributes to breaking down gender stereotypes and promoting equality by examining children's perceptions of a software engineer. This aligns with the value item Equality and its corresponding value Universalism.,"In 'Paper X,' the authors analyze children's perceptions of a software engineer and find that a favorable gender balance exists, with both boys and girls drawing representations of women in the field. This aligns with the value item Equality from Schwartz's Taxonomy, as it demonstrates a recognition of diversity and equal opportunities for both genders. By promoting equality in the software engineering field and challenging gender stereotypes, the paper contributes to the value of Universalism where broadmindedness and equality are valued in society. Therefore, this alignment between the paper's contributions and the value item and corresponding value is evident and directly supported by the findings in the abstract.",Agreed-Clarified,Agree,,Agree,,
Exp A,Exp H,2783,ICSE-SEIS,Accessibility & User Experience,Hackathons as Inclusive Spaces for Prototyping Software in Open Social Innovation with NGOs,"Non-governmental Organizations (NGOs) usually have limited resources that prevent them from investing in software-based innovation. Sometimes hackathons are used as a resource to crowdsource software for NGOs, but often the resulting projects are not usable or not carried on. These events are not seen as a good option in Software Engineering for social good (i.e., software focused on social change) since they are too short to allow an understanding of the social context of the target institution. Taking that limitation into account, after performing 6 months of ethnography to understand the social context of an NGO, by identifying user needs and eliciting requirements, we organized an inclusive hackathon to address two specific challenges identified in that organization. This paper presents an experience report in the context of an interdisciplinary project with researchers from the Psychology, Design, and Computer Science domains, where the goal is to propose and apply an Open Social Innovation process focused on digital innovative solutions in the context of an NGO from Brazil that supports socially vulnerable people living with HIV/AIDS.",Universalism,Protecting the Environment,Through the inclusive hackathon organized in the context of an NGO; the paper aims to propose and apply an Open Social Innovation process focused on digital innovative solutions. This aligns with the value item 'Protecting the Environment' and its corresponding value 'Universalism'; as it demonstrates a concern for the ecological well-being and emphasizes the importance of sustainable practices and actions that contribute to the preservation of the environment.,"The proposed Open Social Innovation process focused on digital innovative solutions in the context of an NGO aligns with the value item 'Protecting the Environment' and its corresponding value 'Universalism' from a ""Software User"" perspective. By emphasizing the importance of sustainable practices and actions that contribute to the preservation of the environment, the paper demonstrates a concern for the ecological well-being. This aligns with the value of Universalism, which promotes broadmindedness, equality, and protecting the environment as important aspects of social change. The inclusive hackathon organized in the paper's context aims to address specific challenges identified in the NGO, showcasing a commitment to digital solutions that can have societal and environmental impacts, thus linking it to the value item of protecting the environment within the Universalism value.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2783,ICSE-SEIS,Accessibility & User Experience,Hackathons as Inclusive Spaces for Prototyping Software in Open Social Innovation with NGOs,"Non-governmental Organizations (NGOs) usually have limited resources that prevent them from investing in software-based innovation. Sometimes hackathons are used as a resource to crowdsource software for NGOs, but often the resulting projects are not usable or not carried on. These events are not seen as a good option in Software Engineering for social good (i.e., software focused on social change) since they are too short to allow an understanding of the social context of the target institution. Taking that limitation into account, after performing 6 months of ethnography to understand the social context of an NGO, by identifying user needs and eliciting requirements, we organized an inclusive hackathon to address two specific challenges identified in that organization. This paper presents an experience report in the context of an interdisciplinary project with researchers from the Psychology, Design, and Computer Science domains, where the goal is to propose and apply an Open Social Innovation process focused on digital innovative solutions in the context of an NGO from Brazil that supports socially vulnerable people living with HIV/AIDS.",Benevolence,Helpful,The paper presents an experience report on an interdisciplinary project with researchers from different domains; aiming to address challenges in an NGO that supports socially vulnerable people living with HIV/AIDS. By focusing on the user needs and eliciting requirements through ethnographic research; the paper contributes to the value item 'Helpful' and its corresponding value 'Benevolence' by seeking to create software solutions that contribute to the well-being and welfare of others.,"The justification for aligning 'Paper X' with the value item v9.3 and its corresponding value v9 is based on the explicit contribution of the paper to creating software solutions that are helpful and benevolent towards socially vulnerable people living with HIV/AIDS. Through an interdisciplinary project and extensive ethnographic research, the paper focuses on user needs and requirements, aiming to develop innovative digital solutions that contribute to the well-being and welfare of this specific user group. This alignment directly highlights the paper's commitment to benevolence and providing software that is genuinely helpful for the targeted software users.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2783,ICSE-SEIS,Accessibility & User Experience,Hackathons as Inclusive Spaces for Prototyping Software in Open Social Innovation with NGOs,"Non-governmental Organizations (NGOs) usually have limited resources that prevent them from investing in software-based innovation. Sometimes hackathons are used as a resource to crowdsource software for NGOs, but often the resulting projects are not usable or not carried on. These events are not seen as a good option in Software Engineering for social good (i.e., software focused on social change) since they are too short to allow an understanding of the social context of the target institution. Taking that limitation into account, after performing 6 months of ethnography to understand the social context of an NGO, by identifying user needs and eliciting requirements, we organized an inclusive hackathon to address two specific challenges identified in that organization. This paper presents an experience report in the context of an interdisciplinary project with researchers from the Psychology, Design, and Computer Science domains, where the goal is to propose and apply an Open Social Innovation process focused on digital innovative solutions in the context of an NGO from Brazil that supports socially vulnerable people living with HIV/AIDS.",Achievement,Capable,The paper highlights the limitation of using hackathons as a resource for software-based innovation in NGOs; emphasizing the need for a deeper understanding of the social context of the target institution. Through 6 months of ethnography to understand the social context of an NGO; the paper aims to propose an Open Social Innovation process focused on digital innovative solutions. This aligns with the value item 'Capable' and its corresponding value 'Achievement'; as it demonstrates the aspiration to excel and be acknowledged for the competence in addressing challenges identified by NGOs.,"The alignment with the value item 'Capable' and its corresponding value 'Achievement' is evident in the paper's emphasis on the need for a deeper understanding of the social context of NGOs and the proposal of an Open Social Innovation process. By conducting ethnography and addressing specific challenges, the paper demonstrates the competence and aspiration to excel in providing digital innovative solutions for NGOs, ultimately aiming for social change and acknowledgement for their accomplishments.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2785,ICSE-SEIS,Accessibility & User Experience,Do Users Act Equitably? Understanding User Bias Through a Large In-person Study,"Inequitable software is a common problem. Bias may be caused by developers, or even software users. As a society, it is crucial that we understand and identify the causes and implications of software bias from both users and the software itself. To address the problems of inequitable software, it is essential that we inform and motivate the next generation of software developers regarding bias and its adverse impacts. However, research shows that there is a lack of easily adoptable ethics-focused educational material to support this effort.To address the problem of inequitable software, we created an easily adoptable, self-contained experiential activity that is designed to foster student interest in software ethics, with a specific emphasis on AI/ML bias. This activity involves participants selecting fictitious teammates based solely on their appearance. The participant then experiences bias either against themselves or a teammate by the activityaEUR(tm)s fictitious AI. The created lab was then utilized in this study involving 173 real-world users (age 18-51+) to better understand user bias.The primary findings of our study include: I) Participants from minority ethnic groups have stronger feeling regarding being impacted by inequitable software/AI, II) Participants with higher interest in AI/ML have a higher belief for the priority of unbiased software, III) Users do not act in an equitable manner, as avatars with aEURkhdarkaEUR(tm) skin color are less likely to be selected, and IV) Participants from different demographic groups exhibit similar behavior bias. The created experiential lab activity may be executed using only a browser and internet connection, and is publicly available on our project website: https://all.rit.edu.",Benevolence,A Spiritual Life,The paper addresses the issue of bias in software; specifically with an emphasis on AI/ML bias. By creating an educational activity that allows participants to experience bias based on appearance; the paper aims to foster student interest in software ethics and highlight the importance of unbiased software. This aligns with the value item A Spiritual Life; as it emphasizes the responsibility and moral duty to ensure fairness and equity in software development.,"The justification for aligning 'Paper X' with the value item v9.2 and its corresponding value v9 from a ""Software User"" perspective is based on the paper's focus on addressing bias in software, particularly AI/ML bias. The paper's emphasis on fostering student interest in software ethics and highlighting the importance of unbiased software aligns with the value of A Spiritual Life, as it highlights the responsibility and moral duty to ensure fairness and equity in software development. This value emphasizes the recognition of the inherent value and worth of all individuals, promoting a sense of empathy and ethical responsibility in the context of software usage and development.",Agreed-Justified,Disagree,reconcile with coder_2,Disagree,"This issue highlights the importance of unbiased software, and the justification does not align with A Spiritual Life.",
Exp A,Exp H,2785,ICSE-SEIS,Accessibility & User Experience,Do Users Act Equitably? Understanding User Bias Through a Large In-person Study,"Inequitable software is a common problem. Bias may be caused by developers, or even software users. As a society, it is crucial that we understand and identify the causes and implications of software bias from both users and the software itself. To address the problems of inequitable software, it is essential that we inform and motivate the next generation of software developers regarding bias and its adverse impacts. However, research shows that there is a lack of easily adoptable ethics-focused educational material to support this effort.To address the problem of inequitable software, we created an easily adoptable, self-contained experiential activity that is designed to foster student interest in software ethics, with a specific emphasis on AI/ML bias. This activity involves participants selecting fictitious teammates based solely on their appearance. The participant then experiences bias either against themselves or a teammate by the activityaEUR(tm)s fictitious AI. The created lab was then utilized in this study involving 173 real-world users (age 18-51+) to better understand user bias.The primary findings of our study include: I) Participants from minority ethnic groups have stronger feeling regarding being impacted by inequitable software/AI, II) Participants with higher interest in AI/ML have a higher belief for the priority of unbiased software, III) Users do not act in an equitable manner, as avatars with aEURkhdarkaEUR(tm) skin color are less likely to be selected, and IV) Participants from different demographic groups exhibit similar behavior bias. The created experiential lab activity may be executed using only a browser and internet connection, and is publicly available on our project website: https://all.rit.edu.",Universalism,Social Justice,The paper's findings indicate that participants from minority ethnic groups have stronger feelings regarding the impact of inequitable software/AI. This aligns with the value item Social Justice; as it emphasizes the need for equal treatment and fairness in software systems; particularly in relation to minority groups.,"In the paper, it is explicitly stated that participants from minority ethnic groups have stronger feelings regarding the impact of inequitable software/AI. This aligns with the value item Social Justice, as it emphasizes the importance of equal treatment and fairness in software systems, particularly in relation to minority groups. By highlighting this finding, the paper underscores the need to address biases and ensure fairness for all users, which directly aligns with the value item v10.8 from Schwartz's Taxonomy.",Agreed-Clarified,Agree,,Agree,,
Exp A,Exp H,2785,ICSE-SEIS,Accessibility & User Experience,Do Users Act Equitably? Understanding User Bias Through a Large In-person Study,"Inequitable software is a common problem. Bias may be caused by developers, or even software users. As a society, it is crucial that we understand and identify the causes and implications of software bias from both users and the software itself. To address the problems of inequitable software, it is essential that we inform and motivate the next generation of software developers regarding bias and its adverse impacts. However, research shows that there is a lack of easily adoptable ethics-focused educational material to support this effort.To address the problem of inequitable software, we created an easily adoptable, self-contained experiential activity that is designed to foster student interest in software ethics, with a specific emphasis on AI/ML bias. This activity involves participants selecting fictitious teammates based solely on their appearance. The participant then experiences bias either against themselves or a teammate by the activityaEUR(tm)s fictitious AI. The created lab was then utilized in this study involving 173 real-world users (age 18-51+) to better understand user bias.The primary findings of our study include: I) Participants from minority ethnic groups have stronger feeling regarding being impacted by inequitable software/AI, II) Participants with higher interest in AI/ML have a higher belief for the priority of unbiased software, III) Users do not act in an equitable manner, as avatars with aEURkhdarkaEUR(tm) skin color are less likely to be selected, and IV) Participants from different demographic groups exhibit similar behavior bias. The created experiential lab activity may be executed using only a browser and internet connection, and is publicly available on our project website: https://all.rit.edu.",Security,Sense of Belonging,The paper's findings also show that users do not act in an equitable manner; as avatars with darker skin color are less likely to be selected. This aligns with the value item Sense of Belonging; as it highlights the importance of creating software that fosters inclusivity and a sense of belonging for all users; regardless of their appearance or ethnicity.,"The alignment of 'Paper X' with the value item v6.6 (Sense of Belonging) from a ""Software User"" perspective can be justified based on the paper's findings that avatars with darker skin color are less likely to be selected. This highlights the need for equitable software that promotes inclusivity and ensures that all users feel a sense of belonging, regardless of their appearance or ethnicity. By addressing bias in software and promoting a sense of belonging, 'Paper X' contributes to the value item v6.6 (Sense of Belonging) and its corresponding value v6 (Security) in Schwartz's Taxonomy.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2790,ICSE-SEIS,Software Testing & QA,Metamorphic Testing and Debugging of Tax Preparation Software,"This paper presents a data-driven debugging framework to improve the trustworthiness of US tax preparation software systems. Given the legal implications of bugs in such software on its users, ensuring compliance and trustworthiness of tax preparation software is of paramount importance. The key barriers in developing debugging aids for tax preparation systems are the unavailability of explicit specifications and the difficulty of obtaining oracles. We posit that, since the US tax law adheres to the legal doctrine of precedent, the specifications about the outcome of tax preparation software for an individual taxpayer must be viewed in comparison with individuals that are deemed similar. Consequently, these specifications are naturally available as properties on the software requiring similar inputs provide similar outputs. Inspired by the metamorphic testing paradigm, we dub these relations metamorphic relations as they relate to structurally modified inputs.In collaboration with legal and tax experts, we explicated metamorphic relations for a set of challenging properties from various US Internal Revenue Services (IRS) publications including Form 1040 (U.S. Individual Income Tax Return), Publication 596 (Earned Income Tax Credit), Schedule 8812 (Qualifying Children and Other Dependents), and Form 8863 (Education Credits). While we focus on an open-source tax preparation software for our case study, the proposed framework can be readily extended to other commercial software. We develop a randomized test-case generation strategy to systematically validate the correctness of tax preparation software guided by metamorphic relations. We further aid this test-case generation by visually explaining the behavior of software on suspicious instances using easy-to-interpret decision-tree models. Our tool uncovered several accountability bugs with varying severity ranging from non-robust behavior in corner-cases (unreliable behavior when tax returns are close to zero) to missing eligibility conditions in the updated versions of software.",Power,Wealth,The paper aims to improve the trustworthiness of tax preparation software systems; which is aligned with the value item Wealth and its corresponding value Power. By ensuring compliance and trustworthiness; the software can contribute to the accumulation of wealth for the software users.,"I apologize for any confusion. To clarify, aligning 'Paper X' with the value item Wealth (v5.2) and its corresponding value Power (v5) is justified by the fact that the paper aims to improve the trustworthiness of tax preparation software systems. By ensuring compliance and trustworthiness, the software can provide users with a sense of power and control over their financial matters, potentially leading to wealth accumulation. This aligns with the power dynamics associated with the value item Wealth in Schwartz's Taxonomy.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2790,ICSE-SEIS,Software Testing & QA,Metamorphic Testing and Debugging of Tax Preparation Software,"This paper presents a data-driven debugging framework to improve the trustworthiness of US tax preparation software systems. Given the legal implications of bugs in such software on its users, ensuring compliance and trustworthiness of tax preparation software is of paramount importance. The key barriers in developing debugging aids for tax preparation systems are the unavailability of explicit specifications and the difficulty of obtaining oracles. We posit that, since the US tax law adheres to the legal doctrine of precedent, the specifications about the outcome of tax preparation software for an individual taxpayer must be viewed in comparison with individuals that are deemed similar. Consequently, these specifications are naturally available as properties on the software requiring similar inputs provide similar outputs. Inspired by the metamorphic testing paradigm, we dub these relations metamorphic relations as they relate to structurally modified inputs.In collaboration with legal and tax experts, we explicated metamorphic relations for a set of challenging properties from various US Internal Revenue Services (IRS) publications including Form 1040 (U.S. Individual Income Tax Return), Publication 596 (Earned Income Tax Credit), Schedule 8812 (Qualifying Children and Other Dependents), and Form 8863 (Education Credits). While we focus on an open-source tax preparation software for our case study, the proposed framework can be readily extended to other commercial software. We develop a randomized test-case generation strategy to systematically validate the correctness of tax preparation software guided by metamorphic relations. We further aid this test-case generation by visually explaining the behavior of software on suspicious instances using easy-to-interpret decision-tree models. Our tool uncovered several accountability bugs with varying severity ranging from non-robust behavior in corner-cases (unreliable behavior when tax returns are close to zero) to missing eligibility conditions in the updated versions of software.",Universalism,Protecting the Environment,The paper develops a test-case generation strategy based on metamorphic relations to validate the correctness of tax preparation software. This aligns with the value item Protecting the Environment and its corresponding value Universalism. By ensuring the accuracy of tax preparation software; the paper contributes to the protection of the environment by reducing potential errors that may lead to incorrect tax calculations and unnecessary paper waste.,"In the paper, the authors propose a data-driven debugging framework for tax preparation software, which involves developing metamorphic relations and a test-case generation strategy. This aligns with the value item v10.4, ""Protecting the Environment,"" as it aims to reduce errors in tax calculations and minimize paper waste. By ensuring the accuracy of tax software, the paper indirectly contributes to the protection of the environment by promoting efficient and sustainable tax preparation processes. This aligns with the value of Universalism, which emphasizes the importance of considering broader societal and environmental concerns.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2790,ICSE-SEIS,Software Testing & QA,Metamorphic Testing and Debugging of Tax Preparation Software,"This paper presents a data-driven debugging framework to improve the trustworthiness of US tax preparation software systems. Given the legal implications of bugs in such software on its users, ensuring compliance and trustworthiness of tax preparation software is of paramount importance. The key barriers in developing debugging aids for tax preparation systems are the unavailability of explicit specifications and the difficulty of obtaining oracles. We posit that, since the US tax law adheres to the legal doctrine of precedent, the specifications about the outcome of tax preparation software for an individual taxpayer must be viewed in comparison with individuals that are deemed similar. Consequently, these specifications are naturally available as properties on the software requiring similar inputs provide similar outputs. Inspired by the metamorphic testing paradigm, we dub these relations metamorphic relations as they relate to structurally modified inputs.In collaboration with legal and tax experts, we explicated metamorphic relations for a set of challenging properties from various US Internal Revenue Services (IRS) publications including Form 1040 (U.S. Individual Income Tax Return), Publication 596 (Earned Income Tax Credit), Schedule 8812 (Qualifying Children and Other Dependents), and Form 8863 (Education Credits). While we focus on an open-source tax preparation software for our case study, the proposed framework can be readily extended to other commercial software. We develop a randomized test-case generation strategy to systematically validate the correctness of tax preparation software guided by metamorphic relations. We further aid this test-case generation by visually explaining the behavior of software on suspicious instances using easy-to-interpret decision-tree models. Our tool uncovered several accountability bugs with varying severity ranging from non-robust behavior in corner-cases (unreliable behavior when tax returns are close to zero) to missing eligibility conditions in the updated versions of software.",Tradition,Respect for Tradition,The paper; in collaboration with legal and tax experts; explicated metamorphic relations for tax preparation software based on the US tax law adhering to the legal doctrine of precedent. This aligns with the value item Respect for Tradition and its corresponding value Tradition. The software seeks to ensure compliance with the established tax laws and regulations; respecting the tradition of legal precedents.,"In 'Paper X', the authors specifically mention their collaboration with legal and tax experts to establish metamorphic relations for tax preparation software based on the US tax law adhering to the legal doctrine of precedent. This emphasis on legal precedents and adherence to established tax laws aligns directly with the value item v7.3 - Respect for Tradition and its corresponding value - Tradition. The authors aim to ensure compliance and trustworthiness of tax preparation software by respecting the tradition of legal precedents and upholding established tax regulations.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2790,ICSE-SEIS,Software Testing & QA,Metamorphic Testing and Debugging of Tax Preparation Software,"This paper presents a data-driven debugging framework to improve the trustworthiness of US tax preparation software systems. Given the legal implications of bugs in such software on its users, ensuring compliance and trustworthiness of tax preparation software is of paramount importance. The key barriers in developing debugging aids for tax preparation systems are the unavailability of explicit specifications and the difficulty of obtaining oracles. We posit that, since the US tax law adheres to the legal doctrine of precedent, the specifications about the outcome of tax preparation software for an individual taxpayer must be viewed in comparison with individuals that are deemed similar. Consequently, these specifications are naturally available as properties on the software requiring similar inputs provide similar outputs. Inspired by the metamorphic testing paradigm, we dub these relations metamorphic relations as they relate to structurally modified inputs.In collaboration with legal and tax experts, we explicated metamorphic relations for a set of challenging properties from various US Internal Revenue Services (IRS) publications including Form 1040 (U.S. Individual Income Tax Return), Publication 596 (Earned Income Tax Credit), Schedule 8812 (Qualifying Children and Other Dependents), and Form 8863 (Education Credits). While we focus on an open-source tax preparation software for our case study, the proposed framework can be readily extended to other commercial software. We develop a randomized test-case generation strategy to systematically validate the correctness of tax preparation software guided by metamorphic relations. We further aid this test-case generation by visually explaining the behavior of software on suspicious instances using easy-to-interpret decision-tree models. Our tool uncovered several accountability bugs with varying severity ranging from non-robust behavior in corner-cases (unreliable behavior when tax returns are close to zero) to missing eligibility conditions in the updated versions of software.",Self Direction,Choosing Own Goals,The paper presents a data-driven debugging framework for tax preparation software; aiming to improve its trustworthiness. This aligns with the value item Choosing Own Goals and its corresponding value Self Direction. The software users can choose the goal of accurately preparing their tax returns and rely on the proposed framework to achieve this goal.,"In the paper, ""Paper X"", the authors present a data-driven debugging framework for tax preparation software with the goal of improving its trustworthiness. This aligns with the value item v1.5 (Choosing Own Goals) and its corresponding value v1 (Self Direction) from a ""Software User"" perspective. Users of tax preparation software have the goal of accurately preparing their tax returns, and by leveraging the proposed framework, they have the autonomy and freedom to choose and achieve this goal in a self-directed manner, thus aligning with the value of Self Direction.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2792,ICSE-SEIS,Software Engineering Practices,Contradicting Motivations in Civic Tech Software Development: Analysis of a Grassroots Project,"Grassroots civic tech, or software for social change, is an emerging practice where people create and then use software to create positive change in their community. In this interpretive case study, we apply Engestr'PmaEUR(tm)s expanded activity theory as a theoretical lens to analyze motivations, how they relate to for example group goals or development tool supported processes, and what contradictions emerge. Participants agreed on big picture motivations, such as learning new skills or improving the community. The main contradictions occurred inside activity systems on details of implementation or between system motives, instead of big picture motivations. Two most significant contradictions involved planning, and converging on design and technical approaches. These findings demonstrate the value of examining civic tech development processes as evolving activity systems.",Stimulation,Variation in Life,The paper explores the motivations behind grassroots civic tech development; which includes creating software for social change in a community. This aligns with the value item Variation in Life and its corresponding value Stimulation.,"My justification for aligning 'Paper X' with the value item v2.2 (Variation in Life) and its corresponding value v2 (Stimulation) is based on the fact that grassroots civic tech development, as discussed in the paper, involves creating software for social change in a community. This process of utilizing software for positive change can bring about a sense of excitement, exploration, and new experiences, which directly aligns with the value of stimulation and the desire for variation in life. Users engaging with this software can experience a stimulating and dynamic environment, contributing to their overall satisfaction and motivation in using the technology.",Agreed-Clarified,Agree,,Agree,,
Exp A,Exp H,2792,ICSE-SEIS,Software Engineering Practices,Contradicting Motivations in Civic Tech Software Development: Analysis of a Grassroots Project,"Grassroots civic tech, or software for social change, is an emerging practice where people create and then use software to create positive change in their community. In this interpretive case study, we apply Engestr'PmaEUR(tm)s expanded activity theory as a theoretical lens to analyze motivations, how they relate to for example group goals or development tool supported processes, and what contradictions emerge. Participants agreed on big picture motivations, such as learning new skills or improving the community. The main contradictions occurred inside activity systems on details of implementation or between system motives, instead of big picture motivations. Two most significant contradictions involved planning, and converging on design and technical approaches. These findings demonstrate the value of examining civic tech development processes as evolving activity systems.",Stimulation,Excitement in Life,The paper discusses the main contradictions that occur in the activity systems of civic tech development; such as planning and converging on design and technical approaches. By examining and addressing these contradictions; the paper contributes to creating excitement and stimulation in the process of software development; which aligns with the value item Excitement in Life.,"I apologize for any confusion. In the paper abstract, it is mentioned that participants in grassroots civic tech strive for big picture motivations such as learning new skills and improving the community. The main contradictions that arise are related to planning and converging on design and technical approaches. By addressing these contradictions, the paper contributes to creating excitement and stimulation in the process of software development, as it allows participants to overcome challenges and find innovative solutions. This directly aligns with the value item of Excitement in Life, as it enhances the experience of software users by providing an engaging and dynamic development process.",Agreed-Clarified,Agree,,Agree,,
Exp A,Exp H,2792,ICSE-SEIS,Software Engineering Practices,Contradicting Motivations in Civic Tech Software Development: Analysis of a Grassroots Project,"Grassroots civic tech, or software for social change, is an emerging practice where people create and then use software to create positive change in their community. In this interpretive case study, we apply Engestr'PmaEUR(tm)s expanded activity theory as a theoretical lens to analyze motivations, how they relate to for example group goals or development tool supported processes, and what contradictions emerge. Participants agreed on big picture motivations, such as learning new skills or improving the community. The main contradictions occurred inside activity systems on details of implementation or between system motives, instead of big picture motivations. Two most significant contradictions involved planning, and converging on design and technical approaches. These findings demonstrate the value of examining civic tech development processes as evolving activity systems.",Universalism,Protecting the Environment,The paper highlights the importance of grassroots civic tech development processes being aligned with ecological well-being and sustainable practices. By emphasizing the value of examining these processes as evolving activity systems; the paper contributes to the value item Protecting the Environment and its corresponding value Universalism.,"In the paper abstract, it is explicitly stated that the main contributions of 'Paper X' involve analyzing motivations and contradictions in grassroots civic tech development processes. By emphasizing the value of examining these processes as evolving activity systems, the paper aligns with the value item v10.4 (Protecting the Environment) from Schwartz's Taxonomy. This alignment is supported by the paper's focus on the importance of sustainable practices and ecological well-being in software development for social change, contributing to the corresponding value of Universalism.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2793,ICSE-SEIS,Accessibility & User Experience,Software Engineering for Smart Things in Public Spaces: Initial Insights and Challenges,"Software engineering for mobile applications has its own challenges, different from when we engineer software just for desktop environments. With the emergence of smart things (including smart everyday objects embedded with connectivity, computational ability, sensors, and sometimes actuators, urban robots such as delivery and cleaning robots, smart street lighting, smart vehicles, and smart park benches, and so on) not just within the home but in public spaces, there is a need to consider software engineering challenges for software on such things. Human-centred software engineering and work on ethical behaviours in smart things will need to come together, even as we continue to understand what it takes to effectively develop software (and systems) for such emerging devices. In order to demonstrate how software (and systems) for intelligent devices in public places might be developed, findings from a quantitative survey we performed are discussed in this study. The survey was designed such that the questions focused on the socio-ethical behaviours of smart devices when interacting with people in public places. The survey was based on a supermarket scenario where the participants had to answer the different questions in the questionnaire. There were 250 participants who only completed part of the survey; of them, 60 participants finished it in full. The complete replies have been examined and analysed in this paper. To determine how people feel about employing smart technology in public places, a variety of smart devices, including robots, smart cameras, smart speakers, and smart trolleys, are utilised in the survey questions. According to the findings, more than 80 percent of respondents think it important for smart gadgets to be socially-aware and ethical in public places.General Abstract This paper examines the survey results conducted to explore if smart devices such as robots or smart cameras can be deployed in public areas. The respondents reply to survey questions asking them whether they believe it is crucial to keep smart robots, smart carts, or any other smart devices in the supermarket. The surveyaEUR(tm)s questions are constructed in such a manner that participants are asked to imagine themselves as either a customer shopping for groceries at a store or a manager running the business and dealing with the friendly robot. This survey was created with the intention of thinking carefully about how intelligent software systems may be designed from the standpoint of software engineering for public settings. Later in this article, the survey findings and insights are discussed.",Universalism,Protecting the Environment,The paper emphasizes the importance of designing intelligent software systems for public settings that consider ethical behaviors and social awareness. This aligns with the value item Protecting the Environment and its corresponding value Universalism.,"The paper's focus on designing intelligent software systems with ethical behaviors and social awareness in public settings aligns with the value item ""Protecting the Environment"" from Schwartz's Taxonomy. This is because considering the ethical behavior of smart devices in public places contributes to the protection of the environment by promoting sustainable and responsible technology usage. Furthermore, this alignment also corresponds with the value of ""Universalism"" as it emphasizes the importance of considering the well-being of society as a whole, including the environment, when designing software for public settings.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2793,ICSE-SEIS,Accessibility & User Experience,Software Engineering for Smart Things in Public Spaces: Initial Insights and Challenges,"Software engineering for mobile applications has its own challenges, different from when we engineer software just for desktop environments. With the emergence of smart things (including smart everyday objects embedded with connectivity, computational ability, sensors, and sometimes actuators, urban robots such as delivery and cleaning robots, smart street lighting, smart vehicles, and smart park benches, and so on) not just within the home but in public spaces, there is a need to consider software engineering challenges for software on such things. Human-centred software engineering and work on ethical behaviours in smart things will need to come together, even as we continue to understand what it takes to effectively develop software (and systems) for such emerging devices. In order to demonstrate how software (and systems) for intelligent devices in public places might be developed, findings from a quantitative survey we performed are discussed in this study. The survey was designed such that the questions focused on the socio-ethical behaviours of smart devices when interacting with people in public places. The survey was based on a supermarket scenario where the participants had to answer the different questions in the questionnaire. There were 250 participants who only completed part of the survey; of them, 60 participants finished it in full. The complete replies have been examined and analysed in this paper. To determine how people feel about employing smart technology in public places, a variety of smart devices, including robots, smart cameras, smart speakers, and smart trolleys, are utilised in the survey questions. According to the findings, more than 80 percent of respondents think it important for smart gadgets to be socially-aware and ethical in public places.General Abstract This paper examines the survey results conducted to explore if smart devices such as robots or smart cameras can be deployed in public areas. The respondents reply to survey questions asking them whether they believe it is crucial to keep smart robots, smart carts, or any other smart devices in the supermarket. The surveyaEUR(tm)s questions are constructed in such a manner that participants are asked to imagine themselves as either a customer shopping for groceries at a store or a manager running the business and dealing with the friendly robot. This survey was created with the intention of thinking carefully about how intelligent software systems may be designed from the standpoint of software engineering for public settings. Later in this article, the survey findings and insights are discussed.",Universalism,A World at Peace,The paper highlights the need for smart gadgets to be socially-aware and ethical in public places; indicating a desire for a world at peace. This aligns with the value item A World at Peace and its corresponding value Universalism.,"In the paper abstract, it is explicitly mentioned that more than 80 percent of respondents believe it is important for smart gadgets to be socially-aware and ethical in public places. This desire for ethical behavior aligns with the value item of A World at Peace from Schwartz's Taxonomy. The value of Universalism, which includes social justice, harmony, and peace, is directly reflected in this alignment. This demonstrates that the main contribution of the paper, regarding the need for ethical and socially-aware smart devices, aligns with the value item v10.7 and its corresponding value v10 from the perspective of a Software User within a software context.",Agreed-Clarified,Agree,,Agree,,
Exp A,Exp H,2794,ICSE-SEIS,Accessibility & User Experience,A Novel Approach to Improving the Digital Literacy of Older Adults,"Digitalisation offers opportunities for older adults (OA) to retain an active role in their lives and alleviate the aEURkhburden of careaEUR(tm) associated with ageing. Yet, digital engagement is consistently cited to be inversely related to age. Although interventions to enhance the digital literacy of OA through formal in-person training have emerged, there has been little evaluation of their effectiveness. This paper presents some insights into the attitudes, needs and challenges of OA in becoming digitally literate. We conducted preliminary user studies with OA and a survey of younger adults (YA) to understand their role as an informal support system. Based on these insights, we propose an innovative approach to digital literacy training for OA by combining a senior-friendly learning management system with informal inter-generational learning support. A preliminary evaluation of the system yielded positive feedback and indicates the need for a more extensive exploration of the digital experiences and requirements of OA and the influence of social support systems in their digital engagement process.",Self Direction,Curiosity,The paper proposes an innovative approach to digital literacy training for older adults (OA) by combining a senior-friendly learning management system with informal inter-generational learning support. This aligns with the value item Curiosity and its corresponding value Self Direction.,"The proposal of an innovative approach to digital literacy training for older adults aligns with the value item Curiosity and its corresponding value Self Direction because it empowers older adults to take control of their own learning and exploration in the digital realm. By combining a senior-friendly learning management system with informal inter-generational learning support, the paper aims to foster curiosity in older adults and enable them to independently navigate and engage with digital technologies. This aligns with the value of Self Direction as it promotes autonomy and the freedom to choose one's own goals and activities in the digital space. Ultimately, this approach encourages older adults to be curious about technology and to confidently explore and learn, in line with the value of Curiosity.",Agreed-Clarified,Agree,,Agree,,
Exp A,Exp H,2794,ICSE-SEIS,Accessibility & User Experience,A Novel Approach to Improving the Digital Literacy of Older Adults,"Digitalisation offers opportunities for older adults (OA) to retain an active role in their lives and alleviate the aEURkhburden of careaEUR(tm) associated with ageing. Yet, digital engagement is consistently cited to be inversely related to age. Although interventions to enhance the digital literacy of OA through formal in-person training have emerged, there has been little evaluation of their effectiveness. This paper presents some insights into the attitudes, needs and challenges of OA in becoming digitally literate. We conducted preliminary user studies with OA and a survey of younger adults (YA) to understand their role as an informal support system. Based on these insights, we propose an innovative approach to digital literacy training for OA by combining a senior-friendly learning management system with informal inter-generational learning support. A preliminary evaluation of the system yielded positive feedback and indicates the need for a more extensive exploration of the digital experiences and requirements of OA and the influence of social support systems in their digital engagement process.",Power,Social Recognition,The paper discusses the influence of social support systems in the digital engagement process of older adults (OA). This aligns with the value item Social Recognition and its corresponding value Power.,"The paper explicitly mentions that it explores the role of informal support systems in the digital engagement process of older adults. This aligns with the value item ""Social Recognition"" from Schwartz's Taxonomy, as the support and recognition from others can provide a sense of power and validation to the software user. By focusing on the influence of social support systems, the paper acknowledges the significance of social recognition in the context of digital engagement for older adults.",Agreed-Justified,Agree,,Agree,,
Exp A,Exp H,2794,ICSE-SEIS,Accessibility & User Experience,A Novel Approach to Improving the Digital Literacy of Older Adults,"Digitalisation offers opportunities for older adults (OA) to retain an active role in their lives and alleviate the aEURkhburden of careaEUR(tm) associated with ageing. Yet, digital engagement is consistently cited to be inversely related to age. Although interventions to enhance the digital literacy of OA through formal in-person training have emerged, there has been little evaluation of their effectiveness. This paper presents some insights into the attitudes, needs and challenges of OA in becoming digitally literate. We conducted preliminary user studies with OA and a survey of younger adults (YA) to understand their role as an informal support system. Based on these insights, we propose an innovative approach to digital literacy training for OA by combining a senior-friendly learning management system with informal inter-generational learning support. A preliminary evaluation of the system yielded positive feedback and indicates the need for a more extensive exploration of the digital experiences and requirements of OA and the influence of social support systems in their digital engagement process.",Security,Family Security,The paper addresses the needs and challenges of older adults (OA) in becoming digitally literate; aiming to alleviate the burden of care associated with aging. This aligns with the value item Family Security and its corresponding value Security.,"In the context of a software user, the alignment of 'Paper X' with the value item v6.2 and its corresponding value v6 can be further clarified as follows: The paper recognizes that older adults (OA) face challenges in becoming digitally literate and emphasizes the need to alleviate the burden of care that is associated with aging. In doing so, it aims to provide a sense of family security, as the digital engagement of older adults can enable them to stay connected with their loved ones, access necessary resources, and maintain their independence. By addressing the specific needs and challenges of older adults in digital literacy, the paper directly aligns with the value item Family Security and its corresponding value Security, as it seeks to enhance the quality of life and well-being of the software users in the context of aging.",Agreed-Clarified,Disagree,reconcile with coder_2,Disagree,"The justification aligns with the quality of life and users' welfare, but not the value item Family Security.
",